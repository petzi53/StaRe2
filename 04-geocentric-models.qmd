# Geocentric Models

```{r}
#| label: setup

library(tidyverse)
```

## Why normal distributions are normal?

Why are there so many distribution approximately normal, resulting in a
Gaussian curve? Because there will be more combinations of outcomes that
sum up to a "central" value, rather than to some extreme value.

::: callout-tip
Any process that adds together random values from the same distribution
converges to a normal.
:::

### Normal by addition

Whatever the average value of the source distribution, each sample from
it can be thought of as a fluctuation from that average value. When we
begin to add these fluctuations together, they also begin to cancel one
another out. A large positive fluctuation will cancel a large negative
one. The more terms in the sum, the more chances for each fluctuation to
be canceled by another, or by a series of smaller ones in the opposite
direction. So eventually the most likely sum, in the sense that there
are the most ways to realize it, will be a sum in which every
fluctuation is canceled by another, a sum of zero (relative to the
mean).

It doesn't matter what shape the underlying distribution possesses. It
could be uniform, like in our example above, or it could be (nearly)
anything else. Depending upon the underlying distribution, the
convergence might be slow, but it will be inevitable.

See the excellent article [Why is normal distribution so
ubiquitous?](https://ekamperi.github.io/mathematics/2021/01/29/why-is-normal-distribution-so-ubiquitous.html)
which also explains the example of random walks from SR2. See also the
scientific paper [Why are normal distribution
normal?](https://www.journals.uchicago.edu/doi/pdf/10.1093/bjps/axs046)
of the The British Journal for the Philosophy of Science.

### Normal by multiplication

This is not only valid for addition but also for multiplication of small
values: Multiplying small numbers is approximately the same as addition.

### Normal by log-multipliation

But even the multiplication of large values tend to produce Gaussian
distributions on the log scale.

### Using Gaussian distribution

The justifications for using the Gaussian distribution fall into two
broad categories:

1.  **Ontological justification**: The world is full of Gaussian
    distributions, approximately. We're never going to experience a
    perfect Gaussian distribution. But it is a widespread pattern,
    appearing again and again at different scales and in different
    domains. Measurement errors, variations in growth, and the
    velocities of molecules all tend towards Gaussian distributions.

There are many other patterns in nature, so make no mistake in assuming
that the Gaussian pattern is universal. In later chapters, we'll see how
other useful and common patterns, like the exponential and gamma and
Poisson, also arise from natural processes. The Gaussian is a member of
a family of fundamental natural distributions known as the **Exponential
family**. All of the members of this family are important for working
science, because they populate our world.

2.  **Epistemological justification**: The Gaussian represents a
    particular state of ignorance. When all we know or are willing to
    say about a distribution of measures (measures are continuous values
    on the real number line) is their mean and variance, then the
    Gaussian distribution arises as the most consistent with our
    assumptions. It is the least surprising and least informative
    assumption to make. --- If you don't think the distribution should
    be Gaussian, then that implies that you know something else that you
    should tell your golem about, something that would improve
    inference.

::: callout-caution
Although the Gaussian distribution is common in nature and has some nice
properties, there are some risks in using it as a default data model.
The Gaussian distribution has some very thin tails---there is very
little probability in them. Instead most of the mass in the Gaussian
lies within one standard deviation of the mean. Many natural (and
unnatural) processes have much heavier tails.
:::

The Gaussian is a continuous distribution, unlike the discrete
distributions of earlier chapters. Probability distributions with only
discrete outcomes, like the binomial, are called *probability mass*
functions and denoted `Pr`. Continuous ones like the Gaussian are called
*probability density* functions, denoted with *`p`* or just plain old
*`f`*, depending upon author and tradition. For mathematical reasons,
probability densities can be greater than 1. Try `dnorm(0,0,0.1)`", for
example, which is the way to make R calculate *`p`*`(0|0, 0.1)`. The
answer, about 4, is no mistake. Probability *density* is the rate of
change in cumulative probability. So where cumulative probability is
increasing rapidly, density can easily exceed 1. But if we calculate the
area under the density function, it will never exceed 1. Such areas are
also called *probability mass*.

## A language describing models

1.  First, we recognize a set of variables to work with. Some of these
    variables are observable. We call these data. Others are
    unobservable things like rates and averages. We call these
    parameters.
2.  We define each variable either in terms of the other variables or in
    terms of a probability distribution.
3.  The combination of variables and their probability distributions
    defines a joint generative model that can be used both to simulate
    hypothetical observations as well as analyze real ones.

Models are mappings of one set of variables through a probability
distribution onto another set of variables. Fundamentally, these models
define the ways values of some variables can arise, given values of
other variables.

### Re-describing the globe tossing model

Recall the proportion of the water problem from previous chapters. The
model in that case was always:

------------------------------------------------------------------------

::: {#def-glob-tossing-model}
Describe the globe tossing model from previous chapter

$$
\begin{align*}
W \sim \operatorname{Binomial}(N, p) \space \space (1)\\
p \sim \operatorname{Uniform}(0, 1)  \space \space (2)
\end{align*}
$$ {#eq-globe-tossing-model}

-   `W`: observed count of water
-   `N`: total number of tosses
-   `p`: proportion of water on the globe

Read the above statement as:

1.  **First line**: The count W is distributed binomially with sample
    size `N` and probability `p`.
2.  **Second line**: The prior for `p` is assumed to be uniform between
    zero and one.
:::

------------------------------------------------------------------------

The first line in these kind of models always defines the likelihood
function used in Bayes' theorem. The other lines define priors.

Both of the lines in the model of @eq-globe-tossing-model are
**stochastic**, as indicated by the `~` symbol. A stochastic
relationship is just a mapping of a variable or parameter onto a
distribution. It is stochastic because no single instance of the
variable on the left is known with certainty. Instead, the mapping is
probabilistic: Some values are more plausible than others, but very many
different values are plausible under any model. Later, we'll have models
with deterministic definitions in them.

## Gaussian model of height {#sec-gaussian-model-of-height}

There are an infinite number of possible Gaussian distributions. Some
have small means. Others have large means. Some are wide, with a large
`σ`. Others are narrow. We want our Bayesian machine to consider every
possible distribution, each defined by a combination of `μ` and `σ`, and
rank them by posterior plausibility. Posterior plausibility provides a
measure of the logical compatibility of each possible distribution with
the data and model.

### The data

#### Original

The data contained in `data(Howell1)` are partial census data for the
Dobe area !Kung San, compiled from interviews conducted by Nancy Howell
in the late 1960s. Much more raw data is available for download from
https://tspace.library.utoronto.ca/handle/1807/10395.

For the non-anthropologists reading along, the !Kung San are the most
famous foraging population of the twentieth century, largely because of
detailed quantitative studies by people like Howell.

::: callout-caution
Loading data from a package with `data()` is only possible if you have
already loaded the package. In our example:

```{r}
#| label: loading-data-from-package1_a
#| eval: false


## R code 4.7 #######################
library(rethinking)
data(Howell1)
d_a <- Howell1
```

Because of many function name conflicts with {**brms**} I do not want to
load {**rethinking**} and will call the function of these conflicted
packages with `<package name>::<function name>()` Therefore I have to
use another, not so usual loading strategy of the data set:

```{r}
#| label: loading-data-from-package2_a
#| attr-source: '#lst-loading-data-from-package2_a lst-cap="Load data `Howell1` from the {**rethinking**} package without loading the package and assign the data to an object. Rethinking"'

data(package = "rethinking", list = "Howell1")
d_a <- Howell1
```

The advantage of this strategy is that I have not always to detach the
{**rethinking**} package and to make sure {**rethinking**} is detached
before using {**brms**} as it is necessary in the Kurz's {**tidyverse**}
/ {**brms**} version.
:::

##### Show the data

```{r}
#| label: show-howell-data-a
#| attr-source: '#lst-show-howell-data-a lst-cap="Show and inspect the data: rethinking"'

## R code 4.8 ####################
str(d_a)

## R code 4.9 ###################
rethinking::precis(d_a)
```

This data frame contains four columns. Each column has 544 entries, so
there are 544 individuals in these data. Each individual has a recorded
height (centimeters), weight (kilograms), age (years), and "maleness" (0
indicating female and 1 indicating male).

##### Select the height data of adults

We're going to work with just the height column, for the moment. All we
want for now are heights of adults in the sample. The reason to filter
out non-adults for now is that height is strongly correlated with age,
before adulthood.

```{r}
#| label: select-height-adults-a
#| attr-source: '#lst-select-height-adults-a lst-cap="Select the height data of adults (individuals older or equal than 18 years): base R version"'

## R code 4.10 ###################
head(d_a$height)
 
## R code 4.11 ###################
d2_a <- d_a[d_a$age >= 18, ]

```

We'll be working with the data frame d2 now. It should have 352 rows
(individuals) in it. We will check this with `nrow(d2_a)` =
`r nrow(d2_a)`.

#### Tidyverse

##### Show the data

```{r}
#| label: loading-data-from-package_b
#| attr-source: '#lst-loading-data-from-package_b lst-cap="Load data `Howell1` from the {**rethinking**} package without loading the package and assign the data to an object. Tidyverse"'

data(package = "rethinking", list = "Howell1")
d_b <- Howell1
```

```{r}
#| label: show-howell-data1-b

d_b |>
    glimpse()

```

`glimpse()` is the tidyverse analogue for `str()`.

```{r}
#| label: show-howell-data2-b
d_b |> 
    summary()
```

Kurz tells us that the {**brms**} package does not have a function that
works like `rethinking::precis()` for providing numeric and graphical
summaries of variables, as in the second part of
@lst-show-howell-data-a. Kurz suggests `base::summary()` to get some of
the information from `rethinking::precis()`.

```{r}
#| label: show-howell-data3-b
d_b |>            
    skimr::skim() 

```

I think `skimr::skim()` is a better option as an alternative to
`rethinking::precis()` as `base::summary()` because it also has a
graphical summary of the variables. {**skimr**} has many other useful
functions and is very adaptable. I propose to install and to try it out.

##### Select the height data of adults

With {**tidyverse**} we can isolate height values with the
`dplyr::select()` function and we are using the `dplyr::filter()`
function to make an adults-only data frame.

```{r}
#| label: select-height-adults-b
#| attr-source: '#lst-select-height-adults-b lst-cap="Select the height data of adults (individuals older or equal than 18 years): tidyverse version"'

d_b %>%
  select(height) %>% 
  glimpse()

d2_b <- 
  d_b %>%
  filter(age >= 18) 
 
glimpse(d2_b)
```

The two functions of @lst-select-height-adults-b are much more readable
and understandable as the weird base R syntax in
@lst-select-height-adults-a.

### The model

#### Original

Our goal is to model the data in `d2_a` using a Gaussian distribution.

Plot the distribution of heights

```{r}
#| label: fig-dist-heights-a
#| fig-cap: "The distribution of the heights data,overlaid by an ideal Gaussian distribution: rethinking version"
#| attr-source: '#lst-fig-dist-heights-a lst-cap="Plot the distribution of the heights of adults: rethinking version"'

rethinking::dens(d2_a$height, norm.comp = TRUE)
```

With the option `norm.comp = TRUE` I have overlaid a Gaussian
distribution to see the differences to the actual data. There are some
differences locally, especially on the peak of the distribution. But the
tails looks nice and we can say that the overall impression of the curve
is Gaussian.

::: callout-caution
###### Decisions how to model the data

Gawking at the raw data, to try to decide how to model them, is usually
not a good idea. The data could be, for example, a mixture of different
Gaussian distributions. Furthermore, the empirical distribution need not
be actually Gaussian in order to justify using a Gaussian probability
distribution.
:::

Define the heights as normally distributed with a mean `μ` and standard
deviation `σ`

------------------------------------------------------------------------

::: {#def-height-normal-dist}
Heights normally distributed

$$
h_{i} \sim \operatorname{Normal}(σ, μ) 
$$ {#eq-height-normal-dist}
:::

------------------------------------------------------------------------

The symbol `h` refers to the list of heights, and the subscript `i`
means each individual element of this list. It is conventional to use
`i` because it stands for index. The index `i` takes on row numbers, and
so in this example can take any value from 1 to 352 (the number of
heights in `d2_a$height`). As such, the model above is saying that all
the golem knows about each height measurement is defined by the same
normal distribution, with mean `μ` and standard deviation `σ`.

The short model in @def-height-normal-dist assumes that the values
$h_{i}$ are *independent and identically distributed*, abbreviated
`i.i.d.`, `iid`, or `IID`.

To complete the model, we're going to need some priors. The parameters
to be estimated are both `μ` and `σ`, so we need a prior `Pr(μ, σ)`, the
joint prior probability for all parameters. In most cases, priors are
specified independently for each parameter, which amounts to assuming
$Pr(μ,σ) = Pr(μ)Pr(σ)$.

------------------------------------------------------------------------

::: {#def-prior-height-model}
Priors for heights model

$$
\begin{align*}
h_{i} \sim \operatorname{Normal}(μ, σ) \space \space (1) \\ 
μ \sim \operatorname{Normal}(178, 20)  \space \space (2) \\ 
μ \sim \operatorname{Uniform}(0, 50)   \space \space (3)      
\end{align*}
$$ {#eq-prior-height-model}

1.  First line represents the likelihood.
2.  Second line is the chosen `μ`(mu, mean) prior.
3.  Third line is the chosen `σ` (sigma, standard deviation) prior.
:::

------------------------------------------------------------------------

Let's think about the chosen value for the priors more in detail:

The prior for `μ` is a broad Gaussian prior, centered on 178 cm, with
95% of probability between 178 ± 40 cm.

Why 178 cm? Your author is 178 cm tall. And the range from 138 cm to 218
cm encompasses a huge range of plausible mean heights for human
populations. So domain-specific information has gone into this prior.
Everyone knows something about human height and can set a reasonable and
vague prior of this kind. But in many regression problems, as you'll see
later, using prior information is more subtle, because parameters don't
always have such clear physical meaning.

Whatever the prior, it's a very good idea to plot your priors, so you
have a sense of the assumption they build into the model.

**Plot the mu prior (mean)**

```{r}
#| label: fig-mean-prior-a
#| fig-cap: "Plot of the chosen mean prior: base R version"

## R code 4.12 ###############################
curve(dnorm(x, 178, 20), from = 100, to = 250)
```

You can see that the golem is assuming that the average height (not each
individual height) is almost certainly between 140 cm and 220 cm. So
this prior carries a little information, but not a lot.

**Plot the sigma prior (standard deviation)**

A standard deviation like `σ` must be positive, so bounding it at zero
makes sense. How should we pick the upper bound? In this case, a
standard deviation of 50 cm would imply that 95% of individual heights
lie within 100 cm of the average height. That's a very large range.

```{r}
#| label: fig-sd-prior-a
#| fig-cap: "Plot the chosen prior for the standard deviation: base R version"

## R code 4.13 ###########################
curve(dunif(x, 0, 50), from = -10, to = 60)
```

**Prior predictive simulation**

> Once you've chosen priors for *h, μ*, and *σ*, these imply a joint
> prior distribution of individual heights. By simulating from this
> distribution, you can see what your choices imply about observable
> height. This helps you diagnose bad choices.

Okay, so how to do this? You can quickly simulate heights by sampling
from the prior, like you sampled from the posterior back in
@sec-sampling-the-imaginary. Remember, every posterior is also
potentially a prior for a subsequent analysis, so you can process priors
just like posteriors.

```{r}
#| label: fig-prior-predictive-sim-a
#| fig-cap: "Simulate heights by sampling from the prior: rethinking version"

set.seed(4) # to make example reproducible
## R code 4.14 #######################################
sample_mu_a <- rnorm(1e4, 178, 20)
sample_sigma_a <- runif(1e4, 0, 50)
prior_h_a <- rnorm(1e4, sample_mu_a, sample_sigma_a)
rethinking::dens(prior_h_a, norm.comp = TRUE)
```

> It displays a vaguely bell-shaped density with thick tails. It is the
> expected distribution of heights, averaged over the prior. Notice that
> the prior probability distribution of height is not itself Gaussian.
> This is okay. The distribution you see is not an empirical
> expectation, but rather the distribution of relative plausibilities of
> different heights, before seeing the data.

This comment is strange for me as in my point of view the distribution
*is* Gaussian. It is true that the tails are (a little bit?) thicker
than in the standard Gaussian distribution. But in my view
@fig-prior-predictive-sim-a is more Gaussian than @fig-dist-heights-a.
OK, in @fig-dist-heights-a we have just `r nrow(d2_a)` data and in
@fig-prior-predictive-sim-a we sampled 10,000 times. But this is not a
counter argument for @fig-prior-predictive-sim-a not being a a bell
shaped distribution.

**Simulate heights from priors with large sd**

Prior predictive simulation is very useful for assigning sensible
priors, because it can be quite hard to anticipate how priors influence
the observable variables. As an example, consider a much flatter and
less informative prior for `μ`, like $μ \sim Normal(178, 100)$. Priors
with such large standard deviations are quite common in Bayesian models,
but they are hardly ever sensible.

```{r}
#| label: fig-prior-predictive-sim2-a
#| fig-cap: "Simulate heights from priors with a large standard deviation: rethinking version"

set.seed(4) # to make example reproducible
## R code 4.15 ############################
sample_mu2_a <- rnorm(1e4, 178, 100)
prior_h2_a <- rnorm(1e4, sample_mu2_a, sample_sigma_a)
rethinking::dens(prior_h2_a)
```

The results of @fig-prior-predictive-sim2-a contradicts our scientific
knowledge --- but also our common sense --- about possible height values
of humans. Now the model, before seeing the data, expects people to have
negative height. It also expects some giants. One of the tallest people
in recorded history, [Robert Pershing
Wadlow](https://en.wikipedia.org/wiki/Robert_Wadlow) (1918--1940) stood
272 cm tall. In our prior predictive simulation many people are taller
than this.

Does this matter? In this case, we have so much data that the silly
prior is harmless. But that won't always be the case. There are plenty
of inference problems for which the data alone are not sufficient, no
matter how numerous. Bayes lets us proceed in these cases. But only if
we use our scientific knowledge to construct sensible priors. Using
scientific knowledge to build priors is not cheating. The important
thing is that your prior not be based on the values in the data, but
only on what you know about the data before you see it.

#### Tidyverse

The plot of the heights distribution compared with the standard Gaussian
distribution is missing in Kurz's version. I added this plot by using
the last example of [How to Plot a Normal Distribution in
R](https://www.statology.org/plot-normal-distribution-r/).

```{r}
#| label: fig-dist-heights-b
#| fig-cap: "The distribution of the heights data, overlaid by an ideal Gaussian distribution: tidyverse version"
#| attr-source: '#lst-fig-dist-heights-b lst-cap="Plot the distribution of the heights of adults: tidyverse version"'

p0 <- 
    d2_b |> 
    ggplot(aes(height)) +
    geom_density() +

    stat_function(
        fun = dnorm,
        args = with(d2_b, c(mean = mean(height), sd = sd(height)))
        ) +
    scale_x_continuous("Height in cm")

p0
```

Here is the shape for the prior $μ \sim Normal(178, 20)$.

```{r}
#| label: fig-mean-prior-b
#| fig-cap: "Plot of the chosen mean prior: tidyverse version"
#| attr-source: '#lst-fig-mean-prior-b lst-cap="Plot of the chosen mean prior: tidyverse version"'

p1 <-
  tibble(x = seq(from = 100, to = 250, by = .1)) %>% 
    
  ggplot(aes(x = x, y = dnorm(x, mean = 178, sd = 20))) +
  geom_line() +
  scale_x_continuous(breaks = seq(from = 100, to = 250, by = 75)) +
  labs(title = "mu ~ dnorm(178, 20)",
       y = "density")

p1
```

And here's the ggplot2 code for our prior for `σ`, a uniform
distribution with a minimum value of 0 and a maximum value of 50. We
don't really need the `y`-axis when looking at the shapes of a density,
so we'll just remove it with `scale_y_continuous()`.

```{r}
#| label: fig-sd-prior-b
#| fig-cap: "Plot the chosen prior for the standard deviation: tidyverse version"

p2 <-
  tibble(x = seq(from = -10, to = 60, by = .1)) %>%
  
  ggplot(aes(x = x, y = dunif(x, min = 0, max = 50))) +
  geom_line() +
  scale_x_continuous(breaks = c(0, 50)) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("sigma ~ dunif(0, 50)")

p2
```

We can simulate from both priors at once to get a prior probability
distribution of `height`.

```{r}
#| label: fig-prior-predictive-sim-b
#| fig-cap: "Simulate heights by sampling from the prior: tidyverse version"

n <- 1e4
set.seed(4)

sim <-
  tibble(sample_mu_b    = rnorm(n, mean = 178, sd  = 20),
         sample_sigma_b = runif(n, min = 0, max = 50)) %>% 
  mutate(height = rnorm(n, mean = sample_mu_b, sd = sample_sigma_b))
  
p3 <- sim %>% 
  ggplot(aes(x = height)) +
  geom_density(fill = "deepskyblue") +
  scale_x_continuous(breaks = c(0, 73, 178, 283)) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("height ~ dnorm(mu, sigma)") +
  theme(panel.grid = element_blank())

p3
```

If you look at the `x`-axis breaks on the plot in McElreath's lower left
panel in Figure 4.3, you'll notice they're intentional. To compute the
mean and 3 standard deviations above and below, you might do this.

```{r}
#| label: compute-mean-3sd-b
sim %>% 
  summarise(ll   = mean(height) - sd(height) * 3,
            mean = mean(height),
            ul   = mean(height) + sd(height) * 3) %>% 
  mutate_all(round, digits = 1)
```

Here's the work to make the lower right panel of Figure 4.3.

```{r}
#| label: fig-reproduce-4.3-low-right
#| fig-cap: "Reproduce lower right panels of Figure 4.3"


# simulate
set.seed(4)

sim <-
  tibble(sample_mu_b    = rnorm(n, mean = 178, sd = 100),
         sample_sigma_b = runif(n, min = 0, max = 50)) %>% 
  mutate(height = rnorm(n, mean = sample_mu_b, sd = sample_sigma_b))

# compute the values we'll use to break on our x axis
breaks <-
  c(mean(sim$height) - 3 * sd(sim$height), 0, mean(sim$height), mean(sim$height) + 3 * sd(sim$height)) %>% 
  round(digits = 0)

# this is just for aesthetics
text <-
  tibble(height = 272 - 25,
         y      = .0013,
         label  = "tallest man",
         angle  = 90)

# plot
p4 <-
  sim %>% 
  ggplot(aes(x = height)) +
  geom_density(fill = "deepskyblue", color = "black") +
  geom_vline(xintercept = 0, color = "black") +
  geom_vline(xintercept = 272, color = "black", linetype = 3) +
  geom_text(data = text,
            aes(y = y, label = label, angle = angle),
            color = "black") +
  scale_x_continuous(breaks = breaks) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("height ~ dnorm(mu, sigma)\nmu ~ dnorm(178, 100)") +
  theme(panel.grid = element_blank())

p4
```

Let's combine the four to make our version of McElreath's Figure 4.3.

```{r}
#| label: fig-reproduce-3.4
#| fig-cap: "Reproduction of Figure 3.4"

library(patchwork)
(p1 + xlab("mu") | p2 + xlab("sigma")) / (p3 | p4)
```

On page 84, McElreath said his prior simulation indicated 4% of the
heights would be below zero. He also drew the break down compared to the
tallest man on record, [Robert Pershing
Wadlow](https://en.wikipedia.org/wiki/Robert_Wadlow) (1918--1940).

```{r}
#| label: calc-breaks-b

sim %>% 
  count(height < 0) %>% 
  mutate(percent = 100 * n / sum(n))

sim %>% 
  count(height < 272) %>% 
  mutate(percent = 100 * n / sum(n))
```

### Grid approximation of the posterior distribution

#### Original

We are going to map out the posterior distribution through brute force
calculations.

This is not recommended because it is

-   laborious and computationally expensive
-   usually so impractical as to be essentially impossible.

Therefor the grid approximation technique has limited relevance. Later
on we will use the quadratic approximation with `rethinking::quap()`.

```{r}
#| label: grid-approx-posterior-a

## R code 4.16 ##################################

# establish range of μ and σ values, respectively, to calculate over 
# as well as how many points to calculate in-between. 
mu.list_a <- seq(from = 150, to = 160, length.out = 100)
sigma.list_a <- seq(from = 7, to = 9, length.out = 100)

# expands μ & σ values into a matrix of all of the combinations
post_a <- expand.grid(mu_a = mu.list_a, sigma_a = sigma.list_a)

# compute the log-likelihood at each combination of μ and σ
post_a$LL <- sapply(1:nrow(post_a), function(i) {
  sum(
    dnorm(d2_a$height, post_a$mu[i], post_a$sigma[i], log = TRUE)
  )
})

# multiply the prior by the likelihood
# as the priors are on the log scale adding = multiplying
post_a$prod <- post_a$LL + dnorm(post_a$mu_a, 178, 20, TRUE) +
  dunif(post_a$sigma_a, 0, 50, TRUE)

# getting back on the probability scale without rounding error 
post_a$prob <- exp(post_a$prod - max(post_a$prod))

```

> **Comment to the last line**: the obstacle for getting back on the
> probability scale is that rounding error is always a threat when
> moving from log-probability to probability. If you use the obvious
> approach, like `exp( post$prod )`, you'll get a vector full of zeros,
> which isn't very helpful. This is a result of R's rounding very small
> probabilities to zero.

**Plot contour lines**

```{r}
#| label: fig-contour-plot-a
#| fig-cap: "Draw a contour plot: rethinking version"

## R code 4.17 ##################################
rethinking::contour_xyz(post_a$mu_a, post_a$sigma_a, post_a$prob)

```

You can inspect this posterior distribution, now residing in
`post_a$prob`, using a variety of plotting commands.

**Plot heat map**

```{r}
#| label: fig-heat-map-a
#| fig-cap: "Draw a heat map: rethinking version"

## R code 4.18 ##################################
rethinking::image_xyz(post_a$mu_a, post_a$sigma_a, post_a$prob)

```

#### Tidyverse

With grid approximation we are going to use the brute force method for
the calculation of the posterior distribution. This technique has
limited relevance. Later on we will use the quadratic approximation with
`brms::brm()`.

It is the same technique we have use in
@sec-sampling-from-a-grid-approximate-posterior respectively in the
tidyverse version in @sec-grid-approximation-b. As there is no
conceptually new information to learn, I am not going into the details
of the following code. (It combines several code chunk from Kurz's
version.) But I am going to foreshadow the most important differences in
the tidyverse approach of the grid approximation technique:

Instead of `base::grid_expand()` we will use `tidyr::crossing()` Instead
of `base::sapply()` we will use `purr::map2()`

The produced tibble contains data frames in its cells, so that we have
to use the `tidyr::unnest()` function to expand the list-column
containing data frames into rows and columns.

Referring to the plots:

-   Instead of `rethinking::contour_xyz()` we will use
    `ggplot2::geom_contour()`
-   Instead of `rethinking::image_xyz()` we will use
    `ggplot2::geom_raster()`

```{r}
#| label: grid-approx-posterior-b
#| attr-source: '#lst-grid-approx-posterior-b lst-cap="Grid Approximation of the posterior distribution: tidyverse version"'

n <- 200

d_grid_b <-
  # we'll accomplish with `tidyr::crossing()` what McElreath did with base R `expand.grid()`
  crossing(mu_b    = seq(from = 140, to = 160, length.out = n),
           sigma_b = seq(from = 4, to = 9, length.out = n))

glimpse(d_grid_b)

grid_function <- function(mu, sigma) {
  
  dnorm(d2_b$height, mean = mu, sd = sigma, log = TRUE) %>% 
    sum()
  
}

d_grid2_b <-
  d_grid_b %>% 
  mutate(log_likelihood_b = map2(mu_b, sigma_b, grid_function)) %>%
  unnest(log_likelihood_b) %>% 
  mutate(prior_mu_b    = dnorm(mu_b, mean = 178, sd = 20, log = T),
         prior_sigma_b = dunif(sigma_b, min = 0, max = 50, log = T)) %>% 
  mutate(product_b = log_likelihood_b + prior_mu_b + prior_sigma_b) %>% 
  mutate(probability_b = exp(product_b - max(product_b)))
  
head(d_grid2_b)
```

```{r}
#| label: fig-contour-b
#| fig-cap: "Draw 2D contours of a 3D surface"

d_grid2_b %>% 
  ggplot(aes(x = mu_b, y = sigma_b, z = probability_b)) + 
  geom_contour() +
  labs(x = expression(mu),
       y = expression(sigma)) +
  coord_cartesian(xlim = range(d_grid2_b$mu_b),
                  ylim = range(d_grid2_b$sigma_b)) +
  theme(panel.grid = element_blank())

```

```{r}
#| label: fig-heatmap-b
#| fig-cap: "Draw heat map"

d_grid2_b %>% 
  ggplot(aes(x = mu_b, y = sigma_b, fill = probability_b)) + 
  geom_raster(interpolate = TRUE) +
  scale_fill_viridis_c(option = "B") +
  labs(x = expression(mu),
       y = expression(sigma)) +
  theme(panel.grid = element_blank())
```

### Sampling from the posterior

#### Original

To study this posterior distribution in more detail, again I'll push the
flexible approach of sampling parameter values from it. This works just
like it did in @sec-sampling-to-summarize, when you sampled values of
`p` from the posterior distribution for the globe tossing example. The
only new trick is that since there are two parameters, and we want to
sample combinations of them, we first randomly sample row numbers in
post in proportion to the values in \`post_a\$prob´. Then we pull out
the parameter values on those randomly sampled rows.

```{r}
#| label: fig-posterior-sample-a
#| fig-cap: "Samples from the posterior distribution for the heights data. The density of points is highest in the center, reflecting the most plausible combinations of μ and σ. There are many more ways for these parameter values to produce the data, conditional on the model. (rethinking version)"

## R code 4.19 ###########################

# randomly sample row numbers in post_a 
# in proportion to the values in post_a$prob. 
sample.rows <- sample(1:nrow(post_a),
  size = 1e4, replace = TRUE,
  prob = post_a$prob
)

# pull out the parameter values
sample.mu_a <- post_a$mu[sample.rows]
sample.sigma_a <- post_a$sigma[sample.rows]

## R code 4.20 ###########################
plot(sample.mu_a, sample.sigma_a, cex = 0.8, pch = 21, col = rethinking::col.alpha(rethinking:::rangi2, 0.1))

```

The function `col.alpha()` is part of the {**rethinking**} R package.
All it does is make colors transparent, which helps the plot in FIGURE
4.4 (here: @fig-posterior-sample-a) more easily show density, where
samples overlap. Adjust the plot to your tastes by playing around with
`cex` (character expansion, the size of the points), `pch` (plot
character), and the 0.1 transparency value.

**Marginal Posterior Density**

Now that you have these samples, you can describe the distribution of
confidence in each combination of `μ` and `σ` by summarizing the
samples. Think of them like data and describe them, just like in
@sec-sampling-to-summarize. For example, to characterize the shapes of
the marginal posterior densities of `μ` and `σ`, all we need to do is:

```{r}
#| label: fig-marg-post-density-a
#| fig-cap: "Shapes of the marginal posterior densities of μ and σ: rethinking version"

## R code 4.21 #########################
rethinking::dens(sample.mu_a)
rethinking::dens(sample.sigma_a)

```

The jargon "marginal" here means "averaging over the other parameters."
Execute the above code and inspect the plots. These densities are very
close to being normal distributions. And this is quite typical. As
sample size increases, posterior densities approach the normal
distribution. If you look closely, though, you'll notice that the
density for σ has a longer right-hand tail. I'll exaggerate this
tendency a bit later, to show you that this condition is very common for
standard deviation parameters.

**Posterior Compatibility Intervals (PIs)**

To summarize the widths of these densities with posterior compatibility
intervals we use:

```{r}
#| label: post-comp-intervals-a
#| attr-source: '#lst-post-comp-intervals-a lst-cap="Posterior Compatibility Intervals (PIs): rethinking version"'

## R code 4.22 ####################
rethinking::PI(sample.mu_a)
rethinking::PI(sample.sigma_a)
```

Since these samples are just vectors of numbers, you can compute any
statistic from them that you could from ordinary data: `mean`, `median`,
or `quantile`, for example.

**Sample size and the normality of sigmas posterior**

Before moving on to using quadratic approximation `rethinking::quap()`
as shortcut to all of this inference, it is worth repeating the analysis
of the height data above, but now with only a fraction of the original
data. The reason to do this is to demonstrate that, in principle, the
posterior is not always so Gaussian in shape. There's no trouble with
the mean, `μ`. For a Gaussian likelihood and a Gaussian prior on `μ`,
the posterior distribution is always Gaussian as well, regardless of
sample size. It is the standard deviation `σ` that causes problems. So
if you care about `σ`---often people do not---you do need to be careful
of abusing the quadratic approximation.

The deep reasons for the posterior of `σ` tending to have a long
right-hand tail are complex. But a useful way to conceive of the problem
is that variances must be positive. As a result, there must be more
uncertainty about how big the variance (or standard deviation) is than
about how small it is. For example, if the variance is estimated to be
near zero, then you know for sure that it can't be much smaller. But it
could be a lot bigger.

Let's quickly analyze only 20 of the heights from the height data to
reveal this issue. To sample 20 random heights from the original list:

```{r}
#| label: fig-sample-only-20-a
#| fig-cap: "Sample 20 heights: rethinking version"

## R code 4.23 ######################################
d3_a <- sample(d2_a$height, size = 20)

## R code 4.24 ######################################
mu2_a.list <- seq(from = 150, to = 170, length.out = 200)
sigma2_a.list <- seq(from = 4, to = 20, length.out = 200)
post2_a <- expand.grid(mu = mu2_a.list, sigma = sigma2_a.list)
post2_a$LL <- sapply(1:nrow(post2_a), function(i) {
  sum(dnorm(d3_a,
    mean = post2_a$mu[i], sd = post2_a$sigma[i],
    log = TRUE
  ))
})
post2_a$prod <- post2_a$LL + dnorm(post2_a$mu, 178, 20, TRUE) +
  dunif(post2_a$sigma, 0, 50, TRUE)
post2_a$prob <- exp(post2_a$prod - max(post2_a$prod))
sample2_a.rows <- sample(1:nrow(post2_a),
  size = 1e4, replace = TRUE,
  prob = post2_a$prob
)
sample2_a.mu <- post2_a$mu[sample2_a.rows]
sample2_a.sigma <- post2_a$sigma[sample2_a.rows]
plot(sample2_a.mu, sample2_a.sigma,
  cex = 0.5,
  col = rethinking::col.alpha(rethinking:::rangi2, 0.1),
  xlab = "mu", ylab = "sigma", pch = 16
)

```

you'll see another scatter plot of the samples from the posterior
density, but this time you'll notice a distinctly longer tail at the top
of the cloud of points.

**Marginal Posterior Density with only 20 rows**

You should also inspect the marginal posterior density for σ, averaging
over μ, produced with:

```{r}
#| label: fig-marg-post-density-a2
#| fig-cap: "Marginal posterior density for σ, averaging over μ: rethinking version"

## R code 4.25
rethinking::dens(sample2_a.sigma, norm.comp = TRUE)

```

#### Tidyverse

We can use `dplyr::sample_n()` to sample rows, with replacement, from
`d_grid2_b`.

```{r}
#| label: fig-posterior-sample-b
#| fig-cap: "Samples from the posterior distribution for the heights data. The density of points is highest in the center, reflecting the most plausible combinations of μ and σ. There are many more ways for these parameter values to produce the data, conditional on the model. (tidyverse version)"


set.seed(4)

d_grid_samples_b <- 
  d_grid2_b %>% 
  sample_n(size = 1e4, replace = T, weight = probability_b)

d_grid_samples_b %>% 
  ggplot(aes(x = mu_b, y = sigma_b)) + 
  geom_point(size = .9, alpha = 1/15) +
  scale_fill_viridis_c() +
  labs(x = expression(mu[samples]),
       y = expression(sigma[samples])) +
  theme(panel.grid = element_blank())
```

We can use `tidyr::pivot_longer()` and then `ggplot2::facet_wrap()` to
plot the densities for both `mu` and `sigma` at once.

```{r}
#| label: fig-densities-mu-sigma
#| fig-cap: "Shapes of the marginal posterior densities of μ and σ: tidyverse version"

d_grid_samples_b %>% 
  pivot_longer(mu_b:sigma_b) %>% 

  ggplot(aes(x = value)) + 
  geom_density(fill = "deepskyblue", color = "black") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ name, scales = "free", labeller = label_parsed)
```

We'll use the {**tidybayes**} package to compute their posterior modes
and 95% HDIs.

::: {.callout-important}
There is a companion package {**ggdist**} which is imported by {**tidybayes**}. Whenever you cannot find the function in {**tidybayes**} then look at the documentation of {**ggdist**}. This is also the case for the `tidybayes::mode_hdi()` function. In the help files of {**tidybayes**} you will just find notes about a deprecated `tidybayes::mode_hdih()` function but not the arguments of its new version without the last `h` (for horizontal) `tidybayes::mode_hdi()`. But you can look up these details in the {**ggdist**} documentation. This observation is valid for many families of deprecated functions.

There is a division of functionality between {**tidybayes**} and {**ggdist**}:

- {**tidybayes**}: Tidy Data and 'Geoms' for Bayesian Models: Compose data for and extract, manipulate, and visualize posterior draws from Bayesian models in a tidy data format. Functions are provided to help extract tidy data frames of draws from Bayesian models and that generate point summaries and intervals in a tidy format.
- {**ggdist**}: Visualizations of Distributions and Uncertainty: Provides primitives for visualizing distributions using {**ggplot2**} that are particularly tuned for visualizing uncertainty in either a frequentist or Bayesian mode. Both analytical distributions (such as frequentist confidence distributions or Bayesian priors) and distributions represented as samples (such as bootstrap distributions or Bayesian posterior samples) are easily visualized.
:::


```{r}
#| label: post-mode-hdi95-b

d_grid_samples_b %>% 
  pivot_longer(mu_b:sigma_b) %>% 
  group_by(name) %>% 
  tidybayes::mode_hdi(value) 
```

Let's say you wanted their posterior medians and 50% quantile-based
intervals, instead. Just switch out the last line for
`tidybayes::median_qi(value, .width = .5)`.

```{r}
#| label: post-median-qi90-b

d_grid_samples_b %>% 
  pivot_longer(mu_b:sigma_b) %>% 
  group_by(name) %>% 
  tidybayes::median_qi(value, .width = .5)
```

**Sample size and the normality of σ's posterior**

I will skip this part as there is nothing conceptually new in this
section.

### Finding the posterior distribution with `quap()` resp. `brms()`

#### Original

> To build the **quadratic approximation**, we'll use quap, a command in
> the `rethinking` package. The `quap` function works by using the model
> definition you were introduced to earlier in this chapter. Each line
> in the definition has a corresponding definition in the form of R
> code. The engine inside quap then uses these definitions to define the
> posterior probability at each combination of parameter values. Then it
> can climb the posterior distribution and find the peak, its MAP
> (**Maximum A Posteriori** estimate). Finally, it estimates the
> quadratic curvature at the MAP to produce an approximation of the
> posterior distribution. (parenthesis and emphasis are mine)

::: callout-note
The procedure used by `rethinking:quap()` is very similar to what many
non-Bayesian procedures do, just without any priors.
:::

1.  We start with the Howell1 data frame for adults `d2_a` (age \>= 18).
    We will place the R code equivalents into an `alist()` We are going
    to use the @def-prior-height-model. (Code 4.27).
2.  Then we fit the model with `rethinking::quap()` to the data in the
    data frame `d2_a` (Code 4.28) to `m4.1`.
3.  Now we can have a look with `rethinking::precis()` at the posterior
    distribution (Code 4.29).

```{r}
#| label: post-dist-quap-m4-1-a
#| attr-source: '#lst-post-dist-quap-m4-1-a lst-cap="Finding the posterior distribution with rethinking::quap()"'

## R code 4.27 ######################
flist <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(178, 20),
  sigma ~ dunif(0, 50)
)

## R code 4.28 ######################
m4.1 <- rethinking::quap(flist, data = d2_a)

## R code 4.29 ######################
rethinking::precis(m4.1)

```

> These numbers provide Gaussian approximations for each parameter's
> *marginal* distribution. This means the plausibility of each value of
> `_μ_`, after averaging over the plausibilities of each value of `_σ_`,
> is given by a Gaussian distribution with mean 154.6 and standard
> deviation 0.4.
>
> The 5.5% and 94.5% quantiles are percentile interval boundaries,
> corresponding to an 89% compatibility interval. Why 89%? It's just the
> default. It displays a quite wide interval, so it shows a
> high-probability range of parameter values. If you want another
> interval, such as the conventional and mindless 95%, you can use
> `precis(m4.1, prob=0.95)`. But I don't recommend 95% intervals,
> because readers will have a hard time not viewing them as significance
> tests. 89 is also a prime number, so if someone asks you to justify
> it, you can stare at them meaningfully and incant, "Because it is
> prime." That's no worse justification than the conventional
> justification for 95%.

I encourage you to compare these 89% boundaries to the compatibility
intervals from the grid approximation in @lst-post-comp-intervals-a
earlier. You'll find that they are almost identical. When the posterior
is approximately Gaussian, then this is what you should expect.

##### Start values for `rethinking::quap()` {#sec-start-values-rethinking}

Mean and standard deviation are good values to start values for hill
climbing. If you don't specify `rethinking::quap()` will use a random
value.

```{r}
#| label: start-values-quap
#| attr-source: '#start-values-quap lst-cap="Define start values for rethinking::quap()"'

## R code 4.30 ######################
start <- list(
  mu = mean(d2_a$height),
  sigma = sd(d2_a$height)
)
m4.1_2 <- rethinking::quap(flist, data = d2_a, start = start)
rethinking::precis(m4.1_2)

```

::: callout-note
###### list() and alist()

Note that the list of start values is a regular `list`, not an `alist`
like the formula list is. The two functions `alist` and `list` do the
same basic thing: allow you to make a collection of arbitrary R objects.
They differ in one important respect: `list` evaluates the code you
embed inside it, while `alist` does not. So when you define a list of
formulas, you should use `alist`, so the code isn't executed. But when
you define a list of start values for parameters, you should use `list`,
so that code like `mean(d2_a$height)` will be evaluated to a numeric
value.
:::

**Slicing in more information**

> The priors we used before are very weak, both because they are nearly
> flat and because there is so much data. So I'll splice in a more
> informative prior for `*μ*`, so you can see the effect. All I'm going
> to do is change the standard deviation of the prior to 0.1, so it's a
> very narrow prior. I'll also build the formula right into the call to
> `quap` this time.

```{r}
#| label: post-dist-quap-m4.2
#| attr-source: '#lst-post-dist-quap-m4.2 lst-cap="Finding the posterior distribution with a narrower prior rethinking::quap()"'

## R code 4.31 ###########################
m4.2 <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu ~ dnorm(178, 0.1),
    sigma ~ dunif(0, 50)
  ),
  data = d2_a
)
rethinking::precis(m4.2)

```

> Notice that the estimate for `*μ*` has hardly moved off the prior. The
> prior was very concentrated around 178. So this is not surprising. But
> also notice that the estimate for `*σ*` has changed quite a lot, even
> though we didn't change its prior at all. Once the golem is certain
> that the mean is near 178---as the prior insists---then the golem has
> to estimate `*σ*` conditional on that fact. This results in a
> different posterior for `*σ*`, even though all we changed is prior
> information about the other parameter.

::: callout-caution
###### `μ` has hardly moved off the prior

At first I did not understand "that the estimate for `*μ*` has hardly
moved off the prior". I thought this assertion refers to the value of
`*μ*` in both calculation. *μ* has changed considerably from 154.61 to
177.86 and under that assumption the above quote does not make sense.

But in contrast to my wrong assumption the assertion refers to the
difference between the chosen prior (178) and the resulting value of
`*μ*` (177.86).
:::

#### Tidyverse

> In the text, McElreath indexed his models with names like `m4.1`. I
> will largely follow that convention, but will replace the *m* with a
> *b* to stand for the **`brms`** package.

Here's how to fit the first model for this chapter.

```{r}
#| label: post-dist-brms-b4.1
#| att-source: '#lst-post-dist-brms-b4.1 lst-cap="Finding the posterior distribution with brms::brm()"'

b4.1 <- 
  brms::brm(data = d2_b, 
      family = gaussian,
      height ~ 1,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.01")

brms:::plot.brmsfit(b4.1)
```

If you want detailed diagnostics for the HMC chains, call
`brms::launch_shinystan(b4.1)`. That'll keep you busy for a while. See
the [ShinyStan website](https://mc-stan.org/users/interfaces/shinystan)
for more information.

::: callout-caution
###### Launch of shinystan turned off

I turned off the evaluation of the following chunk. It took some time
and the it referred to a local page `http://127.0.0.1:6367/`\`where I
could inspect many details of the model. But this is at the moment too
complex to me: I do not understand all the parameters and the many
configurable options programmed with a {**shiny**) interface.
:::

```{r}
#| label: detailled-diganostic-chains-brms-b4.1
#| eval: false

brms::launch_shinystan(b4.1)

```

```{r}
#| label: print-summary-brms-b4.1

brms:::print.brmsfit(b4.1)

```

```{r}
#| label: print-stan-like-summary-brms-b4.1

b4.1$fit
```

Whereas rethinking defaults to 89% intervals, using `print()` or
`summary()` with {**brms**} models defaults to 95% intervals.

::: callout-note
As I have learned shortly: `print()` or `summary()` are generic
functions where one can add new printing methods with new classes. In
this case `class(b4.1)` = `r class(b4.1)`. This means I do not need to
add `brms::` to secure that I will get the {**brms**} printing or
summary method as I didn't load the {**brms**} package. Quite the
contrary: Adding `brms::` would result into the message: "Error:
'summary' is not an exported object from 'namespace:brms'".

As I really want to specify explicitly the method these generic
functions should use, I need to use the syntax `brms:::print.brmsfit()`
or `brms:::summary.brmsfit()` respectively.

In this respect I have to learn more about S3 classes. There are many
important web resources about this subject that I have found with the
search string "r what is s3 class". Maybe I should start with the [S3
chapter in Advanced R](https://adv-r.hadley.nz/s3.html).
:::

Unless otherwise specified, Kurz will stick with 95% intervals
throughout. To get those 89% intervals or McElreath approach, one could
use the `prob` argument within `summary()` or `print()`.

```{r}
#| label: summary-interval-.89-brms-b4.1

brms:::summary.brmsfit(b4.1, prob = .89)

```

Here's the `brms::brm()` code for the model with the very narrow `_μ_`
prior corresponding to the `rethinking::quap()` code in
@lst-post-dist-quap-m4.2.

```{r}
#| label: fig-post-dist-brms-b4.2
#| fig-cap: "Finding the posterior distribution with a narrower prior using brms::brm()"
#| att-source: '#lst-post-dist-brms-b4.2 lst-cap="Finding the posterior distribution with a narrower prior using brms::brm()"'

b4.2 <- 
  brms::brm(data = d2_b, 
      family = gaussian,
      height ~ 1,
      prior = c(brms::prior(normal(178, 0.1), class = Intercept),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.02")

brms:::plot.brmsfit(b4.2, widths = c(1, 2))

```

And here's the model `summary()`.

```{r}
#| label: summary-narrow-prior

brms:::summary.brmsfit(b4.2)

```

Subsetting the `summary()` output with `$fixed` provides a convenient
way to compare the Intercept summaries between `b4.1` and `b4.2`.

```{r}
#| label: compare-summaries-b4.1-b4.2

rbind(brms:::summary.brmsfit(b4.1)$fixed,
      brms:::summary.brmsfit(b4.2)$fixed)

```

### Sampling from a quap

#### Original

The above explains how to get a quadratic approximation of the
posterior, using `rethinking::quap()`. But how do we then get samples
from the quadratic approximate posterior distribution? --- When R
constructs a quadratic approximation, it calculates not only standard
deviations for all parameters, but also the covariances among all pairs
of parameters. Just like a mean and standard deviation (or its square, a
variance) are sufficient to describe a one-dimensional Gaussian
distribution, a list of means and a matrix of variances and covariances
are sufficient to describe a multi-dimensional Gaussian distribution.

```{r}
#| label: calc-var-cov-m4.1-a
#| attr-source: '#lst-calc-var-cov-m4.1-a lst-cap="Calculation of the variance-covariance matrix: rethinking version"'

## R code 4.32 ###################
rethinking::vcov(m4.1)
```

`vcov()` returns the variance-covariance matrix of the main parameters
of a fitted model object. In the above {**rethinking**} version is uses
the class `map2stan` for a fitted Stan model as `m4.1` is of class
`map`.

::: callout-caution
###### rethinking::vcov

In @lst-calc-var-cov-m4.1-a I am explicitly using the package
{**rethinking**} for the `vcov()` function. The same function is also
available as a base R function with `stats::vcov()`. But this generates
an error because there is no method known for an object of class `map`
from the rethinking package. The help file for `stats::vcov()` only says
that the `vcov` object is an S3 method for classes `lm`, `glm`, `mlm`
and `aov` but not for `map`.

> Error in UseMethod("vcov") : no applicable method for 'vcov' applied
> to an object of class "map"

I could have used only `vcov()`. But this only works when the
{**rethinking**} package is already loaded. In that case R knows because
of the class of the object which `vcov()` version to use. In this case:
class of object = `class(m4.1)` `r class(m4.1)`.
:::

@lst-calc-var-cov-m4.1-a results in a variance-covariance matrix. It is
the multi-dimensional glue of a quadratic approximation, because it
tells us how each parameter relates to every other parameter in the
posterior distribution. A variance-covariance matrix can be factored
into two elements: (1) a vector of variances for the parameters and (2)
a correlation matrix that tells us how changes in any parameter lead to
correlated changes in the others.

```{r}
#| label: vcov-decomp-m4.1-a

## R code 4.33 #######################
base::diag(rethinking::vcov(m4.1))      # <1>
stats::cov2cor(rethinking::vcov(m4.1))  # <2>
```

1.  `base::diag()` extracts the diagonal of the (variance-covariance)
    matrix. The two-element vector in the output is the list of
    variances. If you take the square root of this vector, you get the
    standard deviations that are shown in `rethinking::precis()` output.
2.  `stats::cov2cor()` scales a covariance matrix into the corresponding
    correlation matrix. The two-by-two matrix in the output is this
    correlation matrix. Each entry shows the correlation, bounded
    between −1 and +1, for each pair of parameters. The 1's indicate a
    parameter's correlation with itself. If these values were anything
    except 1, we would be worried. The other entries are typically
    closer to zero, and they are very close to zero in this example.
    This indicates that learning μ tells us nothing about σ and likewise
    that learning σ tells us nothing about μ. This is typical of simple
    Gaussian models of this kind. But it is quite rare more generally,
    as you'll see in later chapters.

Okay, so how do we get samples from this multi-dimensional posterior?
Now instead of sampling single values from a simple Gaussian
distribution, we sample vectors of values from a multi-dimensional
Gaussian distribution.

```{r}
#| label: extract-samples-m4.1-a
#| attr-source: '#lst-extract-samples-m4.1-a lst-cap="Extract sample vectors of values from the multi-dimensional Gaussian distribution of m4.1: rethinking version"'


## R code 4.34 #######################
post3_a <- rethinking::extract.samples(m4.1, n = 1e4)
head(post3_a)
```

You end up with a data frame, post, with 10,000 (1e4) rows and two
columns, one column for `_μ_` and one for `_σ_`. Each value is a sample
from the posterior, so the mean and standard deviation of each column
will be very close to the MAP values from before. You can confirm this
by summarizing the samples:

```{r}
#| label: summary-samples-m4.1-a
#| attr-source: '#lst-summary-samples-m4.1-a lst-cap="Summary the extracted samples: rethinking version"'

## R code 4.35 ##################
rethinking::precis(post3_a)
```

Compare these values to the output from @lst-post-dist-quap-m4-1-a. And
you can use `plot(post)` to see how much they resemble the samples from
the grid approximation in FIGURE 4.4 (here @fig-posterior-sample-a).
These samples also preserve the covariance between `_μ_` and `_σ_`. This
hardly matters right now, because `_μ_` and `_σ_` don't co-vary at all
in this model. But once you add a predictor variable to your model,
covariance will matter a lot.

```{r}
#| label: fig-posterior-sample-vectors-a
#| fig-cap: "Samples from the vectors of values from a multi-dimensional Gaussian distribution. The density of points is highest in the center, reflecting the most plausible combinations of μ and σ. It is very similar to @fig-posterior-sample-a (rethinking version)"

base::plot(post3_a)
```

##### Under the hood with multivariate sampling {#sec-under-the-hood-multivariate-sampling}

The function `rethinking::extract.samples()` is for convenience. It is
just running a simple simulation of the sort you conducted near the end
of @sec-sampling-the-imaginary with @lst-sim-pred-samples-a. Here's a
peak at the motor. The work is done by a multi-dimensional version of
`stats::rnorm()`, `MASS::mvrnorm()`. The function `stats::rnorm()`
simulates random Gaussian values, while `MASS::mvrnorm()` simulates
random vectors of multivariate Gaussian values. Here's how to use it the
{**MASS**} function to do what `rethinking::extract.samples()` does:

```{r}
#| label: fig-posterior-sample-vectors-MASS-a
#| fig-cap: "Extract samples from the vectors of values from a multi-dimensional Gaussian distribution using the {**MASS**} package. It is same calculation as in @fig-posterior-sample-a (rethinking version)"


## R code 4.36 ######################
post4_a <- MASS::mvrnorm(n = 1e4, mu = rethinking::coef(m4.1), 
                      Sigma = rethinking::vcov(m4.1))
plot(post4_a)
```

#### Tidyverse

{**brms**} doesn't seem to have a convenience function that works the
way `vcov()` does for {**rethinking**}.

```{r}
#|label: calc-var-cov-m4.1-b
#|attr-source: '#lst-calc-var-cov-m4.1-b lst-cap="Calculation of vcov(): tidyverse version."'

brms:::vcov.brmsfit(b4.1)
```

This only returns the first element in the matrix it did for
{**rethinking**}. That is, it appears `brms::vcov()` only returns the
variance/covariance matrix for the single-level `_β_` parameters.

::: callout-caution
###### brms::vcov()

Referring to a similar situation with `rethinking::vcov()` in
@lst-calc-var-cov-m4.1-a I cannot write `brms::vcov()`, but have to use
either `brms:::vcov.brmsfit(b4.1)` or just `vcov(b4.1)`. The weird thing
is that the first time it also works with `brms::vcov()` but only the
first time!
:::

However, if you really wanted this information, you could get it after
putting the Hamilton Monte Carlo (HMC) chains in a data frame. We do
that with the `brms::as_draws_df()` function:

```{r}
#| label: put-hmc-into-df-b
#| attr-source: '#lst-put-hmc-into-df-b lst-cap="Extract the iteration of the Hamiliton Monte Carlo (HMC) chains into a data frame"'

post_b <- brms::as_draws_df(b4.1)

head(post_b)
```

::: callout-tip
###### draws object

The functions of the family `as_draws()` transform `brmsfit` objects to
`draws` objects, a format supported by the {**posterior**} package.
{brms} currently imports the family of `as_draws()`functions from the
{**posterior**} package, a tool for working with posterior
distributions.

@lst-put-hmc-into-df-b produced the {**brms**} version of what McElreath
achieved with `extract.samples()` in @lst-extract-samples-m4.1-a.
However, what happened under the hood was different. Whereas rethinking
used the `mvnorm()` function from the {**MASS**} package, with
{**brms**} we just extracted the iterations of the HMC chains and put
them in a data frame.
:::

Now `select()` the columns containing the draws from the desired
parameters `b_Intercept` and `sigma` and feed them into `cov()`.

```{r}
#| label: calc-cov-post-b
#| attr-source: '#lst-calc-cov-post-b lst-cap="Calculate the vector of variances and correlation matrix for b_Intercept and sigma"'

select(post_b, b_Intercept:sigma) %>% 
  stats::cov()
```

@lst-calc-cov-post-b displays "(1) a vector of variances for the
parameters and (2) a correlation matrix" for them (p. 90). Here are just
the variances (i.e., the diagonal elements) and the correlation matrix.

```{r}
#| label: calc-var-post-b
#| attr-source: '#lst-calc-var-post-b lst-cap="Calculate only variances (the diagonal values)"'

select(post_b, b_Intercept:sigma) %>%
  stats::cov() %>%
  base::diag()

```

```{r}
#| label: calc-corr-matrix-post-b
#| attr-source: '#lst-calc-corr-matrix-post-b lst-cap="Calculate only crrelation"'

# correlation
post_b %>%
  select(b_Intercept, sigma) %>%
  stats::cor()
```

```{r}
#| label: show-data-structure

str(post_b)
```

The `post_b` object is not just a data frame, but also of class
`draws_df`, which means it contains three metadata variables ----
`.chain`, `.iteration`, and `.draw` --- which are often hidden from
view, but are there in the background when needed. As you'll see, we'll
make good use of the `.draw` variable in the future. Notice how our post
data frame also includes a vector named `lp__`. That's the log
posterior.

For details, see: - The [Log-Posterior (function and
gradient)](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html#the-log-posterior-function-and-gradient)
section of the Stan Development Team's (2023) vignette [RStan: the R
interface to
Stan](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html)
and - Stephen Martin's [explanation of the log
posterior](https://discourse.mc-stan.org/t/basic-question-what-is-lp-in-posterior-samples-of-a-brms-regression/17567/2)
on the Stan Forums.

::: callout-caution
###### Summaries of {brms} and {posterior} packages

Kurz claims that `summary()` function doesn't work for {**brms**}
posterior data frames quite the way `rethinking::precis()` does for
posterior data frames from the {\*\*rethinking\*} package. But I this
observation is somewhat misleading.

The posterior data frame `post_b` is not of class `brms`. Let's check
this:

```{r}
#| label: class-post_b-b

class(post_b)
```

The class `draws_df`and `draws` refers to the {**posterior**} and not to
the {**brms**} package. Remember: In @lst-put-hmc-into-df-b we
transformed with the function `as_draws_df` the `brms` object into a
`draws_df` and `draws` object.

Therefore Kurz's claim should be read: The `summary()` function doesn't
work for {**posterior**} posterior data frames quite the way
`rethinking::precis()` does for posterior data frames from the
{**rethinking**} package. Instead of calling `brms:::summary.brmsfit()`
I will use `posterior:::summary.draws()`.

I wouldn't have noticed this difference if I hadn't mentioned explicitly
the name of the packages in front of the function, because in that case
R would have used `base::summary()` as in Kurz's text.
:::

```{r}
#| label: base-summary-samples-b4.1-b
#| attr-source: '#lst-base-summary-samples-b4.1-b lst-cap="Summary the extracted iterations of the HMC chains: base version"' 

base::summary(post_b[, 1:2])

```

```{r}
#| label: posterior-summary-samples-b4.1-b
#| attr-source: '#lst-posterior-summary-samples-b4.1-b lst-cap="Summary the extracted iterations of the HMC chains: posterior version"' 

posterior:::summary.draws(post_b[, 1:2])


```

To get a similar summary with tiny histograms Kurz offers different
solutions:

-   A base R approach by using the transpose of a `stats::quantile()`
    call nested within `base::apply()`
-   A {**tidyverse**} approach
-   A {**brms**} approach by just putting the `brm()` fit object into
    `posterior_summary()`
-   A {**tidybayes**} approach using `tidybayes::mean_hdi()` if you're
    willing to drop the posterior `sd` and
-   Using additionally the [function
    `histospark()`](https://github.com/hadley/precis/blob/master/R/histospark.R)
    (from the unfinished {**precis**} package by Hadley Wickham supposed
    to replace `base::summary()`) to get the tiny histograms and to add
    them into the tidyverse approach.

Additionally I will propose using the {**skimr**} packages:

```{r}
#| label: skim-summary-samples-b4.1-b
#| attr-source: '#lst-skim-summary-samples-b4.1-b lst-cap="Summary the extracted iterations of the HMC chains: skimr version"' 

skimr::skim(post_b[, 1:2])
```

Kurz refers only shortly to both `overthinking` blocks of this section:

-   Start values for `rethinking::quap()` resp. `brms::brm()` (See
    @sec-start-values-rethinking): Within the `brm()` function, you use
    the `init` argument fpr the start values.
-   Under the hood with multivariate sampling (See
    @sec-under-the-hood-multivariate-sampling): Again Kurz remarked that
    `brms::as_draws_df()` is not the same as
    `rethinking::extract.samples()`. What this exactly means will
    (hopefully) explained later in @sec-markov-chain-monte-carlo.

## Linear prediction

### Introduction

#### Original

What we've done until now is just a Gaussian model of height in a
population of adults. But typically, we are interested in modeling how
an outcome is related to some other variable, a predictor variable. If
the predictor variable has any statistical association with the outcome
variable, then we can use it to predict the outcome. When the predictor
variable is built inside the model in a particular way, we'll have
linear regression.

Let's look at how height in these Kalahari foragers (the outcome
variable) co-varies with weight (the predictor variable).

```{r}
#| label: fig-height-against-weight-a
#| fig-cap: "Adult height and weight against one another"

## R code 4.37 #####################
plot(d2_a$height ~ d2_a$weight)
```

There's obviously a relationship: Knowing a person's weight helps you
predict height. To make this vague observation into a more precise
quantitative model that relates values of `weight` to plausible values
of `height`, we need some more technology. How do we take our Gaussian
model from @sec-gaussian-model-of-height and incorporate predictor
variables?

#### Tidyverse

```{r}
#| label: fig-height-against-weight-b
#| fig-cap: "Adult height and weight against one another"


d2_b |> 
    ggplot(aes(height, weight)) + 
    geom_point()
```

### The linear model strategy

#### Original

##### Model definition

Recall @def-prior-height-model for the Gaussian height model. How do we
get `weight` into this model? Let `_x_` be the name for the column of
weight measurements, `d2_a$weight`. Let the average of the `_x_` values
be $\overline{x}$, "ex bar". Now we have a predictor variable `_x_`,
which is a list of measures of the same length as `_h_`. To get weight
into the model, we define the mean `_μ_` as a function of the values in
`_x_`.

------------------------------------------------------------------------

::: {#def-height-weight-linear-model}
Linear model height against weight

$$
\begin{align*}
h_{i} \sim \operatorname{Normal}(μ_{i}, σ) \space \space (1) \\ 
μ_{i} = \alpha + \beta(x_{i}-\overline{x}) \space \space (2) \\
\alpha \sim \operatorname{Normal}(178, 20) \space \space (3)  \\ 
\beta \sim \operatorname{Normal}(0,10) \space \space (4) \\
\sigma \sim \operatorname{Uniform}(0, 50) \space \space (5)      
\end{align*}
$$ {#eq-height-weight-linear-model}

(1) **Likelihood (Probability of the data)**: The first line is nearly
    identical to before, except now there is a little index $i$ on the
    $μ$ as well as the $h$. You can read $h_{i}$ as "each height" and
    $\mu_{i}$ as "each $μ$" The mean $μ$ now depends upon unique values
    on each row $i$. So the little $i$ on $\mu_{i}$ indicates that *the
    mean depends upon the row*.

(2) **Linear model**: The mean $μ$ is no longer a parameter to be
    estimated. Rather, as seen in the second line of the model, $\mu{i}$
    is constructed from other parameters, $\alpha$ and $\beta$, and the
    observed variable $x$. This line is not a stochastic relationship
    ----- there is no `~` in it, but rather an `=` in it ----- because
    the definition of $\mu{i}$ is deterministic. That is to say that,
    once we know $\alpha$ and $\beta$ and $x_{i}$, we know $\mu{i}$ with
    certainty. (More details in @sec-linear-model-a.)

(3) **includes (3),(4) and(5) with** $\alpha, \beta, \sigma$ priors: The
    remaining lines in the model define distributions for the unobserved
    variables. These variables are commonly known as parameters, and
    their distributions as priors. There are three parameters:
    $\alpha, \beta, \sigma$. You've seen priors for $\alpha$ and $\beta$
    before, although $\sigma$ was called $\mu$ back then. (More details
    in @sec-priors-a)
:::

------------------------------------------------------------------------

##### Linear model {#sec-linear-model-a}

The value $x_{i}$ in the second line of @def-height-weight-linear-model
is just the weight value on row $i$. It refers to the same individual as
the height value, $h_{i}$, on the same row. The parameters $\alpha$ and
$\beta$ are more mysterious. Where did they come from? We made them up.
The parameters $\mu$ and $\sigma$ are necessary and sufficient to
describe a Gaussian distribution. But $\alpha$ and $\beta$ are instead
devices we invent for manipulating $\mu$, allowing it to vary
systematically across cases in the data.

You'll be making up all manner of parameters as your skills improve. One
way to understand these made-up parameters is to think of them as
targets of learning. Each parameter is something that must be described
in the posterior distribution. So when you want to know something about
the data, you ask your golem by inventing a parameter for it. This will
make more and more sense as you progress.

What does the second line of @def-height-weight-linear-model? It tells
the regression golem that you are asking two questions about the mean of
the outcome.

1.  What is the expected height when $x_{i} = \overline{x}$? The
    parameter $\alpha$ answers this question, because when
    $x_{i} = \overline{x}$, $\mu_{i} = \alpha$. For this reason,
    $\alpha$ is often called the **intercept**. But we should think not
    in terms of some abstract line, but rather in terms of the meaning
    with respect to the observable variables.
2.  What is the change in expected height, when $x_{i}$ changes by 1
    unit? The parameter $\beta$ answers this question. It is often
    called a **slope**, again because of the abstract line. Better to
    think of it as a rate of change in expectation.

Jointly these two parameters ask the golem to find a line that relates
$x$ to $h$, a line that passes through $\alpha$ when
$x_{i} = \overline{x}$ and has slope $\beta$. That is a task that golems
are very good at. It's up to you, though, to be sure it's a good
question.

##### Priors {#sec-priors-a}

The prior for $\beta$ in @def-height-weight-linear-model deserves
explanation. Why have a Gaussian prior with mean zero? This prior places
just as much probability below zero as it does above zero, and when
$\beta = 0$, weight has no relationship to height. To figure out what
this prior implies, we have to simulate the prior predictive
distribution.

The goal is to simulate heights from the model, using only the priors.
First, let's consider a range of weight values to simulate over. The
range of observed weights will do fine. Then we need to simulate a bunch
of lines, the lines implied by the priors for $\alpha$ and $\beta$.
Here's how to do it, setting a seed so you can reproduce it exactly:

```{r}
#| label: fig-sim-heights-only-with-priors-a
#| fig-cap: "Simulating heights from the model, using only the priors: rethinking version"

## R code 4.38 #####################
set.seed(2971)
N_100_a <- 100 # 100 lines
a <- rnorm(N_100_a, 178, 20)
b <- rnorm(N_100_a, 0, 10)


## R code 4.39 #####################
plot(NULL,
  xlim = range(d2_a$weight), ylim = c(-100, 400),
  xlab = "weight", ylab = "height"
)
abline(h = 0, lty = 2)
abline(h = 272, lty = 1, lwd = 0.5)
mtext("b ~ dnorm(0,10)")
xbar <- mean(d2_a$weight)
for (i in 1:N_100_a) {
  curve(a[i] + b[i] * (x - xbar),
    from = min(d2_a$weight), to = max(d2_a$weight), add = TRUE,
    col = rethinking::col.alpha("black", 0.2)
  )
}

```

For reference, I've added a dashed line at zero---no one is shorter than
zero---and the "Wadlow" line at 272 cm for the world's tallest person.
The pattern doesn't look like any human population at all. It
essentially says that the relationship between weight and height could
be absurdly positive or negative. Before we've even seen the data, this
is a bad model. Can we do better?

We can do better immediately. We know that average height increases with
average weight, at least up to a point. Let's try restricting it to
positive values. The easiest way to do this is to define the prior as
Log-Normal instead. Defining $\beta$ as `Log-Normal(0,1)` means to claim
that the logarithm of $\beta$ has a Normal(0,1) distribution.

------------------------------------------------------------------------

::: {#def-prior-log-normal-dist}
Defining the prior as Log-Normal distribution

$$
\beta \sim \operatorname{Log-Normal}(0,1)
$$ {#eq-prior-log-normal-dist}
:::

------------------------------------------------------------------------

Base R provides the `dlnorm()` and `rlnorm()` densities for working with
log-normal distributions. You can simulate this relationship to see what
this means for $\beta$:

```{r}
#| label: fig-log-normal-a
#| fig-cap: "Log-Normal distribution: rethinking version"


set.seed(4) # to reproduce with tidyverse version

## R code 4.40 ####################
b <- rlnorm(1e4, 0, 1)
rethinking::dens(b, xlim = c(0, 5), adj = 0.1)
```

If the logarithm of $\beta$ is normal, then $\beta$ itself is strictly
positive. The reason is that `exp(x)` is greater than zero for any real
number `x`. This is the reason that Log-Normal priors are commonplace.
They are an easy way to enforce positive relationships.

So what does this earn us? Do the prior predictive simulation again, now
with the Log-Normal prior:

```{r}
#| label: fig-prior-pred-sim-a
#| fig-cap: "Prior predictive simulation again, now with the Log-Normal prior: rethinking version"


## R code 4.41 ###################
set.seed(2971)
N_100_a <- 100 # 100 lines
a <- rnorm(N_100_a, 178, 20)
b <- rlnorm(N_100_a, 0, 1)

## R code 4.39 ###################
plot(NULL,
  xlim = range(d2_a$weight), ylim = c(-100, 400),
  xlab = "weight", ylab = "height"
)
abline(h = 0, lty = 2)
abline(h = 272, lty = 1, lwd = 0.5)
mtext("b ~ dnorm(0,10)")
xbar <- mean(d2_a$weight)
for (i in 1:N_100_a) {
  curve(a[i] + b[i] * (x - xbar),
    from = min(d2_a$weight), to = max(d2_a$weight), add = TRUE,
    col = rethinking::col.alpha("black", 0.2)
  )
}


```

This is much more sensible. There is still a rare impossible
relationship. But nearly all lines in the joint prior for $\alpha$ and
$\beta$ are now within human reason.

::: callout-note
###### What is the correct prior?

There is no more a uniquely correct prior than there is a uniquely
correct likelihood. Statistical models are machines for inference. Many
machines will work, but some work better than others. Priors can be
wrong, but only in the same sense that a kind of hammer can be wrong for
building a table.

Priors encode states of information before seeing data. So priors allow
us to explore the consequences of beginning with different information.
In cases in which we have good prior information that discounts the
plausibility of some parameter values, like negative associations
between height and weight, we can encode that information directly into
priors. When we don't have such information, we still usually know
enough about the plausible range of values. And you can vary the priors
and repeat the analysis in order to study how different states of
initial information influence inference. Frequently, there are many
reasonable choices for a prior, and all of them produce the same
inference.
:::

::: callout-note
###### Prior predictive simulation and p-hacking

When the model is adjusted in light of the observed data, then p-values
no longer retain their original meaning. False results are to be
expected. This is valid for Bayesian and Non-Bayesian statistics. Even
if Bayesian statistics don't pay any attention to p-values, the danger
remains. We could choose our priors conditional on the observed sample,
just to get some desired (wrong) result. It is therefore important to
choose priors conditional on pre-data knowledge of the variables---their
constraints, ranges, and theoretical relationships. We should always
judge our priors against general facts, not the sample.
:::

#### Tidyverse

##### Model definition (empty)

Nothing to add. Remains empty.

##### Linear model (empty)

Nothing to add. Remains empty.

##### Priors {#sec-priors-b}

```{r}
#| label: fig-sim-heights-only-with-priors-b
#| fig-cap: "Simulating heights from the model, using only the priors: tidyverse version"

set.seed(2971)
# how many lines would you like?
n_lines <- 100

lines <-
  tibble(n = 1:n_lines,
         a = rnorm(n_lines, mean = 178, sd = 20),
         b = rnorm(n_lines, mean = 0, sd = 10)) %>% 
  expand_grid(weight = range(d2_b$weight)) %>% 
  mutate(height = a + b * (weight - mean(d2_b$weight)))


lines %>% 
  ggplot(aes(x = weight, y = height, group = n)) +
  geom_hline(yintercept = c(0, 272), linetype = 2:1, linewidth = 1/3) +
  geom_line(alpha = 1/10) +
  coord_cartesian(ylim = c(-100, 400)) +
  ggtitle("b ~ dnorm(0, 10)") +
  theme_classic()

```

Using the Log-Normal distribution prohibits negative values. This is an
important constraint for height and weight as these variables cannot be
under 0.

```{r}
#| label: fig-log-normal-b
#| fig-cap: "Log-Normal distribution: tidyverse version"

set.seed(4)

tibble(b = rlnorm(1e4, meanlog = 0, sdlog = 1)) %>% 
  ggplot(aes(x = b)) +
  geom_density(fill = "grey92") +
  coord_cartesian(xlim = c(0, 5)) +
  theme_classic()

```

::: callout-note
Kurz used with `mean`and `sd`an abbreviated version of the argument
names `meanlog` and `sdlog`.
:::

I am not very skilled with the Log-Normal distribution, and so I am
happy that Kurz added some explanations:

> If you're unfamiliar with the log-normal distribution, it is the
> distribution whose logarithm is normally distributed. For example,
> here's what happens when we compare Normal(0,1) with
> log(Log-Normal(0,1)).

```{r}
#| label: fig-normal-log-normal
#| fig-cap: "Compare Normal(0,1) with log(Log-Normal(0,1))"

set.seed(4)

tibble(rnorm           = rnorm(1e5, mean = 0, sd = 1),
       `log(rlognorm)` = log(rlnorm(1e5, meanlog = 0, sdlog = 1))) %>% 
  pivot_longer(everything()) %>% 

  ggplot(aes(x = value)) +
  geom_density(fill = "grey92") +
  coord_cartesian(xlim = c(-3, 3)) +
  theme_classic() +
  facet_wrap(~ name, nrow = 2)
```

> Those values are ~~what~~ the mean and standard deviation of the
> output from the `rlnorm()` function **after** they are log
> transformed. The formulas for the actual mean and standard deviation
> for the log-normal distribution itself are complicated (see
> [Wikipedia](https://en.wikipedia.org/wiki/Log-normal_distribution)).

------------------------------------------------------------------------

::: {#def-mean-sd}
$$
\begin{align*}
\text{mean}               & = \exp \left (\mu + \frac{\sigma^2}{2} \right) \\
\text{standard deviation} & = \sqrt{[\exp(\sigma ^{2})-1] \; \exp(2\mu +\sigma ^{2})}
\end{align*}
$$ {#eq-formula-mean-sd}
:::

------------------------------------------------------------------------

see: @eq-formula-mean-sd

Let's try our hand at those formulas and compute the mean and standard
deviation for Log-Normal(0,1):

```{r}
#| label: compute-mu-sigma-for-log-normal-manually-b

mu    <- 0
sigma <- 1

# mean
exp(mu + (sigma^2) / 2)

# sd
sqrt((exp(sigma^2) - 1) * exp(2 * mu + sigma^2))
```

Let's confirm with simulated draws from `rlnorm()`.

```{r}
#| label: compute-log-normal-b
#| fig-cap: "Compute mean and standard deviation of the Log-Normal distribution: tidyverse version"

set.seed(4)

tibble(x = rlnorm(1e7, meanlog = 0, sdlog = 1)) %>% 
  summarise(mean = mean(x),
            sd   = sd(x))
```

```{r}
#| label: fig-prior-pred-sim-b
#| fig-cap: "Prior predictive simulation again, now with the Log-Normal prior: tidyverse version"


# make a tibble to annotate the plot
text <-
  tibble(weight = c(34, 43),
         height = c(0 - 25, 272 + 25),
         label  = c("Embryo", "World's tallest person (272 cm)"))

# simulate
set.seed(2971)

tibble(n = 1:n_lines,
       a = rnorm(n_lines, mean = 178, sd = 20),
       b = rlnorm(n_lines, mean = 0, sd = 1)) %>% 
  expand_grid(weight = range(d2_b$weight)) %>% 
  mutate(height = a + b * (weight - mean(d2_b$weight))) %>%
  
  # plot
  ggplot(aes(x = weight, y = height, group = n)) +
  geom_hline(yintercept = c(0, 272), linetype = 2:1, linewidth = 1/3) +
  geom_line(alpha = 1/10) +
  geom_text(data = text,
            aes(label = label),
            size = 3) +
  coord_cartesian(ylim = c(-100, 400)) +
  ggtitle("log(b) ~ dnorm(0, 1)") +
  theme_classic()
```

::: callout-tip
###### Literature reference

The paper by Simmons, Nelson and Simonsohn (2011), [False-positive
psychology: Undisclosed flexibility in data collection and analysis
allows presenting anything as
significant](https://journals.sagepub.com/doi/10.1177/0956797611417632),
is often cited as an introduction to the problem.
:::

### Finding the posterior distribution

#### Original

The code needed to approximate the posterior is a straightforward
modification of the kind of code you've already seen. All we have to do
is incorporate our new model for the mean into the model specification
inside `rethinking::quap()` and be sure to add a prior for the new
parameter, `β`. Let's repeat the model definition, now with the
corresponding R code as footnotes:

------------------------------------------------------------------------

::: {#def-find-post-dist}
Finding the posterior distribution

$$
\begin{align*}
h_{i} \sim \operatorname{Normal}(μ_{i}, σ) \space \space (1) \\ 
μ_{i} = \alpha + \beta(x_{i}-\overline{x}) \space \space (2) \\
\alpha \sim \operatorname{Normal}(178, 20) \space \space (3)  \\ 
\beta \sim \operatorname{Log-Normal}(0,10) \space \space (4) \\
\sigma \sim \operatorname{Uniform}(0, 50)  \space \space (5) \\    
\end{align*} 
$$ {#eq-find-post-dist-a}

```         
height ~ dnorm(mu, sigma)     # <1>
mu <- a + b * (weight - xbar) # <2>
a ~ dnorm(178, 20)            # <3>
b ~ dlnorm(0, 10)             # <4>
sigma ~ dunif(0, 50)          # <5>
```
:::

------------------------------------------------------------------------

Notice that the linear model, in the R code on the right-hand side, uses
the R assignment operator, `<-` instead of the symbol `=`.

```{r}
#| label: find-post-dist-a
#| attr-source: '#lst-find-post-dist-a lst-cap="Find the posterior distribution of the linear height-weight model: rethinking version"'

## R code 4.42 #############################

# define the average weight, x-bar
xbar <- mean(d2_a$weight)

# fit model
m4.3 <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - xbar),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d2_a
)

# summary result
## R code 4.44 ############################
rethinking::precis(m4.3)
```

You can usefully think of as assigning to $y = log(x)$ the order of
magnitude of $x = exp(y)$. The function is the reverse, turning a
magnitude into a value. These definitions will make a mathematician
shriek. But much of our computational work relies only on these
intuitions.

These definitions allow the Log-Normal prior for `β` to be coded another
way. Instead of defining a parameter `β`, we define a parameter that is
the logarithm of `β` and then assign it a normal distribution. Then we
can reverse the logarithm inside the linear model. It looks like this:

```{r}
#| label: find-post-dist2-a
#| attr-source: '#lst-find-post-dist2-a lst-cap="Find the posterior distribution of the linear height-weight model (log version): rethinking version"'

## R code 4.43 ############################
m4.3b <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + exp(log_b) * (weight - xbar),
    a ~ dnorm(178, 20),
    log_b ~ dnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d2_a
)

rethinking::precis(m4.3b)
```

Note the `exp(log_b)` in the definition of `mu`. This is the same model
as `m4.3`. It will make the same predictions. But instead of `β` in the
posterior distribution, you get `log((β)`. It is easy to translate
between the two, because $\beta = exp(log(\beta))$. In code form:
`b <- exp(log_b)`.

#### Tidyverse

Unlike with McElreath's `rethinking::quap()` formula syntax, Kurz is not
aware that we can just specify something like `weight – xbar` in the
`formula` argument in `brms::brm()`. 


However, the alternative is easy:
Just make a new variable in the data that is equivalent to
`weight – mean(weight)`. We'll call it `weight_c`.

```{r}
#| label: create-weight-diff-var-b
#| attr-source: '#lst-create-weight-diff-var-b lst-cap="Create a new variable in the data equivalent to weight - mean(height): tidyverse version"'


d2_b <-
  d2_b %>% 
  mutate(weight_c = weight - mean(weight))
```
Unlike with McElreath’s {**rethinking**} package, the conventional `brms::brm()` syntax doesn’t mirror the statistical notation. But here are the analogues to the exposition at the bottom of page 97:

$$
* $\text{height}_i \sim \operatorname{Normal}(\mu_i, \sigma)$: `family = gaussian`,
* $\mu_i = \alpha + \beta \text{weight}_i$: `height ~ 1 + weight_c`,
* $\alpha \sim \operatorname{Normal}(178, 20)$: `prior(normal(178, 20), class = Intercept`,
* $\beta \sim \operatorname{Log-Normal}(0, 1)$: `prior(lognormal(0, 1), class = b)`, and
* $\sigma \sim \operatorname{Uniform}(0, 50)$: `prior(uniform(0, 50), class = sigma)`.
$$ {#eq-find-post-dist-b}



```{r}
#| label: find-and summarize-post-dist-b
#| attr-source: '#lst-find-and-summarize-post-dist-b lst-cap="Find the posterior distribution of the linear height-weight model: tidyverse version"'

b4.3 <- 
  brms::brm(data = d2_b, 
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(lognormal(0, 1), class = b),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03")

brms:::summary.brmsfit(b4.3)
```

Here are the trace plots.

```{r}
#| label: fig-find-post-dist-b
#| fig-cap: "Find the posterior distribution of the linear height-weight model: tidyverse version"

plot(b4.3, widths = c(1, 2))
```

{**brms**} does not allow users to insert coefficients into functions
like exp() within the conventional `formula` syntax. We can fit a
{**brms**} model like McElreath's `m4.3b` if we adopt what's called the
[non-linear
syntax](https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html).
The non-linear syntax is a lot like the syntax McElreath uses in
{**rethinking**} in that it typically includes both predictor and
variable names in the `formula`. Since this is so early in the book, I
won't go into a full-blown explanation, here. There will be many more
opportunities to practice with the non-linear syntax in the chapters to
come.

```{r}
#| label: find-post-dist2-b
#| attr-source: '#lst-find-post-dist2-b lst-cap="Find the posterior distribution of the linear height-weight model (log version): tidyverse version"'

b4.3b <- 
  brms::brm(data = d2_b, 
      family = gaussian,
      brms::bf(height ~ a + exp(lb) * weight_c,
         a ~ 1,
         lb ~ 1,
         nl = TRUE),
      prior = c(brms::prior(normal(178, 20), class = b, nlpar = a),
                brms::prior(normal(0, 1), class = b, nlpar = lb),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03b")
```

If you execute `summary(b4.3b)`, you\'ll see the intercept and `σ`
summaries for this model are about the same as those for `b4.3`, above.

```{r}
#| label: summarize-post-dist2-b
#| attr-source: '#lst-summarize-post-dist2-b lst-cap="Summarize the posterior distribution of the linear height-weight model (log version): tidyverse version"'

brms:::summary.brmsfit(b4.3b)
```

The difference is for the β parameter, which we called `lb` in the
`b4.3b` model. If we term that parameter from `b4.3` as $\beta^{b4.3}$
and the one from our new model $\beta^{b4.3b}$, it turns out that
$\beta^{b4.3} = exp(\beta^{b4.3b})$.

```{r}
#| label: extract-fixed-effects-b
#| attr-source: '#lst-extract-foxed-effects-b lst-cap="Extract and compare the population-level (fixed) effects from object b4.3 and b4.3b"'

brms::fixef(b4.3)["weight_c", "Estimate"]
brms::fixef(b4.3b)["lb_Intercept", "Estimate"] %>% exp()
```

They\'re the same within simulation variance.

### Interpretating the posterior distribution

#### Original

Statistical models are hard to interpret. There are two broad categories of interpretation:
- reading tables
- plotting simulation

Plotting posterior distributions and posterior predictions is better than attempting to understand a table. 

##### Table of marginal distributions

I have already included the summary (`precis()`) for the `m4.3` model in @lst-find-post-dist-a. But I will repeat it to inspect the marginal posterior distributions of the parameters in detail:

```{r}
#| label: display-precis-m4.3
#| attr-source: '#lst-display-precis-m4.3 lst-cap="Display the marginal posterior distributions of the parameters: rethinking version"'


## R code 4.44 ################
rethinking::precis(m4.3)
```
1. First row: quadratic approximation for $\alpha$
2. Second row: quadratic approximation for $\beta$
3. Third row: quadratic approximation for $\sigma$


Let’s focus on b ($\beta$), because it’s the new parameter. Since ($\beta$) is a slope, the value 0.90 can be read as *a person 1 kg heavier is expected to be 0.90 cm taller*. 89% of the posterior probability ($94.5-5.5$) lies between 0.84 and 0.97. That suggests that ($\beta$) values close to zero or greatly above one are highly incompatible with these data and this model. It is most certainly not evidence that the relationship between weight and height is linear, because the model only considered lines. It just says that, if you are committed to a line, then lines with a slope around 0.9 are plausible ones.

Remember, the numbers in the default precis output aren’t sufficient to describe the quadratic posterior completely. For that, we also require the variance-covariance matrix. 

```{r}
#| label: var-cov-matrix-m4.3
#| attr-source: '#lst-var-cov-matrix-m4.3 lst-cap="Calculate the variance-covariance matrix for model m4.3"'

## R code 4.45 ######################
round(rethinking::vcov(m4.3), 3)
```

```{r}
#| label: marg-post-cov-m4.3
#| attr-source: '#lst-var-cov-matrix-m4.3 lst-cap="Calculate the marginal posteriors and covariance matrix for model m4.3"'

rethinking::pairs(m4.3)
```

##### Plotting posterior inference against the data

It’s almost always much more useful to plot the posterior inference against the data. Not only does plotting help in interpreting the posterior, but it also provides an informal check on model assumptions. 

We’re going to start with a simple version of that task, superimposing just the posterior mean values over the height and weight data. Then we’ll slowly add more and more information to the prediction plots, until we’ve used the entire posterior distribution.

We’ll start with just the raw data and a single line. The code below plots the raw data, computes the posterior mean values for `a` and `b`, then draws the implied line:

```{r}
#| label: fig-raw-data-line-m4.3
#| fig-cap: "Height in centimeters (vertical) plotted against weight in kilograms (horizontal), with the line at the posterior mean plotted in black."

## R code 4.46 ############################################
plot(height ~ weight, data = d2_a, col = rethinking::rangi2)
post_m4.3 <- rethinking::extract.samples(m4.3)
a_map <- mean(post_m4.3$a)
b_map <- mean(post_m4.3$b)
curve(a_map + b_map * (x - xbar), add = TRUE)
```
Each point in this plot is a single individual. The black line is defined by the mean slope `β` and mean intercept `α`. This is not a bad line. It certainly looks highly plausible. But there are an infinite number of other highly plausible lines near it. Let’s draw those too.

##### Adding uncertainty around the mean

Plots of the average line, like in @fig-raw-data-line-m4.3, are useful for getting an impression of the magnitude of the estimated influence of a variable. But they do a poor job of communicating uncertainty. Remember, the posterior distribution considers every possible regression line connecting height to weight. It assigns a relative plausibility to each. This means that each combination of `α` and `β` has a posterior probability. It could be that there are many lines with nearly the same posterior probability as the average line. Or it could be instead that the posterior distribution is rather narrow near the average line.

So how can we get that uncertainty onto the plot? Together, a combination of `α` and `β` define a line. And so we could sample a bunch of lines from the posterior distribution. Then we could display those lines on the plot, to visualize the uncertainty in the regression relationship.

To better appreciate how the posterior distribution contains lines, we work with all of the samples from the model. 

```{r}
#| label: collect-post-samples-a

## R code 4.47 ################
# post_m4.3 <- rethinking::extract.samples(m4.3) # already in previous listing
post_m4.3[1:5, ]
```

Each row is a correlated random sample from the joint posterior of all three parameters, using the covariances provided by `rethinking::vcov(m4.3)` (@lst-var-cov-matrix-m4.3). The paired values of `a` and `b` on each row define a line. The average of very many of these lines is the posterior mean line. But the scatter around that average is meaningful, because it alters our confidence in the relationship between the predictor and the outcome.

So now let’s display a bunch of these lines, so you can see the scatter. This lesson will be easier to appreciate, if we use only some of the data to begin. Then you can see how adding in more data changes the scatter of the lines. So we’ll begin with just the first 10 cases in `d2_a`. The following code extracts the first 10 cases and re-estimates the model:

```{r}
#| label: extract-10-re-est-mod-a

## R code 4.48 ##########################
N10_a <- 10
dN10_a <- d2_a[1:N10_a, ]
mN10_a <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - mean(weight)),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = dN10_a
)

```

Now let’s plot 20 of these lines for the 10 data points, to see what the uncertainty looks like.

```{r}
#| label: fig-plot-lines-10-points-a
#| fig-cap: "Samples from the quadratic approximate posterior distribution for the height/weight model, m4.3. 20 lines sampled from 10 data points of the posterior distribution, showing the uncertainty in the regression relationship: rethinking version"


## R code 4.49 ##############################
# extract 20 samples from the posterior
post_20_m4.3 <- rethinking::extract.samples(mN10_a, n = 20)

# display raw data and sample size
plot(dN10_a$weight, dN10_a$height,
  xlim = range(d2_a$weight), ylim = range(d2_a$height),
  col = rethinking::rangi2, xlab = "weight", ylab = "height"
)
mtext(rethinking:::concat("N = ", N10_a))

# plot the lines, with transparency
for (i in 1:20) {
  curve(post_20_m4.3$a[i] + post_20_m4.3$b[i] * (x - mean(dN10_a$weight)),
    col = rethinking::col.alpha("black", 0.3), add = TRUE
  )
}

```
The result is shown in the upper-left plot in FIGURE 4.7. By plotting multiple regression lines, sampled from the posterior, it is easy to see both the highly confident aspects of the relationship and the less confident aspects. The cloud of regression lines displays greater uncertainty at extreme values for weight.

The other plots in FIGURE 4.7 show the same relationships, but for increasing amounts of data. Just re-use the code from before, but change N <- 10 to some other value. Notice that the cloud of regression lines grows more compact as the sample size increases. This is a result of the model growing more confident about the location of the mean.

Plot 20 lines sampled from 50 data points of the posterior distribution

```{r}
#| label: fig-plot-lines-50-points-a
#| fig-cap: "Samples from the quadratic approximate posterior distribution for the height/weight model, m4.3. 20 lines sampled from 50 data points of the posterior distribution, showing the uncertainty in the regression relationship: rethinking version"

## R code 4.48, 4.49 ######################
N50_a <- 50
dN50_a <- d2_a[1:N50_a, ]
mN50_a <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - mean(weight)),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = dN50_a
)

# extract 20 samples from the posterior
post_50_m4.3 <- rethinking::extract.samples(mN50_a, n = 20)

# display raw data and sample size
plot(dN50_a$weight, dN50_a$height,
  xlim = range(d2_a$weight), ylim = range(d2_a$height),
  col = rethinking::rangi2, xlab = "weight", ylab = "height"
)
mtext(rethinking:::concat("N = ", N50_a))

# plot the lines, with transparency
for (i in 1:20) {
  curve(post_50_m4.3$a[i] + post_50_m4.3$b[i] * (x - mean(dN50_a$weight)),
    col = rethinking::col.alpha("black", 0.3), add = TRUE
  )
}

```

Plot 20 lines sampled from 352 data points of the posterior distribution

```{r}
#| label: fig-plot-lines-all-352-points-a
#| fig-cap: "Samples from the quadratic approximate posterior distribution for the height/weight model, m4.3. 20 lines sampled from all 352 data points of the posterior distribution, showing the uncertainty in the regression relationship."

## R code 4.48, 4.49 ###########################
N352_a <- 352
dN352_a <- d2_a[1:N352_a, ]
mN352_a <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - mean(weight)),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = dN352_a
)

# extract 20 samples from the posterior
post_352_m4.3 <- rethinking::extract.samples(mN352_a, n = 20)

# display raw data and sample size
plot(dN352_a$weight, dN352_a$height,
  xlim = range(d2_a$weight), ylim = range(d2_a$height),
  col = rethinking::rangi2, xlab = "weight", ylab = "height"
)
mtext(rethinking:::concat("N = ", N352_a))

# plot the lines, with transparency
for (i in 1:20) {
  curve(post_352_m4.3$a[i] + post_352_m4.3$b[i] * (x - mean(dN352_a$weight)),
    col = rethinking::col.alpha("black", 0.3), add = TRUE
  )
}

```


#### Tidyverse

Instead of `rethinking::extract.samples()` the {**brms**} packages extract all the posterior draws with `brms::as_draws_df()`. We have already done this with @lst-put-hmc-into-df-b. We just repeat this code here using `dplyr::slice(1:6)` instead of `utils::head()`


```{r}
#| label: put-hmc-into-df2-b
#| attr-source: '#lst-put-hmc-into-df2-b lst-cap="Extract the iteration of the Hamiliton Monte Carlo (HMC) chains into a data frame"'

post_b <- brms::as_draws_df(b4.3)
post_b %>%
  slice(1:6)
```

Here are the four models:

```{r}
#| label: calc-all-four-models-b
#| attr-source: '#lst-calc-all-four-models-b lst-cap="Calculate all four models"'

dN10_b <- 10

b4.3_010 <- 
  brms::brm(data = d2_b %>%
        slice(1:dN10_b),  # note our tricky use of `N` and `slice()`
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(lognormal(0, 1), class = b),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03_010")

dN50_b <- 50

b4.3_050 <- 
  brms::brm(data = d2_b %>%
        slice(1:dN50_b), 
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(lognormal(0, 1), class = b),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03_050")

dN150_b <- 150

b4.3_150 <- 
  brms::brm(data = d2_b %>%
        slice(1:dN150_b), 
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(lognormal(0, 1), class = b),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03_150")

dN352_b <- 352

b4.3_352 <- 
  brms::brm(data = d2_b %>%
        slice(1:dN352_b), 
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(lognormal(0, 1), class = b),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03_352")
```


Here are the trace plots and coefficient summaries from these four models.

```{r}
#| label: trace-plots-cov-sum-b
#| attr-source: '#lst-trace-plots-cov-sum-b lst-cap="Trace plots and coefficient summaries from all four models"'


plot(b4.3_010)
print(b4.3_010)

plot(b4.3_050)
print(b4.3_050)

plot(b4.3_150)
print(b4.3_150)

plot(b4.3_352)
print(b4.3_352)
```


We’ll need to put the chains of each model into data frames.

```{r}
#| label: put-chain-into-model-b
#| #| attr-source: #lst-put-chain-into-model-b lst-cap="Put the chains of each model into data frames"'

post010_b4.3 <- brms::as_draws_df(b4.3_010)
post050_b4.3 <- brms::as_draws_df(b4.3_050)
post150_b4.3 <- brms::as_draws_df(b4.3_150)
post352_b4.3 <- brms::as_draws_df(b4.3_352)
```


Here is the code for the four individual plots:

```{r}
#| label: calc-code-for plots-b
#| attr-source: '#lst-calc-code-for plots-b lst-cap="Prepare data for four individual plots"'

p1 <- 
  ggplot(data =  d2_b[1:10, ], 
         aes(x = weight_c, y = height)) +
  geom_abline(data = post010_b4.3 %>% slice(1:20),
              aes(intercept = b_Intercept, slope = b_weight_c),
              linewidth = 1/3, alpha = .3) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  coord_cartesian(xlim = range(d2_b$weight_c),
                  ylim = range(d2_b$height)) +
  labs(subtitle = "N = 10")

p2 <-
  ggplot(data =  d2_b[1:50, ], 
         aes(x = weight_c, y = height)) +
  geom_abline(data = post050_b4.3 %>% slice(1:20),
              aes(intercept = b_Intercept, slope = b_weight_c),
              linewidth = 1/3, alpha = .3) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  coord_cartesian(xlim = range(d2_b$weight_c),
                  ylim = range(d2_b$height)) +
  labs(subtitle = "N = 50")

p3 <-
  ggplot(data =  d2_b[1:150, ], 
         aes(x = weight_c, y = height)) +
  geom_abline(data = post150_b4.3 %>% slice(1:20),
              aes(intercept = b_Intercept, slope = b_weight_c),
              linewidth = 1/3, alpha = .3) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  coord_cartesian(xlim = range(d2_b$weight_c),
                  ylim = range(d2_b$height)) +
  labs(subtitle = "N = 150")

p4 <- 
  ggplot(data =  d2_b[1:352, ], 
         aes(x = weight_c, y = height)) +
  geom_abline(data = post352_b4.3 %>% slice(1:20),
              aes(intercept = b_Intercept, slope = b_weight_c),
              linewidth = 1/3, alpha = .3) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  coord_cartesian(xlim = range(d2_b$weight_c),
                  ylim = range(d2_b$height)) +
  labs(subtitle = "N = 352")
```


Now we can combine the ggplots with patchwork syntax to make the full version of Figure 4.7.

```{r}
#| label: fig-draw-plots-figure-4.7-b
#| fig-cap: "Samples from the quadratic approximate posterior distribution for the height/weight model, b4.3, with increasing amounts of data. In each plot, 20 lines sampled from the posterior distribution, showing the uncertainty in the regression relationship. Tidyverse version."



library(patchwork)

(p1 + p2 + p3 + p4) &
  scale_x_continuous("weight",
                     breaks = c(-10, 0, 10),
                     labels = labels) &
  theme_classic()
```



##### Plotting regression intervals and contours

The cloud of regression lines in @fig-plot-lines-10-points-a, @fig-plot-lines-50-points-a and @fig-plot-lines-all-352-points-a is an appealing display, because it communicates uncertainty about the relationship in a way that many people find intuitive. But it’s more common, and often much clearer, to see the uncertainty displayed by plotting an interval or contour around the average regression line.

```{r}
#| label: calc-PI-around-regr-line-a
#| attr-source: '#lst-calc-PI-around-regr-line-a lst-cap="Calculate uncertainty around the average regression line"'

## R code 4.50 ##########################

post_m4.3 <- rethinking::extract.samples(m4.3)      # <1>
mu_at_50_a <- post_m4.3$a + post_m4.3$b * (50 - xbar) # <2>
head(mu_at_50_a)                                      # <3>
```

1. Repeating the code for drawing (extracting and collecting) from the fitted model m4.3 (already done in @fig-raw-data-line-m4.3)
2. The code to the right of the `<-` takes its form from the equation for $\mu_{i} = \alpha + \beta(x_{i} - \overline{x})$. The value of $x_{i}$ in this case is 50.
3. The result is a vector of predicted means, one for each random sample from the posterior. Since joint `a` and `b` went into computing each, the variation across those means incorporates the uncertainty in and correlation between both parameters.

It might be helpful at this point to actually plot the density for this vector of means:

```{r}
#| label: fig-density-vector-mean-50-a
#| fig-cap: "The quadratic approximate posterior distribution of the mean height, μ, when weight is 50 kg. This distribution represents the relative plausibility of different values of the mean. Rehtinking version"

## R code 4.51 ##################
rethinking::dens(mu_at_50_a, col = rethinking::rangi2, lwd = 2, xlab = "mu|weight=50")
```

Since the components of `μ` have distributions, so too does `μ`. And since the distributions of `α` and `β` are Gaussian, so too is the distribution of `μ` (adding Gaussian distributions always produces a Gaussian distribution).

Since the posterior for `μ` is a distribution, you can find intervals for it, just like for any posterior distribution. To find the 89% compatibility interval of `μ` at 50 kg, just use the `PI()` command as usual:

```{r}
#| label: PI-mu-at-50-a
#| attr-source: '#lst-PI-mu-at-50-a lst-cap="89% compatibility interval of μ at 50 kg"'

## R code 4.52 ##############
rethinking::PI(mu_at_50_a, prob = 0.89)

```

What these numbers mean is that the central 89% of the ways for the model to produce the data place the average height between about 159 cm and 160 cm (conditional on the model and data), assuming the weight is 50 kg.

That’s good so far, but we need to repeat the above calculation for every weight value on the horizontal axis, not just when it is 50 kg. We want to draw 89% intervals around the average slope in @fig-raw-data-line-m4.3.

This is made simple by strategic use of the `rethinking::`link()` function, a part of the {**rethinking**} package. What `rethinking::link()` will do is take your `rethinking::quap()` approximation, sample from the posterior distribution, and then compute `μ` for each case in the data and sample from the posterior distribution. Here’s what it looks like for the data you used to fit the model:

```{r}
#| label: calc-mu-with-link-a
#| attr-source: '#lst-calc-mu-with-link-a lst-cap="Calculate μ for each case in the data and sample from the posterior distribution: Rethinking version"'

## R code 4.53 ##############
mu_a <- rethinking::link(m4.3)
str(mu_a)
```


You end up with a big matrix of values of `μ`. Each row is a sample from the posterior distribution. The default is 1000 samples, but you can use as many or as few as you like. Each column is a case (row) in the data. There are 352 rows in `d2_a`, corresponding to 352 individuals. So there are 352 columns in the matrix mu above.

The function `rethinking::link()` provides a posterior distribution of `μ` for each case we feed it. So above we have a distribution of `μ` for each individual in the original data. We actually want something slightly different: a distribution of `μ` for each unique weight value on the horizontal axis. It’s only slightly harder to compute that, by just passing `rethinking::link()` some new data:

```{r}
#| label: calc-dist-mu-unique-with-link-a
#| attr-source: '#lst-calc-dist-mu-unique-with-link-a lst-cap="Calculate a distribution of μ for each unique weight value on the horizontal axis: rethinking version"'

## R code 4.54 ###############################
# define sequence of weights to compute predictions for
# these values will be on the horizontal axis
weight.seq <- seq(from = 25, to = 70, by = 1)

# use link to compute mu
# for each sample from posterior
# and for each weight in weight.seq
mu2_a <- rethinking::link(m4.3, data = data.frame(weight = weight.seq))
str(mu2_a)
```

And now there are only 46 columns in mu, because we fed it 46 different values for weight. To visualize what you’ve got here, let’s plot the distribution of `μ` values at each height.

```{r}
#| label: fig-dist-mu-height-100-a
#| fig-cap: "The first 100 values in the distribution of μ at each weight value. Rethinking version"

## R code 4.55
# use type="n" to hide raw data
base::plot(height ~ weight, d2_a, type = "n")

# loop over samples and plot each mu value
for (i in 1:100) {
  graphics::points(weight.seq, mu2_a[i, ], pch = 16, col = rethinking::col.alpha(rethinking::rangi2, 0.1))
}

```
At each weight value in `weight.seq`, a pile of computed `μ` values are shown. Each of these piles is a Gaussian distribution, like that in @fig-density-vector-mean-50-a. You can see now that the amount of uncertainty in `μ` depends upon the value of weight. And this is the same fact you saw in @fig-plot-lines-10-points-a, @fig-plot-lines-50-points-a and @fig-plot-lines-all-352-points-a.

The final step is to summarize the distribution for each weight value. We’ll use `base::apply()`, which applies a function of your choice to a matrix.

```{r}
#| label: sum-dist-weight-a
#| attr-source: '#lst-sum-dist-weight-a lst-cap="Summary of the distribution for each weight value. Rethinking version"'

## R code 4.56 #####################
# summarize the distribution of mu
mu2.mean <- apply(mu2_a, 2, mean)                      # <1>
mu2.PI <- apply(mu2_a, 2, rethinking::PI, prob = 0.89) # <2>
mu2.mean                                               # <3>
mu2.PI                                                 # <4>
```

1. Read `apply(mu2,2,mean)` as "compute the mean of each column (dimension '2') of the matrix mu". Now `mu2.mean` contains the average `μ` at each weight value.
2. `mu2.PI` contains 89% lower and upper bounds for each weight value.
3. `mu2.mean` and `mu2.PI` are just different kinds of summaries of the distributions in `mu2_a`, with each column being for a different weight value. These summaries are only summaries. The "estimate" is the entire distribution.


```{r}
#| label: fig-summaries-on-data-top-a
#| fig-cap: "Plot of the summaries on top of the !Kung height data again, now with 89% compatibility interval of the mean indicated by the shaded region. Compare this region to the distributions of blue points in @fig-dist-mu-height-100-a."

## R code 4.57 ###########################
# plot raw data
# fading out points to make line and interval more visible
plot(height ~ weight, data = d2_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))

# plot the MAP line, aka the mean mu for each weight
graphics::lines(weight.seq, mu2.mean)

# plot a shaded region for 89% PI
rethinking::shade(mu2.PI, weight.seq)

```
::: callout-caution
There is very little uncertainty about the average height as a function of average weight. But keep in mind that these inferences are always conditional on the model. Think of the regression line in @fig-summaries-on-data-top-a as saying: *Conditional on the assumption that height and weight are related by a straight line, then this is the most plausible line, and these are its plausible bounds.*

:::


Using this approach, you can derive and plot posterior prediction means and intervals for quite complicated models, for any data you choose. As long as you know the structure of the model —-- how parameters relate to the data —-- you can use samples from the posterior to describe any aspect of the model’s behavior.

Summarizing the three steps for generating predictions and intervals from the posterior of a fit model:

1. Use `rethinking::link()` to generate distributions of posterior values for `μ`. The default behavior of `rethinking::link()` is to use the original data, so you have to pass it a list of new horizontal axis values you want to plot posterior predictions across.
2. Use summary functions like `mean` or `PI` to find averages and lower and upper bounds of `μ` for each value of the predictor variable.
3. Finally, use plotting functions like `graphics::lines()` and `rethinking::shade()` to draw the lines and intervals. Or you might plot the distributions of the predictions, or do further numerical calculations with them. It’s really up to you.

::: callout-note
You could write a function that accomplish the same thing as `rethinking::link()`:

```{r}
#| label: writing-link-function-a
#| attr-source: '#lst-writing-link-function-a lst-cap="Code to perform the same steps as the rethinking::link() function"'
#| 
## R code 4.58 ################################
post_m4.3 <- rethinking::extract.samples(m4.3)
mu.link <- function(weight) post_m4.3$a + post_m4.3$b * (weight - xbar)
weight.seq <- seq(from = 25, to = 70, by = 1)
mu3_a <- sapply(weight.seq, mu.link)
mu3.mean <- apply(mu3_a, 2, mean)
mu3.CI <- apply(mu3_a, 2, rethinking::PI, prob = 0.89)
mu3.mean
mu3.CI
```

And the values in `mu3.mean` and `mu3.CI` should be very similar (allowing for simulation variance) to what you got the automated way, using `rethinking::link()` in @lst-sum-dist-weight-a.

Knowing this manual method is useful both for (1) understanding and (2) sheer power. Whatever the model you find yourself with, this approach can be used to generate posterior predictions for any component of it. Automated tools like link save effort, but they are never as flexible as the code you can write yourself.
:::

#### Tidyverse

Since we used `weight_c` to fit our model, we might first want to understand what exactly the mean value is for weight.

```{r}
#| label: calc-mean-weight-b
#| attr-source: '#lst-calc-mean-weight-b lst-cap="Calculate mean of weight"'

mean(d2_b$weight)
```

Just a hair under 45. If we're interested in $\mu$ at `weight` = 50, that implies we're also interested in $\mu$ at `weight_c` + 5.01. Within the context of our model, we compute this with $\alpha + \beta \cdot 5.01$. Here's what that looks like with `post_b`.

```{r}
#| label: calc-mean-weight-at-50-b
#| attr-source: '#lst-calc-mean-weight-at-50-b lst-cap="Calculate the mean at weight 50 kg"'

mu_at_50_b <- 
  post_b %>% 
  transmute(mu_at_50_b = b_Intercept + b_weight_c * 5.01)
 
head(mu_at_50_b)
```

And here is a version McElreath’s Figure 4.8 density plot.

```{r}
#| label: fig-density-vector-mean-50-b
#| fig-cap: "The quadratic approximate posterior distribution of the mean height, μ, when weight is 50 kg. This distribution represents the relative plausibility of different values of the mean. Tidyverse version"

mu_at_50_b %>%
  ggplot(aes(x = mu_at_50_b)) +
  geom_density(linewidth = 0, fill = "deepskyblue") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(mu["height | weight = 50"])) +
  theme_classic()
```

We’ll use `tidybayes::mean_hdi()` to get both 89% and 95% HPDIs along with the mean.

```{r}
#| label: calc-mean-and-HPDI-b
#| attr-source: '#lst-calc-mean-and-HPDI-b lst-cap="Calculate both 89% and 95% Highest Priority Intensity Intervals (HPDIs) along with the mean."'

tidybayes::mean_hdi(mu_at_50_b[, 1], .width = c(.89, .95))
```
If you wanted to express those sweet 95% HPDIs on your density plot, you might use `tidybayes::stat_halfeye()`. Since `tidybayes::stat_halfeye()` also returns a point estimate, we’ll throw in the mode.

```{r}
#| label: fig-half-eye-b
#| fig-cap: "Plot of half-eye (density + interval) geometry"
mu_at_50_b %>%
  ggplot(aes(x = mu_at_50_b, y = 0)) +
  tidybayes::stat_halfeye(point_interval = tidybayes::mode_hdi, .width = .95,
               fill = "deepskyblue") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(mu["height | weight = 50"])) +
  theme_classic()
```
With {**brms**}, you would use fitted() to do what McElreath accomplished with `rethinking::link()`.

::: {.callout-caution}
Kurz applies the function `fitted()` in the code, but in the text he uses twice `brms::fitted()` which doesn't exist. I used both `brms:::fitted.brmsfit()` and `stats::fitted()` to get the same results. 

The object `b4.3` is of class `brmsfit` but in the help file of `stats::fitted()` you can read: "`fitted` is a generic function which extracts fitted values from objects returned by modeling functions.  **All object classes which are returned by model fitting functions should provide a `fitted` method.** (emphasis is mine)

My interpretation therefore is that `stats::fitted()` is using `brms:::fitted.brmsfit()`. Thts why the results are identical.

:::

```{r}
#| label: calc-mu-with-fitted-b
#| attr-source: '#lst-calc-mu-with-fitted-b lst-cap="Calculate μ for each case in the data and sample from the posterior distribution: Tidyverse version"'

mu2_b <- brms:::fitted.brmsfit(b4.3, summary = F)
mu2.1_b <- stats::fitted(b4.3, summary = F)
str(mu2_b)
str(mu2.1_b)
```

When you specify `summary = F`, `brms:::fitted.brmsfit()` returns a matrix of values with as many rows as there were post-warmup draws across your Hamilton Monte Carlo (HMC) chains and as many columns as there were cases in your analysis. Because we had 4,000 post-warmup draws and $n=352$, `brms:::fitted.brmsfit()` returned a matrix of 4,000 rows and 352 vectors. If you omitted the `summary = F` argument, the default is TRUE and `brms:::fitted.brmsfit()` will return summary information instead.

Much like `rethinking::link()`, `brms:::fitted.brmsfit()` can accommodate custom predictor values with its `newdata` argument.

```{r}
#| label: calc-dist-mu-unique-with-fitted.brmsfit-b
#| attr-source: '#lst-calc-dist-mu-unique-with-fitted.brmsfit-b lst-cap="Calculate a distribution of μ for each unique weight value on the horizontal axis: tidyverse version"'

weight_seq <- 
  tibble(weight = 25:70) %>% 
  mutate(weight_c = weight - mean(d2_b$weight))

mu3_b <-
  brms:::fitted.brmsfit(b4.3,
         summary = F,
         newdata = weight_seq) %>%
  data.frame() %>%
  # here we name the columns after the `weight` values from which they were computed
  set_names(25:70) %>% 
  mutate(iter = 1:n())

head(mu3_b)
```

Anticipating {**ggplot2**}, we went ahead and converted the output to a data frame. But we might do a little more data processing with the aid of `tidyr::pivot_longer()`, which will convert the data from the wide format to the long format. 

::: callout-tip
###### Literature references

If you are new to the distinction between wide and long data, you can learn more from the [Pivot data from wide to long](https://tidyr.tidyverse.org/reference/pivot_longer.html) vignette from the tidyverse team (2020); Simon Ejdemyr’s blog post, [Wide & long data](https://sejdemyr.github.io/r-tutorials/basics/wide-and-long/); or Karen Grace-Martin’s blog post, [The wide and long data format for repeated measures data](https://www.theanalysisfactor.com/wide-and-long-data/).
:::

```{r}
#| label: convert-wide-to-long-b
#| attr-source: '#lst-convert-wide-to-long-b lst-cap="Data processing: Convert data from wide to long format: tidyverse version"'

mu4_b <- 
  mu3_b %>%
  pivot_longer(-iter,
               names_to = "weight",
               values_to = "height") %>% 
  # we might reformat `weight` to numerals
  mutate(weight = as.numeric(weight))

head(mu4_b)
```

Now our data processing is done, here we reproduce McElreath’s Figure 4.9.a.

```{r}
#| label: fig-dist-mu-height-100-b
#| fig-cap: "The first 100 values in the distribution of μ at each weight value. Tidyverse version"
d2_b %>%
  ggplot(aes(x = weight, y = height)) +
  geom_point(data = mu4_b %>% filter(iter < 101), 
             color = "navyblue", alpha = .05) +
  coord_cartesian(xlim = c(30, 65)) +
  theme(panel.grid = element_blank())
```
With `brms:::fitted.brmsfit()`, it’s quite easy to plot a regression line and its intervals. Just omit the `summary = T` argument.

```{r}
#| label: sum-dist-weight-b
#| attr-source: '#lst-sum-dist-weight-b lst-cap="Summary of the distribution for each weight value. Tidyverse version"'

mu_summary <-
  brms:::fitted.brmsfit(b4.3, 
         newdata = weight_seq) %>%
  data.frame() %>%
  bind_cols(weight_seq)

head(mu_summary)
```

Here it is, our analogue to Figure 4.9.b.

```{r}
#| label: fig-summaries-on-data-top-b
#| fig-cap: "Plot of the summaries on top of the !Kung height data again, now with 89% compatibility interval of the mean indicated by the shaded region. Compare this region to the distributions of blue points in @fig-dist-mu-height-100-b."


d2_b %>%
  ggplot(aes(x = weight, y = height)) +
  geom_smooth(data = mu_summary,
              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              stat = "identity",
              fill = "grey70", color = "black", alpha = 1, linewidth = 1/2) +
  geom_point(color = "navyblue", shape = 1, size = 1.5, alpha = 2/3) +
  coord_cartesian(xlim = range(d2_b$weight)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())
```

If you wanted to use intervals other than the default 95% ones, you’d include the probs argument like this: `stats::fitted(b4.3, newdata = weight.seq, probs = c(.25, .75))`. The resulting third and fourth vectors from the `fitted()` object would be named `Q25` and `Q75` instead of the default `Q2.5` and `Q97.5`. The [Q prefix](https://github.com/paul-buerkner/brms/issues/425) stands for quantile.

Similar to `rethinking::link()`, `stats::fitted()` uses the formula from your model to compute the model expectations for a given set of predictor values. I use it a lot in this project. If you follow along, you’ll get a good handle on it. But to dive deeper, you can [go here for the documentation](https://rdrr.io/cran/brms/man/fitted.brmsfit.html). Though we won’t be using it in this project, {**brms**} users might want to know that fitted() is also an alias for the `brms::posterior_epred()` function, about which you might [learn more here](https://rdrr.io/cran/brms/man/posterior_epred.brmsfit.html). Users can always learn more about them and other functions in the [{**brms**} reference manual](https://cran.r-project.org/web/packages/brms/brms.pdf).

## I STOPPED THIS CHAPTER HERE! (2023-08-11)

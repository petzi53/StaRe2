---
format: html
execute: 
  cache: true
---


# Geocentric Models {#sec-chap04}

## File setup {.unnumbered}

```{r}
#| label: setup

library(tidyverse)
library(patchwork)
```

## `ORIGINAL`

### Why normal distributions? {#sec-why-normal-dist-a}

Why are there so many distribution approximately [normal]{.smallcaps},
resulting in a Gaussian curve? Because there will be more combinations
of outcomes that sum up to a "central" value, rather than to some
extreme value.

****
:::: {#prp-why-normal}
Why are normal distribution normal?

::: callout-important
Any process that adds together random values from the same distribution
converges to a normal.
:::

::::
***


#### Normal by addition

Whatever the average value of the source distribution, each sample from
it can be thought of as a fluctuation from that average value. When we
begin to add these fluctuations together, they also begin to cancel one
another out. A large positive fluctuation will cancel a large negative
one. The more terms in the sum, the more chances for each fluctuation to
be canceled by another, or by a series of smaller ones in the opposite
direction. So eventually the most likely sum, in the sense that there
are the most ways to realize it, will be a sum in which every
fluctuation is canceled by another, a sum of zero (relative to the
mean).

It doesn't matter what shape the underlying distribution possesses. It
could be uniform, like in our example above, or it could be (nearly)
anything else. Depending upon the underlying distribution, the
convergence might be slow, but it will be inevitable.

See the excellent article [Why is normal distribution so
ubiquitous?](https://ekamperi.github.io/mathematics/2021/01/29/why-is-normal-distribution-so-ubiquitous.html)
which also explains the example of random walks from SR2. See also the
scientific paper [Why are normal distribution
normal?](https://www.journals.uchicago.edu/doi/pdf/10.1093/bjps/axs046)
of the The British Journal for the Philosophy of Science.

#### Normal by multiplication

This is not only valid for addition but also for multiplication of small
values: Multiplying small numbers is approximately the same as addition.

#### Normal by log-multipliation

But even the multiplication of large values tend to produce Gaussian
distributions on the log scale.

#### Using Gaussian distributions

The justifications for using the Gaussian distribution fall into two
broad categories:

1.  **Ontological justification**: The world is full of Gaussian
    distributions, approximately. We're never going to experience a
    perfect Gaussian distribution. But it is a widespread pattern,
    appearing again and again at different scales and in different
    domains. Measurement errors, variations in growth, and the
    velocities of molecules all tend towards Gaussian distributions.

There are many other patterns in nature, so make no mistake in assuming
that the Gaussian pattern is universal. In later chapters, we'll see how
other useful and common patterns, like the exponential and gamma and
Poisson, also arise from natural processes. The Gaussian is a member of
a family of fundamental natural distributions known as the **Exponential
family**. All of the members of this family are important for working
science, because they populate our world.

2.  **Epistemological justification**: The Gaussian represents a
    particular state of ignorance. When all we know or are willing to
    say about a distribution of measures (measures are continuous values
    on the real number line) is their mean and variance, then the
    Gaussian distribution arises as the most consistent with our
    assumptions. It is the least surprising and least informative
    assumption to make. --- If you don't think the distribution should
    be Gaussian, then that implies that you know something else that you
    should tell your golem about, something that would improve
    inference.

::: callout-caution
Although the Gaussian distribution is common in nature and has some nice
properties, there are some risks in using it as a default data model.
The Gaussian distribution has some very thin tails---there is very
little probability in them. Instead most of the mass in the Gaussian
lies within one standard deviation of the mean. Many natural (and
unnatural) processes have much heavier tails.
:::

The Gaussian is a continuous distribution, unlike the discrete
distributions of earlier chapters. Probability distributions with only
discrete outcomes, like the binomial, are called *probability mass*
functions and denoted `Pr`. Continuous ones like the Gaussian are called
*probability density* functions, denoted with *`p`* or just plain old
*`f`*, depending upon author and tradition. For mathematical reasons,
probability densities can be greater than 1. Try `dnorm(0,0,0.1)`", for
example, which is the way to make R calculate *`p`*`(0|0, 0.1)`. The
answer, about 4, is no mistake. Probability *density* is the rate of
change in cumulative probability. So where cumulative probability is
increasing rapidly, density can easily exceed 1. But if we calculate the
area under the density function, it will never exceed 1. Such areas are
also called *probability mass*.

### Model describing language

1.  First, we recognize a set of variables to work with. Some of these
    variables are observable. We call these *data.* Others are
    unobservable things like rates and averages. We call these
    *parameters*.
2.  We define each variable either in terms of the other variables or in
    terms of a *probability distribution*.
3.  The combination of variables and their probability distributions
    defines a *joint generative model* that can be used both to simulate
    hypothetical observations as well as analyze real ones.

This outline applies to models in every field, from astronomy to art
history. The biggest difficulty usually lies in the subject
matter---which variables matter and how does theory tell us to connect
them?---not in the mathematics.

The mathy way to summarize models will be something like: (taken from
Kurz's version as it defines the general approach more clearly.)

------------------------------------------------------------------------

::: {#def-summarize-a-model}
How to summarize a model mathematically?

$$
\begin{align*}
\text{criterion}_i & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i  & = \beta \times \text{predictor}_i \\
\beta &  \sim \operatorname{Normal}(0, 10) \\
\sigma & \sim \operatorname{Exponential}(1) \\
x_i   &  \sim \operatorname{Normal}(0, 1).
\end{align*}
$$ {#eq-summarize-a-model}
:::

------------------------------------------------------------------------

::: callout-tip
The ampersand sign `&` in the code of Kurz' version is used for
horizontal alignment of different parts for case statements. It wouldn't
be necessary in @eq-summarize-a-model because there is no conditional
statement.
:::

`r glossary("Statistical Model", "Models")` are mappings of one set of
variables through a probability distribution onto another set of
variables. Fundamentally, these models define the ways values of some
variables can arise, given values of other variables.

#### Re-describing the globe tossing model

Recall the proportion of the water problem from previous chapters. The
model in that case was always:

------------------------------------------------------------------------

::: {#def-glob-tossing-model}
Describe the globe tossing model from @sec-chap03

$$
\begin{align*}
W \sim \operatorname{Binomial}(N, p) \space \space (1)\\
p \sim \operatorname{Uniform}(0, 1)  \space \space (2)
\end{align*}
$$ {#eq-globe-tossing-model}

-   `W`: observed count of water
-   `N`: total number of tosses
-   `p`: proportion of water on the globe

Read the above statement as:

1.  **First line**: The count W is distributed binomially with sample
    size `N` and probability `p`.
2.  **Second line**: The prior for `p` is assumed to be uniform between
    zero and one.
:::

------------------------------------------------------------------------

::: callout-important
The first line in these kind of models always defines the likelihood
function used in `r glossary("Bayes’ theorem")`. The other lines define priors.
:::

Both of the lines in the model of @eq-globe-tossing-model are
`r glossary("stochastic")`, as indicated by the `~` symbol. A stochastic
relationship is just a mapping of a variable or parameter onto a
distribution. It is stochastic because no single instance of the
variable on the left is known with certainty. Instead, the mapping is
probabilistic: Some values are more plausible than others, but very many
different values are plausible under any model. Later, we'll have models
with deterministic definitions in them.

##### From model definition to Bayes’ theorem

To relate the mathematical format of @eq-globe-tossing-model to `r glossary("Bayes’ theorem")`, you could use the model definition to define the posterior
distribution:

------------------------------------------------------------------------

::: {#def-from-model-to-bayes-theorem}
From model definition to Bayes’ theorem

$$
Pr(p|w,n) = \frac{\operatorname{Binomial(w|n,p)}\operatorname{Uniform(p|0,1)}}{\int\operatorname{Binomial(w|n,p)}\operatorname{Uniform(p|0,1)}dp}
$$ {#eq-from-model-to-bayes-theorem}
:::

------------------------------------------------------------------------

That monstrous denominator is just the average likelihood again. It
standardizes the posterior to sum to 1. The action is in the numerator,
where the posterior probability of any particular value of `p` is seen
again to be proportional to the product of the likelihood and prior. In
R code form, this is the same grid approximation calculation you've been
using all along.

We will write it in a form that is compatible and therefore better
recognizable with the expression in @eq-from-model-to-bayes-theorem:

```{r}
#| label: model-bayes-theorem-a
#| attr-source: '#lst-model-bayes-theorem-a lst-cap="Calculate the posterior distribution in a way that is regognizable with the Bayes theorem"'

## R code 4.6 ##############
w <- 6
n <- 9
p_grid_a <- seq(from = 0, to = 1, length.out = 100)
posterior_num <- dbinom(w, n, p_grid_a) * dunif(p_grid_a, 0, 1)
posterior_a <- posterior_num / sum(posterior_num)

```

Compare to the calculations in earlier chapters, for example with
@lst-grid-approx-a.

### Gaussian model of height {#sec-gaussian-model-of-height-a}

In this section we want a single measurement variable to model as a Gaussian distribution. It is a preparation for the linear regression model in @sec-linear-prediction-a  where we will construct and add a predictor variable to the model. 

For the moment, we want just a single measurement variable to model as a Gaussian distribution. There will be two parameters describing the distribution’s shape, the `r glossary("arithmetic mean", "mean")` `μ` and the `r glossary("standard deviation")` `σ`. `r glossary("Bayesian updating")` will allow us to consider every possible combination of values for `μ` and `σ` and to score each combination by its relative plausibility, in light of the data. These relative plausibilities are the `r glossary("posterior probability", "posterior probabilities")` of each combination of values `μ`, `σ`.

There are an infinite number of possible Gaussian distributions. Some
have small means. Others have large means. Some are wide, with a large
`σ`. Others are narrow. We want our Bayesian machine to consider every
possible distribution, each defined by a combination of `μ` and `σ`, and
rank them by posterior plausibility. Posterior plausibility provides a
measure of the logical compatibility of each possible distribution with
the data and model.

Keep in mind that the “estimate” here will be the entire posterior distribution, not any point within it. And as a result, the posterior distribution will be a distribution of Gaussian distributions. Yes, a distribution of distributions. 

#### The data

The data contained in `data(Howell1)` are partial census data for the
Dobe area !Kung San, compiled from interviews conducted by Nancy Howell
in the late 1960s. Much more raw data is available for download from
https://tspace.library.utoronto.ca/handle/1807/10395.

For the non-anthropologists reading along, the !Kung San are the most
famous foraging population of the twentieth century, largely because of
detailed quantitative studies by people like Howell.

::: callout-caution
Loading data from a package with `data()` is only possible if you have
already loaded the package. In our example:

```{r}
#| label: loading-data-from-package1_a
#| eval: false


## R code 4.7 #######################
library(rethinking)
data(Howell1)
d_a <- Howell1
```

Because of many function name conflicts with {**brms**} I do not want to
load {**rethinking**} and will call the function of these conflicted
packages with `<package name>::<function name>()` Therefore I have to
use another, not so usual loading strategy of the data set:

```{r}
#| label: loading-data-from-package2_a
#| attr-source: '#lst-loading-data-from-package2_a lst-cap="Load data `Howell1` from the {**rethinking**} package without loading the package and assign the data to an object. Rethinking"'

data(package = "rethinking", list = "Howell1")
d_a <- Howell1
```

The advantage of this strategy is that I have not always to detach the
{**rethinking**} package and to make sure {**rethinking**} is detached
before using {**brms**} as it is necessary in the Kurz's {**tidyverse**}
/ {**brms**} version.
:::

##### Show the data

```{r}
#| label: show-howell-data-a
#| attr-source: '#lst-show-howell-data-a lst-cap="Show and inspect the data: rethinking"'

## R code 4.8 ####################
str(d_a)

## R code 4.9 ###################
rethinking::precis(d_a)
```

This data frame contains four columns. Each column has 544 entries, so
there are 544 individuals in these data. Each individual has a recorded
height (centimeters), weight (kilograms), age (years), and "maleness" (0
indicating female and 1 indicating male).

##### Select the height data of adults

We're going to work with just the height column, for the moment. All we
want for now are heights of adults in the sample. The reason to filter
out non-adults for now is that height is strongly correlated with age,
before adulthood.

```{r}
#| label: select-height-adults-a
#| attr-source: '#lst-select-height-adults-a lst-cap="Select the height data of adults (individuals older or equal than 18 years): base R version"'

## R code 4.10 ###################
head(d_a$height)
 
## R code 4.11 ###################
d2_a <- d_a[d_a$age >= 18, ]

```

We'll be working with the data frame d2 now. It should have 352 rows
(individuals) in it. We will check this with `nrow(d2_a)` =
`r nrow(d2_a)`.

#### The model

Our goal is to model the data in `d2_a` using a Gaussian distribution.

##### Plot the distribution of heights

```{r}
#| label: fig-dist-heights-a
#| fig-cap: "The distribution of the heights data, overlaid by an ideal Gaussian distribution: rethinking version"
#| attr-source: '#lst-fig-dist-heights-a lst-cap="Plot the distribution of the heights of adults, overlaid by an ideal Gaussian distribution: rethinking version"'

rethinking::dens(d2_a$height, norm.comp = TRUE)
```

With the option `norm.comp = TRUE` I have overlaid a Gaussian
distribution to see the differences to the actual data. There are some
differences locally, especially on the peak of the distribution. But the
tails looks nice and we can say that the overall impression of the curve
is Gaussian.

::: callout-caution
###### Decisions how to model the data

Gawking at the raw data, to try to decide how to model them, is usually
not a good idea. The data could be, for example, a mixture of different
Gaussian distributions. Furthermore, the empirical distribution need not
be actually Gaussian in order to justify using a Gaussian probability
distribution.
:::

Define the heights as normally distributed with a mean `μ` and standard
deviation `σ`

------------------------------------------------------------------------

::: {#def-height-normal-dist}
Heights normally distributed

$$
h_{i} \sim \operatorname{Normal}(σ, μ) 
$$ {#eq-height-normal-dist}
:::

------------------------------------------------------------------------

The symbol `h` refers to the list of heights, and the subscript `i`
means each individual element of this list. It is conventional to use
`i` because it stands for index. The index `i` takes on row numbers, and
so in this example can take any value from 1 to 352 (the number of
heights in `d2_a$height`). As such, the model above is saying that all
the golem knows about each height measurement is defined by the same
normal distribution, with mean `μ` and standard deviation `σ`.

The short model in @def-height-normal-dist assumes that the values
$h_{i}$ are *independent and identically distributed*, abbreviated
`i.i.d.`, `iid`, or `IID`.

To complete the model, we're going to need some priors. The parameters
to be estimated are both `μ` and `σ`, so we need a prior `Pr(μ, σ)`, the
joint prior probability for all parameters. In most cases, priors are
specified independently for each parameter, which amounts to assuming
$Pr(μ,σ) = Pr(μ)Pr(σ)$.

------------------------------------------------------------------------

::: {#def-height-linear-model-m4.1}
###### Linear heights model

$$
\begin{align*}
h_{i} \sim \operatorname{Normal}(μ, σ) \space \space (1) \\ 
μ \sim \operatorname{Normal}(178, 20)  \space \space (2) \\ 
μ \sim \operatorname{Uniform}(0, 50)   \space \space (3)      
\end{align*}
$$ {#eq-height-linear-model-m4.1}

1.  First line represents the likelihood.
2.  Second line is the chosen `μ`(mu, mean) prior.
3.  Third line is the chosen `σ` (sigma, standard deviation) prior.
:::

------------------------------------------------------------------------

Let's think about the chosen value for the priors more in detail:

The prior for `μ` is a broad Gaussian prior, centered on 178 cm, with
95% of probability between 178 ± 40 cm.

Why 178 cm? Your author is 178 cm tall. And the range from 138 cm to 218
cm encompasses a huge range of plausible mean heights for human
populations. So domain-specific information has gone into this prior.
Everyone knows something about human height and can set a reasonable and
vague prior of this kind. But in many regression problems, as you'll see
later, using prior information is more subtle, because parameters don't
always have such clear physical meaning.

Whatever the prior, it's a very good idea to plot your priors, so you
have a sense of the assumption they build into the model.

##### Plot the mean prior (mu)

```{r}
#| label: fig-mean-prior-a
#| fig-cap: "Plot of the chosen mean prior: base R version"
#| attr-source: '#lst-fig-mean-prior-a lst-cap="Plot the chosen mean prior: base R version"'

## R code 4.12 ###############################
curve(dnorm(x, 178, 20), from = 100, to = 250)
```

You can see that the golem is assuming that the average height (not each
individual height) is almost certainly between 140 cm and 220 cm. So
this prior carries a little information, but not a lot.

##### Plot the prior of the standard deviation (sigma)

A standard deviation like `σ` must be positive, so bounding it at zero
makes sense. How should we pick the upper bound? In this case, a
standard deviation of 50 cm would imply that 95% of individual heights
lie within 100 cm of the average height. That's a very large range.

```{r}
#| label: fig-sd-prior-a
#| fig-cap: "Plot the chosen prior for the standard deviation: base R version"
#| attr-source: '#lst-fig-sd-prior-a lst-cap="Plot chosen prior for the standard deviation: base R version"'

## R code 4.13 ###########################
curve(dunif(x, 0, 50), from = -10, to = 60)
```

##### Prior predictive simulation

> Once you've chosen priors for `h`, `μ`, and `σ`, these imply a joint
> prior distribution of individual heights. By simulating from this
> distribution, you can see what your choices imply about observable
> height. This helps you diagnose bad choices.

Okay, so how to do this? You can quickly simulate heights by sampling
from the prior, like you sampled from the posterior back in
@sec-chap03. Remember, every posterior is also
potentially a prior for a subsequent analysis, so you can process priors
just like posteriors.

```{r}
#| label: fig-prior-predictive-sim-a
#| fig-cap: "Simulate heights by sampling from the prior: rethinking version"
#| attr-source: '#lst-fig-prior-predictive-sim-a lst-cap="Heights by sampling from the prior: rethinking version"'

set.seed(4) # to make example reproducible
## R code 4.14 #######################################
sample_mu_a <- rnorm(1e4, 178, 20)
sample_sigma_a <- runif(1e4, 0, 50)
prior_h_a <- rnorm(1e4, sample_mu_a, sample_sigma_a)
rethinking::dens(prior_h_a, norm.comp = TRUE)
```

> It displays a vaguely bell-shaped density with thick tails. It is the
> expected distribution of heights, averaged over the prior. Notice that
> the prior probability distribution of height is not itself Gaussian.
> This is okay. The distribution you see is not an empirical
> expectation, but rather the distribution of relative plausibilities of
> different heights, before seeing the data.

This comment is strange for me as in my point of view the distribution
*is* Gaussian. It is true that the tails are (a little bit?) thicker
than in the standard Gaussian distribution. But in my view
@fig-prior-predictive-sim-a is more Gaussian than @fig-dist-heights-a.
OK, in @fig-dist-heights-a we have just `r nrow(d2_a)` data and in
@fig-prior-predictive-sim-a we sampled 10,000 times. But this is not a
counter argument for @fig-prior-predictive-sim-a not being a a bell
shaped distribution.

##### Simulate heights from priors with large sd

`r glossary("Prior predictive simulation")` is very useful for assigning sensible
priors, because it can be quite hard to anticipate how priors influence
the observable variables. As an example, consider a much flatter and
less informative prior for `μ`, like $μ \sim Normal(178, 100)$. Priors
with such large standard deviations are quite common in Bayesian models,
but they are hardly ever sensible.

```{r}
#| label: fig-prior-predictive-sim2-a
#| fig-cap: "Simulate heights from priors with a large standard deviation: rethinking version"
#| attr-source: '#lst-fig-prior-predictive-sim2-a lst-cap="Simulate heights from priors with a large standard deviation: rethinking version"'

set.seed(4) # to make example reproducible
## R code 4.15 ############################
sample_mu2_a <- rnorm(1e4, 178, 100)
prior_h2_a <- rnorm(1e4, sample_mu2_a, sample_sigma_a)
rethinking::dens(prior_h2_a)
```

The results of @fig-prior-predictive-sim2-a contradicts our scientific
knowledge --- but also our common sense --- about possible height values
of humans. Now the model, before seeing the data, expects people to have
negative height. It also expects some giants. One of the tallest people
in recorded history, [Robert Pershing
Wadlow](https://en.wikipedia.org/wiki/Robert_Wadlow) (1918--1940) stood
272 cm tall. In our prior predictive simulation many people are taller
than this.

Does this matter? In this case, we have so much data that the silly
prior is harmless. But that won't always be the case. There are plenty
of inference problems for which the data alone are not sufficient, no
matter how numerous. Bayes lets us proceed in these cases. But only if
we use our scientific knowledge to construct sensible priors. Using
scientific knowledge to build priors is not cheating. The important
thing is that your prior not be based on the values in the data, but
only on what you know about the data before you see it.

#### Grid approximation of the posterior distribution

We are going to map out the posterior distribution through brute force
calculations.

This is not recommended because it is

-   laborious and computationally expensive
-   usually so impractical as to be essentially impossible.

Therefor the grid approximation technique has limited relevance. Later
on we will use the quadratic approximation with `rethinking::quap()`.

The strategy is the same grid approximation strategy as before in @lst-grid-approx-base-demo. But now there are two dimensions, and so there is a geometric (literally) increase in bother. The next code chunk is complex, so I will explain it line by line:

```{r}
#| label: grid-approx-posterior-a
#| attr-source: '#lst-grid-approx-posterior-a lst-cap="Grid approximation of the posterior distribution: rethinking version"'

## R code 4.16 ##################################

# establish range of μ and σ values, respectively, to calculate over 
# as well as how many points to calculate in-between. 
mu.list_a <- seq(from = 150, to = 160, length.out = 100)  # <1>
sigma.list_a <- seq(from = 7, to = 9, length.out = 100)   # <1>

# expands μ & σ values into a matrix of all of the combinations
post_a <- expand.grid(mu_a = mu.list_a, sigma_a = sigma.list_a) # <2>

# compute the log-likelihood at each combination of μ and σ
post_a$LL <- sapply(1:nrow(post_a), function(i) {                 # <3>
  sum(                                                            # <3>
    dnorm(d2_a$height, post_a$mu[i], post_a$sigma[i], log = TRUE) # <3>
  )                                                               # <3>
})                                                                # <3>

# multiply the prior by the likelihood
# as the priors are on the log scale adding = multiplying
post_a$prod <- post_a$LL + dnorm(post_a$mu_a, 178, 20, TRUE) +    # <4>
  dunif(post_a$sigma_a, 0, 50, TRUE)                              # <4>

# getting back on the probability scale without rounding error 
post_a$prob <- exp(post_a$prod - max(post_a$prod))                # <5>

```

1. Establish the range of `μ` and `σ` values, respectively, to calculate over, as well as how many points to calculate in-between. 
2. Expand those chosen `μ` and `σ` values into a matrix of all of the combinations of `μ` and `σ`. This matrix is stored in a data frame, `post_a`. 
3. Compute the log-likelihood at each combination of `μ` and `σ`. This line looks so awful, because we have to be careful here to do everything on the log scale. Otherwise rounding error will quickly make all of the posterior probabilities zero. So what `sapply()` does is pass the unique combination of `μ` and `σ` on each row of post to a function that computes the log-likelihood of each observed height, and adds all of these log-likelihoods together (`sum()`). 
4. Multiply the prior by the likelihood to get the product that is proportional to the posterior density. The priors are also on the log scale, and so we add them to the log-likelihood, which is equivalent to multiplying the raw densities by the likelihood. 
5. Finally, the obstacle for getting back on the probability scale is that rounding error is always a threat when moving from log-probability to probability. If you use the obvious approach, like `exp(post_a$prod)`, you’ll get a vector full of zeros, which isn’t very helpful. This is a result of R’s rounding very small probabilities to zero. Remember, in large samples, all unique samples are unlikely. This is why you have to work with log-probability. The code in the box dodges this problem by scaling all of the log-products by the maximum log-product. As a result, the values in `post_a$prob` are not all zero, but they also aren’t exactly probabilities. Instead they are relative posterior probabilities. But that’s good enough for what we wish to do with these values.


**Plot contour lines**

```{r}
#| label: fig-contour-plot-a
#| fig-cap: "Draw a contour plot: rethinking version"
#| attr-source: '#lst-fig-contour-plot-a lst-cap="Draw a contour plot: rethinking version"'

## R code 4.17 ##################################
rethinking::contour_xyz(post_a$mu_a, post_a$sigma_a, post_a$prob)

```

You can inspect this posterior distribution, now residing in
`post_a$prob`, using a variety of plotting commands.

**Plot heat map**

```{r}
#| label: fig-heat-map-a
#| fig-cap: "Draw a heat map: rethinking version"
#| attr-source: '#lst-fig-heat-map-a lst-cap="Draw a heat map: rethinking version"'

## R code 4.18 ##################################
rethinking::image_xyz(post_a$mu_a, post_a$sigma_a, post_a$prob)

```

#### Sampling from the posterior

To study this posterior distribution in more detail, again I'll push the
flexible approach of sampling parameter values from it. This works just
like it did in @sec-sampling-to-summarize, when you sampled values of
`p` from the posterior distribution for the globe tossing example. The
only new trick is that since there are two parameters, and we want to
sample combinations of them, we first randomly sample row numbers in
`post_a` in proportion to the values in `post_a$prob`. Then we pull out
the parameter values on those randomly sampled rows.

```{r}
#| label: fig-posterior-sample-a
#| fig-cap: "Samples from the posterior distribution for the heights data. The density of points is highest in the center, reflecting the most plausible combinations of μ and σ. There are many more ways for these parameter values to produce the data, conditional on the model. (rethinking version)"
#| attr-source: '#lst-fig-posterior-sample-a lst-cap="Samples from the posterior distribution for the heights data"'

## R code 4.19 ###########################

# randomly sample row numbers in post_a 
# in proportion to the values in post_a$prob. 
sample.rows <- sample(1:nrow(post_a),
  size = 1e4, replace = TRUE,
  prob = post_a$prob
)

# pull out the parameter values
sample.mu_a <- post_a$mu[sample.rows]
sample.sigma_a <- post_a$sigma[sample.rows]

## R code 4.20 ###########################
plot(sample.mu_a, sample.sigma_a, cex = 0.8, pch = 21, col = rethinking::col.alpha(rethinking:::rangi2, 0.1))

```

The function `col.alpha()` is part of the {**rethinking**} R package.
All it does is make colors transparent, which helps the plot in FIGURE
4.4 (here: @fig-posterior-sample-a) more easily show density, where
samples overlap. `rangi2` itself is just the [definition of a hex color
code](https://github.com/rmcelreath/rethinking/blob/2f01a9c5dac4bc6e9a6f95eec7cae268200a8181/R/colors.r#L22)
("#8080FF") specifying the shade of blue.

Adjust the plot to your tastes by playing around with `cex` (character
expansion, the size of the points), `pch` (plot character), and the 0.1
transparency value.

**Marginal Posterior Density**

Now that you have these samples, you can describe the distribution of
confidence in each combination of `μ` and `σ` by summarizing the
samples. Think of them like data and describe them, just like in
@sec-sampling-to-summarize. For example, to characterize the shapes of
the marginal posterior densities of `μ` and `σ`, all we need to do is:

```{r}
#| label: fig-marg-post-density-a
#| fig-cap: "Shapes of the marginal posterior densities of μ and σ: rethinking version"
#| attr-source: '#lst-fig-marg-post-density-a lst-cap="Plot shapes of the marginal posterior densities of μ and σ"'

## R code 4.21 #########################
rethinking::dens(sample.mu_a)
rethinking::dens(sample.sigma_a)

```

The jargon "marginal" here means "averaging over the other parameters."
Execute the above code and inspect the plots. These densities are very
close to being normal distributions. And this is quite typical. As
sample size increases, posterior densities approach the normal
distribution. If you look closely, though, you'll notice that the
density for σ has a longer right-hand tail. I'll exaggerate this
tendency a bit later, to show you that this condition is very common for
standard deviation parameters.

**Posterior Compatibility Intervals (PIs)**

To summarize the widths of these densities with posterior compatibility
intervals we use:

```{r}
#| label: post-comp-intervals-a
#| attr-source: '#lst-post-comp-intervals-a lst-cap="Posterior Compatibility Intervals (PIs): rethinking version"'

## R code 4.22 ####################
rethinking::PI(sample.mu_a)
rethinking::PI(sample.sigma_a)
```

Since these samples are just vectors of numbers, you can compute any
statistic from them that you could from ordinary data: `mean`, `median`,
or `quantile`, for example.

**Sample size and the normality of sigmas posterior**

Before moving on to using quadratic approximation `rethinking::quap()`
as shortcut to all of this inference, it is worth repeating the analysis
of the height data above, but now with only a fraction of the original
data. The reason to do this is to demonstrate that, in principle, the
posterior is not always so Gaussian in shape. There's no trouble with
the mean, `μ`. For a Gaussian likelihood and a Gaussian prior on `μ`,
the posterior distribution is always Gaussian as well, regardless of
sample size. It is the standard deviation `σ` that causes problems. So
if you care about `σ`---often people do not---you do need to be careful
of abusing the quadratic approximation.

The deep reasons for the posterior of `σ` tending to have a long
right-hand tail are complex. But a useful way to conceive of the problem
is that variances must be positive. As a result, there must be more
uncertainty about how big the variance (or standard deviation) is than
about how small it is. For example, if the variance is estimated to be
near zero, then you know for sure that it can't be much smaller. But it
could be a lot bigger.

Let's quickly analyze only 20 of the heights from the height data to
reveal this issue. To sample 20 random heights from the original list:

```{r}
#| label: fig-sample-only-20-a
#| fig-cap: "Sample 20 heights: rethinking version"
#| attr-source: '#lst-fig-sample-only-20-a lst-cap="Sample 20 heights: rethinking version"'

## R code 4.23 ######################################
d3_a <- sample(d2_a$height, size = 20)

## R code 4.24 ######################################
mu2_a.list <- seq(from = 150, to = 170, length.out = 200)
sigma2_a.list <- seq(from = 4, to = 20, length.out = 200)
post2_a <- expand.grid(mu = mu2_a.list, sigma = sigma2_a.list)
post2_a$LL <- sapply(1:nrow(post2_a), function(i) {
  sum(dnorm(d3_a,
    mean = post2_a$mu[i], sd = post2_a$sigma[i],
    log = TRUE
  ))
})
post2_a$prod <- post2_a$LL + dnorm(post2_a$mu, 178, 20, TRUE) +
  dunif(post2_a$sigma, 0, 50, TRUE)
post2_a$prob <- exp(post2_a$prod - max(post2_a$prod))
sample2_a.rows <- sample(1:nrow(post2_a),
  size = 1e4, replace = TRUE,
  prob = post2_a$prob
)
sample2_a.mu <- post2_a$mu[sample2_a.rows]
sample2_a.sigma <- post2_a$sigma[sample2_a.rows]
plot(sample2_a.mu, sample2_a.sigma,
  cex = 0.5,
  col = rethinking::col.alpha(rethinking:::rangi2, 0.1),
  xlab = "mu", ylab = "sigma", pch = 16
)

```

you'll see another scatter plot of the samples from the posterior
density, but this time you'll notice a distinctly longer tail at the top
of the cloud of points.

**Marginal Posterior Density with only 20 rows**

You should also inspect the marginal posterior density for σ, averaging
over μ, produced with:

```{r}
#| label: fig-marg-post-density-a2
#| fig-cap: "Marginal posterior density for σ, averaging over μ: rethinking version"
#| attr-source: '#lst-fig-marg-post-density-a2 lst-cap="Marginal posterior density for σ, averaging over μ: rethinking version"'

## R code 4.25 ############
rethinking::dens(sample2_a.sigma, norm.comp = TRUE)

```

#### Finding the posterior distribution with quap()

> To build the **quadratic approximation**, we'll use quap, a command in
> the `rethinking` package. The `quap` function works by using the model
> definition you were introduced to earlier in this chapter. Each line
> in the definition has a corresponding definition in the form of R
> code. The engine inside quap then uses these definitions to define the
> posterior probability at each combination of parameter values. Then it
> can climb the posterior distribution and find the peak, its `r glossary("MAP")`
> (**Maximum A Posteriori** estimate). Finally, it estimates the
> quadratic curvature at the MAP to produce an approximation of the
> posterior distribution. (parenthesis and emphasis are mine)

::: callout-note
The procedure used by `rethinking:quap()` is very similar to what many
non-Bayesian procedures do, just without any priors.
:::

1.  We start with the Howell1 data frame for adults `d2_a` (age \>= 18).
    We will place the R code equivalents into an `alist()` We are going
    to use the @def-height-linear-model-m4.1. (Code 4.27).
2.  Then we fit the model with `rethinking::quap()` to the data in the
    data frame `d2_a` (Code 4.28) to `m4.1`.
3.  Now we can have a look with `rethinking::precis()` at the posterior
    distribution (Code 4.29).

```{r}
#| label: post-dist-quap-m4-1
#| attr-source: '#lst-post-dist-quap-m4-1 lst-cap="Finding the posterior distribution with rethinking::quap(): Model m4.1"'

## R code 4.27 ######################
flist <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(178, 20),
  sigma ~ dunif(0, 50)
)

## R code 4.28 ######################
m4.1 <- rethinking::quap(flist, data = d2_a)

## R code 4.29 ######################
rethinking::precis(m4.1)

```

> These numbers provide Gaussian approximations for each parameter's
> `r glossary("marginal distribution")`. This means the plausibility of
> each value of `_μ_`, after averaging over the plausibilities of each
> value of `_σ_`, is given by a Gaussian distribution with mean 154.6
> and standard deviation 0.4.
>
> The 5.5% and 94.5% quantiles are percentile interval boundaries,
> corresponding to an 89% compatibility interval. Why 89%? It's just the
> default. It displays a quite wide interval, so it shows a
> high-probability range of parameter values. If you want another
> interval, such as the conventional and mindless 95%, you can use
> `precis(m4.1, prob=0.95)`. But I don't recommend 95% intervals,
> because readers will have a hard time not viewing them as significance
> tests. 89 is also a prime number, so if someone asks you to justify
> it, you can stare at them meaningfully and incant, "Because it is
> prime." That's no worse justification than the conventional
> justification for 95%.

> I encourage you to compare these 89% boundaries to the compatibility
intervals from the grid approximation in @lst-post-comp-intervals-a
earlier. You'll find that they are almost identical. When the posterior
is approximately Gaussian, then this is what you should expect.

##### Start values for `rethinking::quap()` {#sec-start-values-rethinking}

Mean and standard deviation are good values to start values for hill
climbing. If you don't specify `rethinking::quap()` will use a random
value.

```{r}
#| label: start-values-quap
#| attr-source: '#start-values-quap lst-cap="Define start values for rethinking::quap()"'

## R code 4.30 ######################
start <- list(
  mu = mean(d2_a$height),
  sigma = sd(d2_a$height)
)
m4.1_2 <- rethinking::quap(flist, data = d2_a, start = start)
rethinking::precis(m4.1_2)

```

::: callout-note
###### list() and alist()

Note that the list of start values is a regular `list`, not an `alist`
like the formula list is. The two functions `alist` and `list` do the
same basic thing: allow you to make a collection of arbitrary R objects.
They differ in one important respect: `list` evaluates the code you
embed inside it, while `alist` does not. So when you define a list of
formulas, you should use `alist`, so the code isn't executed. But when
you define a list of start values for parameters, you should use `list`,
so that code like `mean(d2_a$height)` will be evaluated to a numeric
value.
:::

**Slicing in more information**

> The priors we used before are very weak, both because they are nearly
> flat and because there is so much data. So I'll splice in a more
> informative prior for `*μ*`, so you can see the effect. All I'm going
> to do is change the standard deviation of the prior to 0.1, so it's a
> very narrow prior. I'll also build the formula right into the call to
> `quap` this time.

```{r}
#| label: post-dist-quap-m4.2
#| attr-source: '#lst-post-dist-quap-m4.2 lst-cap="Finding the posterior distribution with a narrower prior rethinking::quap(): Model m4.2"'

## R code 4.31 ###########################
m4.2 <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu ~ dnorm(178, 0.1),
    sigma ~ dunif(0, 50)
  ),
  data = d2_a
)
rethinking::precis(m4.2)

```

> Notice that the estimate for `*μ*` has hardly moved off the prior. The
> prior was very concentrated around 178. So this is not surprising. But
> also notice that the estimate for `*σ*` has changed quite a lot, even
> though we didn't change its prior at all. Once the golem is certain
> that the mean is near 178---as the prior insists---then the golem has
> to estimate `*σ*` conditional on that fact. This results in a
> different posterior for `*σ*`, even though all we changed is prior
> information about the other parameter.

::: callout-caution
###### `μ` has hardly moved off the prior

At first I did not understand "that the estimate for `*μ*` has hardly
moved off the prior". I thought this assertion refers to the value of
`*μ*` in both calculation. *μ* has changed considerably from 154.61 to
177.86 and under that assumption the above quote does not make sense.

But in contrast to my wrong assumption the assertion refers to the
difference between the chosen prior (178) and the resulting value of
`*μ*` (177.86).
:::

#### Sampling from a quap

The above explains how to get a quadratic approximation of the
posterior, using `rethinking::quap()`. But how do we then get samples
from the quadratic approximate posterior distribution? --- When R
constructs a quadratic approximation, it calculates not only standard
deviations for all parameters, but also the covariances among all pairs
of parameters. Just like a mean and standard deviation (or its square, a
variance) are sufficient to describe a one-dimensional Gaussian
distribution, a list of means and a matrix of variances and covariances
are sufficient to describe a multi-dimensional Gaussian distribution.



```{r}
#| label: vcov-matrix-m4.1
#| attr-source: '#lst-vcov-matrix-m4.1 lst-cap="Calculation of the variance-covariance matrix: rethinking version"'

## R code 4.32 ###################
rethinking::vcov(m4.1)
```

`vcov()` returns the variance-covariance matrix of the main parameters
of a fitted model object. In the above {**rethinking**} version is uses
the class `map2stan` for a fitted Stan model as `m4.1` is of class
`map`.

***
:::: {#prp-vcov-function)
Two different `vcov()` functions

::: callout-warning
In @lst-vcov-matrix-m4.1 I am explicitly using the package
{**rethinking**} for the `vcov()` function. The same function is also
available as a base R function with `stats::vcov()`. But this generates
an error because there is no method known for an object of class `map`
from the rethinking package. The help file for `stats::vcov()` only says
that the `vcov` object is an S3 method for classes `lm`, `glm`, `mlm`
and `aov` but not for `map`.

> Error in UseMethod("vcov") : no applicable method for 'vcov' applied
> to an object of class "map"

I could have used only `vcov()`. But this only works when the
{**rethinking**} package is already loaded. In that case R knows because
of the class of the object which `vcov()` version to use. In this case:
class of object = `class(m4.1)` `r class(m4.1)`.
:::

::::
***

@lst-vcov-matrix-m4.1 results in a variance-covariance matrix. It is
the multi-dimensional glue of a quadratic approximation, because it
tells us how each parameter relates to every other parameter in the
posterior distribution. A variance-covariance matrix can be factored
into two elements: (1) a vector of variances for the parameters and (2)
a correlation matrix that tells us how changes in any parameter lead to
correlated changes in the others.



```{r}
#| label: vcov-decomp-m4.1
#| attr-source: '#lst-vcov-decomp-m4.1 lst-cap="Variance-Covariance Matrix of model m4.1 decomposed"'

## R code 4.33 #######################
base::diag(rethinking::vcov(m4.1))      # <1>
stats::cov2cor(rethinking::vcov(m4.1))  # <2>
```

1.  `base::diag()` extracts the diagonal of the (variance-covariance)
    matrix. The two-element vector in the output is the list of
    variances. If you take the square root of this vector, you get the
    standard deviations that are shown in the `rethinking::precis()` output of
    @lst-post-dist-quap-m4-1.
2.  `stats::cov2cor()` scales a covariance matrix into the corresponding
    correlation matrix. The two-by-two matrix in the output is this
    correlation matrix. Each entry shows the correlation, bounded
    between −1 and +1, for each pair of parameters. The 1's indicate a
    parameter's correlation with itself. If these values were anything
    except 1, we would be worried. The other entries are typically
    closer to zero, and they are very close to zero in this example.
    This indicates that learning `μ` tells us nothing about `σ` and likewise
    that learning `σ` tells us nothing about `μ`. This is typical of simple
    Gaussian models of this kind. But it is quite rare more generally,
    as you'll see in later chapters.

***
:::: {#prp-why-cor-almost-0}

Confusing m4.1 (Gaussian distribution) with m4.3 (height-weight against each other)

::: callout-warning

Frankly speaking I had troubles to understand why the correlation is almost 0. It turned out that I had unconsciously in mind a correlation between height and weight, an issue that is raised later in this chapter with `m4.3`. 

But here there is only a Gaussian distribution of height under discussion, and not the correlation between height and weight.
:::
::::

***

But wait, there is another problem in my understanding:

***
:::: {#prp-from-vcov-to-cor}

How to compute correlation from variance-covariance matrix?

::: {.callout-warning}

I wonder how to compute the correlation matrix by hand form the covariance-variance matrix. I thought that I have to use `sqrt()`, but it didn't work. After I inspected the  code of the `cov2cor()` function I noticed that it uses the expression `sqrt(1/diag(V))`. I don't understand why it is using the inverse value and not just `sqrt()`. Maybe it has to do with matrix calculation, where I am not firm about it?
:::
::::

***

Let's check the assertion that the square root of the variance vector, will get us the standard deviations that are shown in the `rethinking::precis()` output of @lst-post-dist-quap-m4-1 (0.41 for $\mu$ and 0.29 for $\sigma$):

```{r}
#| label: sqrt-var-m4.1
#| attr-source: '#lst-sqrt-var-m41 lst-cap="Square root of variance vector of model m4.1"'
sqrt(base::diag(rethinking::vcov(m4.1)))
```

In fact: The square roots of the variance vector results in the standard deviation and therefore in the same output as in @lst-post-dist-quap-m4-1.

> Scaling a covariance matrix into a correlation one can be achieved in many ways, mathematically most appealing by multiplication with a diagonal matrix from left and right, or more efficiently by using `base::sweep(.., FUN = "/")` twice. The `stats::cov2cor()` function is even a bit more efficient, and provided mostly for didactical reasons.

For computing the covariance matrix with `base::sweep()` see the answer in  [StackOverflow](https://stats.stackexchange.com/a/407954/207389).

```{r}
#| label: compute-vcov-with-sweep
#| attr-source: '#lst-compute-vcov-with-sweep lst-cap="Compute covariance matrix using sweep()"'

R <- stats::cov2cor(rethinking::vcov(m4.1))
S <- sqrt(base::diag(rethinking::vcov(m4.1)))

sweep(sweep(R, 1, S, "*"), 2, S, "*")
```



:::: {#prp-vcov-interpretation}
How to interpret covariances?

::: callout-caution
A large covariance can mean a strong relationship between variables. However, you can’t compare variances over data sets with different scales (like pounds and inches).

The main problem with interpretation is that the wide range of results that it takes on makes it hard to interpret. For example, your data set could return a value of 3, or 3,000. This wide range of values is cause by a simple fact: *The larger the X and Y values, the larger the covariance*. A value of 300 tells us that the variables are correlated, but unlike the correlation coefficient, that number doesn’t tell us exactly how strong that relationship is. The problem can be fixed by dividing the covariance by the standard deviation to get the correlation coefficient. ([Statistics How To](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/covariance/))

:::

::::
***

Okay, so how do we get samples from this multi-dimensional posterior?
Now instead of sampling single values from a simple Gaussian
distribution, we sample vectors of values from a multi-dimensional
Gaussian distribution.

```{r}
#| label: extract-samples-m4.1-a
#| attr-source: '#lst-extract-samples-m4.1-a lst-cap="Extract sample vectors of values from the multi-dimensional Gaussian distribution of m4.1: rethinking version"'


## R code 4.34 #######################
post3_a <- rethinking::extract.samples(m4.1, n = 1e4)
head(post3_a)
```

You end up with a data frame, post, with 10,000 (1e4) rows and two
columns, one column for `_μ_` and one for `_σ_`. Each value is a sample
from the posterior, so the mean and standard deviation of each column
will be very close to the `r glossary("MAP")` values from before. You can confirm this by summarizing the samples:

```{r}
#| label: summary-samples-m4.1-a
#| attr-source: '#lst-summary-samples-m4.1-a lst-cap="Summary the extracted samples: rethinking version"'

## R code 4.35 ##################
rethinking::precis(post3_a)
```

Compare these values to the output from @lst-post-dist-quap-m4-1. And
you can use `plot(post)` to see how much they resemble the samples from
the grid approximation in FIGURE 4.4 (here @fig-posterior-sample-a).
These samples also preserve the covariance between `_μ_` and `_σ_`. This
hardly matters right now, because `_μ_` and `_σ_` don't co-vary at all
in this model. But once you add a predictor variable to your model,
covariance will matter a lot.

```{r}
#| label: fig-posterior-sample-vectors-a
#| fig-cap: "Samples from the vectors of values from a multi-dimensional Gaussian distribution. The density of points is highest in the center, reflecting the most plausible combinations of μ and σ. It is very similar to @fig-posterior-sample-a (rethinking version)"
#| attr-source: '#lst-fig-posterior-sample-vectors-a lst-cap="Samples from the vectors of values from a multi-dimensional Gaussian distribution"'

base::plot(post3_a)
```

##### Under the hood with multivariate sampling {#sec-under-the-hood-multivariate-sampling-a}

The function `rethinking::extract.samples()` is for convenience. It is
just running a simple simulation of the sort you conducted near the end
of @sec-chap03 with @lst-sim-pred-samples-a. Here's a
peak at the motor. The work is done by a multi-dimensional version of
`stats::rnorm()`, `MASS::mvrnorm()`. The function `stats::rnorm()`
simulates random Gaussian values, while `MASS::mvrnorm()` simulates
random vectors of multivariate Gaussian values. Here's how to use it the
{**MASS**} function to do what `rethinking::extract.samples()` does:

```{r}
#| label: fig-posterior-sample-vectors-MASS-a
#| fig-cap: "Extract samples from the vectors of values from a multi-dimensional Gaussian distribution using the {**MASS**} package. It is the same calculation as in @fig-posterior-sample-a (rethinking version)"
#| attr-source: '#lst-fig-posterior-sample-vectors-MASS-a lst-cap="Extract samples from the vectors of values from a multi-dimensional Gaussian distribution (rethinking version)"'


## R code 4.36 ######################
post4_a <- MASS::mvrnorm(n = 1e4, mu = rethinking::coef(m4.1), 
                      Sigma = rethinking::vcov(m4.1))
plot(post4_a)
```

### Linear prediction {#sec-linear-prediction-a}

What we've done until now is just a Gaussian model of height in a
population of adults. But it doesn’t really have the usual feel of "regression" to it. Typically, we are interested in modeling how
an outcome is related to some other variable, a `r glossary("predictor variable")`. If the predictor variable has any statistical association with the outcome
variable, then we can use it to predict the outcome. When the predictor
variable is built inside the model in a particular way, we'll have
linear regression.

Let's look at how height in these Kalahari foragers (the outcome
variable) co-varies with weight (the predictor variable).

```{r}
#| label: fig-height-against-weight-a
#| fig-cap: "Adult height and weight against one another"
#| attr-source: '#lst-fig-height-against-weight-a lst-cap="Scatterplot of height versus weight (Base R version)"'

## R code 4.37 #####################
plot(d2_a$height ~ d2_a$weight)
```

There's obviously a relationship: Knowing a person's weight helps you
predict height. To make this vague observation into a more precise
quantitative model that relates values of `weight` to plausible values
of `height`, we need some more technology. How do we take our Gaussian
model from @sec-gaussian-model-of-height-a and incorporate predictor
variables?

#### The linear model strategy

##### Model definition

The strategy is to make the parameter for the mean of a Gaussian distribution, `μ`, into a linear function of the predictor variable and other, new parameters that we invent. This strategy is often simply called the `r glossary("linear model")`. The linear model strategy instructs the golem to assume that the predictor variable has a constant and additive relationship to the mean of the outcome. We ask the golem: “Consider all the lines that relate one variable to the other. Rank all of these lines by plausibility, given these data.” The golem answers with a posterior distribution.

Recall @def-height-linear-model-m4.1 for the Gaussian height model. How do we
get `weight` into this model? Let $x$ be the name for the column of
weight measurements, `d2_a$weight`. Let the average of the $x$ values
be $\overline{x}$, "ex bar". Now we have a predictor variable `$x$,
which is a list of measures of the same length as $h$. To get weight
into the model, we define the mean $\mu$ as a function of the values in
$x$.

------------------------------------------------------------------------

::: {#def-height-weight-linear-model1-m4.3}
###### Linear model height against weight (Version 1 of m4.3)

$$
\begin{align*}
h_{i} \sim \operatorname{Normal}(μ_{i}, σ) \space \space (1) \\ 
μ_{i} = \alpha + \beta(x_{i}-\overline{x}) \space \space (2) \\
\alpha \sim \operatorname{Normal}(178, 20) \space \space (3)  \\ 
\beta \sim \operatorname{Normal}(0,10) \space \space (4) \\
\sigma \sim \operatorname{Uniform}(0, 50) \space \space (5)      
\end{align*}
$$ {#eq-height-weight-linear-model1-m4.3}

(1) **Likelihood (Probability of the data)**: The first line is nearly
    identical to before, except now there is a little index $i$ on the
    $μ$ as well as on the $h$. You can read $h_{i}$ as "each height" and
    $\mu_{i}$ as "each $μ$" The mean $μ$ now depends upon unique values
    on each row $i$. So the little $i$ on $\mu_{i}$ indicates that *the
    mean depends upon the row*.

(2) **Linear model**: The mean $μ$ is no longer a parameter to be
    estimated. Rather, as seen in the second line of the model, $\mu{i}$
    is constructed from other parameters, $\alpha$ and $\beta$, and the
    observed variable $x$. This line is not a stochastic relationship
    ----- there is no `~` in it, but rather an `=` in it ----- because
    the definition of $\mu{i}$ is deterministic. That is to say that,
    once we know $\alpha$ and $\beta$ and $x_{i}$, we know $\mu{i}$ with
    certainty. (More details in @sec-linear-model-a.)

(3) **includes (3),(4) and(5) with** $\alpha, \beta, \sigma$ priors: The
    remaining lines in the model define distributions for the unobserved
    variables. These variables are commonly known as parameters, and
    their distributions as priors. There are three parameters:
    $\alpha, \beta, \sigma$. You've seen priors for $\alpha$ and $\sigma$
    before, although $\sigma$ was called $\mu$ back then. (More details
    in @sec-priors-a)
:::

------------------------------------------------------------------------

##### Linear model {#sec-linear-model-a}

The mean $\mu$ is no longer a parameter to be estimated. Rather, as seen in the second line of the model, $\mu_{i}$ is constructed from other parameters, $\alpha$ and $\beta$, and the observed variable $x$. This line is not a stochastic relationship—there is no `~` in it, but rather an `=` in it—because the definition of $\mu_{i}$ is deterministic. That is to say that, once we know $\alpha$ and $\beta$ and $x_{i}$, we know $\mu_{i}$ with certainty.

The value $x_{i}$ in the second line of @def-height-weight-linear-model1-m4.3
is just the weight value on row $i$. It refers to the same individual as
the height value, $h_{i}$, on the same row. The parameters $\alpha$ and
$\beta$ are more mysterious. Where did they come from? We made them up.
The parameters $\mu$ and $\sigma$ are necessary and sufficient to
describe a Gaussian distribution. But $\alpha$ and $\beta$ are instead
devices we invent for manipulating $\mu$, allowing it to vary
systematically across cases in the data.

You'll be making up all manner of parameters as your skills improve. One
way to understand these made-up parameters is to think of them as
targets of learning. Each parameter is something that must be described
in the posterior distribution. So when you want to know something about
the data, you ask your golem by inventing a parameter for it. This will
make more and more sense as you progress.

What does the second line of @def-height-weight-linear-model1-m4.3? It tells
the regression golem that you are asking two questions about the mean of
the outcome.

1.  What is the expected height when $x_{i} = \overline{x}$? The
    parameter $\alpha$ answers this question, because when
    $x_{i} = \overline{x}$, $\mu_{i} = \alpha$. For this reason,
    $\alpha$ is often called the **intercept**. But we should think not
    in terms of some abstract line, but rather in terms of the meaning
    with respect to the observable variables.
2.  What is the change in expected height, when $x_{i}$ changes by 1
    unit? The parameter $\beta$ answers this question. It is often
    called a **slope**, again because of the abstract line. Better to
    think of it as a rate of change in expectation.

Jointly these two parameters ask the golem to find a line that relates
$x$ to $h$, a line that passes through $\alpha$ when
$x_{i} = \overline{x}$ and has slope $\beta$. That is a task that golems
are very good at. It's up to you, though, to be sure it's a good
question.

##### Priors {#sec-priors-a}

The prior for $\beta$ in @def-height-weight-linear-model1-m4.3 deserves
explanation. Why have a Gaussian prior with mean zero? This prior places
just as much probability below zero as it does above zero, and when
$\beta = 0$, weight has no relationship to height. To figure out what
this prior implies, we have to simulate the prior predictive
distribution.

The goal is to simulate heights from the model, using only the priors.
First, let's consider a range of weight values to simulate over. The
range of observed weights will do fine. Then we need to simulate a bunch
of lines, the lines implied by the priors for $\alpha$ and $\beta$.
Here's how to do it, setting a seed so you can reproduce it exactly:

```{r}
#| label: fig-sim-heights-only-with-priors-a
#| fig-cap: "Simulating heights from the model, using only the priors: rethinking version"
#| attr-source: '#lst-fig-sim-heights-only-with-priors-a lst-cap="Simulating heights from the model, using only the priors: rethinking version"'

## R code 4.38 #####################
set.seed(2971)
N_100_a <- 100 # 100 lines
a <- rnorm(N_100_a, 178, 20)
b <- rnorm(N_100_a, 0, 10)


## R code 4.39 #####################
plot(NULL,
  xlim = range(d2_a$weight), ylim = c(-100, 400),
  xlab = "weight", ylab = "height"
)
abline(h = 0, lty = 2)
abline(h = 272, lty = 1, lwd = 0.5)
mtext("b ~ dnorm(0,10)")
xbar <- mean(d2_a$weight)
for (i in 1:N_100_a) {
  curve(a[i] + b[i] * (x - xbar),
    from = min(d2_a$weight), to = max(d2_a$weight), add = TRUE,
    col = rethinking::col.alpha("black", 0.2)
  )
}

```

For reference, I've added a dashed line at zero---no one is shorter than
zero---and the "Wadlow" line at 272 cm for the world's tallest person.
The pattern doesn't look like any human population at all. It
essentially says that the relationship between weight and height could
be absurdly positive or negative. Before we've even seen the data, this
is a bad model. Can we do better?

We can do better immediately. We know that average height increases with
average weight, at least up to a point. Let's try restricting it to
positive values. The easiest way to do this is to define the prior as
Log-Normal instead. Defining $\beta$ as `Log-Normal(0,1)` means to claim
that the logarithm of $\beta$ has a Normal(0,1) distribution.

------------------------------------------------------------------------

::: {#def-prior-log-normal-dist}
Defining the prior as Log-Normal distribution

$$
\beta \sim \operatorname{Log-Normal}(0,1)
$$ {#eq-prior-log-normal-dist}
:::

------------------------------------------------------------------------

Base R provides the `dlnorm()` and `rlnorm()` densities for working with
log-normal distributions. You can simulate this relationship to see what
this means for $\beta$:

```{r}
#| label: fig-log-normal-a
#| fig-cap: "Log-Normal distribution: rethinking version"
#| attr-source: '#lst-ig-log-normal-a lst-cap="Log-Normal distribution: rethinking version"'


set.seed(4) # to reproduce with tidyverse version

## R code 4.40 ####################
b <- rlnorm(1e4, 0, 1)
rethinking::dens(b, xlim = c(0, 5), adj = 0.1)
```

If the logarithm of $\beta$ is normal, then $\beta$ itself is strictly
positive. The reason is that `exp(x)` is greater than zero for any real
number `x`. This is the reason that Log-Normal priors are commonplace.
They are an easy way to enforce positive relationships.

So what does this earn us? Do the prior predictive simulation again, now
with the Log-Normal prior:

```{r}
#| label: fig-prior-pred-sim-a
#| fig-cap: "Prior predictive simulation again, now with the Log-Normal prior: rethinking version"
#| attr-source: '#lst-fig-prior-pred-sim-a lst-cap="Prior predictive simulation with Log-Normal prior: rethinking version"'


## R code 4.41 ###################
set.seed(2971)
N_100_a <- 100 # 100 lines
a <- rnorm(N_100_a, 178, 20)
b <- rlnorm(N_100_a, 0, 1)

## R code 4.39 ###################
plot(NULL,
  xlim = range(d2_a$weight), ylim = c(-100, 400),
  xlab = "weight", ylab = "height"
)
abline(h = 0, lty = 2)
abline(h = 272, lty = 1, lwd = 0.5)
mtext("b ~ dnorm(0,10)")
xbar <- mean(d2_a$weight)
for (i in 1:N_100_a) {
  curve(a[i] + b[i] * (x - xbar),
    from = min(d2_a$weight), to = max(d2_a$weight), add = TRUE,
    col = rethinking::col.alpha("black", 0.2)
  )
}


```

This is much more sensible. There is still a rare impossible
relationship. But nearly all lines in the joint prior for $\alpha$ and
$\beta$ are now within human reason.

::: callout-note
###### What is the correct prior?

There is no more a uniquely correct prior than there is a uniquely
correct likelihood. Statistical models are machines for inference. Many
machines will work, but some work better than others. Priors can be
wrong, but only in the same sense that a kind of hammer can be wrong for
building a table.

Priors encode states of information before seeing data. So priors allow
us to explore the consequences of beginning with different information.
In cases in which we have good prior information that discounts the
plausibility of some parameter values, like negative associations
between height and weight, we can encode that information directly into
priors. When we don't have such information, we still usually know
enough about the plausible range of values. And you can vary the priors
and repeat the analysis in order to study how different states of
initial information influence inference. Frequently, there are many
reasonable choices for a prior, and all of them produce the same
inference.
:::

::: callout-note
###### Prior predictive simulation and p-hacking

When the model is adjusted in light of the observed data, then p-values
no longer retain their original meaning. False results are to be
expected. This is valid for Bayesian and Non-Bayesian statistics. Even
if Bayesian statistics don't pay any attention to p-values, the danger
remains. We could choose our priors conditional on the observed sample,
just to get some desired (wrong) result. It is therefore important to
choose priors conditional on pre-data knowledge of the variables---their
constraints, ranges, and theoretical relationships. We should always
judge our priors against general facts, not the sample.
:::

#### Finding the posterior distribution

The code needed to approximate the posterior is a straightforward
modification of the kind of code you've already seen. All we have to do
is incorporate our new model for the mean into the model specification
inside `rethinking::quap()` and be sure to add a prior for the new
parameter, `β`. Let's repeat the model definition, now with the
corresponding R code as footnotes:

------------------------------------------------------------------------

::: {#def-height-weight-linear-model2-m4.3}
###### Linear model height against weight (Version 2 of m4.3)

$$
\begin{align*}
h_{i} \sim \operatorname{Normal}(μ_{i}, σ) \space \space (1) \\ 
μ_{i} = \alpha + \beta(x_{i}-\overline{x}) \space \space (2) \\
\alpha \sim \operatorname{Normal}(178, 20) \space \space (3)  \\ 
\beta \sim \operatorname{Log-Normal}(0,10) \space \space (4) \\
\sigma \sim \operatorname{Uniform}(0, 50)  \space \space (5) \\    
\end{align*} 
$$ {#eq-height-weight-linear-model2-m4.3}

```         
height ~ dnorm(mu, sigma)     # <1>
mu <- a + b * (weight - xbar) # <2>
a ~ dnorm(178, 20)            # <3>
b ~ dlnorm(0, 10)             # <4>
sigma ~ dunif(0, 50)          # <5>
```
:::

------------------------------------------------------------------------

Notice that the linear model, in the R code on the right-hand side, uses
the R assignment operator, `<-` instead of the symbol `=`.

```{r}
#| label: find-post-dist-m4.3
#| attr-source: '#lst-find-post-dist-m4.3 lst-cap="Find the posterior distribution of the linear height-weight model: rethinking version"'

## R code 4.42 #############################

# define the average weight, x-bar
xbar_a <- mean(d2_a$weight)

# fit model
m4.3 <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - xbar_a),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d2_a
)

# summary result
## R code 4.44 ############################
rethinking::precis(m4.3)
```

You can usefully think of as assigning to $y = log(x)$ the order of
magnitude of $x = exp(y)$. The function is the reverse, turning a
magnitude into a value. These definitions will make a mathematician
shriek. But much of our computational work relies only on these
intuitions.

These definitions allow the Log-Normal prior for `β` to be coded another
way. Instead of defining a parameter `β`, we define a parameter that is
the logarithm of `β` and then assign it a normal distribution. Then we
can reverse the logarithm inside the linear model. It looks like this:

```{r}
#| label: find-post-dist2-a
#| attr-source: '#lst-find-post-dist2-a lst-cap="Find the posterior distribution of the linear height-weight model (log version): rethinking version"'

## R code 4.43 ############################
m4.3b <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + exp(log_b) * (weight - xbar),
    a ~ dnorm(178, 20),
    log_b ~ dnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d2_a
)

rethinking::precis(m4.3b)
```

Note the `exp(log_b)` in the definition of `mu`. This is the same model
as `m4.3`. It will make the same predictions. But instead of `β` in the
posterior distribution, you get `log((β)`. It is easy to translate
between the two, because $\beta = exp(log(\beta))$. In code form:
`b <- exp(log_b)`.

#### Interpretating the posterior distribution

Statistical models are hard to interpret. There are two broad categories
of interpretation: - reading tables - plotting simulation

Plotting posterior distributions and posterior predictions is better
than attempting to understand a table.

##### Table of marginal distributions

I have already included the summary (`precis()`) for the `m4.3` model in
@lst-find-post-dist-m4.3. But I will repeat it to inspect the marginal
posterior distributions of the parameters in detail:

```{r}
#| label: table-summary-m4.3
#| attr-source: '#lst-table-summary-m4.3 lst-cap="Display the marginal posterior distributions of the parameters: rethinking version"'


## R code 4.44 ################
rethinking::precis(m4.3)

```

**Interpreting the result of `rethinking::precis(m4.3)`**

1.  First row: quadratic approximation for $\alpha$
2.  Second row: quadratic approximation for $\beta$
3.  Third row: quadratic approximation for $\sigma$

Let's focus on b ($\beta$), because it's the new parameter. Since
($\beta$) is a slope, the value 0.90 can be read as *a person 1 kg
heavier is expected to be 0.90 cm taller*. 89% of the posterior
probability ($94.5-5.5$) lies between 0.84 and 0.97. That suggests that
($\beta$) values close to zero or greatly above one are highly
incompatible with these data and this model. It is most certainly not
evidence that the relationship between weight and height is linear,
because the model only considered lines. It just says that, if you are
committed to a line, then lines with a slope around 0.9 are plausible
ones.

Remember, the numbers in the default precis output aren't sufficient to
describe the quadratic posterior completely. For that, we also require
the variance-covariance matrix.

```{r}
#| label: var-cov-matrix-m4.3
#| attr-source: '#lst-var-cov-matrix-m4.3 lst-cap="Calculate the variance-covariance matrix for model m4.3"'

## R code 4.45 ######################
round(rethinking::vcov(m4.3), 3)
```

There is very little covariation among the parameters in this case. The
lack of covariance among the parameters results from
`r glossary("centering")`.

```{r}
#| label: fig-marg-post-cov-m4.3
#| fig-cap: "The marginal posteriors and the covariance matrix for model m4.3"
#| attr-source: '#lst-var-cov-matrix-m4.3 lst-cap="Show the marginal posteriors and covariance matrix for model m4.3"'
#| warning: false

rethinking::pairs(m4.3)
```

##### Plotting posterior inference against the data

It's almost always much more useful to plot the posterior inference
against the data. Not only does plotting help in interpreting the
posterior, but it also provides an informal check on model assumptions.

We're going to start with a simple version of that task, superimposing
just the posterior mean values over the height and weight data. Then
we'll slowly add more and more information to the prediction plots,
until we've used the entire posterior distribution.

We'll start with just the raw data and a single line. The code below
plots the raw data, computes the posterior mean values for `a` and `b`,
then draws the implied line:

```{r}
#| label: fig-raw-data-line-m4.3
#| fig-cap: "Height in centimeters (vertical) plotted against weight in kilograms (horizontal), with the line at the posterior mean plotted in black: rethinking version"
#| attr-source: '#lst-fig-raw-data-line-m4.3 lst-cap="Height against weight with linear regression"'

## R code 4.46 ############################################
plot(height ~ weight, data = d2_a, col = rethinking::rangi2)
post_m4.3 <- rethinking::extract.samples(m4.3)
a_map <- mean(post_m4.3$a)
b_map <- mean(post_m4.3$b)
curve(a_map + b_map * (x - xbar), add = TRUE)
```

Each point in this plot is a single individual. The black line is
defined by the mean slope `β` and mean intercept `α`. This is not a bad
line. It certainly looks highly plausible. But there are an infinite
number of other highly plausible lines near it. Let's draw those too.

##### Adding uncertainty around the mean

Plots of the average line, like in @fig-raw-data-line-m4.3, are useful
for getting an impression of the magnitude of the estimated influence of
a variable. But they do a poor job of communicating uncertainty.
Remember, the posterior distribution considers every possible regression
line connecting height to weight. It assigns a relative plausibility to
each. This means that each combination of `α` and `β` has a posterior
probability. It could be that there are many lines with nearly the same
posterior probability as the average line. Or it could be instead that
the posterior distribution is rather narrow near the average line.

So how can we get that uncertainty onto the plot? Together, a
combination of `α` and `β` define a line. And so we could sample a bunch
of lines from the posterior distribution. Then we could display those
lines on the plot, to visualize the uncertainty in the regression
relationship.

To better appreciate how the posterior distribution contains lines, we
work with all of the samples from the model.

```{r}
#| label: collect-post-samples-a

## R code 4.47 ################
# post_m4.3 <- rethinking::extract.samples(m4.3) # already in previous listing
post_m4.3[1:5, ]
```

Each row is a correlated random sample from the joint posterior of all
three parameters, using the covariances provided by
`rethinking::vcov(m4.3)` (@lst-var-cov-matrix-m4.3). The paired values
of `a` and `b` on each row define a line. The average of very many of
these lines is the posterior mean line. But the scatter around that
average is meaningful, because it alters our confidence in the
relationship between the predictor and the outcome.

So now let's display a bunch of these lines, so you can see the scatter.
This lesson will be easier to appreciate, if we use only some of the
data to begin. Then you can see how adding in more data changes the
scatter of the lines. So we'll begin with just the first 10 cases in
`d2_a`. The following code extracts the first 10 cases and re-estimates
the model:

```{r}
#| label: extract-10-re-est-mod-a

## R code 4.48 ##########################
N10_a <- 10
dN10_a <- d2_a[1:N10_a, ]
mN10_a <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - mean(weight)),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = dN10_a
)

```

Now let's plot 20 of these lines for the 10 data points, to see what the
uncertainty looks like.

```{r}
#| label: fig-plot-lines-10-points-a
#| fig-cap: "Samples from the quadratic approximate posterior distribution for the height/weight model, m4.3. 20 lines sampled from 10 data points of the posterior distribution, showing the uncertainty in the regression relationship: rethinking version"
#| attr-source: '#lst-fig-plot-lines-10-points-a lst-cap="20 samples from the quadratic approximate posterior distribution for m4.3 with 10 data points"'

## R code 4.49 ##############################
# extract 20 samples from the posterior
post_20_m4.3 <- rethinking::extract.samples(mN10_a, n = 20)

# display raw data and sample size
plot(dN10_a$weight, dN10_a$height,
  xlim = range(d2_a$weight), ylim = range(d2_a$height),
  col = rethinking::rangi2, xlab = "weight", ylab = "height"
)
mtext(rethinking:::concat("N = ", N10_a))

# plot the lines, with transparency
for (i in 1:20) {
  curve(post_20_m4.3$a[i] + post_20_m4.3$b[i] * (x - mean(dN10_a$weight)),
    col = rethinking::col.alpha("black", 0.3), add = TRUE
  )
}

```

The result is shown in the upper-left plot in FIGURE 4.7. By plotting
multiple regression lines, sampled from the posterior, it is easy to see
both the highly confident aspects of the relationship and the less
confident aspects. The cloud of regression lines displays greater
uncertainty at extreme values for weight.

The other plots in FIGURE 4.7 show the same relationships, but for
increasing amounts of data. Just re-use the code from before, but change
N \<- 10 to some other value. Notice that the cloud of regression lines
grows more compact as the sample size increases. This is a result of the
model growing more confident about the location of the mean.

Plot 20 lines sampled from 50 data points of the posterior distribution

```{r}
#| label: fig-plot-lines-50-points-a
#| fig-cap: "Samples from the quadratic approximate posterior distribution for the height/weight model, m4.3. 20 lines sampled from 50 data points of the posterior distribution, showing the uncertainty in the regression relationship: rethinking version"
#| attr-source: '#lst-fig-plot-lines-50-points-a lst-cap="20 samples from the quadratic approximate posterior distribution for m4.3 with 50 data points"'



## R code 4.48, 4.49 ######################
N50_a <- 50
dN50_a <- d2_a[1:N50_a, ]
mN50_a <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - mean(weight)),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = dN50_a
)

# extract 20 samples from the posterior
post_50_m4.3 <- rethinking::extract.samples(mN50_a, n = 20)

# display raw data and sample size
plot(dN50_a$weight, dN50_a$height,
  xlim = range(d2_a$weight), ylim = range(d2_a$height),
  col = rethinking::rangi2, xlab = "weight", ylab = "height"
)
mtext(rethinking:::concat("N = ", N50_a))

# plot the lines, with transparency
for (i in 1:20) {
  curve(post_50_m4.3$a[i] + post_50_m4.3$b[i] * (x - mean(dN50_a$weight)),
    col = rethinking::col.alpha("black", 0.3), add = TRUE
  )
}

```

Plot 20 lines sampled from 352 data points of the posterior distribution

```{r}
#| label: fig-plot-lines-all-352-points-a
#| fig-cap: "Samples from the quadratic approximate posterior distribution for the height/weight model, m4.3. 20 lines sampled from all 352 data points of the posterior distribution, showing the uncertainty in the regression relationship."
#| attr-source: '#lst-fig-plot-lines-all-352-points-a lst-cap="20 Samples from the quadratic approximate posterior distribution for m4.3 with all data"'

## R code 4.48, 4.49 ###########################
N352_a <- 352
dN352_a <- d2_a[1:N352_a, ]
mN352_a <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - mean(weight)),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = dN352_a
)

# extract 20 samples from the posterior
post_352_m4.3 <- rethinking::extract.samples(mN352_a, n = 20)

# display raw data and sample size
plot(dN352_a$weight, dN352_a$height,
  xlim = range(d2_a$weight), ylim = range(d2_a$height),
  col = rethinking::rangi2, xlab = "weight", ylab = "height"
)
mtext(rethinking:::concat("N = ", N352_a))

# plot the lines, with transparency
for (i in 1:20) {
  curve(post_352_m4.3$a[i] + post_352_m4.3$b[i] * (x - mean(dN352_a$weight)),
    col = rethinking::col.alpha("black", 0.3), add = TRUE
  )
}

```

##### Plotting regression intervals and contours

The cloud of regression lines in @fig-plot-lines-10-points-a,
@fig-plot-lines-50-points-a and @fig-plot-lines-all-352-points-a is an
appealing display, because it communicates uncertainty about the
relationship in a way that many people find intuitive. But it's more
common, and often much clearer, to see the uncertainty displayed by
plotting an interval or contour around the average regression line.

```{r}
#| label: calc-PI-around-regr-line-a
#| attr-source: '#lst-calc-PI-around-regr-line-a lst-cap="Calculate uncertainty around the average regression line"'

## R code 4.50 ##########################

post_m4.3 <- rethinking::extract.samples(m4.3)      # <1>
mu_at_50_a <- post_m4.3$a + post_m4.3$b * (50 - xbar) # <2>
head(mu_at_50_a)                                      # <3>
```

1.  Repeating the code for drawing (extracting and collecting) from the
    fitted model m4.3 (already done in @fig-raw-data-line-m4.3)
2.  The code to the right of the `<-` takes its form from the equation
    for $\mu_{i} = \alpha + \beta(x_{i} - \overline{x})$. The value of
    $x_{i}$ in this case is 50.
3.  The result is a vector of predicted means, one for each random
    sample from the posterior. Since joint `a` and `b` went into
    computing each, the variation across those means incorporates the
    uncertainty in and correlation between both parameters.

It might be helpful at this point to actually plot the density for this
vector of means:

```{r}
#| label: fig-density-vector-mean-50-a
#| fig-cap: "The quadratic approximate posterior distribution of the mean height, μ, when weight is 50 kg. This distribution represents the relative plausibility of different values of the mean. Rethinking version"
#| attr-source: '#lst-ig-density-vector-mean-50-a fig-cap="Quadratic approximation of the posterior distribution for mean height, μ, when weight is 50 kg"'

## R code 4.51 ##################
rethinking::dens(mu_at_50_a, col = rethinking::rangi2, lwd = 2, xlab = "mu|weight=50")
```

Since the components of `μ` have distributions, so too does `μ`. And
since the distributions of `α` and `β` are Gaussian, so too is the
distribution of `μ` (adding Gaussian distributions always produces a
Gaussian distribution).

Since the posterior for `μ` is a distribution, you can find intervals
for it, just like for any posterior distribution. To find the 89%
compatibility interval of `μ` at 50 kg, just use the `PI()` command as
usual:

```{r}
#| label: PI-mu-at-50-a
#| attr-source: '#lst-PI-mu-at-50-a lst-cap="89% compatibility interval of μ at 50 kg"'

## R code 4.52 ##############
rethinking::PI(mu_at_50_a, prob = 0.89)

```

What these numbers mean is that the central 89% of the ways for the
model to produce the data place the average height between about 159 cm
and 160 cm (conditional on the model and data), assuming the weight is
50 kg.

That's good so far, but we need to repeat the above calculation for
every weight value on the horizontal axis, not just when it is 50 kg. We
want to draw 89% intervals around the average slope in
@fig-raw-data-line-m4.3.

This is made simple by strategic use of the `rethinking::link()`
function, a part of the {**rethinking**} package.
What`rethinking::link()` will do is take your`rethinking::quap()`
approximation, sample from the posterior distribution, and then compute
`μ\` for each case in the data and sample from the posterior
distribution. Here's what it looks like for the data you used to fit the
model:

```{r}
#| label: calc-mu-with-link-a
#| attr-source: '#lst-calc-mu-with-link-a lst-cap="Calculate μ for each case in the data and sample from the posterior distribution: Rethinking version"'

## R code 4.53 ##############
mu_a <- rethinking::link(m4.3)
str(mu_a)
```

You end up with a big matrix of values of `μ`. Each row is a sample from
the posterior distribution. The default is 1000 samples, but you can use
as many or as few as you like. Each column is a case (row) in the data.
There are 352 rows in `d2_a`, corresponding to 352 individuals. So there
are 352 columns in the matrix mu above.

The function `rethinking::link()` provides a posterior distribution of
`μ` for each case we feed it. So above we have a distribution of `μ` for
each individual in the original data. We actually want something
slightly different: a distribution of `μ` for each unique weight value
on the horizontal axis. It's only slightly harder to compute that, by
just passing `rethinking::link()` some new data:

```{r}
#| label: calc-dist-mu-unique-with-link-a
#| attr-source: '#lst-calc-dist-mu-unique-with-link-a lst-cap="Calculate a distribution of μ for each unique weight value on the horizontal axis: rethinking version"'

## R code 4.54 ###############################
# define sequence of weights to compute predictions for
# these values will be on the horizontal axis
weight.seq <- seq(from = 25, to = 70, by = 1)

# use link to compute mu
# for each sample from posterior
# and for each weight in weight.seq
mu2_a <- rethinking::link(m4.3, data = data.frame(weight = weight.seq))
str(mu2_a)
```

And now there are only 46 columns in mu, because we fed it 46 different
values for weight. To visualize what you've got here, let's plot the
distribution of `μ` values at each height.

```{r}
#| label: fig-dist-mu-height-100-a
#| fig-cap: "The first 100 values in the distribution of μ at each weight value. Rethinking version"
#| attr-source: '#lst-fig-dist-mu-height-100-a lst-cap="The first 100 values in the distribution of μ at each weight value. Rethinking version"'

## R code 4.55 ##################
# use type="n" to hide raw data
base::plot(height ~ weight, d2_a, type = "n")

# loop over samples and plot each mu value
for (i in 1:100) {
  graphics::points(weight.seq, mu2_a[i, ], pch = 16, col = rethinking::col.alpha(rethinking::rangi2, 0.1))
}

```

At each weight value in `weight.seq`, a pile of computed `μ` values are
shown. Each of these piles is a Gaussian distribution, like that in
@fig-density-vector-mean-50-a. You can see now that the amount of
uncertainty in `μ` depends upon the value of weight. And this is the
same fact you saw in @fig-plot-lines-10-points-a,
@fig-plot-lines-50-points-a and @fig-plot-lines-all-352-points-a.

The final step is to summarize the distribution for each weight value.
We'll use `base::apply()`, which applies a function of your choice to a
matrix.

```{r}
#| label: sum-dist-weight-a
#| attr-source: '#lst-sum-dist-weight-a lst-cap="Summary of the distribution for each weight value. Rethinking version"'

## R code 4.56 #####################
# summarize the distribution of mu
mu2.mean <- apply(mu2_a, 2, mean)                      # <1>
mu2.PI <- apply(mu2_a, 2, rethinking::PI, prob = 0.89) # <2>
mu2.mean                                               # <3>
mu2.PI                                                 # <4>
```

1.  Read `apply(mu2,2,mean)` as "compute the mean of each column
    (dimension '2') of the matrix mu". Now `mu2.mean` contains the
    average `μ` at each weight value.
2.  `mu2.PI` contains 89% lower and upper bounds for each weight value.
3.  `mu2.mean` and `mu2.PI` are just different kinds of summaries of the
    distributions in `mu2_a`, with each column being for a different
    weight value. These summaries are only summaries. The "estimate" is
    the entire distribution.

```{r}
#| label: fig-summaries-on-data-top-a
#| fig-cap: "Plot of the summaries on top of the !Kung height data again, now with 89% compatibility interval of the mean indicated by the shaded region. Compare this region to the distributions of blue points in @fig-dist-mu-height-100-a."
#| attr-source: '#lst-fig-summaries-on-data-top-a lst-cap="Plot of the summaries on top of the !Kung height data with 89% compatibility interval"'

## R code 4.57 ###########################
# plot raw data
# fading out points to make line and interval more visible
plot(height ~ weight, data = d2_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))

# plot the MAP line, aka the mean mu for each weight
graphics::lines(weight.seq, mu2.mean)

# plot a shaded region for 89% PI
rethinking::shade(mu2.PI, weight.seq)

```

::: callout-caution
There is very little uncertainty about the average height as a function
of average weight. But keep in mind that these inferences are always
conditional on the model. Think of the regression line in
@fig-summaries-on-data-top-a as saying: *Conditional on the assumption
that height and weight are related by a straight line, then this is the
most plausible line, and these are its plausible bounds.*
:::

Using this approach, you can derive and plot posterior prediction means
and intervals for quite complicated models, for any data you choose. As
long as you know the structure of the model ----- how parameters relate
to the data ----- you can use samples from the posterior to describe any
aspect of the model's behavior.

Summarizing the three steps for generating predictions and intervals
from the posterior of a fit model:

1.  Use `rethinking::link()` to generate distributions of posterior
    values for `μ`. The default behavior of `rethinking::link()` is to
    use the original data, so you have to pass it a list of new
    horizontal axis values you want to plot posterior predictions
    across.
2.  Use summary functions like `mean` or `PI` to find averages and lower
    and upper bounds of `μ` for each value of the predictor variable.
3.  Finally, use plotting functions like `graphics::lines()` and
    `rethinking::shade()` to draw the lines and intervals. Or you might
    plot the distributions of the predictions, or do further numerical
    calculations with them. It's really up to you.

::: callout-note
You could write a function that accomplish the same thing as
`rethinking::link()`:

```{r}
#| label: writing-link-function-a
#| attr-source: '#lst-writing-link-function-a lst-cap="Code to perform the same steps as the rethinking::link() function"'
#| 
## R code 4.58 ################################
post_m4.3 <- rethinking::extract.samples(m4.3)
mu.link <- function(weight) post_m4.3$a + post_m4.3$b * (weight - xbar)
weight.seq <- seq(from = 25, to = 70, by = 1)
mu3_a <- sapply(weight.seq, mu.link)
mu3.mean <- apply(mu3_a, 2, mean)
mu3.CI <- apply(mu3_a, 2, rethinking::PI, prob = 0.89)
mu3.mean
mu3.CI
```

And the values in `mu3.mean` and `mu3.CI` should be very similar
(allowing for simulation variance) to what you got the automated way,
using `rethinking::link()` in @lst-sum-dist-weight-a.

Knowing this manual method is useful both for (1) understanding and (2)
sheer power. Whatever the model you find yourself with, this approach
can be used to generate posterior predictions for any component of it.
Automated tools like link save effort, but they are never as flexible as
the code you can write yourself.
:::

##### Prediction intervals

Now let's walk through generating an 89% prediction interval for actual
heights, not just the average height, `μ`. This means we'll incorporate
the standard deviation `σ` and its uncertainty as well. Remember, the
first line of the statistical model in @eq-height-weight-linear-model2-m4.3 is:

------------------------------------------------------------------------

::: {#def-height-weight-linear-model-first-line-m4.3}
###### Remember the first line of @eq-height-weight-linear-model2-m4.3

$$
h_{i} \sim \operatorname{Normal}(μ_{i}, σ)
$$ {#eq-height-weight-linear-model-first-line-m4.3}
:::

------------------------------------------------------------------------

What you've done so far is just use samples from the posterior to
visualize the uncertainty in $μ_{i}$, the linear model of the mean. But
actual predictions of heights depend also upon the distribution in the
first line. The Gaussian distribution on the first line tells us that
the model expects observed heights to be distributed around `μ`, not
right on top of it. And the spread around `μ` is governed by `σ`. All of
this suggests we need to incorporate `σ` in the predictions.

Imagine simulating heights. For any unique weight value, you sample from
a Gaussian distribution with the correct mean `μ` for that weight, using
the correct value of σ sampled from the same posterior distribution. If
you do this for every sample from the posterior, for every weight value
of interest, you end up with a collection of simulated heights that
embody the uncertainty in the posterior as well as the uncertainty in
the Gaussian distribution of heights.

There is a tool called `rethinking::sim()` which does this simulation of
the posterior observations:

```{r}
#| label: simulate-post-dist-a
#| attr-source: '#lst-simulate-post-dist-a lst-cap="Simulation of the posterior observations"'

## R code 4.59 #######################
sim.height <- rethinking::sim(m4.3, data = list(weight = weight.seq))
str(sim.height)
```

This matrix is much like the earlier one, `mu`, but it contains
simulated heights, not distributions of plausible average height, `μ`.

We can summarize these simulated heights in the same way we summarized
the distributions of `μ`, by using apply:

```{r}
#| label: sum-sim-heights-a
#| attr-source: '#lst-sum-sim-heights-a lst-cap="Summarize simulted heights"'

## R code 4.60 ###################
height.PI <- apply(sim.height, 2, rethinking::PI, prob = 0.89)
```

Now `height.PI` contains the 89% posterior prediction interval of
observable (according to the model) heights, across the values of weight
in `weight.seq`.

Let's plot everything we've built up: (1) the average line, (2) the
shaded region of 89% plausible `μ`, and (3) the boundaries of the
simulated heights the model expects.

```{r}
#| label: fig-pred-interval-height-a
#| fig-cap: "89% prediction interval for height, as a function of weight. The solid line is the average line for the mean height at each weight. The two shaded regions show different 89% plausible regions. The narrow shaded interval around the line is the distribution of μ. The wider shaded region represents the region within which the model expects to find 89% of actual heights in the population, at each weight."
#| attr-source: '#lst-fig-pred-interval-height-a lst-cap="89% prediction interval for height, as a function of weight"'

## R code 4.61 ##################
# plot raw data
plot(height ~ weight, d2_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))

# draw MAP line
graphics::lines(weight.seq, mu3.mean)

# draw HPDI region for line
rethinking::shade(mu3.CI, weight.seq)

# draw PI region for simulated heights
rethinking::shade(height.PI, weight.seq)


```

Notice that the outline for the wide shaded interval is a little rough.
This is the simulation variance in the tails of the sampled Gaussian
values. If it really bothers you, increase the number of samples you
take from the posterior distribution. The optional n parameter for
`sim.height` controls how many samples are used. Try for example 1e4
samples and run the plotting code again. You'll see the shaded boundary
smooth out some.

With extreme percentiles, it can be very hard to get out all of the
roughness. Luckily, it hardly matters, except for aesthetics. Moreover,
it serves to remind us that all statistical inference is approximate.
The fact that we can compute an expected value to the 10th decimal place
does not imply that our inferences are precise to the 10th decimal
place.

```{r}
#| label: fig-pred-interval-height2-a
#| fig-cap: "89% prediction interval for height, as a function of weight. Shaded boundary smoothed out by sampling 1e4 times instead of the standard value of 1e3"
#| attr-source: '#lst-fig-pred-interval-height2-a lst-cap="89% prediction interval for height, as a function of weight"'

## R code 4.62 (4.59 & 4.60) ###################

## R code 4.59 ################
sim2.height <- rethinking::sim(m4.3, data = list(weight = weight.seq), n = 1e4)
height2.PI <- apply(sim2.height, 2, rethinking::PI, prob = 0.89)

## R code 4.61 ##################
# plot raw data
plot(height ~ weight, d2_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))

# draw MAP line
graphics::lines(weight.seq, mu3.mean)

# draw HPDI region for line
rethinking::shade(mu3.CI, weight.seq)

# draw PI region for simulated heights
rethinking::shade(height2.PI, weight.seq)
```

Just like with `rethinking::link()`, it's useful to know a little about
how `rethinking::sim()` operates. For every distribution like `dnorm`,
there is a companion simulation function. For the Gaussian distribution,
the companion is `rnorm`, and it simulates sampling from a Gaussian
distribution. What we want R to do is simulate a height for each set of
samples, and to do this for each value of weight. The following will do
it:

```{r}
#| label: writing-sim-function-a
#| attr-source: '#lst-writing-sim-function-a lst-cap="Writing you own rethinking:sim() function"'

## R code 4.63 ########################################

post_m4.3 <- rethinking::extract.samples(m4.3)
# post <- extract.samples(m4.3)
# weight.seq <- 25:70
sim3.height <- sapply(weight.seq, function(weight) {
  rnorm(
    n = nrow(post_m4.3),
    mean = post_m4.3$a + post_m4.3$b * (weight - xbar),
    sd = post_m4.3$sigma
  )
})
height3.PI <- apply(sim3.height, 2, rethinking::PI, prob = 0.89)
```

The values in `height.PI` will be practically identical to the ones
computed in the main text and displayed in FIGURE 4.10.

```{r}
#| label: compare-height-PIs-str--a
#| attr-source: '#lst-compare-height-PIs-str-a lst-cap="Compare height3.PI with height.PI using str()"'

str(height.PI)
str(height3.PI)

```

Small differences are the result of the randomized sampling process:

```{r}
#| label: compare-height-PIs-head-a
#| attr-source: '#lst-compare-height-PIs-head-a lst-cap="Compare height3.PI with height.PI using head()"'

head(height.PI)[ , 1:6]
head(height3.PI)[ , 1:6]
```

### Curves from lines

We'll consider two commonplace methods that use linear regression to
build curves. The first is `r glossary("polynomial regression")`. The
second is `r glossary("spline")`. Both approaches work by transforming a
single predictor variable into several synthetic variables. But splines
have some clear advantages.

#### Polynomial regression

Polynomial regression uses powers of a variable -- squares and cubes --
as extra predictors. This is an easy way to build curved associations.
Polynomial regressions are very common, and understanding how they work
will help scaffold later models.

```{r}
#| label: fig-scatterplot-height-weight-a
#| fig-cap: "Height in centimeters (vertical) plotted against weight in kilograms (horizontal). This time with the full data, e.g., the non-adults data."
#| attr-source: '#fig-scatterplot-height-weight-a lst-cap="Height in centimeters (vertical) plotted against weight in kilograms (horizontal): rethinking version"'

plot(height ~ weight, data = d_a, col = rethinking::rangi2)
```

The relationship is visibly curved, now that we've included the
non-adult individuals. (Compare with adult data in
@fig-raw-data-line-m4.3)

The most common polynomial regression is a parabolic model of the mean.
Let `x` be standardized body weight. Then the parabolic equation for the
mean height is:

------------------------------------------------------------------------

::: {#def-parabolic-mean}
Parabolic equation for the mean height

$$
\mu_{i} = \alpha + \beta_{1}x_{i} + \beta_{2}x_{i}^2
$$ {#eq-parabolic-mean}
:::

------------------------------------------------------------------------

@eq-parabolic-mean is a parabolic (second order) polynomial. The
$\alpha + \beta_{1}x_{i}$ part is the same linear function of x in a
linear regression, just with a little "1" subscript added to the
parameter name, so we can tell it apart from the new parameter. The
additional term uses the square of $x_{i}$ to construct a parabola,
rather than a perfectly straight line. The new parameter $\beta_{2}$
measures the curvature of the relationship.

> Fitting these models to data is easy. Interpreting them can be hard.

::: callout-tip
Polynomial parameters are in general very difficult to understand. But
prior predictive simulation does help a lot.
:::

**Fitting a parabolic model of height on weight**

The first thing to do is to
`r glossary("Standardization", "standardize")` the predictor variable.
We've done this in previous examples. But this is especially helpful for
working with polynomial models. When predictor variables have very large
values in them, there are sometimes numerical glitches. Even well-known
statistical software can suffer from these glitches, leading to mistaken
estimates. These problems are very common for polynomial regression,
because the square or cube of a large number can be truly massive.
Standardizing largely resolves this issue. It should be your default
behavior.

> A quadratic function (also called a quadratic, a quadratic polynomial,
> or a polynomial of degree 2) is special type of polynomial function
> where the highest-degree term is second degree. ... The graph of a
> quadratic function is a parabola, a 2-dimensional curve that looks
> like either a cup(∪) or a cap(∩). ([Statistis How
> To](https://www.statisticshowto.com/quadratic-function/))

To define the parabolic model, just modify the definition of $\mu_{i}$.

------------------------------------------------------------------------

::: {#def-parabolic-model}
Fitting a parabolic model of height on weight

$$
\begin{align*}
h_{i} \sim \operatorname{Normal}(μ_{i}, σ) \space \space (1) \\ 
μ_{i} = \alpha + \beta_{1}x_{i} + \beta_{2}x_{i}^2 \space \space (2) \\
\alpha \sim \operatorname{Normal}(178, 20) \space \space (3)  \\ 
\beta_{1} \sim \operatorname{Log-Normal}(0,10) \space \space (4) \\
\beta_{2} \sim \operatorname{Normal}(0,10) \space \space (5) \\
\sigma \sim \operatorname{Uniform}(0, 50) \space \space (6)      
\end{align*}
$$ {#eq-parabolic-model}

```         
height ~ dnorm(mu, sigma)                  # <1>
mu <- a + b1 * weight.s + b2 * weight.s2^2 # <2>
a ~ dnorm(178, 20)                         # <3>
b1 ~ dlnorm(0, 10)                         # <4>
b2 ~ dnorm(0, 10)                          # <5>
sigma ~ dunif(0, 50)                       # <6>
```
:::

------------------------------------------------------------------------

The confusing issue here is assigning a prior for $\beta_{2}$, the
parameter on the squared value of `x`. Unlike $\beta_{1}$, we don't want
a positive constraint.

Approximating the posterior is straightforward. Just modify the
definition of `mu` so that it contains both the linear and quadratic
terms. But in general it is better to pre-process any variable
transformations -- you don't need the computer to recalculate the
transformations on every iteration of the fitting procedure. So I'll
also build the square of `weight_s` as a separate variable:

```{r}
#| label: quap-parabolic
#| attr-source: '#lst-quap-parabolic lst-cap="Finding the posterior distribution of a parabolic model of height on weight with rethinking::quap()"'

## R code 4.65 #######################
d_a$weight_s <- (d_a$weight - mean(d_a$weight)) / sd(d_a$weight)
d_a$weight_s2 <- d_a$weight_s^2
m4.5 <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1 * weight_s + b2 * weight_s2,
    a ~ dnorm(178, 20),
    b1 ~ dlnorm(0, 1),
    b2 ~ dnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d_a
)
```

Now, since the relationship between the outcome height and the predictor
weight depends upon two slopes, b1 and b2, it isn't so easy to read the
relationship off a table of coefficients:

```{r}
#| label: show-result-m4.5
#| attr-source: '#lst-show-result-m4.5 lst-cap="Show table of coefficients for model m4.5"'

## R code 4.66 ################
rethinking::precis(m4.5)
```

The parameter $\alpha$ (a) is still the intercept, so it tells us the
expected value of height when weight is at its mean value. But it is no
longer equal to the mean height in the sample, since there is no
guarantee it should in a polynomial regression. And those $\beta_{1}$
and $\beta_{2}$ parameters are the linear and square components of the
curve. But that doesn't make them transparent.

You have to plot these model fits to understand what they are saying. So
let's do that. We'll calculate the mean relationship and the 89%
intervals of the mean and the predictions, like in the previous section.
Here's the working code:

```{r}
#| label: fig-polynomial-regression-a
#| fig-cap: "Polynomial regressions of height on weight (standardized), for the full !Kung data."
#| attr-source: '#lst-fig-polynomial-regression-a lst-cap="Polynomial regressions of height on weight (standardized), for the full !Kung data."'

## R code 4.67 ################
weight5.seq <- seq(from = -2.2, to = 2, length.out = 30)
pred_dat_a <- list(weight_s = weight5.seq, weight_s2 = weight5.seq^2)
mu5_a <- rethinking::link(m4.5, data = pred_dat_a)
mu5.mean <- apply(mu5_a, 2, mean)
mu5.PI <- apply(mu5_a, 2, rethinking::PI, prob = 0.89)
sim5.height <- rethinking::sim(m4.5, data = pred_dat_a)
height5.PI <- apply(sim5.height, 2, rethinking::PI, prob = 0.89)


## R code 4.68 #################
plot(height ~ weight_s, d_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))
graphics::lines(weight5.seq, mu5.mean)
rethinking::shade(mu5.PI, weight5.seq)
rethinking::shade(height5.PI, weight5.seq)

```

The quadratic regression does a pretty good job. It is much better than
a linear regression for the full Howell1 data set.

```{r}
#| label: fig-find-post-full-data-a
#| fig-cap: "Find the posterior distribution of the linear height-weight model (full data set): rethinking version"
#| attr-source: '#lst-fig-find-post-full-data-a lst-cap="Posterior distribution of the linear height-weight model: rethinking version"'

## R code 4.42 #############################

# define the average weight, x-bar
xbar_full <- mean(d_a$weight)

# fit model
m4.3_full <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - xbar_full),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d_a
)

## R code 4.46 ############################################
plot(height ~ weight, data = d_a, col = rethinking::rangi2)
post_m4.3_full <- rethinking::extract.samples(m4.3_full)
a_map_full <- mean(post_m4.3_full$a)
b_map_full <- mean(post_m4.3_full$b)
curve(a_map_full + b_map_full * (x - xbar_full), add = TRUE)



## R code 4.58 ################################
mu.link_full <- function(weight) post_m4.3_full$a + post_m4.3_full$b * (weight - xbar_full)
weight.seq_full <- seq(from = 0, to = 70, by = 1)
mu3_a_full <- sapply(weight.seq_full, mu.link_full)
mu3.mean_full <- apply(mu3_a_full, 2, mean)
mu3.CI_full <- apply(mu3_a_full, 2, rethinking::PI, prob = 0.89)

## R code 4.59 #######################
sim.height_full <- rethinking::sim(m4.3_full, data = list(weight = weight.seq_full))

## R code 4.60 ###################
height.PI_full <- apply(sim.height_full, 2, rethinking::PI, prob = 0.89)

## R code 4.61 ##################
# plot raw data
# plot(height ~ weight, d2_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))

# draw MAP line
graphics::lines(weight.seq_full, mu3.mean_full)

# draw HPDI region for line
rethinking::shade(mu3.CI_full, weight.seq_full)

# draw PI region for simulated heights
rethinking::shade(height.PI_full, weight.seq_full)


```

I had `xbar` to recalculate with the new data set. All the other code is
the same as in @lst-find-post-dist-m4.3 and @lst-fig-raw-data-line-m4.3.

@fig-find-post-full-data-a shows the familiar linear regression from
earlier in the chapter, but now with the standardized predictor and full
data with both adults and non-adults. The linear model makes some
spectacularly poor predictions, at both very low and middle weights.
Compare this to @fig-polynomial-regression-a, the new quadratic
regression. The curve does a better job of finding a central path
through the data.

Let's see what will happen if we are using a higher-order polynomial
regression, a cubic regression on weight. The model is:

------------------------------------------------------------------------

::: {#def-cubic-regression}
Cubic regression on weigth

$$
\begin{align*}
h_{i} \sim \operatorname{Normal}(μ_{i}, σ) \space \space (1) \\ 
μ_{i} = \alpha + \beta_{1}x_{i} + \beta_{2}x_{i}^2 + \beta_{3}x_{i}^3 \space \space (2) \\
\alpha \sim \operatorname{Normal}(178, 20) \space \space (3)  \\ 
\beta_{1} \sim \operatorname{Log-Normal}(0,10) \space \space (4) \\
\beta_{2} \sim \operatorname{Normal}(0,10) \space \space (5) \\
\beta_{3} \sim \operatorname{Normal}(0,10) \space \space (6) \\
\sigma \sim \operatorname{Uniform}(0, 50) \space \space (7)      
\end{align*}
$$ {#eq-cubic-regression}

```         
height ~ dnorm(mu, sigma)               # <1>
mu <- a + b1 * weight.s + b2 * weight.s2^2 + b3 * weight.s3^3 # <2>
a ~ dnorm(178, 20)                      # <3>
b1 ~ dlnorm(0, 10)                      # <4>
b2 ~ dnorm(0, 10)                       # <5>
b3 ~ dnorm(0, 10)                       # <6>
sigma ~ dunif(0, 50)                    # <7>
```
:::

------------------------------------------------------------------------

Fit the model accordingly. It is only a slight modification of the
parabolic model's code:

```{r}
#| label: fig-cubic-regression-a
#| fig-cap: "Cubic regressions of height on weight (standardized), for the full !Kung data."
#| attr-source: '#lst-fig-cubic-regression-a lst-cap="Cubic regressions of height on weight (standardized), for the full !Kung data."'

## R code 4.69 ####################
d_a$weight_s3 <- d_a$weight_s^3
m4.6 <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1 * weight_s + b2 * weight_s2 + b3 * weight_s3,
    a ~ dnorm(178, 20),
    b1 ~ dlnorm(0, 1),
    b2 ~ dnorm(0, 10),
    b3 ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ),
  data = d_a
)

## R code 4.67 ################
weight6.seq <- seq(from = -2.2, to = 2, length.out = 30)
pred_dat6_a <- list(weight_s = weight6.seq, weight_s2 = weight6.seq^2,
                    weight_s3 = weight6.seq^3)
mu6_a <- rethinking::link(m4.6, data = pred_dat6_a)
mu6.mean <- apply(mu6_a, 2, mean)
mu6.PI <- apply(mu6_a, 2, rethinking::PI, prob = 0.89)
sim5.height <- rethinking::sim(m4.6, data = pred_dat6_a)
height6.PI <- apply(sim5.height, 2, rethinking::PI, prob = 0.89)


## R code 4.68 #################
plot(height ~ weight_s, d_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))
graphics::lines(weight6.seq, mu6.mean)
rethinking::shade(mu6.PI, weight6.seq)
rethinking::shade(height6.PI, weight6.seq)


```

This cubic curve is even more flexible than the parabola, so it fits the
data even better.

But it's not clear that any of these models make a lot of sense. They
are good geocentric descriptions of the sample, yes. But there are two
problems. First, a better fit to the sample might not actually be a
better model. That's the subject of @sec-chap07-ulisses-compass. Second, the
model contains no biological information. We aren't learning any causal
relationship between height and weight. We'll deal with this second
problem much later, in @sec-chap16-genealized-linear-madness.

**Converting back to natural scale**

The plots @fig-find-post-full-data-a, @fig-polynomial-regression-a and
@fig-cubic-regression-a have standard units on the horizontal axis.
These units are sometimes called `z-scores`. But suppose you fit the
model using standardized variables, but want to plot the estimates on
the original scale. All that's really needed is first to turn off the
horizontal axis when you plot the raw data:

```{r}
#| label: fig-x-axis-turned-off-a
#| fig-cap: "Cubic regression with x-axis turned off: Rethinking version"
#| attr-source: '#lst-fig-x-axis-turned-off-a lst-cap="Cubic regression with x-axis turned off: Rethinking version"'

## R code 4.70 ###########
plot(height ~ weight_s, d_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5), xaxt = "n")
```

The `xaxt` at the end there turns off the horizontal axis. Then you
explicitly construct the axis, using the `graphics::axis()` function.

```{r}
#| label: fig-cubic-regression-natural-scale-a
#| fig-cap: "Cubic regression with x-axis in natural scale: Rehtinking version"
#| attr-source: '#lst-fig-cubic-regression-natural-scale-a lst-cap="Cubic regression with x-axis in natural scale: Rehtinking version"'

## R code 4.71 #############

plot(height ~ weight_s, d_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5), xaxt = "n")

at_a <- c(-2, -1, 0, 1, 2)                           # <1>
labels <- at_a * sd(d_a$weight) + mean(d_a$weight)   # <2>
axis(side = 1, at = at_a, labels = round(labels, 1)) # <3>
```

1.  Defines the location of the labels, in standardized units.
2.  Takes standardized units and converts them back to the original
    scale.
3.  Draws the axis.

Take a look at the help `?axis` for more details.

#### Splines

The second way to introduce a curve is to construct something known as a
`r glossary("B-spline", "spline")`. The word spline originally referred
to a long, thin piece of wood or metal that could be anchored in a few
places in order to aid drafters or designers in drawing curves. In
statistics, a spline is a smooth function built out of smaller,
component functions. There are actually many types of splines. The
`r glossary("B-spline")` we'll look at here is commonplace. The "B"
stands for "basis," which here just means "component." B-splines build
up wiggly functions from simpler less-wiggly components. Those
components are called basis functions. While there are fancier splines,
we want to start B-splines because they force you to make a number of
choices that other types of splines automate. You'll need to understand
B-splines before you can understand fancier splines.

To see how B-splines work, we'll need an example that is much
wigglier---that's a scientific term---than the !Kung stature data.
Cherry trees blossom all over Japan in the spring each year, and the
tradition of flower viewing follows. The timing of the blossoms can vary
a lot by year and century. Let's load a thousand years of blossom dates:

```{r}
#| label: load-cherry-blossoms-data-a
#| attr-source: '#lst-load-cherry-blossoms-data-a lst-cap="Load Cherry Blossoms data and display summary (rethinking version)"'

## R code 4.72 modified ######################
data(package = "rethinking", list = "cherry_blossoms")
d4_a <- cherry_blossoms
rethinking::precis(d4_a)
```

See `?cherry_blossoms` for details and sources. We're going to work with
the historical record of the **d**ay **o**f **y**ear of first bloom,
`doy`, for now. It ranges from 86 (late March) to 124 (early May). The
years with recorded blossom dates run from 812 CE to 2015 CE. You should
go ahead and plot `doy` against year to see. There might be some wiggly
trend in that cloud. It's hard to tell. (For the abbreviation CE see:
[Common Era (CE) and Before Common Era
(BCE)](https://www.timeanddate.com/calendar/ce-bce-what-do-they-mean.html))

```{r}
#| label: fig-scatterplot-cbl-a
#| fig-cap: "Display raw data for `doy` (Day of the year of first blossom) against the year: base R version"
#| attr-source: '#lst-fig-scatterplot-cbl-a lst-cap="Display raw data for `doy` against the year: base R version"'

plot(doy ~ year, data = d4_a)
```

There might be some wiggly trend in that cloud. It's hard to tell.

Let's try extracting a trend with a `r glossary("B-spline")`. B-splines
divide the full range of some predictor variable, like `year`, into
parts. Then they assign a parameter to each part. These parameters are
gradually turned on and off in a way that makes their sum into a fancy,
wiggly curve.

***
:::: {#prp-procedure-for-generating-b-splines}
Procedure for generating B-splines

::: callout-important


1.  Choose the number and distribution of knots
2.  Choose the polynomial degree
3.  Get the parameter weights for each basis function (define the model
    and make it run)
:::
::::
***

##### Choice of knots

Remember, the knots are just values of year that serve as pivots for our
spline. Where should the knots go? There are different ways to answer
this question. You can, in principle, put the knots wherever you like.
Their locations are part of the model, and you are responsible for them.
Let's do what we did in the simple example above, place the knots at
different evenly-spaced quantiles of the predictor variable. This gives
you more knots where there are more observations. We used only 5 knots
in the first example. Now let's go for 15:

```{r}
#| label: choose-knots-a
#| attr-source: '#lst-choose-knots-a lst-cap="Choose the knots that serve as pivots for the spline: rethinking version"'

## R code 4.73 ####################
d5_a <- d4_a[complete.cases(d4_a$doy), ] # complete cases on doy
num_knots15_a <- 15
(knot_list15_a <- quantile(d5_a$year, 
                           probs = seq(0, 1, length.out = num_knots15_a)))
```

##### Choice of polynomial degree

The next choice is polynomial degree. This determines how basis
functions combine, which determines how the parameters interact to
produce the spline. For degree 1, as in FIGURE 4.12, two basis functions
combine at each point. For degree 2, three functions combine at each
point. For degree 3, four combine. R already has a nice function that
will build basis functions for any list of knots and degree. This code
will construct the necessary basis functions for a degree 3 (cubic)
spline:

```{r}
#| label: compute-b-spline-matrix-a
#| attr-source: '#lst-compute-b-spline-matrix-a lst-cap="Compute the B-spline basis matrix for a cubic spline (degree 3): rethinking version"'

## R code 4.74 #######################
B_a <- splines::bs(d5_a$year,
  knots = knot_list15_a[-c(1, num_knots15_a)],
  degree = 3, intercept = TRUE
)
```

The B-spline basis matrix B_a should have 827 rows and 17 columns. Lets
check it:

-   Number of rows = `r nrow(B_a)`
-   Number of columns = `r ncol(B_a)`

Each row is a year, corresponding to the rows in the d5_a data frame.
Each column is a basis function, one of our synthetic variables defining
a span of years within which a corresponding parameter will influence
prediction.

To display the basis functions, just plot each column against year:

```{r}
#| label: fig-b-spline-matrix-a
#| fig-cap: "B-spline basis matrix for a cubic spline (degree 3): rethinking version"
#| attr-source: '#lst-fig-b-spline-matrix-a lst-cap="B-spline basis matrix for a cubic spline (degree 3): rethinking version"'

## R code 4.75 #############################
plot(NULL, xlim = range(d5_a$year), ylim = c(0, 1), xlab = "year", ylab = "basis")
for (i in 1:ncol(B_a)) lines(d5_a$year, B_a[, i])

```

The plot shows, just like the top in FIGURE 4.12., the basis functions.
However now more of these functions overlap.

##### Parameter weights for each basis function

Now to get the parameter weights for each basis function, we need to
actually define the model and make it run. The model is just a linear
regression. The synthetic basis functions do all the work. We'll use
each column of the matrix `B_a` as a variable. We'll also have an
intercept to capture the average blossom day. This will make it easier
to define priors on the basis weights, because then we can just conceive
of each as a deviation from the intercept.

This is also the first time we've used an
`r glossary("exponential distribution")` as a prior. Exponential
distributions are useful priors for scale parameters, parameters that
must be positive. The prior for `σ` is exponential with a rate of 1. The
way to read an exponential distribution is to think of it as containing
no more information than an average deviation. That average is the
inverse of the rate. So in this case it is $1/1 = 2$. If the rate were
0.5, the mean would be$1/0.5 = 2$. We'll use exponential priors for the
rest of the book, in place of uniform priors. It is much more common to
have a sense of the average deviation than of the maximum.

To build this model in `rethinking::quap()`, we just need a way to do
that sum. The easiest way is to use matrix multiplication.
@sec-matrix-multiplication has more details about why this works. The
only other trick is to use a start list for the weights to tell
`rethinking::quap()` how many there are.

```{r}
#| label: fit-model-m4.7
#| attr-source: '#lst-fit-model-m4.7 lst-cap="Fit the model of B-spline basis matrix m4.7"'

## R code 4.76 ################################
m4.7 <- rethinking::quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + B %*% w,
    a ~ dnorm(100, 10),
    w ~ dnorm(0, 10),
    sigma ~ dexp(1)
  ),
  data = list(D = d5_a$doy, B = B_a),
  start = list(w = rep(0, ncol(B_a)))
)
```

A summary with `rethinking::precis` won't reveal much.

```{r}
#| label: summarize-model-m4.7
#| attr-source: '#lst-summarize-model-m4.7 lst-cap="Summarize model m4.7"'

rethinking::precis(m4.7,depth = 2)
```

You should see 17 `w` parameters. But you can't tell what the model
thinks from the parameter summaries. Instead we need to plot the
posterior predictions. First, here are the weighted basis functions:

```{r}
#| label: fig-weighted-basis-function
#| fig-cap: "Weighted basis functions"
#| attr-source: '#lst-fig-weighted-basis-function lst-cap="Weighted basis functions"'

## R code 4.77 #################
post5_a <- rethinking::extract.samples(m4.7)
w <- apply(post5_a$w, 2, mean)
plot(NULL,
  xlim = range(d5_a$year), ylim = c(-6, 6),
  xlab = "year", ylab = "basis * weight"
)
for (i in 1:ncol(B_a)) lines(d5_a$year, w[i] * B_a[, i])

```

And finally we will plot the 97% posterior interval for `μ`, at each
year:

```{r}
#| label: fig-PI-cbl-a
#| fig-cap: "97% posterior interval for μ, at each year"
#| attr-source: '#lst-fig-PI-cbl-a lst-cap="97% posterior interval for μ, at each year"'

## R code 4.78 ######################
mu_cbl_a <- rethinking::link(m4.7)
mu_PI_cbl_a <- apply(mu_cbl_a, 2, rethinking::PI, 0.97)
plot(d5_a$year, d5_a$doy, 
     col = rethinking::col.alpha(rethinking::rangi2, 0.3), pch = 16)
rethinking::shade(mu_PI_cbl_a, d5_a$year, 
                  col = rethinking::col.alpha("black", 0.5))

```

The spline is much wigglier now. Something happened around 1500, for
example. If you add more knots, you can make this even wigglier. You
might wonder how many knots is correct. We'll be ready to (re)address
that question in a few more chapters.

Distilling the trend across years provides a lot of information. But
year is not really a causal variable, only a proxy for features of each
year. In the practice problems below, you'll compare this trend to the
temperature record, in an attempt to explain those wiggles.

##### Matrix multiplication in the spline model {#sec-matrix-multiplication}

Matrix algebra is a new way to represent ordinary algebra. It is often
much more compact. So to make model m4.7 easier to program, we used a
matrix multiplication of the basis matrix `B_a` by the vector of
parameters w: `B %*% w`. This notation is just linear algebra shorthand
for (1) multiplying each element of the vector `w` by each value in the
corresponding row of `B_a` and then (2) summing up each result. You
could also fit the same model with the following less-elegant code:

```{r}
#| label: fit-model-m4.7alt-a
#| attr-source: '#lst-fit-model-m4.7alt-a lst-cap="Fit model m4.7 with less-elegant code in matrix multiplication"'

## R code 4.79 ################################
m4.7alt <- rethinking::quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + sapply(1:827, function(i) sum(B[i, ] * w)),
    a ~ dnorm(100, 1),
    w ~ dnorm(0, 10),
    sigma ~ dexp(1)
  ),
  data = list(D = d5_a$doy, B = B_a),
  start = list(w = rep(0, ncol(B_a)))
)
rethinking::precis(m4.7alt, depth = 2)

```
Besides that @lst-fit-model-m4.7alt-a is not elegant you will notice that it need also more time to compile. Although all models are cached it seems that @lst-fit-model-m4.7alt-a needs more time to check that it is the same code.

```{r}
#| label: fig-PI-cbl2-a
#| fig-cap: "97% posterior interval for μ, at each year: Alternate matrix multiplication"
#| attr-source: '#lst-fig-PI-cbl-a lst-cap="97% posterior interval for μ, at each year with matrix multiplication manually"'

## R code 4.78 ######################
mu_cbl2_a <- rethinking::link(m4.7alt)
mu_PI_cbl2_a <- apply(mu_cbl2_a, 2, rethinking::PI, 0.97)
plot(d5_a$year, d5_a$doy, 
     col = rethinking::col.alpha(rethinking::rangi2, 0.3), pch = 16)
rethinking::shade(mu_PI_cbl2_a, d5_a$year, 
                  col = rethinking::col.alpha("black", 0.5))

```

#### Smooth functions for a rough world

The splines in the previous section are just the beginning. A entire
class of models, `r glossary("generalized additive model")`, focuses on
predicting an outcome variable using smooth functions of some predictor
variables.

***
:::: {#prp-resources-for-gams}
Resources for GAMs

::: callout-tip

-   Wood, S. N. (2017). Generalized Additive Models: An Introduction
    with R, Second Edition (2nd ed.). Taylor & Francis Inc.
-   SemanticScholar: [Series of paper dedicated to
    GAMs](https://www.semanticscholar.org/paper/Generalized-Additive-Models%3A-An-Introduction-with-R-G%C3%B3mez%E2%80%90Rubio/025f25133a5c1da746eb7e7719bb715b71a7f518)
-   Anish Singh Walia: [Generalized Additive
    Model](https://datascienceplus.com/generalized-additive-models/)
-   Noam Ross: [GAMs in
    R](https://noamross.github.io/gams-in-r-course/): A Free,
    Interactive Course using `mgcv`
-   Michael Clark: [Generalized Additive
    Models](https://m-clark.github.io/generalized-additive-models/)
-   Dheeraj Vaidya: [Generalized Additive
    Model](https://www.wallstreetmojo.com/generalized-additive-model/)

------------------------------------------------------------------------

-   Adam Shaif: What is Gneralised Additive Model? ([Medium member
    story](https://towardsdatascience.com/generalised-additive-models-6dfbedf1350a))
-   Eugenio Anello: Generalized Additive Models with R ([Medium member
    story](https://pub.towardsai.net/generalized-additive-models-with-r-5f01c8e52089))
:::

::::
***

## `TIDYVERSE`

### Why normal distributions?

I am not going to replicate the examples of @sec-why-normal-dist-a. They
do not help conceptually about the main issues why normal distributions
are wide-spread. But I will mention even the empty sub-chapter to get a
symmetry in the table of content.

#### EMPTY: Normal by addition

#### EMPTY: Normal by multiplication

#### EMPTY: Normal by log-multiplication

#### EMPTY: Using Gaussian distributions

### Model describing language

#### From model definition to Bayes’ theorem

We can use grid approximation to work through our globe tossing model.

```{r}
#| label: model-bayes-b
#| attr-source: '#lst-model-bayes-b lst-cap="Calculate globe tossing model with syntax from Bayes theorem"'


# how many `p_grid` points would you like?
n_points <- 100

d_bayes_b <-
  tibble(p_grid = seq(from = 0, to = 1, length.out = n_points),
         w      = 6, 
         n      = 9) %>% 
  mutate(prior      = dunif(p_grid, min = 0, max = 1),
         likelihood = dbinom(w, size = n, prob = p_grid)) %>%
  mutate(posterior = likelihood * prior / sum(likelihood * prior))

head(d_bayes_b)
```

In case you were curious, here's what they look like.

```{r}
#| label: fig-prior-likelihood-posterior-b
#| fig-cap: "Prior, likelihood and posterior distribution for globe tossing model calculated with syntax accoridng to Bayes' theorem"
#| attr-source: '#lst-fig-prior-likelihood-posterior-b lst-cap="Prior, likelihood and posterior distribution for globe tossing model calculated with syntax accoridng to Bayes theorem"'

d_bayes_b %>% 
  pivot_longer(prior:posterior) %>% 
  
  # dictate the order in which the panels will appear
  mutate(name = factor(name, levels = c("prior", "likelihood", "posterior"))) %>% 
    
  ggplot(aes(x = p_grid, y = value, fill = name)) +
  geom_area() +
  scale_fill_manual(values = c("blue", "red", "purple")) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(legend.position = "none") +
  facet_wrap(~ name, scales = "free")
```

The posterior is a combination of the prior and the likelihood. When the
prior is flat across the parameter space, the posterior is just the
likelihood re-expressed as a probability. As we go along, you'll see we
almost never use flat priors in practice. Be warned that eschewing flat
priors is a recent development, however. You only have to look at the
literature from a couple decades ago to see mounds and mounds of flat
priors.

### Gaussian model of height

#### The data

##### Show the data

```{r}
#| label: loading-data-from-package_b
#| attr-source: '#lst-loading-data-from-package_b lst-cap="Load data `Howell1` from the {**rethinking**} package without loading the package and assign the data to an object. Tidyverse"'

data(package = "rethinking", list = "Howell1")
d_b <- Howell1
```

```{r}
#| label: show-howell-data1-b

d_b |>
    glimpse()

```

`glimpse()` is the tidyverse analogue for `str()`.

```{r}
#| label: show-howell-data2-b
d_b |> 
    summary()
```

Kurz tells us that the {**brms**} package does not have a function that
works like `rethinking::precis()` for providing numeric and graphical
summaries of variables, as in the second part of
@lst-show-howell-data-a. Kurz suggests `base::summary()` to get some of
the information from `rethinking::precis()`.

```{r}
#| label: show-howell-data3-b
d_b |>            
    skimr::skim() 

```

I think `skimr::skim()` is a better option as an alternative to
`rethinking::precis()` as `base::summary()` because it also has a
graphical summary of the variables. {**skimr**} has many other useful
functions and is very adaptable. I propose to install and to try it out.

##### Select the height data of adults

With {**tidyverse**} we can isolate height values with the
`dplyr::select()` function and we are using the `dplyr::filter()`
function to make an adults-only data frame.

```{r}
#| label: select-height-adults-b
#| attr-source: '#lst-select-height-adults-b lst-cap="Select the height data of adults (individuals older or equal than 18 years): tidyverse version"'

d_b %>%
  select(height) %>% 
  glimpse()

d2_b <- 
  d_b %>%
  filter(age >= 18) 
 
glimpse(d2_b)
```

The two functions of @lst-select-height-adults-b are much more readable
and understandable as the weird base R syntax in
@lst-select-height-adults-a.

#### The model

##### Plot the distribution of heights

The plot of the heights distribution compared with the standard Gaussian
distribution is missing in Kurz's version. I added this plot by using
the last example of [How to Plot a Normal Distribution in
R](https://www.statology.org/plot-normal-distribution-r/). It uses the `ggplot2::stat_function()` to compute and draw a function as a continuous curve. This makes it easy to superimpose a function on top of an existing plot.

```{r}
#| label: fig-dist-heights-b
#| fig-cap: "The distribution of the heights data, overlaid by an ideal Gaussian distribution: tidyverse version"
#| attr-source: '#lst-fig-dist-heights-b lst-cap="Plot the distribution of the heights of adults, overlaid by an ideal Gaussian distribution: tidyverse version"'

p0 <- 
    d2_b |> 
    ggplot(aes(height)) +
    geom_density() +

    stat_function(
        fun = dnorm,
        args = with(d2_b, c(mean = mean(height), sd = sd(height)))
        ) +
    scale_x_continuous("Height in cm")

p0
```

##### Plot the mean prior (mu)

Here is the shape for the prior $μ \sim Normal(178, 20)$.

```{r}
#| label: fig-mean-prior-b
#| fig-cap: "Plot of the chosen mean prior: tidyverse version"
#| attr-source: '#lst-fig-mean-prior-b lst-cap="Plot of the chosen mean prior: tidyverse version"'

p1 <-
  tibble(x = seq(from = 100, to = 250, by = .1)) %>% 
    
  ggplot(aes(x = x, y = dnorm(x, mean = 178, sd = 20))) +
  geom_line() +
  scale_x_continuous(breaks = seq(from = 100, to = 250, by = 25)) +
  labs(title = "mu ~ dnorm(178, 20)",
       y = "density")

p1
```

As there is only one variable y (= `dnorm(x, mean = 178, sd = 20)`) we need to specify x as a sequence of 1501 points t provide a x and y aesthetic for the plot.


##### Plot the prior of the standard deviation (sigma)

And here's the ggplot2 code for our prior for `σ`, a uniform
distribution with a minimum value of 0 and a maximum value of 50. We
don't really need the `y`-axis when looking at the shapes of a density,
so we'll just remove it with `scale_y_continuous()`.

```{r}
#| label: fig-sd-prior-b
#| fig-cap: "Plot the chosen prior for the standard deviation: tidyverse version"
#| attr-source: '#lst-fig-sd-prior-b lst-cap="Plot the chosen prior for the standard deviation: tidyverse version"'

p2 <-
  tibble(x = seq(from = -10, to = 60, by = .1)) %>%
  
  ggplot(aes(x = x, y = dunif(x, min = 0, max = 50))) +
  geom_line() +
  scale_x_continuous(breaks = c(0, 50)) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("sigma ~ dunif(0, 50)")

p2
```

##### Prior predictive simulation

`r glossary("Prior predictive simulation")` is very useful for assigning sensible priors, because it can be quite hard to anticipate how priors influence the observable variables. 

We can simulate from both priors at once to get a prior probability
distribution of `height`.

```{r}
#| label: fig-prior-predictive-sim-b
#| fig-cap: "Simulate heights by sampling from the prior: tidyverse version"
#| attr-source: '#lst-fig-prior-predictive-sim-b lst-cap="Simulate heights by sampling from the prior: tidyverse version"'

n <- 1e4
set.seed(4)

sim <-
  tibble(sample_mu_b    = rnorm(n, mean = 178, sd  = 20),
         sample_sigma_b = runif(n, min = 0, max = 50)) %>% 
  mutate(height = rnorm(n, mean = sample_mu_b, sd = sample_sigma_b))
  
p3 <- sim %>% 
  ggplot(aes(x = height)) +
  geom_density(fill = "deepskyblue") +
  scale_x_continuous(breaks = c(0, 73, 178, 283)) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("height ~ dnorm(mu, sigma)") +
  theme(panel.grid = element_blank())

p3
```

`ggplot2::geom_density()` computes and draws kernel density estimates,  which is a smoothed version of the histogram. Note that there is no data mentioned explicitly in the call of `ggplot2::geom_density()`. When this is the case (data = `NULL`) then the data will be inherited from the plot data as specified in the call to `ggplot2::ggplot()`. Otherwise the function needs a data frame to override the plot data or a function with a single argument, the plot data. ([geom_density() help file](https://ggplot2.tidyverse.org/reference/geom_density.html)).


If you look at the `x`-axis breaks on the plot in McElreath's lower left
panel in Figure 4.3, you'll notice they're intentional. To compute the
mean and 3 standard deviations above and below, you might do this.

```{r}
#| label: compute-mean-3sd-b
sim %>% 
  summarise(ll   = mean(height) - sd(height) * 3,
            mean = mean(height),
            ul   = mean(height) + sd(height) * 3) %>% 
  mutate_all(round, digits = 1)
```

Here's the work to make the lower right panel of Figure 4.3.

```{r}
#| label: fig-reproduce-4.3-low-right
#| fig-cap: "Reproduce lower right panels of Figure 4.3"
#| attr-source: '#lst-fig-reproduce-4.3-low-right lst-cap="Reproduce lower right panels of Figure 4.3"'


# simulate
set.seed(4)

sim <-
  tibble(sample_mu_b    = rnorm(n, mean = 178, sd = 100),
         sample_sigma_b = runif(n, min = 0, max = 50)) %>% 
  mutate(height = rnorm(n, mean = sample_mu_b, sd = sample_sigma_b))

# compute the values we'll use to break on our x axis
breaks <-
  c(mean(sim$height) - 3 * sd(sim$height), 0, mean(sim$height), mean(sim$height) + 3 * sd(sim$height)) %>% 
  round(digits = 0)

# this is just for aesthetics
text <-
  tibble(height = 272 - 25,
         y      = .0013,
         label  = "tallest man",
         angle  = 90)

# plot
p4 <-
  sim %>% 
  ggplot(aes(x = height)) +
  geom_density(fill = "deepskyblue", color = "black") +
  geom_vline(xintercept = 0, color = "black") +
  geom_vline(xintercept = 272, color = "black", linetype = 3) +
  geom_text(data = text,
            aes(y = y, label = label, angle = angle),
            color = "black") +
  scale_x_continuous(breaks = breaks) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("height ~ dnorm(mu, sigma)\nmu ~ dnorm(178, 100)") +
  theme(panel.grid = element_blank())

p4
```

Let's combine the four to make our version of McElreath's Figure 4.3.

```{r}
#| label: fig-reproduce-3.4
#| fig-cap: "Reproduction of Figure 3.4"
#| attr-source: '#lst-fig-reproduce-3.4 lst-cap="Reproduction of Figure 3.4"'

# library(patchwork) ## already in setup chunk
(p1 + xlab("mu") | p2 + xlab("sigma")) / (p3 | p4)
```

On page 84, McElreath said his prior simulation indicated 4% of the
heights would be below zero. He also drew the break down compared to the
tallest man on record, [Robert Pershing
Wadlow](https://en.wikipedia.org/wiki/Robert_Wadlow) (1918--1940).

```{r}
#| label: calc-breaks-b

sim %>% 
  count(height < 0) %>% 
  mutate(percent = 100 * n / sum(n))

sim %>% 
  count(height < 272) %>% 
  mutate(percent = 100 * n / sum(n))
```

#### Grid approximation of the posterior distribution

With grid approximation we are going to use the brute force method for
the calculation of the posterior distribution. This technique has
limited relevance. Later on we will use the quadratic approximation with
`brms::brm()`.

It is the same technique we have used in
@sec-sampling-from-a-grid-approximate-posterior respectively in the
tidyverse version in @sec-grid-approximation-b. As there is no
conceptually new information to learn, I am not going into the details
of the following code. (It combines several code chunk from Kurz's
version.) But I am going to foreshadow the most important differences in
the tidyverse approach of the grid approximation technique:

Instead of `base::grid_expand()` we will use `tidyr::crossing()` Instead
of `base::sapply()` we will use `purr::map2()`

The produced tibble contains data frames in its cells, so that we have
to use the `tidyr::unnest()` function to expand the list-column
containing data frames into rows and columns.

Referring to the plots:

-   Instead of `rethinking::contour_xyz()` we will use
    `ggplot2::geom_contour()`
-   Instead of `rethinking::image_xyz()` we will use
    `ggplot2::geom_raster()`

```{r}
#| label: fig-grid-approx-posterior-b
#| attr-source: '#lst-fig-grid-approx-posterior-b lst-cap="Grid Approximation of the posterior distribution: tidyverse version"'
#| fig-cap: "Grid Approximation of the posterior distribution"
#| results: hold

n <- 200

d_grid_b <-
  # we'll accomplish with `tidyr::crossing()` what McElreath did with base R `expand.grid()`
  crossing(mu_b    = seq(from = 140, to = 160, length.out = n),
           sigma_b = seq(from = 4, to = 9, length.out = n))

glimpse(d_grid_b)

grid_function <- function(mu, sigma) {
  
  dnorm(d2_b$height, mean = mu, sd = sigma, log = TRUE) %>% 
    sum()
  
}

d_grid2_b <-
  d_grid_b %>% 
  mutate(log_likelihood_b = map2(mu_b, sigma_b, grid_function)) %>%
  unnest(log_likelihood_b) %>% 
  mutate(prior_mu_b    = dnorm(mu_b, mean = 178, sd = 20, log = T),
         prior_sigma_b = dunif(sigma_b, min = 0, max = 50, log = T)) %>% 
  mutate(product_b = log_likelihood_b + prior_mu_b + prior_sigma_b) %>% 
  mutate(probability_b = exp(product_b - max(product_b)))
  
head(d_grid2_b)
```

```{r}
#| label: fig-contour-b
#| fig-cap: "Draw 2D contours of a 3D surface"
#| attr-source: '#lst-fig-contour-b lst-cap="Draw 2D contours of a 3D surface: tidyverse version"'

d_grid2_b %>% 
  ggplot(aes(x = mu_b, y = sigma_b, z = probability_b)) + 
  geom_contour() +
  labs(x = expression(mu),
       y = expression(sigma)) +
  coord_cartesian(xlim = range(d_grid2_b$mu_b),
                  ylim = range(d_grid2_b$sigma_b)) +
  theme(panel.grid = element_blank())

```

```{r}
#| label: fig-heatmap-b
#| fig-cap: "Draw heat map"
#| attr-source: '#lst-fig-heatmap-b lst-cap="Draw heat map: tidyverse version"'

d_grid2_b %>% 
  ggplot(aes(x = mu_b, y = sigma_b, fill = probability_b)) + 
  geom_raster(interpolate = TRUE) +
  scale_fill_viridis_c(option = "B") +
  labs(x = expression(mu),
       y = expression(sigma)) +
  theme(panel.grid = element_blank())
```

#### Sampling from the posterior

We can use `dplyr::sample_n()` to sample rows, with replacement, from
`d_grid2_b`.

```{r}
#| label: fig-posterior-sample-b
#| fig-cap: "Samples from the posterior distribution for the heights data. The density of points is highest in the center, reflecting the most plausible combinations of μ and σ. There are many more ways for these parameter values to produce the data, conditional on the model. (tidyverse version)"
#| attr-source: '#lst-fig-posterior-sample-b lst-cap="Samples from the posterior distribution for the heights data"'


set.seed(4)

d_grid_samples_b <- 
  d_grid2_b %>% 
  sample_n(size = 1e4, replace = T, weight = probability_b)

d_grid_samples_b %>% 
  ggplot(aes(x = mu_b, y = sigma_b)) + 
  geom_point(size = .9, alpha = 1/15) +
  scale_fill_viridis_c() +
  labs(x = expression(mu[samples]),
       y = expression(sigma[samples])) +
  theme(panel.grid = element_blank())
```

We can use `tidyr::pivot_longer()` and then `ggplot2::facet_wrap()` to
plot the densities for both `mu` and `sigma` at once.

```{r}
#| label: fig-densities-mu-sigma
#| fig-cap: "Shapes of the marginal posterior densities of μ and σ: tidyverse version"
#| attr-source: '#lst-fig-densities-mu-sigma lst-cap="Plot shapes of the marginal posterior densities of μ and σ: tidyverse version"'

d_grid_samples_b %>% 
  pivot_longer(mu_b:sigma_b) %>% 

  ggplot(aes(x = value)) + 
  geom_density(fill = "deepskyblue", color = "black") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ name, scales = "free", labeller = label_parsed)
```

We'll use the {**tidybayes**} package to compute their posterior modes
and 95% HDIs.

::: callout-important
There is a companion package {**ggdist**} which is imported completely
by {**tidybayes**}. Whenever you cannot find the function in
{**tidybayes**} then look at the documentation of {**ggdist**}. This is
also the case for the `tidybayes::mode_hdi()` function. In the help
files of {**tidybayes**} you will just find notes about a deprecated
`tidybayes::mode_hdih()` function but not the arguments of its new
version without the last `h` (for horizontal) `tidybayes::mode_hdi()`.
But you can look up these details in the {**ggdist**} documentation.
This observation is valid for many families of deprecated functions.

There is a division of functionality between {**tidybayes**} and
{**ggdist**}:

-   {**tidybayes**}: Tidy Data and 'Geoms' for Bayesian Models: Compose
    data for and extract, manipulate, and visualize posterior draws from
    Bayesian models in a tidy data format. Functions are provided to
    help extract tidy data frames of draws from Bayesian models and that
    generate point summaries and intervals in a tidy format.
-   {**ggdist**}: Visualizations of Distributions and Uncertainty:
    Provides primitives for visualizing distributions using
    {**ggplot2**} that are particularly tuned for visualizing
    uncertainty in either a frequentist or Bayesian mode. Both
    analytical distributions (such as frequentist confidence
    distributions or Bayesian priors) and distributions represented as
    samples (such as bootstrap distributions or Bayesian posterior
    samples) are easily visualized.
:::

::: callout-todo
###### TODO: ggdist \<-\> tidybayes {.unnumbered}

Replace `tidyverse::` with `ggdist::` where appropriate. This is
important when you want to find the related help file.
:::

```{r}
#| label: post-mode-hdi95-b

d_grid_samples_b %>% 
  pivot_longer(mu_b:sigma_b) %>% 
  group_by(name) %>% 
  tidybayes::mode_hdi(value) 
```

Let's say you wanted their posterior medians and 50% quantile-based
intervals, instead. Just switch out the last line for
`tidybayes::median_qi(value, .width = .5)`.

```{r}
#| label: post-median-qi90-b

d_grid_samples_b %>% 
  pivot_longer(mu_b:sigma_b) %>% 
  group_by(name) %>% 
  tidybayes::median_qi(value, .width = .5)
```

**Sample size and the normality of σ's posterior**

I will skip this part as there is nothing conceptually new in this
section.

#### Finding the posterior distribution with brm()

> In the text, McElreath indexed his models with names like `m4.1`. I
> will largely follow that convention, but will replace the *m* with a
> *b* to stand for the **`brms`** package.

Here's how to fit the first model for this chapter.

::: callout-tip
###### Code slightly changed

I have worked my way through the `brms::brm()` help file. From its more than 40 (!) arguments I have concentrated my learning on those arguments, that are used explicitly by Kurz.

But to understand the syntax I had made some minor changes:

1. Instead of `height ~ 1` I wrote the argument with it argument name `formula = height ~ 1" and changed put it as first argument.
2. Instead of `family = gaussian` I wrote `family = gaussian()`as in the help file mentioned.

To understand the syntax it is also quite revealing to look at @tbl-mirror-rethinking-tidyverse.

To learn more read [Defining statistical models; formulae](https://cran.r-project.org/doc/manuals/R-intro.html#Formulae-for-statistical-models) and Chapter 11 of [Introduction to R](https://cran.r-project.org/doc/manuals/R-intro.pdf).
:::

The following table I have inserted here from Kurz's explanation of model `b4.3`. It is the first time that I used brms::brm() and therefore I need slow down here and go into the gritty details.

> Unlike with McElreath's {**rethinking**} package, the conventional
`brms::brm()` syntax doesn't mirror the statistical notation. But here
are the analogues to the exposition at the bottom of page 97:

| {**rethinking**} package                                    | {**brms**} package: `brms::brm()`          |
|------------------------------------|------------------------------------|
| $\text{height}_i \sim \operatorname{Normal}(\mu_i, \sigma)$ | `family = gaussian`                        |
| $\mu_i = \alpha + \beta \text{weight}_i$                    | `height ~ 1 + weight_c`                 |
| $\alpha \sim \operatorname{Normal}(178, 20)$                | `prior(normal(178, 20), class = Intercept` |
| $\beta \sim \operatorname{Log-Normal}(0, 1)$                | `prior(lognormal(0, 1), class = b)`        |
| $\sigma \sim \operatorname{Uniform}(0, 50)$                 | `prior(uniform(0, 50), class = sigma)`     |

: Mirror the statistical notation of the rethinking package with the
tidyverse approach using the brms package {#tbl-mirror-rethinking-tidyverse}

```{r}
#| label: post-dist-brms-b4.1
#| attr-source: '#lst-post-dist-brms-b4.1 lst-cap="Finding the posterior distribution with brms::brm()"'

b4.1 <- 
  brms::brm(
      formula = height ~ 1,                                           # <1>
      data = d2_b,                                                    # <2>
      family = gaussian(),                                            # <3>
      prior = c(brms::prior(normal(178, 20), class = Intercept),      # <4>
                brms::prior(uniform(0, 50), class = sigma, ub = 50)), # <4>
      iter = 2000,               # <5>
      warmup = 1000,             # <6>
      chains = 4,                # <7>
      cores = 4,                 # <8>  
      seed = 4,                  # <9>
      file = "fits/b04.01")      # <10>
```

1. **formula** describes the relation between dependent and independent variables in the form of a linear model. The left hand side are the dependent variables, the right hand side the independent. The independent variables are used to calculate the trend component of the linear model, the residuals are then assumed to have some kind of distribution. When the independent are equal to one `~ 1`, the trend component is a single value, e.g. the mean value of the data, i.e. the linear model only has an intercept. ([StackOverflow](https://stackoverflow.com/a/13366973/7322615)) In other words, it is the value the dependent variable is expected to have when the independent variables are zero or have no influence.  ([StackOverflow](https://stackoverflow.com/a/13367260/7322615)). The formula `y ~ 1` is just a model with a constant (intercept) and no regressor ([StackOverflow](https://stackoverflow.com/questions/53812741/tilde-operator-in-r)).
2. **data**: A data frame that contains all the variables used in the model.
3. **family**: A description of the response distribution and link function to be used in the model. This can be a family function, a call to a family function or a character string naming the family. By default a linear `gaussian` model is applied. So this line would not have been necessary. There are [standard family functions `stats::family()`](https://www.stat.ethz.ch/R-manual/R-devel/library/stats/html/family.html)that will work with {**brms**}, but there are also [special family functions `brms::brmsfamily()`](https://paul-buerkner.github.io/brms/reference/brmsfamily.html) that work only for {**brms**} models. Additionally you can [specify custom families](https://paul-buerkner.github.io/brms/reference/custom_family.html) for use in brms with the `brms::custom_family()` function.
4. **prior**: The next two lines specify priors for the normal and the uniform distribution. As you can see this is another place where parts for the formula are provided for the `brms::brm()` function work. --- `class` specifies the parameter class. It defaults to "b" ((i.e. population-level -- 'fixed' -- effects)). (There is also the argument `group` for grouping of factors for group-level effects. Not used in this code example.) --- Besides the "b" class there are other classes for the "Intercept" and the standard deviation "Sigma" on the population level: There is also a "sd" class for the standard deviation of group-level effects. Finally there is the special case of `class = "cor"` to set the same prior on every correlation matrix. --- `ub = 50` sets the upper bound to 50. There is also a `lb` (lower bound). Both bounds are for parameter restriction, so that population-level effects must fall within a certain interval using the `lb` and `ub` arguments. `lb` and `ub` default to `NULL`, i.e. there is no restriction.
5. **iter**: Number of total iterations per chain (including `warmup`; defaults to 2000).
6. **warmup**: A positive integer specifying number of warmup iterations. This also specifies the number of iterations used for stepsize adaptation, so warmup draws should not be used for inference. The number of warmup should not be larger than iter and the default is iter/2.
7. **chains**: Number of `r glossary("Markov chain", "Markov chains")` (defaults to 4). 
8. **cores**: Number of cores to use when executing the chains in parallel, which defaults to 1 but we recommend setting the `mc.cores` option to be as many processors as the hardware and RAM allow (up to the number of chains).
9. **seed**: The seed for random number generation to make results reproducible. Kurz has always used for `set.seed()` in other code chunks the chapter number. If `NA` (the default), Stan will set the seed randomly.
10. **file**: Either `NULL` or a character string. In the latter case, the fitted model object is saved via `base::saveRDS()` in a file named after the string supplied in file. The `.rds` extension is added automatically. If the file already exists, `brms::brm()` will load and return the saved model object instead of refitting the model. Unless you specify the `file_refit` argument as well, the existing files won't be overwritten, you have to manually remove the file in order to refit and save the model under an existing file name. The file name is stored in the brmsfit object for later usage.

If you want detailed diagnostics for the HMC chains, call
`brms::launch_shinystan(b4.1)`. That'll keep you busy for a while. See
the [ShinyStan website](https://mc-stan.org/users/interfaces/shinystan)
for more information.

***
:::: {#prp-shinystan}
Launch of shinystan turned off

::: callout-warning

I turned off the evaluation of the following chunk that should launch {*shinystan*). Besides that the evaluation takes time
I am not yet ready to understand the many configurable options programmed with a {**shiny**) interface. 
::::

:::
***


A short investigation showed that there are several pages of documentation of different packages that I would need to study. These include:

:::: {prp-stan-literature}

Package documentation of Stan and friends

::: callout-tip

- [Interface to shinystan](https://paul-buerkner.github.io/brms/reference/launch_shinystan.brmsfit.html) (`brms::launch_shinystan`)
- I believe that it is also very important to understand [RStan: the R interface to Stan](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html).
- Possibly I should read [Using the ShinyStan GUI with rstanarm models](https://mc-stan.org/rstanarm/reference/launch_shinystan.stanreg.html)
- And maybe it could be also helpful to read selected chapters from the [rstanarm documentation](https://mc-stan.org/rstanarm/index.html), from the [{**rstan**} documentation](https://mc-stan.org/rstan/) or generally from [Stan User’s Guide](https://mc-stan.org/docs/stan-users-guide/index.html) resp. [Stan Language Reference Manual](https://mc-stan.org/docs/reference-manual/index.html. 

Ooops, this opens up Pandora's box!

I do not even understand completely what the different packages do. What follows is a first try where I copied from the documentation pages:

> {**rstanarm**} is an R package that emulates other R model-fitting functions but uses Stan (via the {**rstan**} package) for the back-end estimation. The primary target audience is people who would be open to Bayesian inference if using Bayesian software were easier but would use frequentist software otherwise.

> RStan is the R interface to Stan. It is distributed on CRAN as the {**rstan**} package and its source code is hosted on GitHub.

> Stan is a state-of-the-art platform for statistical modeling and high-performance statistical computation.

:::

::::
***

```{r}
#| label: detailed-diganostic-chains-brms-b4.1
#| attr-source: '#lst-detailed-diganostic-chains-brms-b4.1 lst-cap="Detailed diagnostic using {**shinystan**}"'

# brms::launch_shinystan(b4.1)

```

```{r}
#| label: print-summary-brms-b4.1

brms:::print.brmsfit(b4.1)

```
For the interpretation of this output I am going to use the explication in the [How to use brms](https://github.com/paul-buerkner/brms#how-to-use-brms) section of the {**brms**} GitHup page.

1. **Top**: On the top of the output, some general information on the model is given, such as family, formula, number of iterations and chains.
2. **Upper Middle**: If the data were grouped the next part would display group-level effects separately for each grouping factor in terms of standard deviations and (in case of more than one group-level effect per grouping factor) correlations between group-level effects. (This part is absent above as there are no grouping factors.)
3. **Lower Middle: here Middle**: Next follow the display of the population-level effects (i.e. regression coefficients). If incorporated, autocorrelation effects and family specific parameters (e.g., the residual standard deviation ‘sigma’ in normal models) are also given.
    In general, every parameter is summarized using the mean (`Estimate`) and the standard deviation (`Est.Error`) of the posterior distribution as well as two-sided 95% credible intervals (`l-95% CI` and `u-95% CI`) based on quantiles. The last three values (`ESS_bulk`, `ESS_tail`, and `Rhat`) provide information on how well the algorithm could estimate the posterior distribution of this parameter. If `Rhat` is considerably greater than 1, the algorithm has not yet converged and it is necessary to run more iterations and / or set stronger priors.
4. **Bottom**: The last part is some short explanation of the sampling procedure. `r glossary("NUTS")` stands for **No U-Turn Sampler** and is a Hamiltonian Monte Carlo (HMC) Method. This means that it is not a `r glossary("Markov Chain")` method and thus, this algorithm avoids the random walk part, which is often deemed as inefficient and slow to converge.
    Instead of doing the random walk, NUTS does jumps of length x. Each jump doubles as the algorithm continues to run. This happens until the trajectory reaches a point where it wants to return to the starting point.

```{r}
#| label: print-stan-like-summary-brms-b4.1

b4.1$fit
```



```{r}
#| label: fig-plot-b4.1
#| fig-cap: "Plot model b4.1"
#| attr-source: '#lst-fig-plot-b4.1 lst-cap="Plot model b4.1"'

brms:::plot.brmsfit(b4.1)
```

Whereas rethinking defaults to 89% intervals, using `print()` or
`summary()` with {**brms**} models defaults to 95% intervals.

::: callout-note
As I have learned shortly: `print()` or `summary()` are generic
functions where one can add new printing methods with new classes. In
this case `class(b4.1)` = `r class(b4.1)`. This means I do not need to
add `brms::` to secure that I will get the {**brms**} printing or
summary method as I didn't load the {**brms**} package. Quite the
contrary: Adding `brms::` would result into the message: "Error:
'summary' is not an exported object from 'namespace:brms'".

As I really want to specify explicitly the method these generic
functions should use, I need to use the syntax with *three* columns, like `brms:::print.brmsfit()` or `brms:::summary.brmsfit()` respectively.

In this respect I have to learn more about S3 classes. There are many
important web resources about this subject that I have found with the
search string "r what is s3 class". Maybe I should start with the [S3
chapter in Advanced R](https://adv-r.hadley.nz/s3.html).
:::

Unless otherwise specified, Kurz will stick with 95% intervals
throughout. To get those 89% intervals or McElreath approach, one could
use the `prob` argument within `summary()` or `print()`.

```{r}
#| label: summary-interval-.89-brms-b4.1

brms:::summary.brmsfit(b4.1, prob = .89)

```

Here's the `brms::brm()` code for the model with the very narrow `μ`
prior corresponding to the `rethinking::quap()` code in
@lst-post-dist-quap-m4.2.

```{r}
#| label: fig-post-dist-brms-b4.2
#| fig-cap: "Finding the posterior distribution with a narrower prior using brms::brm()"
#| att-source: '#lst-post-dist-brms-b4.2 lst-cap="Finding the posterior distribution with a narrower prior using brms::brm()"'

b4.2 <- 
  brms::brm(data = d2_b, 
      family = gaussian,
      height ~ 1,
      prior = c(brms::prior(normal(178, 0.1), class = Intercept),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.02")

brms:::plot.brmsfit(b4.2, widths = c(1, 2))

```

And here's the model `summary()`.

```{r}
#| label: summary-narrow-prior

brms:::summary.brmsfit(b4.2)

```

Subsetting the `summary()` output with `$fixed` provides a convenient
way to compare the Intercept summaries between `b4.1` and `b4.2`.

```{r}
#| label: compare-summaries-b4.1-b4.2

rbind(brms:::summary.brmsfit(b4.1)$fixed,
      brms:::summary.brmsfit(b4.2)$fixed)

```

#### Sampling from a ~~quap()~~ `brm()` fit

{**brms**} doesn't seem to have a convenience function that works the
way `vcov()` does for {**rethinking**}.

```{r}
#| label: calc-var-cov-m4.1-b
#| attr-source: '#lst-calc-var-cov-m4.1-b lst-cap="Calculation of vcov(): tidyverse version."'

brms:::vcov.brmsfit(b4.1)
```

This only returns the first element in the matrix it did for
{**rethinking**}. That is, it appears `brms::vcov()` only returns the
variance/covariance matrix for the single-level `_β_` parameters.

::: callout-caution
###### brms::vcov()

Referring to a similar situation with `rethinking::vcov()` in
@lst-vcov-matrix-m4.1 I cannot write `brms::vcov()`, but have to use
either `brms:::vcov.brmsfit(b4.1)` or just `vcov(b4.1)`. The weird thing
is that the first time it also works with `brms::vcov()` but only the
first time!
:::

However, if you really wanted this information, you could get it after
putting the Hamilton Monte Carlo (HMC) chains in a data frame. We do
that with the `brms::as_draws_df()` function:

```{r}
#| label: put-hmc-into-df-b
#| attr-source: '#lst-put-hmc-into-df-b lst-cap="Extract the iteration of the Hamiliton Monte Carlo (HMC) chains into a data frame"'

post_b <- brms::as_draws_df(b4.1)

head(post_b)
```

::: callout-tip
###### draws object

The functions of the family `as_draws()` transform `brmsfit` objects to
`draws` objects, a format supported by the {**posterior**} package.
{**brms**} currently imports the family of `as_draws()`functions from the
{**posterior**} package, a tool for working with posterior
distributions, i.e. for fitting Bayesian models or working with output from Bayesian models. (See as an introduction [The posterior R package](https://mc-stan.org/posterior/articles/posterior.html))

@lst-put-hmc-into-df-b produced the {**brms**} version of what McElreath
achieved with `extract.samples()` in @lst-extract-samples-m4.1-a.
However, what happened under the hood was different. Whereas {**rethinking**} used the `mvnorm()` function from the {**MASS**} package with {**brms**} we just extracted the iterations of the `r glossary("HMC")` chains and put them in a data frame. 

It’s also noteworthy that the `as_draws_df()` is part of a larger class of `as_draws()` functions {**brms**} currently imports from the {**posterior**} package. 

```{r}
#| label: show-class-post-dist
#| attr-source: '#lst-show-class-post-dist lst-cap="Show classes of the posterior distribution post_b, an object created by as_draws() function"'

class(post_b)
```

Besides of class `tbl_df` and `tbl`, [subclasses of data.frame with different behavior](https://tibble.tidyverse.org/reference/tbl_df-class.html) the `as_draws_df()` function has created the `draws` class, the parent class of all supported [draws formats](https://mc-stan.org/posterior/articles/posterior.html#draws-formats). 
:::

Now `select()` the columns containing the draws from the desired
parameters `b_Intercept` and `sigma` and feed them into `cov()`.

```{r}
#| label: calc-cov-post-b
#| attr-source: '#lst-calc-cov-post-b lst-cap="Calculate the vector of variances and correlation matrix for b_Intercept and sigma"'

select(post_b, b_Intercept:sigma) %>% 
  stats::cov()
```

@lst-calc-cov-post-b displays "(1) a vector of variances for the
parameters and (2) a correlation matrix" for them (p. 90). Here are just
the variances (i.e., the diagonal elements) and the correlation matrix.

```{r}
#| label: calc-var-post-b
#| attr-source: '#lst-calc-var-post-b lst-cap="Calculate only variances (the diagonal values)"'

select(post_b, b_Intercept:sigma) %>%
  stats::cov() %>%
  base::diag()

```

```{r}
#| label: calc-corr-matrix-post-b
#| attr-source: '#lst-calc-corr-matrix-post-b lst-cap="Calculate only crrelation"'

# correlation
post_b %>%
  select(b_Intercept, sigma) %>%
  stats::cor()
```

```{r}
#| label: show-data-structure

str(post_b)
```

The `post_b` object is not just a data frame, but also of class
`draws_df`, which means it contains three metadata variables ----
`.chain`, `.iteration`, and `.draw` --- which are often hidden from
view, but are there in the background when needed. As you'll see, we'll
make good use of the `.draw` variable in the future. Notice how our post
data frame also includes a vector named `lp__`. That's the log
posterior.

For details, see: - The [Log-Posterior (function and
gradient)](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html#the-log-posterior-function-and-gradient)
section of the Stan Development Team's (2023) vignette [RStan: the R
interface to
Stan](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html)
and - Stephen Martin's [explanation of the log
posterior](https://discourse.mc-stan.org/t/basic-question-what-is-lp-in-posterior-samples-of-a-brms-regression/17567/2)
on the Stan Forums.

::: callout-caution
###### Summaries of {brms} and {posterior} packages

Kurz claims that `summary()` function doesn't work for {**brms**}
posterior data frames quite the way `rethinking::precis()` does for
posterior data frames from the {**rethinking**} package. But I think his
observation is somewhat misleading.

The posterior data frame `post_b` is not of class `brms`. Let's check
this:

```{r}
#| label: class-post_b-b

class(post_b)
```

The class `draws_df`and `draws` refers to the {**posterior**} and not to
the {**brms**} package. Remember: In @lst-put-hmc-into-df-b we
transformed with the function `as_draws_df` the `brms` object into a
`draws_df` and `draws` object.

Therefore Kurz's claim should be read: The `summary()` function doesn't
work for {**posterior**} posterior data frames quite the way
`rethinking::precis()` does for posterior data frames from the
{**rethinking**} package. Instead of calling `brms:::summary.brmsfit()`
I will use `posterior:::summary.draws()`.

I wouldn't have noticed this difference if I hadn't mentioned explicitly
the name of the packages in front of the function, because in that case
R would have used `base::summary()` as in Kurz's text.
:::

```{r}
#| label: base-summary-samples-b4.1-b
#| attr-source: '#lst-base-summary-samples-b4.1-b lst-cap="Summary the extracted iterations of the HMC chains: base version"' 

base::summary(post_b[, 1:2])

```

```{r}
#| label: posterior-summary-samples-b4.1-b
#| attr-source: '#lst-posterior-summary-samples-b4.1-b lst-cap="Summary the extracted iterations of the HMC chains: posterior version"' 

posterior:::summary.draws(post_b[, 1:2])


```

To get a similar summary with tiny histograms Kurz offers different
solutions:

-   A base R approach by using the transpose of a `stats::quantile()`
    call nested within `base::apply()`
-   A {**tidyverse**} approach
-   A {**brms**} approach by just putting the `brm()` fit object into
    `posterior_summary()`
-   A {**tidybayes**} approach using `tidybayes::mean_hdi()` if you're
    willing to drop the posterior `sd` and
-   Using additionally the [function
    `histospark()`](https://github.com/hadley/precis/blob/master/R/histospark.R)
    (from the unfinished {**precis**} package by Hadley Wickham supposed
    to replace `base::summary()`) to get the tiny histograms and to add
    them into the tidyverse approach.

Additionally I will propose using the {**skimr**} packages:

```{r}
#| label: skim-summary-samples-b4.1-b
#| attr-source: '#lst-skim-summary-samples-b4.1-b lst-cap="Summary the extracted iterations of the HMC chains: skimr version"' 

skimr::skim(post_b[, 1:2])
```

Kurz refers only shortly to both `overthinking` blocks of this section:

-   Start values for `rethinking::quap()` resp. `brms::brm()` (See
    @sec-start-values-rethinking): Within the `brm()` function, you use
    the `init` argument fpr the start values.
-   Under the hood with multivariate sampling (See
    @sec-under-the-hood-multivariate-sampling-a): Again Kurz remarked
    that `brms::as_draws_df()` is not the same as
    `rethinking::extract.samples()`. What this exactly means will
    (hopefully) explained later in @sec-chap09-markov-chain-monte-carlo.

### Linear prediction

```{r}
#| label: fig-height-against-weight-b
#| fig-cap: "Adult height and weight against one another"
#| attr-source: '#lst-fig-height-against-weight-b lst-cap="Scatterplot of adult height and weight"'


d2_b |> 
    ggplot(aes(height, weight)) + 
    geom_point()
```

#### The linear model strategy

##### EMPTY: Model definition

##### EMPTY: Linear model

##### Priors {#sec-priors-b}

```{r}
#| label: fig-sim-heights-only-with-priors-b
#| fig-cap: "Simulating heights from the model, using only the priors: tidyverse version"
#| attr-source: '#lst-fig-sim-heights-only-with-priors-b lst-cap="Simulate heights from the model, using only the priors: tidyverse version"'

set.seed(2971)
# how many lines would you like?
n_lines <- 100

lines <-
  tibble(n = 1:n_lines,
         a = rnorm(n_lines, mean = 178, sd = 20),
         b = rnorm(n_lines, mean = 0, sd = 10)) %>% 
  expand_grid(weight = range(d2_b$weight)) %>% 
  mutate(height = a + b * (weight - mean(d2_b$weight)))


lines %>% 
  ggplot(aes(x = weight, y = height, group = n)) +
  geom_hline(yintercept = c(0, 272), linetype = 2:1, linewidth = 1/3) +
  geom_line(alpha = 1/10) +
  coord_cartesian(ylim = c(-100, 400)) +
  ggtitle("b ~ dnorm(0, 10)") +
  theme_classic()

```

Using the Log-Normal distribution prohibits negative values. This is an
important constraint for height and weight as these variables cannot be
under 0.

```{r}
#| label: fig-log-normal-b
#| fig-cap: "Log-Normal distribution: tidyverse version"

set.seed(4)

tibble(b = rlnorm(1e4, meanlog = 0, sdlog = 1)) %>% 
  ggplot(aes(x = b)) +
  geom_density(fill = "grey92") +
  coord_cartesian(xlim = c(0, 5)) +
  theme_classic()

```

::: callout-note
Kurz wrote just `mean` and `sd` instead of `meanlog` and `sdlog.` These shorter argument names work because of the [partial matching feature in argument evaluation](https://cran.r-project.org/doc/manuals/R-lang.html#Argument-matching) of R functions. But for educational reason (misunderstanding, clashing with other matching arguments and less readable code) I apply this technique only sometimes in interactive use.
:::

I am not very skilled with the Log-Normal distribution, and so I am
happy that Kurz added some explanations:

> If you're unfamiliar with the log-normal distribution, it is the
> distribution whose logarithm is normally distributed. For example,
> here's what happens when we compare Normal(0,1) with
> log(Log-Normal(0,1)).

```{r}
#| label: fig-normal-log-normal
#| fig-cap: "Compare Normal(0,1) with log(Log-Normal(0,1))"

set.seed(4)

tibble(rnorm           = rnorm(1e5, mean = 0, sd = 1),
       `log(rlognorm)` = log(rlnorm(1e5, meanlog = 0, sdlog = 1))) %>% 
  pivot_longer(everything()) %>% 

  ggplot(aes(x = value)) +
  geom_density(fill = "grey92") +
  coord_cartesian(xlim = c(-3, 3)) +
  theme_classic() +
  facet_wrap(~ name, nrow = 2)
```

> Those values are ~~what~~ the mean and standard deviation of the
> output from the `rlnorm()` function **after** they are log
> transformed. The formulas for the actual mean and standard deviation
> for the log-normal distribution itself are complicated (see
> [Wikipedia](https://en.wikipedia.org/wiki/Log-normal_distribution)).

------------------------------------------------------------------------

::: {#def-mean-sd}
Calculate mean and standard deviation

$$
\begin{align*}
\text{mean}               & = \exp \left (\mu + \frac{\sigma^2}{2} \right) \\
\text{standard deviation} & = \sqrt{[\exp(\sigma ^{2})-1] \; \exp(2\mu +\sigma ^{2})}
\end{align*}
$$ {#eq-formula-mean-sd}
:::

------------------------------------------------------------------------

Let's try our hand at those formulas and compute the mean and standard
deviation for Log-Normal(0,1):

```{r}
#| label: compute-mu-sigma-for-log-normal-manually-b

mu    <- 0
sigma <- 1

# mean
exp(mu + (sigma^2) / 2)

# sd
sqrt((exp(sigma^2) - 1) * exp(2 * mu + sigma^2))
```

Let's confirm with simulated draws from `rlnorm()`.

```{r}
#| label: compute-log-normal-b
#| fig-cap: "Compute mean and standard deviation of the Log-Normal distribution: tidyverse version"

set.seed(4)

tibble(x = rlnorm(1e7, meanlog = 0, sdlog = 1)) %>% 
  summarise(mean = mean(x),
            sd   = sd(x))
```

```{r}
#| label: fig-prior-pred-sim-b
#| fig-cap: "Prior predictive simulation again, now with the Log-Normal prior: tidyverse version"


# make a tibble to annotate the plot
text <-
  tibble(weight = c(34, 43),
         height = c(0 - 25, 272 + 25),
         label  = c("Embryo", "World's tallest person (272 cm)"))

# simulate
set.seed(2971)

tibble(n = 1:n_lines,
       a = rnorm(n_lines, mean = 178, sd = 20),
       b = rlnorm(n_lines, mean = 0, sd = 1)) %>% 
  expand_grid(weight = range(d2_b$weight)) %>% 
  mutate(height = a + b * (weight - mean(d2_b$weight))) %>%
  
  # plot
  ggplot(aes(x = weight, y = height, group = n)) +
  geom_hline(yintercept = c(0, 272), linetype = 2:1, linewidth = 1/3) +
  geom_line(alpha = 1/10) +
  geom_text(data = text,
            aes(label = label),
            size = 3) +
  coord_cartesian(ylim = c(-100, 400)) +
  ggtitle("log(b) ~ dnorm(0, 1)") +
  theme_classic()
```
***
:::: {#prp-p-value-problem}
Torturing data until they fit your expectations / desires

::: callout-tip
The paper by Simmons, Nelson and Simonsohn (2011), [False-positive
psychology: Undisclosed flexibility in data collection and analysis
allows presenting anything as
significant](https://journals.sagepub.com/doi/10.1177/0956797611417632),
is often cited as an introduction to the problem.
:::
::::
***

#### Finding the posterior distribution

Unlike with McElreath's `rethinking::quap()` formula syntax, Kurz is not
aware if we can just specify something like `weight – xbar` in the
`formula` argument in `brms::brm()`.

However, the alternative is easy: Just make a new variable in the data
that is equivalent to `weight – mean(weight)`. We'll call it `weight_c`.

```{r}
#| label: create-weight-diff-var-b
#| attr-source: '#lst-create-weight-diff-var-b lst-cap="Create a new variable in the data equivalent to weight - mean(height): tidyverse version"'


d2_b <-
  d2_b %>% 
  mutate(weight_c = weight - mean(weight))
```

Remember: The detailed explication of the syntax for the following `brms::brm()` function are in @tbl-mirror-rethinking-tidyverse and @lst-post-dist-brms-b4.1.


```{r}
#| label: find-and-summarize-post-dist-b
#| attr-source: '#lst-find-and-summarize-post-dist-b lst-cap="Find the posterior distribution of the linear height-weight model: tidyverse version"'

b4.3 <- 
  brms::brm(data = d2_b, 
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(lognormal(0, 1), class = b, lb = 0),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03")

# brms:::summary.brmsfit(b4.3)
```

Here are the trace plots.

```{r}
#| label: fig-find-post-dist-b
#| fig-cap: "Find the posterior distribution of the linear height-weight model: tidyverse version"

plot(b4.3, widths = c(1, 2))
```

{**brms**} does not allow users to insert coefficients into functions
like exp() within the conventional `formula` syntax. We can fit a
{**brms**} model like McElreath's `m4.3b` if we adopt what's called the
[non-linear
syntax](https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html).
The non-linear syntax is a lot like the syntax McElreath uses in
{**rethinking**} in that it typically includes both predictor and
variable names in the `formula`. Since this is so early in the book, I
won't go into a full-blown explanation, here. There will be many more
opportunities to practice with the non-linear syntax in the chapters to
come.

```{r}
#| label: find-post-dist2-b
#| attr-source: '#lst-find-post-dist2-b lst-cap="Find the posterior distribution of the linear height-weight model (log version): tidyverse version"'

b4.3b <- 
  brms::brm(data = d2_b, 
      family = gaussian,
      brms::bf(height ~ a + exp(lb) * weight_c,
         a ~ 1,
         lb ~ 1,
         nl = TRUE),
      prior = c(brms::prior(normal(178, 20), class = b, nlpar = a),
                brms::prior(normal(0, 1), class = b, nlpar = lb),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03b")
```

If you execute `summary(b4.3b)`, you'll see the intercept and `σ`
summaries for this model are about the same as those for `b4.3`, above.

```{r}
#| label: summarize-post-dist2-b
#| attr-source: '#lst-summarize-post-dist2-b lst-cap="Summarize the posterior distribution of the linear height-weight model (log version): tidyverse version"'

brms:::summary.brmsfit(b4.3b)
```

The difference is for the β parameter, which we called `lb` in the
`b4.3b` model. If we term that parameter from `b4.3` as $\beta^{b4.3}$
and the one from our new model $\beta^{b4.3b}$, it turns out that
$\beta^{b4.3} = exp(\beta^{b4.3b})$.

```{r}
#| label: extract-fixed-effects-b
#| attr-source: '#lst-extract-foxed-effects-b lst-cap="Extract and compare the population-level (fixed) effects from object b4.3 and b4.3b"'

brms::fixef(b4.3)["weight_c", "Estimate"]
brms::fixef(b4.3b)["lb_Intercept", "Estimate"] %>% exp()
```

They're the same within simulation variance.

#### Interpreting the posterior distribution

##### Tables of marginal distribution

With a little `[]` subsetting we can exclude the log posterior from our
summary for `b4.3`.

```{r}
#| label: table-summary-b4.3
#| attr-source: '#lst-table-summary-b4.3 lst-cap="Display the marginal posterior distributions of the parameters: brms version"'

brms::posterior_summary(b4.3)[1:3, ] %>% 
  round(digits = 2)
```

Without the subsetting we will get 2 more lines:

```{r}
#| label: table-summary2-b4.3
#| attr-source: '#lst-table-summary2-b4.3 lst-cap="Display the marginal posterior distributions of the parameters: brms version"'

brms::posterior_summary(b4.3) %>% 
  round(digits = 2)
```

::: callout-note
###### TODO: Interpret printout {.unnumbered}

-   `b_Intercept` represents `a` in the rethinking version.
-   `b_weight_c` represents `b` in the rethinking version. But why did
    we have to calculate it different?
-   `sigma` is the quadratic approximation of the standard deviation.
-   `lprior` is -- I assume -- the log prior.
-   `l__` is what??
:::

Looking up `brms::posterior_summary()` I learned that the "function
mainly exists to retain backwards compatibility. It will eventually be
replaced by functions of the {**posterior**} package".

One of the following examples suggests to convert the `brmsfit` object
into a `draws` object and then to use `posterior::summarise_draws()`.
But reading the help file of `posterior::summarise_draws()` it turned
out that it "will convert an object to a draws object if it isn't
already".

```{r}
#| label: table-summary3-b4.3
#| attr-source: '#lst-table-summary3-b4.3 lst-cap="Display the marginal posterior distributions of the parameters: posterior version"'

posterior::summarize_draws(b4.3, posterior::default_summary_measures(), 
                           .num_args = list(sigfig = 2))[1:3, ]


```

@lst-table-summary3-b4.3 is the default call using
`posterior::default_summary_measures()`. This returns the default
measures. But we can adapt this standard summary to get a very similar
result a in @lst-table-summary-m4.3.

```{r}
#| label: table-summary4-b4.3
#| attr-source: '#lst-table-summary4-b4.3 lst-cap="Display the marginal posterior distributions of the parameters: posterior version"'

posterior::summarize_draws(b4.3, "mean", "median", "sd", 
                           ~quantile(., probs = c(0.055, 0.945)),
                           .num_args = list(sigfig = 2))[1:3, ]
```

But don't forget that there exists also `summary()` as an alias either
for `brmsfit` or `draws` objects which can be used for a standardized
output:

```{r}
#| label: table-summary5-b4.3
#| attr-source: '#lst-table-summary5-b4.3 lst-cap="Display and compare the marginal posterior distributions of the parameters of the brms and the posterior version"'

## summary for brmsfit object
summary(b4.3)

## summary for draws object
summary(brms::as_draws_array(b4.3))[1:3, ]

```

If we put our {**brms**} fit into the `brms:::vcov.brmsfit()` function,
we'll get the variance/covariance matrix of the `intercept` and
`weight_c` coefficient.

```{r}
#| label: var-cov-matrix-b4.3
#| attr-source: '#lst-var-cov-matrix-b4.3 lst-cap="Calculate the variance-covariance matrix for model b4.3"'

brms:::vcov.brmsfit(b4.3) %>% 
  round(3)
```

No `σ`, however. To get that, we'll have to extract the posterior draws
and use the `cov()` function, instead.

```{r}
#| label: var-cov-matrix2-b4.3
#| attr-source: '#lst-var-cov-matrix2-b4.3 lst-cap="Calculate the variance-covariance matrix for model b4.3 with {posterior} draws objects"'

brms::as_draws_df(b4.3) %>%
  select(b_Intercept:sigma) %>%
  cov() %>%
  round(digits = 3)
```

The `pairs()` function will work for a brms fit much like it would one
from rethinking. It will show "both the marginal posteriors and the
covariance".

```{r}
#| label: fig-marg-post-cov-b4.3
#| fig-cap: "The marginal posteriors and the covariance matrix for model b4.3"
#| attr-source: '#lst-vfig-marg-post-cov-b4.3 lst-cap="Show the marginal posteriors and covariance matrix for model m4.3"'

brms:::pairs.brmsfit(b4.3)
```

##### Plotting posterior inference around the mean

Here is the code for Figure 4.6.

```{r}
#| label: fig-raw-data-line-b4.3
#| fig-cap: "Height in centimeters (vertical) plotted against weight_c (horizontal), with the line at the posterior mean plotted in black: tidyverse version"


d2_b %>%
  ggplot(aes(x = weight_c, y = height)) +
  geom_abline(intercept = brms:::fixef.brmsfit(b4.3)[1], 
              slope     = brms:::fixef.brmsfit(b4.3)[2]) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  theme_classic()
```

Note how the breaks on our `x`-axis look off. That's because we fit the
model with `weight_c` and we plotted the points in that metric, too.
Since we computed `weight_c` by subtracting the mean of weight from the
data, we can adjust the `x`-axis break point labels by simply adding
that value back.

```{r}
#| label: fig-raw-data-line2-b4.3
#| fig-cap: "Height in centimeters (vertical) plotted against weight in kilograms (horizontal), with the line at the posterior mean plotted in black: tidyverse version"
#| attr-source: '#lst-fig-raw-data-line2-b4.3 lst-cap="Reproducing Figure 4.6 of SR2 with adjusted x-axis"'

labels <-
  c(-10, 0, 10) + mean(d2_b$weight) %>% 
  round(digits = 0)

d2_b %>%
  ggplot(aes(x = weight_c, y = height)) +
  geom_abline(intercept = brms::fixef(b4.3, probs = c(0.055, 0.945))[[1]], 
              slope     = brms::fixef(b4.3, probs = c(0.055, 0.945))[[2]]) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  scale_x_continuous("weight",
                     breaks = c(-10, 0, 10),
                     labels = labels) +
  theme_bw() +
  theme(panel.grid = element_blank())
```

Furthermore note the use of the `brms:::fixef.brmsfit()` function within
`ggplot2::geom_abline()`. The function extracts the population-level
('fixed') effects from a `brmsfit` object. 

::: callout-note
###### Extract population-level ("fixed") effects {#sec-fixed-effects}

I do not know what it means exactly that `brms::fixef()` extracts the population-level ('fixed') effect from a `brmsfit` object. After I googled it turned out that there is a great discussion about [Fixed Effects in Linear Regression](https://statisticsglobe.com/fixed-effects-linear-regression) and generally about fixed, random and mixec models (See Cross Validated [here](https://stats.stackexchange.com/questions/4700/what-is-the-difference-between-fixed-effect-random-effect-and-mixed-effect-mode) and [here](https://stats.stackexchange.com/questions/21760/what-is-a-difference-between-random-effects-fixed-effects-and-marginal-model)).

At the moment I do not understand background and essence of this differentiation. What follows is a quote and summary of short glossary entry in [statistics.com](https://www.statistics.com/glossary/fixed-effects/).

> The term “fixed effects” (as contrasted with “random effects”) is related to how particular coefficients in a model are treated – as fixed or random values. Which approach to choose depends on both the nature of the data and the objective of the study. A fixed effect approach can be used for both random and non-random samples. Random effect models are usually applied only to random samples. 

Following the rest of the glossary entry in statistics.com:

Suppose the data at hand are values of the annual income of 100 school teachers – T1: $N_{1}=30$ males and T2: $N_{2}=70$ females.

1. Suppose the 100 individuals has been drawn randomly from a population, for example, from all school teachers of New York.
    a. If the question of interest is the average income of New York school teachers, then the random effects approach is reasonable. We treat $T_{i}$ as values of a random variable taking on two values – $T_{1}$ and $T_{2}$ . For example, we simply use the mean of the 100 values as an estimate of the average income of New York teachers.
    b. If the question of interest is the average income of female and male teachers separately, then we treat $T_{1}$ and $T_{2}$ as two fixed values.
2. Suppose a researcher decided to pick up $N_{1}=30$ male teachers randomly from all male teachers of New York, and $N_{2}=70$ female teachers from all female teachers. In this case, only the fixed effect approach is reasonable – because the $N_{1}$ values $T_{1}$ and the $N_{2}$ values of $T_{1}$ in the sample of 100 have not been drawn randomly from the population of interest.

There are also academic papers on this topic ([Let's Talk About Fixed Effects](https://link.springer.com/article/10.1007/s11577-020-00699-8)), tutorials ([Fixed Effects Regression](https://www.econometrics-with-r.org/10-3-fixed-effects-regression.html), [Fixed or Random Effects](https://bookdown.org/Yuleng/polimethod/fixed.html)) and R packages ([fixest]https://lrberge.github.io/fixest/index.html, see it's [introduction](https://lrberge.github.io/fixest/articles/fixest_walkthrough.html)) dedicated especially to this subject. 

:::

Let's try and see what `brms::fixef()` produces:

```{r}
#| label: using-fixef-b
#| attr-source: '#lst-using-fixef-b lst-cap="Extract population-level estimates from a brmsfit object"'
#| code-summary: "Extract population-level estimates from a brmsfit object"

brms::fixef(b4.3, probs = c(0.055, 0.945))
```

So the function `ggplot2::geom_abline()` has used the intercept (`a`)
and the slope (`b`) of the estimate column.

::: callout-note
###### My code changes in using the `brms::fixef()` in @lst-fig-raw-data-line2-b4.3

1. Slightly different use of the extracting operator `[`

Instead of using `[1]` and `[2]` I have used `[[1]]` and `[[2]]`.

> `[` selects sub-lists: it always returns a list. If you use it with a
> single positive integer, it returns a list of length one. `[[` selects
> an element within a list. (from [Advanced
> R](https://adv-r.hadley.nz/subsetting.html#subsetting-answers), 2nd
> ed.)

2. Different percentiles

McElreath uses a 89% probability mass as CI (`r glossary("compatibility interval")`, aka credible interval or confidence interval). But the default value for the `probs` arguments is `c(0.025, 0.975)`, a 95% interval. To reproduce the Figure 4.6 I have therefore used `probs = c(0.055, 0.945)`.

3. Direct function call

`brms::fixef(x)` is equivalent to `brms:::fixef.brmsfit(x)` if
`x` is a `brmsfit` object.
:::




##### Adding uncertainty around the mean

Instead of `rethinking::extract.samples()` the {**brms**} packages
extract all the posterior draws with `brms::as_draws_df()`. We have
already done this with @lst-put-hmc-into-df-b. We just repeat this code
here using `dplyr::slice(1:6)` instead of `utils::head()`

```{r}
#| label: put-hmc-into-df2-b
#| attr-source: '#lst-put-hmc-into-df2-b lst-cap="Extract the iteration of the Hamiliton Monte Carlo (HMC) chains into a data frame"'

post_b <- brms::as_draws_df(b4.3)
post_b %>%
  slice(1:6)
```

Here are the four models leading up to Figure 4.7:

```{r}
#| label: calc-all-four-models-b
#| attr-source: '#lst-calc-all-four-models-b lst-cap="Calculate all four models"'

dN10_b <- 10

b4.3_010 <- 
  brms::brm(data = d2_b %>%
        slice(1:dN10_b),  # note our tricky use of `N` and `slice()`
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(lognormal(0, 1), class = b),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03_010")

dN50_b <- 50

b4.3_050 <- 
  brms::brm(data = d2_b %>%
        slice(1:dN50_b), 
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(lognormal(0, 1), class = b),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03_050")

dN150_b <- 150

b4.3_150 <- 
  brms::brm(data = d2_b %>%
        slice(1:dN150_b), 
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(lognormal(0, 1), class = b),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03_150")

dN352_b <- 352

b4.3_352 <- 
  brms::brm(data = d2_b %>%
        slice(1:dN352_b), 
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(lognormal(0, 1), class = b),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03_352")
```

Here are the trace plots and coefficient summaries from these four
models.

```{r}
#| label: trace-plots-cov-sum-b
#| attr-source: '#lst-trace-plots-cov-sum-b lst-cap="Trace plots and coefficient summaries from all four models"'


plot(b4.3_010)
print(b4.3_010)

plot(b4.3_050)
print(b4.3_050)

plot(b4.3_150)
print(b4.3_150)

plot(b4.3_352)
print(b4.3_352)
```

::: callout-note
###### Checking MCMC chains with trace plots and trank plots

At the moment I didn't learn how to interpret these types of graphic output. McElreath's explains in later video lectures that it is important that `r glossary("trace plot")` cover the same location in the vertical axis (e.g. they do not jump around) and show that all different chains alternate in their (top) positions. I mentioned here the top position because this is the place where irregularities can be detected more easily. 

In the above example it is difficult to decide if this is the case because the color differences of the different chains are weak. But it is general difficult to inspect trace plot, therefore McElreath proposes trace rank plots or `r glossary("trank plot")` (his terminus) in @sec-checking-the-chain.
:::


We'll need to put the chains of each model into data frames.

```{r}
#| label: put-chain-into-model-b
#| #| attr-source: #lst-put-chain-into-model-b lst-cap="Put the chains of each model into data frames"'

post010_b4.3 <- brms::as_draws_df(b4.3_010)
post050_b4.3 <- brms::as_draws_df(b4.3_050)
post150_b4.3 <- brms::as_draws_df(b4.3_150)
post352_b4.3 <- brms::as_draws_df(b4.3_352)
```

Here is the code for the four individual plots:

```{r}
#| label: calc-code-for-plots-b
#| attr-source: '#lst-calc-code-for-plots-b lst-cap="Prepare data for four individual plots"'

p5 <- 
  ggplot(data =  d2_b[1:10, ], 
         aes(x = weight_c, y = height)) +
  geom_abline(data = post010_b4.3 %>% slice(1:20),
              aes(intercept = b_Intercept, slope = b_weight_c),
              linewidth = 1/3, alpha = .3) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  coord_cartesian(xlim = range(d2_b$weight_c),
                  ylim = range(d2_b$height)) +
  labs(subtitle = "N = 10")

p6 <-
  ggplot(data =  d2_b[1:50, ], 
         aes(x = weight_c, y = height)) +
  geom_abline(data = post050_b4.3 %>% slice(1:20),
              aes(intercept = b_Intercept, slope = b_weight_c),
              linewidth = 1/3, alpha = .3) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  coord_cartesian(xlim = range(d2_b$weight_c),
                  ylim = range(d2_b$height)) +
  labs(subtitle = "N = 50")

p7 <-
  ggplot(data =  d2_b[1:150, ], 
         aes(x = weight_c, y = height)) +
  geom_abline(data = post150_b4.3 %>% slice(1:20),
              aes(intercept = b_Intercept, slope = b_weight_c),
              linewidth = 1/3, alpha = .3) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  coord_cartesian(xlim = range(d2_b$weight_c),
                  ylim = range(d2_b$height)) +
  labs(subtitle = "N = 150")

p8 <- 
  ggplot(data =  d2_b[1:352, ], 
         aes(x = weight_c, y = height)) +
  geom_abline(data = post352_b4.3 %>% slice(1:20),
              aes(intercept = b_Intercept, slope = b_weight_c),
              linewidth = 1/3, alpha = .3) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  coord_cartesian(xlim = range(d2_b$weight_c),
                  ylim = range(d2_b$height)) +
  labs(subtitle = "N = 352")
```

Now we can combine the ggplots with patchwork syntax to make the full
version of Figure 4.7.

```{r}
#| label: fig-draw-plots-figure-4.7-b
#| fig-cap: "Samples from the quadratic approximate posterior distribution for the height/weight model, b4.3, with increasing amounts of data. In each plot, 20 lines sampled from the posterior distribution, showing the uncertainty in the regression relationship. Tidyverse version."



# library(patchwork) ## already in setup chunk

(p5 + p6 + p7 + p8) &
  scale_x_continuous("weight",
                     breaks = c(-10, 0, 10),
                     labels = labels) &
  theme_classic()
```

##### Plotting regression intervals and contours

Since we used `weight_c` to fit our model, we might first want to
understand what exactly the mean value is for weight.

```{r}
#| label: calc-mean-weight-b
#| attr-source: '#lst-calc-mean-weight-b lst-cap="Calculate mean of weight"'

mean(d2_b$weight)
```

Just a hair under 45. If we're interested in $\mu$ at `weight` = 50,
that implies we're also interested in $\mu$ at `weight_c` + 5.01. Within
the context of our model, we compute this with
$\alpha + \beta \cdot 5.01$. Here's what that looks like with `post_b`.

```{r}
#| label: calc-mean-weight-at-50-b
#| attr-source: '#lst-calc-mean-weight-at-50-b lst-cap="Calculate the mean at weight 50 kg"'

mu_at_50_b <- 
  post_b %>% 
  transmute(mu_at_50_b = b_Intercept + b_weight_c * 5.01)
 
head(mu_at_50_b)
```

And here is a version McElreath's Figure 4.8 density plot.

```{r}
#| label: fig-density-vector-mean-50-b
#| fig-cap: "The quadratic approximate posterior distribution of the mean height, μ, when weight is 50 kg. This distribution represents the relative plausibility of different values of the mean. Tidyverse version"

mu_at_50_b %>%
  ggplot(aes(x = mu_at_50_b)) +
  geom_density(linewidth = 0, fill = "deepskyblue") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(mu["height | weight = 50"])) +
  theme_classic()
```

We'll use `tidybayes::mean_hdi()` to get both 89% and 95% HPDIs along
with the mean.

```{r}
#| label: calc-mean-and-HPDI-b
#| attr-source: '#lst-calc-mean-and-HPDI-b lst-cap="Calculate both 89% and 95% Highest Priority Intensity Intervals (HPDIs) along with the mean."'

tidybayes::mean_hdi(mu_at_50_b[, 1], .width = c(.89, .95))
```

If you wanted to express those sweet 95% HPDIs on your density plot, you
might use `tidybayes::stat_halfeye()`. Since `tidybayes::stat_halfeye()`
also returns a point estimate, we'll throw in the mode.

```{r}
#| label: fig-half-eye-b
#| fig-cap: "Plot of half-eye (density + interval) geometry"
mu_at_50_b %>%
  ggplot(aes(x = mu_at_50_b, y = 0)) +
  tidybayes::stat_halfeye(point_interval = tidybayes::mode_hdi, .width = .95,
               fill = "deepskyblue") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(mu["height | weight = 50"])) +
  theme_classic()
```

With {**brms**}, you would use fitted() to do what McElreath
accomplished with `rethinking::link()`.

::: callout-caution
Kurz applies the function `fitted()` in the code, but in the text he
uses twice `brms::fitted()` which doesn't exist. I used both
`brms:::fitted.brmsfit()` and `stats::fitted()` to get the same results.

The object `b4.3` is of class `brmsfit` but in the help file of
`stats::fitted()` you can read: "`fitted` is a generic function which
extracts fitted values from objects returned by modeling functions.
**All object classes which are returned by model fitting functions
should provide a `fitted` method.** (emphasis is mine)

My interpretation therefore is that `stats::fitted()` is using
`brms:::fitted.brmsfit()`. Thts why the results are identical.
:::

```{r}
#| label: calc-mu-with-fitted-b
#| attr-source: '#lst-calc-mu-with-fitted-b lst-cap="Calculate μ for each case in the data and sample from the posterior distribution: Tidyverse version"'

mu2_b <- brms:::fitted.brmsfit(b4.3, summary = F)
mu2.1_b <- stats::fitted(b4.3, summary = F)
str(mu2_b)
str(mu2.1_b)
```

When you specify `summary = F`, `brms:::fitted.brmsfit()` returns a
matrix of values with as many rows as there were post-warmup draws
across your Hamilton Monte Carlo (HMC) chains and as many columns as
there were cases in your analysis. Because we had 4,000 post-warmup
draws and $n=352$, `brms:::fitted.brmsfit()` returned a matrix of 4,000
rows and 352 vectors. If you omitted the `summary = F` argument, the
default is TRUE and `brms:::fitted.brmsfit()` will return summary
information instead.

Much like `rethinking::link()`, `brms:::fitted.brmsfit()` can
accommodate custom predictor values with its `newdata` argument.

```{r}
#| label: calc-dist-mu-unique-with-fitted.brmsfit-b
#| attr-source: '#lst-calc-dist-mu-unique-with-fitted.brmsfit-b lst-cap="Calculate a distribution of μ for each unique weight value on the horizontal axis: tidyverse version"'

weight_seq <- 
  tibble(weight = 25:70) %>% 
  mutate(weight_c = weight - mean(d2_b$weight))

mu3_b <-
  brms:::fitted.brmsfit(b4.3,
         summary = F,
         newdata = weight_seq) %>%
  data.frame() %>%
  # here we name the columns after the `weight` values from which they were computed
  set_names(25:70) %>% 
  mutate(iter = 1:n())

head(mu3_b)
```

::: callout-caution
The {rethinking} version uses the variable `weight.seq` whereas the
tidyverse version uses `weight_seq`.
:::

Anticipating {**ggplot2**}, we went ahead and converted the output to a
data frame. But we might do a little more data processing with the aid
of `tidyr::pivot_longer()`, which will convert the data from the wide
format to the long format.

------------------------------------------------------------------------

::: callout-tip
###### Literature references

If you are new to the distinction between wide and long data, you can
learn more from the [Pivot data from wide to
long](https://tidyr.tidyverse.org/reference/pivot_longer.html) vignette
from the tidyverse team (2020); Simon Ejdemyr's blog post, [Wide & long
data](https://sejdemyr.github.io/r-tutorials/basics/wide-and-long/); or
Karen Grace-Martin's blog post, [The wide and long data format for
repeated measures
data](https://www.theanalysisfactor.com/wide-and-long-data/).
:::

------------------------------------------------------------------------

```{r}
#| label: convert-wide-to-long-b
#| attr-source: '#lst-convert-wide-to-long-b lst-cap="Data processing: Convert data from wide to long format: tidyverse version"'

mu4_b <- 
  mu3_b %>%
  pivot_longer(-iter,
               names_to = "weight",
               values_to = "height") %>% 
  # we might reformat `weight` to numerals
  mutate(weight = as.numeric(weight))

head(mu4_b)
```

Now our data processing is done, here we reproduce McElreath's Figure
4.9.a.

```{r}
#| label: fig-dist-mu-height-100-b
#| fig-cap: "The first 100 values in the distribution of μ at each weight value. Tidyverse version"
d2_b %>%
  ggplot(aes(x = weight, y = height)) +
  geom_point(data = mu4_b %>% filter(iter < 101), 
             color = "navyblue", alpha = .05) +
  coord_cartesian(xlim = c(30, 65)) +
  theme(panel.grid = element_blank())
```

With `brms:::fitted.brmsfit()`, it's quite easy to plot a regression
line and its intervals. Just omit the `summary = T` argument.

```{r}
#| label: sum-dist-weight-b
#| attr-source: '#lst-sum-dist-weight-b lst-cap="Summary of the distribution for each weight value. Tidyverse version"'

mu_summary <-
  brms:::fitted.brmsfit(b4.3, 
         newdata = weight_seq) %>%
  data.frame() %>%
  bind_cols(weight_seq)

head(mu_summary)
```

Here it is, our analogue to Figure 4.9.b.

```{r}
#| label: fig-summaries-on-data-top-b
#| fig-cap: "Plot of the summaries on top of the !Kung height data again, now with 89% compatibility interval of the mean indicated by the shaded region. Compare this region to the distributions of blue points in @fig-dist-mu-height-100-b."


d2_b %>%
  ggplot(aes(x = weight, y = height)) +
  geom_smooth(data = mu_summary,
              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              stat = "identity",
              fill = "grey70", color = "black", alpha = 1, linewidth = 1/2) +
  geom_point(color = "navyblue", shape = 1, size = 1.5, alpha = 2/3) +
  coord_cartesian(xlim = range(d2_b$weight)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())
```

If you wanted to use intervals other than the default 95% ones, you'd
include the probs argument like this:
`brms:::fitted.brmsfit(b4.3, newdata = weight.seq, probs = c(.25, .75))`.
The resulting third and fourth vectors from the `fitted()` object would
be named `Q25` and `Q75` instead of the default `Q2.5` and `Q97.5`. The
[Q prefix](https://github.com/paul-buerkner/brms/issues/425) stands for
quantile.

Similar to `rethinking::link()`, `brms:::fitted.brmsfit()` uses the
formula from your model to compute the model expectations for a given
set of predictor values. I use it a lot in this project. If you follow
along, you'll get a good handle on it. But to dive deeper, you can [go
here for the
documentation](https://rdrr.io/cran/brms/man/fitted.brmsfit.html).
Though we won't be using it in this project, {**brms**} users might want
to know that `fitted()` is also an alias for the
`brms::posterior_epred()` function, about which you might [learn more
here](https://rdrr.io/cran/brms/man/posterior_epred.brmsfit.html). Users
can always learn more about them and other functions in the [{**brms**}
reference
manual](https://cran.r-project.org/web/packages/brms/brms.pdf).

##### Prediction intervals

We've only been plotting the `μ` part. In order to bring in the
variability expressed by `σ`, we'll have to switch to the `predict()`
function. Much as `brms:::fitted.brmsfit()` was our analogue to
`rethinking::link()`, `brms:::predict.brmsfit()` is our analogue to
`rethinking::sim()`.

We can reuse our `weight_seq` data from before. The `predict()` code
looks a lot like what we used for `fitted()`. Compare
@lst-calc-dist-mu-unique-with-fitted.brmsfit-b with
@lst-calc-pred-height-with-predict.brmsfit-b.

```{r}
#| label: calc-pred-height-with-predict.brmsfit-b
#| attr-source: '#lst-calc-pred-height-with-predict.brmsfit-b lst-cap="Calculate the prediction of heights: tidyverse version"'

pred_height <-
  brms:::predict.brmsfit(b4.3,
          newdata = weight_seq) %>%
  data.frame() %>%
  bind_cols(weight_seq)
  
pred_height %>%
  slice(1:6)
```

This time the summary information in our data frame is for, as McElreath
put it, "simulated heights, not distributions of plausible average
height, `μ`" (p. 108). Another way of saying that is that these
simulations are the joint consequence of both `μ` and `σ`, unlike the
results of `fitted()`, which only reflect `μ`.

Figure 4.10 shows how you might visualize them:

```{r}
#| label: fig-reproduce-figure-4.10
#| fig-cap: "89% prediction interval for height, as a function of weight. The solid line is the average line for the mean height at each weight. The two shaded regions show different 89% plausible regions. The narrow shaded interval around the line is the distribution of μ. The wider shaded region represents the region within which the model expects to find 89% of actual heights in the population, at each weight. (tidyverse version)"

d2_b %>%
  ggplot(aes(x = weight)) +
  geom_ribbon(data = pred_height, 
              aes(ymin = Q2.5, ymax = Q97.5),
              fill = "grey83") +
  geom_smooth(data = mu_summary,
              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              stat = "identity",
              fill = "grey70", color = "black", alpha = 1, linewidth = 1/2) +
  geom_point(aes(y = height),
             color = "navyblue", shape = 1, size = 1.5, alpha = 2/3) +
  coord_cartesian(xlim = range(d2_b$weight),
                  ylim = range(d2_b$height)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())
```

To smooth out the rough shaded interval we would have in the {**brms**}
model fitting approach to refit `b4.3` after specifying a larger number
of post-warmup iterations with alterations to the `iter` and `warmup`
parameters.

::: callout-note
###### TODO: Smooth boundary {.unnumbered}

I should try to smooth out the grey boundary, because it would give me
more experiences how to use the different parameters to fit models with
the {**brms**} approach.

This experiment would also an occasion to change the intervals from the
default 95% ones to the 89% McElreath's is using. See his hint in the
paragraph before displaying Figure 10 how to change it in the
{**rethinking**} version.

In {**brms**} I would have to change the `probs` argument to
brms:::fitted.brmsfit(b4.3, newdata = weight.seq, probs = c(0.055,
0.945)). The resulting third and fourth vectors from the fitted() object
would be named Q5.5 and Q94.5 instead of the default Q2.5 and Q97.5. The
Q prefix stands for quantile. See [Rename summary columns of predict()
and related methods](https://github.com/paul-buerkner/brms/issues/425).
:::

Next we follow McElreath's example and do our model-based predictions by
hand. Instead of relying on base R `apply()` and `sapply()`, the main
action in the tidyverse approach is in `expand_grid()`, the second
`mutate()` line and the `group_by()` + `summarise()` combination.

```{r}
#| label: fig-predict-manually-b
#| fig-cap: "Model-based predictions without {brms} and pedict(): mean with quantiles of 0.25 and .975"
#| attr-source: '#lst-fig-predict-manually-b lst-cap="Model-based predictions without {brms} and pedict(): mean with quantiles of 0.25 and .975"'

set.seed(4)

post_b %>% 
  tidyr::expand_grid(weight = 25:70) %>% 
  mutate(weight_c = weight - mean(d2_b$weight)) %>% 
  mutate(sim_height = rnorm(n(),
                            mean = b_Intercept + b_weight_c * weight_c,
                            sd   = sigma)) %>% 
  group_by(weight) %>% 
  summarise(mean = mean(sim_height),
            ll   = quantile(sim_height, prob = .025),
            ul   = quantile(sim_height, prob = .975)) %>% 
  
  # plot
  ggplot(aes(x = weight)) +
  geom_smooth(aes(y = mean, ymin = ll, ymax = ul),
              stat = "identity",
              fill = "grey83", color = "black", alpha = 1, linewidth = 1/2) +
  geom_point(data = d2_b,
             aes(y = height),
             color = "navyblue", shape = 1, size = 1.5, alpha = 2/3) +
  coord_cartesian(xlim = range(d2_b$weight),
                  ylim = range(d2_b$height)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())
```

We specifically left out the `fitted()` intervals to make it more
apparent what we were simulating. You might also note that we could have
easily replaced that three-line summarize() code with a single line of
`tidybayes::mean_qi(sim_height)`, or whatever combination of central
tendency and interval type you wanted (e.g.,
`tidybayes::mode_hdi(sim_height, .width = .89)`)

Let's try this out:

```{r}
#| label: fig-predict-manually2-b
#| attr-source: '#lst-fig-predict-manually2-b lst-cap="Model-based predictions without {brms} and pedict(): mean with width .89"'

set.seed(4)

post_b %>% 
  tidyr::expand_grid(weight = 25:70) %>% 
  mutate(weight_c = weight - mean(d2_b$weight)) %>% 
  mutate(sim_height = rnorm(n(),
                            mean = b_Intercept + b_weight_c * weight_c,
                            sd   = sigma)) %>% 
  group_by(weight) %>% 
  tidybayes::mean_qi(sim_height, .width = .89) %>% 
  
  # plot
  ggplot(aes(x = weight)) +
  geom_smooth(aes(y = .point, ymin = .lower, ymax = .upper),
              stat = "identity",
              fill = "grey83", color = "black", alpha = 1, linewidth = 1/2) +
  geom_point(data = d2_b,
             aes(y = height),
             color = "navyblue", shape = 1, size = 1.5, alpha = 2/3) +
  coord_cartesian(xlim = range(d2_b$weight),
                  ylim = range(d2_b$height)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())
```

```{r}
#| label: fig-predict-manually3-b
#| attr-source: '#lst-fig-predict-manually3-b lst-cap="Model-based predictions without {brms} and pedict(): mode with width .89"'

set.seed(4)

post_b %>% 
  tidyr::expand_grid(weight = 25:70) %>% 
  mutate(weight_c = weight - mean(d2_b$weight)) %>% 
  mutate(sim_height = rnorm(n(),
                            mean = b_Intercept + b_weight_c * weight_c,
                            sd   = sigma)) %>% 
  group_by(weight) %>% 
  tidybayes::mode_hdi(sim_height, .width = .89) %>% 
  
  # plot
  ggplot(aes(x = weight)) +
  geom_smooth(aes(y = .point, ymin = .lower, ymax = .upper),
              stat = "identity",
              fill = "grey83", color = "black", alpha = 1, linewidth = 1/2) +
  geom_point(data = d2_b,
             aes(y = height),
             color = "navyblue", shape = 1, size = 1.5, alpha = 2/3) +
  coord_cartesian(xlim = range(d2_b$weight),
                  ylim = range(d2_b$height)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())
```

::: callout-note
###### TODO: .width vs. ll/ul {.unnumbered}
:::

Revise @sec-sampling-to-summarize to understand better the difference
between width (= defined probability mass, for example .89) and ll/ul (=
defined boundaries, for example .025 and .975).

What is the equivalent of width .89 (defined in probability mass) in
defined boundaries under the assumption of a Gaussian distribution?
Solution: .055 and.945!

### Curves from lines

#### Polynomial regression

To see an application for fitting curves instead of lines we are going
to use the `Howell1` data, but this time the full data set. The reason
is that in the non-adult years there is a steeper slope than in the
adult years.

```{r}
#| label: fig-scatterplot-height-weight-b
#| fig-cap: "Height in centimeters (vertical) plotted against weight in kilograms (horizontal). This time with the full data, e.g., the non-adults data."
#| attr-source: '#fig-scatterplot-height-weight-b lst-cap="Height in centimeters (vertical) plotted against weight in kilograms (horizontal): tidyverse version"'

d_b %>% 
  ggplot(aes(x = weight, y = height)) +
  geom_point(color = "navyblue", shape = 1, size = 1.5, alpha = 2/3) +
  annotate(geom = "text",
           x = 42, y = 115,
           label = "This relation is\nvisibly curved.",
           family = "Times") +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())
```

Standardizing will help `brms::brm()` fit the model. We might
standardize our weight variable like so:

```{r}
#| label: standardize-weight-b
#| attr-source: '#lst-standardize-weight-b lst-cap="Stadardize the weight variable"'

d3_b <-
  d_b %>%
  mutate(weight_s = (weight - mean(weight)) / sd(weight)) %>% 
  mutate(weight_s2 = weight_s^2)
```

We fit the quadratic model (se @eq-parabolic-model) with {**brms**}:

```{r}
#| label: brm-parabolic
#| attr-source: '#lst-brm-parabolic lst-cap="Finding the posterior distribution of a parabolic model of height on weight with brms::brm()"'

b4.5 <- 
  brms::brm(data = d3_b, 
      family = gaussian,
      height ~ 1 + weight_s + weight_s2,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(lognormal(0, 1), class = b, coef = "weight_s"),
                brms::prior(normal(0, 1), class = b, coef = "weight_s2"),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.05")
```

Note our use of the coef argument within our prior statements. Since β1
and β2 are both parameters of `class = b` within the {**brms**} set-up,
we need to use the `coef` argument when we want their priors to differ.

```{r}
#| label: fig-brm-parabolic
#| attr-source: '#lst-fig-brm-parabolic lst-cap="Plot the posterior distribution of a parabolic model of height on weight calculated with brms::brm()"'

plot(b4.5, widths = c(1, 2))
```

::: callout-note
###### TODO: Interpret graphic
:::

```{r}
#| label: print-brm-parabolic
#| attr-source: '#lst-print-brm-parabolic lst-cap="Pint the result of the posterior distribution of a parabolic model of height on weight calculated with brms::brm()"'
#| 
print(b4.5)
```

::: callout-note
###### TODO: Interpret printout
:::

Our quadratic plot requires new `fitted()`- and `predict()`-oriented
data wrangling.

```{r}
#| label: wrangling-for-parabolic-model-b
#| attr-source: '#lst-wrangling-for-parabolic-model-b lst-cap="Data wrangling as a preparation for the parabolic (quadratic) model"'

weight_seq_full <- 
  tibble(weight_s = seq(from = -2.5, to = 2.5, length.out = 30)) %>% 
  mutate(weight_s2 = weight_s^2)

fitd_quad <-
  fitted(b4.5, 
         newdata = weight_seq_full) %>%
  data.frame() %>%
  bind_cols(weight_seq_full)

pred_quad <-
  predict(b4.5, 
          newdata = weight_seq_full) %>%
  data.frame() %>%
  bind_cols(weight_seq_full)  
```

Replicating Figure 4.11.b:

```{r}
#| label: fig-replicate-4.11.b
#| fig-cap: "Replicate Figure 4.11.b: Polynomial regressions of height on weight (standardized), for the full !Kung data. The raw data are shown by the circles. The solid curves show the path of μ in each model, and the shaded regions show the 95% interval of the mean (close to the solid curve) and the 95% interval of predictions (wider). (Note: This is slightly different to the original version with a width of .89 and qunatiles at .055 and .945 resp. Q5.5 and Q94.5)"

p10 <-
  ggplot(data = d3_b, 
         aes(x = weight_s)) +
  geom_ribbon(data = pred_quad, 
              aes(ymin = Q2.5, ymax = Q97.5),
              fill = "grey83") +
  geom_smooth(data = fitd_quad,
              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              stat = "identity",
              fill = "grey70", color = "black", alpha = 1, linewidth = 1/2) +
  geom_point(aes(y = height),
             color = "navyblue", shape = 1, size = 1.5, alpha = 1/3) +
  labs(subtitle = "quadratic",
       y = "height") +
  coord_cartesian(xlim = range(d3_b$weight_s),
                  ylim = range(d3_b$height)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())

p10
```

From a formula perspective, the cubic model is a simple extension of the
quadratic (compare @eq-parabolic-model with @eq-cubic-regression). --
Before we fit the model, we need to wrangle the data again.

```{r}
#| label: wrangling-for-cubic-model-b
#| attr-source: '#lst-wrangling-for-cubic-model-b lst-cap="Data wrangling as a preparation for the cubic model."'

d3_b <-
  d3_b %>% 
  mutate(weight_s3 = weight_s^3)
```

Now fit the model:

```{r}
#| label: fit-cubic-regression-model-b
#| attr-source: '#lst-fit-cubic-regression-model-b lst-cap="Fit a cubic regression model of height on weight (standardized), for the full !Kung data."'

b4.6 <- 
  brms::brm(data = d3_b, 
      family = gaussian,
      height ~ 1 + weight_s + weight_s2 + weight_s3,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(lognormal(0, 1), class = b, coef = "weight_s"),
                brms::prior(normal(0, 1), class = b, coef = "weight_s2"),
                brms::prior(normal(0, 1), class = b, coef = "weight_s3"),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.06")
```

Here's the `fitted()`, `predict()`, and {**ggplot2**} code for Figure
4.11.c, the cubic model.

```{r}
#| label: fig-cubic-regression-model-b
#| fig-cap: "Fit a cubic regression model of height on weight (standardized), for the full !Kung data"
#| attr-source: '#lst-fig-cubic-regression-model-b lst-cap="Fit a cubic regression model of height on weight (standardized), for the full !Kung data: tidyverse version"'

weight_seq_full <- 
  weight_seq_full %>% 
  mutate(weight_s3 = weight_s^3)

fitd_cub <-
  fitted(b4.6, 
         newdata = weight_seq_full) %>%
  as_tibble() %>%
  bind_cols(weight_seq_full)

pred_cub <-
  predict(b4.6, 
          newdata = weight_seq_full) %>%
  as_tibble() %>%
  bind_cols(weight_seq_full) 

p11 <-
  ggplot(data = d3_b, 
       aes(x = weight_s)) +
  geom_ribbon(data = pred_cub, 
              aes(ymin = Q2.5, ymax = Q97.5),
              fill = "grey83") +
  geom_smooth(data = fitd_cub,
              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              stat = "identity",
              fill = "grey70", color = "black", alpha = 1, linewidth = 1/4) +
  geom_point(aes(y = height),
             color = "navyblue", shape = 1, size = 1.5, alpha = 1/3) +
  labs(subtitle = "cubic",
       y = "height") +
  coord_cartesian(xlim = range(d3_b$weight_s),
                  ylim = range(d3_b$height)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())

p11
```

And now we'll fit the good old linear model.

```{r}
#| label: fit-linear-regression-model-b
#| attr-source: '#lst-fit-linear-regression-model-b lst-cap="Fit a linear regression model of height on weight (standardized), for the full !Kung data."'

b4.7 <- 
  brms::brm(data = d3_b, 
      family = gaussian,
      height ~ 1 + weight_s,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(lognormal(0, 1), class = b, coef = "weight_s"),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.07")
```

And here's the `fitted()`, `predict()`, and {**ggplot2**} code for
Figure 4.11.a, the linear model.

```{r}
#| label: fig-linear-regression-model-b
#| fig-cap: "Fit a linear regression model of height on weight (standardized), for the full !Kung data"
#| attr-source: '#lst-fig-linear-regression-model-b lst-cap="Fit a linear regression model of height on weight (standardized), for the full !Kung data: tidyverse version"'

fitd_line <-
  fitted(b4.7, 
         newdata = weight_seq_full) %>%
  as_tibble() %>%
  bind_cols(weight_seq_full)

pred_line <-
  predict(b4.7, 
          newdata = weight_seq_full) %>%
  as_tibble() %>%
  bind_cols(weight_seq_full) 

p9 <-
  ggplot(data = d3_b, 
       aes(x = weight_s)) +
  geom_ribbon(data = pred_line, 
              aes(ymin = Q2.5, ymax = Q97.5),
              fill = "grey83") +
  geom_smooth(data = fitd_line,
              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              stat = "identity",
              fill = "grey70", color = "black", alpha = 1, linewidth = 1/4) +
  geom_point(aes(y = height),
             color = "navyblue", shape = 1, size = 1.5, alpha = 1/3) +
  labs(subtitle = "linear",
       y = "height") +
  coord_cartesian(xlim = range(d3_b$weight_s),
                  ylim = range(d3_b$height)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())

p9
```

Did you notice how we labeled each of the past three plots as `p1`,
`p2`, and `p3`? Here we use those names to plot them all together with
{**patchwork**} syntax.

```{r}
#| label: fig-plot-all-together-b
#| fig-cap: "Plot all three models (linear, quadratic and cubic) together"
#| attr-source: '#lst-fig-plot-all-together-b lst-cap="Plot all three models (linear, quadratic and cubic) together, using the {**patchwork**} syntax"'

# library(patchwork) ## already in setup chunk
p9 | p10 | p11
```

**Converting back to natural scale** You can apply McElreath's
conversion trick within the {**ggplot2**} environment, too. Here it is
with the cubic model.

```{r}
#| label: fig-cubic-regression-natural-scale-b
#| attr-source: '#lst-fig-cubic-regression-natural-scale-b lst-cap="Cubic regression with x-axis in natural scale: tidyverse version"'

at_b <- c(-2, -1, 0, 1, 2)

ggplot(data = d3_b, 
       aes(x = weight_s)) +
  geom_ribbon(data = pred_cub, 
              aes(ymin = Q2.5, ymax = Q97.5),
              fill = "grey83") +
  geom_point(aes(y = height),
             color = "navyblue", shape = 1, size = 1.5, alpha = 1/3) +
  coord_cartesian(xlim = range(d3_b$weight_s)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank()) +
  
  # here it is!
  scale_x_continuous("standardized weight converted back",
                     breaks = at_a,
                     labels = round(at_b*sd(d3_b$weight) + mean(d3_b$weight), 1))
```

#### Splines

```{r}
#| label: load-cherry-blossoms-data-b
#| attr-source: '#lst-load-cherry-blossoms-data-b lst-cap="Load Cherry Blossoms data and display summary (tidyverse version)"'

## R code 4.72 modified ######################
data(package = "rethinking", list = "cherry_blossoms")
d4_b <- cherry_blossoms


# ground-up tidyverse way to summarize
(
d4.2_b <- 
    d4_b %>% 
      gather() %>% 
      group_by(key) %>% 
      summarise(mean = mean(value, na.rm = T),
                sd   = sd(value, na.rm = T),
                ll   = quantile(value, prob = .055, na.rm = T),
                ul   = quantile(value, prob = .945, na.rm = T)) %>% 
      mutate_if(is.double, round, digits = 2) 
)

d4.2_b |> 
  skimr::skim()



```

Kurz's version does not have the mini histograms. I added another
summary with `skimr::skim()` to add tiny graphics.

```{r}
#| label: fig-scatterplot-cbl-b
#| fig-cap: "Display raw data for `doy` (Day of the year of first blossom) against the year: tidyverse version"
#| warning: true

d4_b %>% 
  ggplot(aes(x = year, y = doy)) +
  # color from here: https://www.colorhexa.com/ffb7c5
  geom_point(color = "#ffb7c5", alpha = 1/2) +
  theme_bw() +
  theme(panel.grid = element_blank(),
        # color from here: https://www.colordic.org/w/, inspired by https://chichacha.netlify.com/2018/11/29/plotting-traditional-colours-of-japan/
        panel.background = element_rect(fill = "#4f455c"))

```

By default {**ggplots**} removes missing data records with a warning. It
turns out there are cases with missing data for the `doy` variable.

```{r}
#| label: doy-missing-data
#| attr-source: '#lst-doy-missing-data lst-cap="Display missing data of the day of the year variable `doy`"'

d4_b %>% 
  count(is.na(doy)) %>% 
  mutate(percent = 100 * n / sum(n))
```

Let's follow McElreath and make a subset of the data that excludes cases
with missing data in `doy.` Within the tidyverse, we might do so with
the `tidyr::drop_na()` function. -- This is a much easier way than my
approach using `dplyr::filter()` (See
[StackOverflow](https://stackoverflow.com/a/70848085/7322615):

```{r}
#| label: remove-missing-data-records
#| attr-source: '#lst-remove-missing-data-records lst-cap="Remove records of missing data for the two interesting variable `doy` and `year`"'

d5_b <- 
    d4_b %>% 
        filter(if_all(.col = c(year, doy), .fns = Negate(is.na)))
d5_b
```

But it worked and resulted in the same 827 records.

##### Choice of knots

Now we start with the spline procedure as mentioned in
@prp-procedure-for-generating-b-splines with choosing number and
location of knots:

```{r}
#| label: choose-knots-b
#| attr-source: '#lst-choose-knots-b lst-cap="Choose the knots that serve as pivots for the spline: tiyverse version"'

num_knots15_b <- 15
knot_list15_b <- quantile(d5_b$year, 
          probs = seq(from = 0, to = 1, length.out = num_knots15_b))
knot_list15_b
```

```{r}
#| label: fig-chosen-knots-b
#| fig-cap: "Show scatterplot with chosen knots: tiyverse version"
#| attr-source: '#lst-fig-chosen-knots-b lst-cap="Scatterplot with chosen knots: tiyverse version"'

d5_b %>% 
  ggplot(aes(x = year, y = doy)) +
  geom_vline(xintercept = knot_list15_b, 
             color = "white", alpha = 1/2) +
  geom_point(color = "#ffb7c5", alpha = 1/2) +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank())
```

##### Choice of polynomial degree

```{r}
#| label: compute-b-spline-matrix-b
#| attr-source: '#lst-compute-b-spline-matrix-b lst-cap="Compute the B-spline basis matrix for a cubic spline (degree 3): tidyverse version"'

B_b <- splines::bs(d5_b$year,
        knots = knot_list15_b[-c(1, num_knots15_b)], 
        degree = 3, 
        intercept = TRUE)
```

Look closely at McElreath's tricky `knot_list[-c(1, num_knots)]` code.
Whereas `knot_list` contains 15 ordered `year` values, McElreath shaved
off the first and last `year` values with `knot_list[-c(1, num_knots)]`,
leaving 13. This is because, by default, the `bs()` function places
knots at the boundaries. Since the first and 15^th^ values in
`knot_list15_b` were boundary values for `year`, we removed them to
avoid redundancies. We can confirm this with the code, below.

```{r}
#| label: show-data-structure-b
#| attr-source: '#lst-show-data-structure-b lst-cap="Show data structure to verify that `splines::bs()` adds the boundary knots"'

B_b %>% str()
```

Look at the second to last line,
`- attr(*, "Boundary.knots")= int [1:2] 812 2015`. Those default
`"Boundary.knots"` are the same as `knot_list15_b[c(1, num_knots)]`.
Let's confirm.

```{r}
#| label: boundary-knots
#| attr-source: '#lst-boundary-knots lst-cap="Boundary knots"'
knot_list15_b[c(1, num_knots15_b)]
```

By the `degree = 3` argument, we indicated we wanted a cubic spline.
McElreath used `degree = 1` for Figure 4.12. For reasons I'm not
prepared to get into, here, {**splines**} don't always include intercept
parameters. Indeed, the `splines::bs()` default is `intercept = FALSE`.
McElreath's code indicated he wanted to fit a B-spline that included an
intercept. Thus: `intercept = TRUE`.

Here's how we might make our version of the top panel of Figure 4.13.

```{r}
#| label: fig-basis-functions-b
#| fig-cap: "Basis functions of a cubic spline with 15 knots"
#| attr-source: '#lst-fig-basis-functions-b lst-cap="Basis functions of a cubic spline with 15 knots"'

# wrangle a bit
b_b <-
  B_b %>% 
  data.frame() %>% 
  set_names(str_c(0, 1:9), 10:17) %>%  
  bind_cols(select(d5_b, year)) %>% 
  pivot_longer(-year,
               names_to = "bias_function",
               values_to = "bias")

# plot
b_b %>% 
  ggplot(aes(x = year, y = bias, group = bias_function)) +
  geom_vline(xintercept = knot_list15_b, color = "white", alpha = 1/2) +
  geom_line(color = "#ffb7c5", alpha = 1/2, linewidth = 1.5) +
  ylab("bias value") +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank())
```

To elucidate what's going on in that plot, we might break it up with
`ggplot2::facet_wrap()`.

```{r}
#| label: fig-facet-basis-functions-b
#| fig-cap: "Basis functions of a cubic spline with 15 knots broken up in different facets"
#| fig-height: 10
#| attr-source: '#lst-fig-facet-basis-functions-b lst-cap="Basis functions of a cubic spline with 15 knots broken up in different facets"'

b_b %>% 
  mutate(bias_function = str_c("bias function ", 
                               bias_function)) %>% 
  ggplot(aes(x = year, y = bias)) +
  geom_vline(xintercept = knot_list15_b, 
             color = "white", alpha = 1/2) +
  geom_line(color = "#ffb7c5", linewidth = 1.5) +
  ylab("bias value") +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank(),
        strip.background = element_rect(fill = scales::alpha("#ffb7c5", .25), color = "transparent"),
        strip.text = element_text(size = 8, margin = margin(0.1, 0, 0.1, 0, "cm"))) +
  facet_wrap(~ bias_function, ncol = 1)
```

##### Parameter weights for each basis function

To get the parameter weights for each basis function, we need to
actually define the model and make it run. The model is just a linear
regression. The synthetic basis functions do all the work. We'll use
each column of the matrix `B2_b` as a variable. We'll also have an
intercept to capture the average blossom day. This will make it easier
to define priors on the basis weights, because then we can just conceive
of each as a deviation from the intercept.

Our model will follow the form:

------------------------------------------------------------------------

::: {#def-spines-blossom-model}
$$ \begin{align*}
\text{day\_in\_year}_i \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i  = \alpha + {\sum_{k=1}^K w_k B_{k, i}} \\
\alpha \sim \operatorname{Normal}(100, 10) \\
w_j \sim \operatorname{Normal}(0, 10) \\
\sigma \sim \operatorname{Exponential}(1)
\end{align*}
$$ {#eq-spines-blossom-model}

where $\alpha$ is the intercept, $B_{k, i}$ is the value of the
$k^\text{th}$ bias function on the $i^\text{th}$ row of the data, and
$w_k$ is the estimated regression weight for the corresponding
$k^\text{th}$ bias function.

As for the new parameter type for $\sigma$, the exponential distribution
is controlled by a single parameter, $\lambda$, which is also called the
*rate*. As it turns out, the mean of the exponential distribution is the
inverse of the rate, $1 / \lambda$.
:::

------------------------------------------------------------------------

We are going to use the `dexp()` function to get a sense of what that
prior looks like.

```{r}
#| label: fig-exponential-prior-b
#| fig-cap: "Using the density of the exponential distribution `dexp()` to show the prior"
#| fig-width: 4
#| fig-height: 2.5
#| attr-source: '#lst-fig-exponential-prior-b lst-cap="Using the density of the exponential distribution to show the prior"'

tibble(x = seq(from = 0, to = 10, by = 0.1)) %>% 
  mutate(d = dexp(x, rate = 1)) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = "#ffb7c5") +
  scale_y_continuous(NULL, breaks = NULL) +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank())
```

> We'll use exponential priors for the rest of the book, in place of
> uniform priors. It is much more common to have a sense of the average
> deviation than of the maximum. (McElreath)

Before fitting this model in {**brms**}, we will take a minor detour on
the data structure. In his R code 4.76 (@lst-fit-model-m4.7), McElreath
defined his data in a list, `list( D=d5_a$doy , B = B_a)`. Our approach
will be a little different. Here, we'll add the `B_b` matrix to our
`d5_b` data frame and name the results as `d6_b`.

```{r}
#| label: add-matrix-to-df-b
#| attr-source: '#lst-add-matrix-to-df-b lst-cap="Add B-splines matrix for 15 knots to the data frame by creating a new data frame"'

d6_b <-
  d5_b %>% 
  mutate(B = B_b) 

# take a look at the structure of the new data frame d6_b
d6_b %>% glimpse()
```

In the `d6_b` data, columns `year` through `temp_lower` are all standard
data columns. The `B` column is a *matrix column*, which contains the
same number of rows as the others, but also smuggled in 17 columns
*within* that column. Each of those 17 columns corresponds to one of our
synthetic $B_{k}$ variables. The advantage of such a data structure is
we can simply define our `formula` argument as $doy \sim 1 + B$, where
`B` is a stand-in for `B.1 + B.2 + ... + B.17`.

Here's how we fit the model:

```{r}
#| label: fit-model-b4.8
#| attr-source: '#lst-fit-model-b4.8 lst-cap="Fit model b4.8"'

b4.8 <- 
  brms::brm(data = d6_b,
      family = gaussian,
      doy ~ 1 + B,
      prior = c(brms::prior(normal(100, 10), class = Intercept),
                brms::prior(normal(0, 10), class = b),
                brms::prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.08")
```

Here is the model summary:

```{r}
#| label: summarize-model-b4.8
#| attr-source: '#lst-summarize-model-b4.8 lst-cap="Summarize model b4.8"'

brms:::print.brmsfit(b4.8)
```

In @lst-summarize-model-b4.8 I have used this time the full call for the
generic function of `print()`. Just using `print()` would have been
enough, but I wanted to check if I have understood the concept of a
generic S3 method. The help file pf `print()` says:

> It is a generic function which means that new printing methods can be
> easily added for new classes.

::: callout-note
TODO: Read [S3 Chapter](https://adv-r.hadley.nz/s3.html) of Advanced R
:::

Look at that. Each of the 17 columns in our `B` matrix was assigned its
own parameter. If you fit this model using McElreath's rethinking code,
you'll see the results are very similar. Anyway, McElreath's comments
are in line with the general consensus on spline modes: the parameter
estimates are very difficult to interpret directly. It's often easier to
just plot the results.

First we'll use `brms::as_draws_df()` to transform `d6_b` to a `draw`
object so that it can processed easier by the {**posterior**} package.
(??)

```{r}
#| label: transform-to-draw-objects
#| attr-source: '#lst-transform-to-draw-objects lst-cap="Transform data of model b4.8 to draw objects"'

post6_b <- brms::as_draws_df(b4.8)

glimpse(post6_b)
```

With a little wrangling, we can use summary information from `post6_b`
to make our version of the middle panel of Figure 4.13.

```{r}
#| label: fig-basis-fun-weighted-b
#| fig-cap: "Each basis function weighted by its corresponding parameter"
#| attr-source: '#lst-fig-basis-fun-weighted-b lst-cap="Weight each basis function by its corresponding parameter"'

post6_b %>% 
  select(b_B1:b_B17) %>% 
  set_names(c(str_c(0, 1:9), 10:17)) %>% 
  pivot_longer(everything(), names_to = "bias_function") %>% 
  group_by(bias_function) %>% 
  summarise(weight = mean(value)) %>% 
  full_join(b_b, by = "bias_function") %>% 
  
  # plot
  ggplot(aes(x = year, y = bias * weight, group = bias_function)) +
  geom_vline(xintercept = knot_list15_b, color = "white", alpha = 1/2) +
  geom_line(color = "#ffb7c5", alpha = 1/2, linewidth = 1.5) +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank()) 
```

In case you missed it, the main action in the {**ggplot2**} code was
`y = bias * weight`, where we defined the $y$-axis as the product of
`bias` and `weight`. This is fulfillment of the $w_k B_{k, i}$ parts of
the model.

Now here's how we might use `brms:::fitted.brmsfit()` to get the
*expected values of the posterior predictive distribution* to make the
lower plot of Figure 4.13.

```{r}
#| label: fig-post-pred-dist-b
#| fig-cap: "Expected values of the posterior predictive distribution"
#| attr-source: '#lst-fig-post-pred-dist-b lst-cap="Expected values of the posterior predictive distribution"'

f <- brms:::fitted.brmsfit(b4.8)

f %>% 
  data.frame() %>% 
  bind_cols(d6_b) %>% 
  
  ggplot(aes(x = year, y = doy, ymin = Q2.5, ymax = Q97.5)) + 
  geom_vline(xintercept = knot_list15_b, color = "white", alpha = 1/2) +
  geom_hline(yintercept = brms::fixef(b4.8)[1, 1], color = "white", linetype = 2) +
  geom_point(color = "#ffb7c5", alpha = 1/2) +
  geom_ribbon(fill = "white", alpha = 2/3) +
  labs(x = "year",
       y = "day in year") +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank())
```

If it wasn't clear, the dashed horizontal line intersecting a little
above 100 on the $y$-axis is the posterior mean for the intercept.

##### Model with five knots

Now let's use our skills to remake the simpler model expressed in Figure
4.12. This model, recall, is based on 5 knots.

```{r}
#| label: fit-5-knot-model-b
#| attr-source: '#lst-fit-5-knot-model-b lst-cap="Fit model with only 5 knots"'

# redo the `B` splines
num_knots5_b <- 5
knot_list5_b <- quantile(d5_b$year, 
             probs = seq(from = 0, to = 1, length.out = num_knots5_b))

B5_b <- splines::bs(d5_b$year,
        knots = knot_list5_b[-c(1, num_knots5_b)], 
        # this makes the splines liner rater than cubic
        degree = 1, 
        intercept = TRUE)

# define a new data frame (d7_b)
d7_b <- 
  d5_b %>% 
  mutate(B = B5_b)

b4.9 <- 
  brms::brm(data = d7_b,
      family = gaussian,
      formula = doy ~ 1 + B,
      prior = c(brms::prior(normal(100, 10), class = Intercept),
                brms::prior(normal(0, 10), class = b),
                brms::prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.09")
```

Review the new model summary.

```{r}
#| label: print-5-knot-model-b
#| attr-source: '#lst-print-5-knot-model-b lst-cap="Print summary of 5 knot model"'

print(b4.9)
```

Here we do all the work in bulk to make and save the three subplots for
Figure 4.12.

```{r}
#| label: prepare-3-subplots-figure-4.12-b
#| attr-source: '#lst-prepare-3-subplots-figure-4.12-b lst-cap="Prepare three subplots for Figure 4.12"'

## top
## wrangle a bit

b5_b <-
  invoke(data.frame, d7_b) %>% 
  pivot_longer(starts_with("B"),
               names_to = "bias_function",
               values_to = "bias")
  
# plot
p10 <- 
  b5_b %>% 
  ggplot(aes(x = year, y = bias, group = bias_function)) +
  geom_vline(xintercept = knot_list5_b, color = "white", alpha = 1/2) +
  geom_line(color = "#ffb7c5", alpha = 1/2, linewidth = 1.5) +
  scale_x_continuous(NULL, breaks = NULL) +
  ylab("bias value")

## middle
# wrangle
p11 <-
  brms::as_draws_df(b4.9) %>% 
  select(b_B1:b_B5) %>% 
  set_names(str_c("B.", 1:5)) %>% 
  pivot_longer(everything(), names_to = "bias_function") %>% 
  group_by(bias_function) %>% 
  summarise(weight = mean(value)) %>% 
  full_join(b5_b, by = "bias_function") %>% 
  
  # plot
  ggplot(aes(x = year, y = bias * weight, group = bias_function)) +
  geom_vline(xintercept = knot_list5_b, color = "white", alpha = 1/2) +
  geom_line(color = "#ffb7c5", alpha = 1/2, linewidth = 1.5) +
  scale_x_continuous(NULL, breaks = NULL)

## bottom
# wrangle
f2_b <- fitted(b4.9)

p12 <-
  f2_b %>% 
  data.frame() %>% 
  bind_cols(d7_b) %>% 
  
  # plot
  ggplot(aes(x = year, y = doy, ymin = Q2.5, ymax = Q97.5)) + 
  geom_vline(xintercept = knot_list5_b, color = "white", alpha = 1/2) +
  geom_hline(yintercept = brms::fixef(b4.9)[1, 1], color = "white", linetype = 2) +
  geom_point(color = "#ffb7c5", alpha = 1/2) +
  geom_ribbon(fill = "white", alpha = 2/3) +
  labs(x = "year",
       y = "day in year")
```

Now combine the subplots with {**patchwork**} syntax and behold their
glory.

```{r}
#| label: fig-subplots-figure-4.12-b
#| fig-cap: "Subplots of Figure 4.12"
#| attr-source: '#lst-fig-subplots-figure-4.12-b lst-cap="Subplots of Figure 4.12"'

# library(patchwork) ## already in setup chunk
(p10 / p11 / p12) &
  theme_bw() &
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank())
```

#### Smooth functions for a rough world

Taking the reference to Woods Book about Additive Generalized Models
(see @prp-resources-for-gams) Kurz adds two bonus section:

-   Smooth functions with `brms::s()` and
-   Group predictors with matrix columns

I will include these two addition without completely understanding the
code at the moment (2023-08-18) hoping that I will come later to these
two section with a richer knowledge.

But there is already one important learning I have gotten from the bonus
section: It is evident that fitting models in different circumstances is
-- with minor changes -- follow always the same procedures with almost
the same R code.

### Bonus sections

#### Smooth functions with `brms::s()`

We might use the `brms::get_prior()` function to get a sense of how to
set up the priors when using `brms::s()`

```{r}
#| label: get-priors-bonus
#| attr-source: '#lst-get-prior-bonus lst-cap="Get priors when using `brms::s()`"'

brms::get_prior(data = d5_b,
          family = gaussian,
          doy ~ 1 + s(year))
```

We have an overall intercept (`class = Intercept`), a single $\beta$
parameter for year (`class = b`), a $\sigma$ parameter
(`class = sigma`), and an unfamiliar parameter of `class = sds`. I'm not
going to go into that last parameter in any detail, here. We'll need to
work our way up through @sec-chap13-models-with-memory and the multilevel model
to get a full picture of what it means. The important thing to note here
is that the priors for our `s()`-based alternative to the B-spline
models, above, are going to look a little different. Here's how we might
fit a model `b4.10` as an alternative to `b4.8`.

```{r}
#| label: fit-bosus-model-b4.10
#| attr-source: '#lst-fit-bosus-model-b4.10 lst-cap="Fit bonus model using `brms::s()`"'

b4.10 <-
  brms::brm(data = d5_b,
      family = gaussian,
      doy ~ 1 + s(year),
      prior = c(brms::prior(normal(100, 10), class = Intercept),
                brms::prior(normal(0, 10), class = b),
                brms::prior(student_t(3, 0, 5.9), class = sds),
                brms::prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      control = list(adapt_delta = .99),
      file = "fits/b04.10")

```

```{r}
#| label: print-summary-model-b4.10
#| attr-source: '#lst-print-summary-model-b4.10 lst-cap="Summarize b4.10 model"'

print(b4.10)
```

Our intercept and σ summaries are similar to those we got from `b4.8`.
The rest looks different and maybe a little disorienting. Here's what
happens when we use `brms:::fitted.brmsfit()` to plot the implications
of the model.

```{r}
#| label: plot-implications-model-b4.10
#| attr-source: '#lst-plot-implications-model-b4.10 lst-cap="Implication of model b4.10"'

fitted(b4.10) %>% 
  data.frame() %>% 
  bind_cols(select(d5_b, year, doy)) %>% 
  
  ggplot(aes(x = year, y = doy, ymin = Q2.5, ymax = Q97.5)) +
  geom_hline(yintercept = brms::fixef(b4.10)[1, 1], color = "white", linetype = 2) +
  geom_point(color = "#ffb7c5", alpha = 1/2) +
  geom_ribbon(fill = "white", alpha = 2/3) +
  labs(subtitle = "b4.7 using s(year)",
       y = "day in year") +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"), 
        panel.grid = element_blank())
```

That smooth doesn't look quite the same. Hopefully this isn't terribly
surprising. We used a function from a different package and ended up
with a different underlying statistical model. In fact, we didn't even
use a B-spline. The default for `s()` is to use what's called a *thin
plate* regression spline. If we'd like to fit a B-spline, we have to set
`bs = "bs"`. Here's an example:

```{r}
#| label: fit-model-b4.11
#| attr-source: '#lst-fit-model-b4.11 lst-cap="Fit bonus model b4.11"'

b4.11 <-
  brms::brm(data = d5_b,
      family = gaussian,
      doy ~ 1 + s(year, bs = "bs", k = 19),
      prior = c(brms::prior(normal(100, 10), class = Intercept),
                brms::prior(normal(0, 10), class = b),
                brms::prior(student_t(3, 0, 5.9), class = sds),
                brms::prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      control = list(adapt_delta = .99),
      file = "fits/b04.11")
```

```{r}
#| label: print-summary-model-b4.11
#| attr-source: '#lst-print-summary-model-b4.11 lst-cap="Summarize b4.11 model"'

print(b4.11)
```

Now here's the depiction of our `s()`-based B-spline model:

```{r}
#| label: plot-bonus-model-b4.11
#| attr-source: '#lst-plot-bonus-model-b4.11 lst-cap="B-splines of model b4.11 using `brms::s()`"'

fitted(b4.11) %>% 
  data.frame() %>% 
  bind_cols(select(d5_b, year, doy)) %>% 
  
  ggplot(aes(x = year, y = doy, ymin = Q2.5, ymax = Q97.5)) +
  geom_hline(yintercept = brms::fixef(b4.11)[1, 1], color = "white", linetype = 2) +
  geom_point(color = "#ffb7c5", alpha = 1/2) +
  geom_ribbon(fill = "white", alpha = 2/3) +
  labs(subtitle = 'b4.7_bs using s(year, bs = "bs")',
       y = "day in year") +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"), 
        panel.grid = element_blank())
```

There are still important differences between the underlying statistical
model for `b4.11` and the earlier `b4.8`. Kurz says that he doesn't want
to go into the details why this is the case. For me this missing
explication is both confusing and frustrating! I hope that with more
knowledge and experience in fitting models I will be able to fill the
gaps.

***
:::: {#prp-resources-for-gams2}
Again: Resources for GAMs

::: callout-tip

This is an addition to the in @prp-resources-for-gams already collected
resources on `r glossary("Generalized Additive Model", "GAMS")`.

-   For more on the B-splines and smooths, more generally, check out the
    blog post by the great [Gavin Simpson](https://twitter.com/ucfagls),
    [Extrapolating with B splines and
    GAMs](https://fromthebottomoftheheap.net/2020/06/03/extrapolating-with-gams/).
-   For a high-level introduction to the models you can fit with
    {**mgcv**}, check out the nice talk by [Noam
    Ross](https://twitter.com/noamross), [Nonlinear models in R: The
    wonderful world of mgcv](https://youtu.be/q4_t8jXcQgc), or the
    equally-nice presentation by Simpson, [Introduction to generalized
    additive models with R and mgcv](https://youtu.be/sgw4cu8hrZM).
-   Ross offers a free online course covering {**mgcv**}, called [GAMS
    in R](https://noamross.github.io/gams-in-r-course/), and he
    maintains a GitHub repo cataloguing other GAM-related resources,
    called [Resources for learning about and using GAMs in
    R](https://github.com/noamross/gam-resources).
-   For specific examples of fitting various GAMS with {**brms**}, check
    out Simpson's blog post, [Fitting GAMs with brms: part
    1](https://fromthebottomoftheheap.net/2018/04/21/fitting-gams-with-brms/).
-   Finally, [Tristan Mahr](https://twitter.com/tjmahr) has a nice blog
    post called [Random effects and penalized splines are the same
    thing](https://www.tjmahr.com/random-effects-penalized-splines-same-thing/),
    where he outlined the connections between penalized smooths, such as
    you might fit with {**mgcv**}, with the multilevel model, which
    we'll learn all about starting in @sec-chap13-models-with-memory, which
    helps explain what's going on with the `s()` function in our last
    two models, `b4.10` and `b4.11`.
:::

::::
***

#### Group predictors with matrix columns

When we fit `b4.8`, our direct {**brms**} analogue to McElreath's
`m4.7`, we used a compact syntax to pass a matrix column of predictors
into the `formula.` If memory serves, this is one of the only places in
the text where we see this. It would be easy for the casual reader to
think this was only appropriate for something like a spline model. But
that's not the case. One could use the matrix-column trick as a general
approach. In this bonus section, we'll explore how.

In Section 11.2.6 of their book "Regression and Other Stories", Gelman,
Hill, and Vehtari worked through an example of a multiple regression
model,

------------------------------------------------------------------------

::: {#def-multiple-regression-model}
Example of a multiple regression model

$$
\begin{align*}
y_i & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i & = \alpha + \theta z_i + \sum_{k = 1}^K b_k x_{k, i} \\
& \text{<priors>},
\end{align*}
$$ {#eq-multiple-regression-model}
:::

------------------------------------------------------------------------

where $y_i$ was some continuous variable collected across participants,
$i$. The $\alpha$ term was the intercept and the $\theta$ term was the
regression slope for a binary variable $z$--we'll practice with binary
predictors in the section for categorical-variables in @sec-chap05. More to our interest, the
last portion of the equation is a compact way to convey there are $K$
additional predictors and their associated regression coefficients,
which we might more explicitly express as
$\beta_1 x_{1, i} + \cdots + \beta_k x_{k, i}$, where $K \geq 1$. In
this particular example, $K = 10$, meaning there were ten $x_{k, i}$
predictors, making this an example of a model with 11 total predictor
variables.

Riffing off of Gelman and colleagues, here's how you might simulate data
of this kind.

```{r}
#| label: simulate-data-multiple-regression-b
#| attr-source: '#lst-simulate-data-multiple-regression-b lst-cap="Simulate data for multiple regression analysis"'

# how many cases would you like?
n8_b <- 100

# how many continuous x predictor variables would you like?
k8_b <- 10

# simulate a dichotomous dummy variable for z
# simulate an n by k array for X
set.seed(4)

d8_b <- 
  tibble(z = sample(0:1, size = n8_b, replace = T),
         X = array(runif(n8_b * k8_b, min = 0, max = 1), dim = c(n8_b, k8_b)))

# set the data-generating parameter values
a8_b     <- 1
theta8_b <- 5
b8_b     <- 1:k8_b
sigma8_b <- 2

# simulate the criterion
d9_b <-
  d8_b %>% 
  mutate(y = as.vector(a8_b + X %*% b8_b + theta8_b * z + rnorm(n8_b, mean = 0, sd = sigma8_b)))

# check the data structure
d9_b %>% 
  glimpse()
```

Although our `d9_b` tibble has only three columns, the `X` column is a
matrix column into which we've smuggled ten columns more. Here's how we
might access them more directly.

```{r}
#| label: access-nested-df
#| attr-source: '#lst-access-nested-df lst-cap="Acess nested data frame in tibble"'

d9_b %>% 
  pull(X) %>% 
  glimpse()
```

See? There's an $100 \times 10$ data matrix in there.

Here's how to fit the full model with {**brms**} where we use the
compact matrix-column syntax in the `formula` argument.

```{r}
#| label: fit-model-b4.12-b
#| attr-source: '#lst-fit-model-b4.12-b lst-cap="Fit model b4.12"'

b4.12 <-
  brms::brm(data = d9_b,
      family = gaussian,
      y ~ 1 + z + X,
      prior = c(brms::prior(normal(0, 2), class = Intercept),
                brms::prior(normal(0, 10), class = b),
                brms::prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.12")
```

Check the parameter summary.

```{r}
#| label: print-summary-model-b4.12-b
#| attr-source: '#lst-print-summary-model-b4.12-b lst-cap="Summarize model b4.12"'

print(b4.12)
```

**brms** automatically numbered our $K = 10$ `X` variables as `X1`
through `X10`. As far as applications go, I'm not sure where I'd use
this way of storing and modeling data in real life. But maybe some of
y'all work in domains where this is just the right way to approach your
data needs. If so, good luck and happy modeling.

## `SYNOPSIS`

### Original

The chapter introduces linear regression as a Bayesian procedure.

#### Why are normal distributions normal?

It starts with answering the question: Why are normal distributions
normal? Any process that adds together random values from the same
distribution converges to a normal. Why? Because the different
fluctuations around the mean cancel one another out. The result is a
Gaussian distribution.

#### A language for describing models

Then a formal language for describing models is presented. To know this
language is important because it closes the gap between scientific
knowledge resp. assumptions and the statistical model. Models are
mappings of one set of variables through a probability distribution onto
another set of variables. In these language for models, defines the
first line the likelihood function used in `r glossary("Bayes’ theorem")`. The other
lines define priors.

The procedure: 1. First, we recognize a set of variables to work with.
Some of these variables are observable. We call these *data.* Others are
unobservable things like rates and averages. We call these *parameters.*
2. We define each variable either in terms of the other variables or in
terms of a probability distribution. 3. The combination of variables and
their probability distributions defines a *joint generative model* that
can be used both to simulate hypothetical observations as well as
analyze real ones.

#### Gaussian model of heights {#sec-gaussian-model-of-heights-r}

With the dataset of adults (age \>=18) of the !Kung San foraging people
our goal is to model the height values using a Gaussian distribution. To
define the heights as normally distributed with a mean `μ` and standard
deviation `σ`, we write $h_i \sim \operatorname{Normal}(\mu, \sigma)$.
To complete the model we need prior for the mean
$\mu \sim \operatorname{Normal}(178, 20)$ and the standard deviation
$\sigma \sim \operatorname{Uniform}(0, 50)$.

The prior for `μ` is a broad Gaussian prior, centered on 178 cm, with
95% of probability between 178 ± 40 cm. It range from 138 cm to 218 cm
encompasses a huge range of plausible mean heights for human
populations. So domain-specific information has gone into this prior.
The `σ` prior is a flat prior, an uniform one, that functions just to
constrain `σ` to have positive probability between zero and 50 cm.


##### Plotting the priors

***
:::: {#prp-always-plot-priors}
Always plot the priors!

::: callout-important
Ir is important to plot your priors, so you have a sense of the assumption they
build into your model.
:::
::::
***

`graphics::curve()` draws a curve corresponding to a function over the
interval `[from, to]`. How to use the interval? Experiment with it to
get a range that will show all possible values.

***
:::: {#prp-knowledge-priors}

Using scientific knowledge to build priors

::: callout-important
Very useful is the knowledge that if you double $\sigma$ and subtract resp. add that value to the mean, then you will get an area where 95% of all expected values should be found.

For instance a standard deviation of 20 cm with a mean of 178 cm would
imply that 95% of individual heights lie between 138 cm and 218 cm.
That's a very large range. The same calculation happens with $\sigma$
where a standard deviation of 50 means that 95% of all values lie in the
range of 100 cm.

Using scientific knowledge to build priors is not cheating. The
important thing is that your priors are not based on the values in the
data, but only on what you know about the data before you see it.

:::

::::
***

```{r}
#| label: fig-priors-r
#| fig-cap: "Plotting priors of the height model: rethinking version"
#| attr-source: '#lst-ig-priors-r lst-cap="Plot priors of height model: rethinking version"'

## R code 4.12 & 4.13 ##############
curve(dnorm(x, 178, 20), from = 100, to = 250)
curve(dunif(x, 0, 50), from = -10, to = 60)
```


##### Prior predictive simulation

It'll help to see what these priors imply about the distribution of
individual heights. The `r glossary("prior predictive simulation")` is
an essential part of your modeling. Once you've chosen priors these
imply a joint prior distribution of individual heights. By simulating
from this distribution, you can see what your choices imply about
observable height.

You can quickly simulate heights by sampling from the prior, like you
sampled from the posterior back in @sec-sampling-the-imaginary.
Remember, every posterior is also potentially a prior for a subsequent
analysis, so you can process priors just like posteriors.

Instead of using the density family of distributions when plotting the
priors we are going now to use the random family of distributions (i.e., using `rnorm()` and `runif()` instead of `dnorm()` and `dunif()`.

```{r}
#| label: fig-prior-predictive-sim-r
#| fig-cap: "Simulate heights by sampling from the prior: rethinking version"
#| attr-source: '#lst-fig-prior-predictive-sim-r lst-cap="Heights by sampling from the prior: rethinking version"'

set.seed(4) # to make example reproducible
## R code 4.14 #######################################
sample_mu_r <- rnorm(1e4, 178, 20)
sample_sigma_r <- runif(1e4, 0, 50)
prior_h_r <- rnorm(1e4, sample_mu_r, sample_sigma_r)
rethinking::dens(prior_h_r, norm.comp = TRUE)
```


#### EMPTY: Grid approximation of the posterior distribution

#### EMPTY: Sampling from the posterior

#### Finding the posterior distribution with quap

I skipped the grid approximation and the sampling from it. It has mostly only educational value as it works only with a few parameters and is computationally expensive. Besides it has complex code that has no additional conceptual value to understand the purpose of the procedure.

***
:::: {#prp-using-quap}

`rethinking:quap()` has three parts to complete

::: note-important

1.  A formula or `base::alist()` of formulas that define the likelihood
    and priors.
2.  A data frame or list containing the data.
3.  Some options like start values of method for search optimization.
    Note that the list of start values is a regular `list()`, not an
    `alist()` like the formula list is.
:::

::::
***

```{r}
#| label: post-dist-quap-r4.1
#| attr-source: '#lst-post-dist-quap-r4-1 lst-cap="Finding the posterior distribution with rethinking::quap(): Model r4.1"'

## R code 4.26 modified #############
data(package = "rethinking", list = "Howell1")
d_r <- Howell1
d2_r <- d_r[d_r$age >= 18, ]

## R code 4.27 & 4.28 modified ##############
r4.1 <- rethinking::quap(
    alist(
        height ~ dnorm(mu, sigma),
        mu ~ dnorm(178, 20),
        sigma ~ dunif(0, 50)
    ),
    data = d2_r
)

## R code 4.29 ###########
rethinking::precis(r4.1)
```

#### Sampling from the quap

I skip the long and diffusing discussion about the `r glossary("dispersion matrix", "covariance matrix")`.

Instead of sampling single values from a simple Gaussian distribution, we sample vectors of values from a multi-dimensional Gaussian distribution. The {**rethinking**} package provides a convenience function to do exactly that:

```{r}
#| label: extract-samples-r4.1
#| attr-source: '#lst-extract-samples-r4.1 lst-cap="Extract sample vectors of values from the multi-dimensional Gaussian distribution of r4.1: rethinking version"'


## R code 4.34 #######################
post_r <- rethinking::extract.samples(r4.1, n = 1e4)
rethinking::precis(post_r)
```
We have ended up with a data frame, `post_t`, with 10,000 (1e4) rows and two columns, one column for `μ` and one for `σ`. Each value is a sample from the posterior, so the mean and standard deviation of each column will be very close to the `r glossary("MAP")` values from @lst-post-dist-quap-r4-1.

#### Linear prediction

How do we take our Gaussian model from @sec-gaussian-model-of-heights-r that finally resulted in the `r glossary("prior predictive simulation")` of @fig-prior-predictive-sim-r and add `r glossary("predictor variable", "predictor variables")` to it?

I we plot height against weight of our data in `d2_r` we will intuitively see that there is a relationship. How to make this vague observation into a more precise quantitative `r glossary("statistical model")`?

We are using the same procedure as in {@sec-gaussian-model-height-r}:

1. Construct the model specification: As always we have to construct sensible priors. It helps to plot a `r glossary("prior predictive simulation")`. In comparison to @eq-height-linear-model-m4.1 we have to add a prior for the new
parameter, `β`. It turned out that this prior has to be in log-scale to prevent height and weight values below zero.
2. Then we have to incorporate our new model @eq-height-weight-linear-model2-m4.3 for the mean into the model specification inside `rethinking::quap()`
3. With the resulting model we are able to plot the posterior inference
against the data. 

##### Step 1: Construct model with sensible priors

***
::: {#def-height-weight-linear-model-r4.3}

###### Linear model height against weight (model r4.3)

$$
\begin{align*}
h_{i} \sim \operatorname{Normal}(μ_{i}, σ) \space \space (1) \\ 
μ_{i} = \alpha + \beta(x_{i}-\overline{x}) \space \space (2) \\
\alpha \sim \operatorname{Normal}(178, 20) \space \space (3)  \\ 
\beta \sim \operatorname{Log-Normal}(0,10) \space \space (4) \\
\sigma \sim \operatorname{Uniform}(0, 50)  \space \space (5) \\    
\end{align*} 
$$ {#eq-height-weight-linear-model-r4.3}

:::
***

##### Step 2: Incorporate new model into quap function

```{r}
#| label: find-post-dist-r4.3
#| attr-source: '#lst-find-post-dist-r4.3 lst-cap="Find the posterior distribution of the linear height-weight model: rethinking version"'

## R code 4.42 #############################

# define the average weight, x-bar
xbar_r <- mean(d2_r$weight)

# fit model
r4.3 <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - xbar_r),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d2_r
)

# summary result
## R code 4.44 ############################
rethinking::precis(r4.3)
```
##### Step 3: Plot the posterior inference

```{r}
#| label: fig-raw-data-line-r4.3
#| fig-cap: "Height in centimeters (vertical) plotted against weight in kilograms (horizontal), with the line at the posterior mean plotted in black: rethinking version"
#| attr-source: '#lst-fig-raw-data-line-r4.3 lst-cap="Height against weight with linear regression"'

## R code 4.46 ############################################
plot(height ~ weight, data = d2_r, col = rethinking::rangi2)
post_r4.3 <- rethinking::extract.samples(r4.3)
a_map <- mean(post_r4.3$a)
b_map <- mean(post_r4.3$b)
curve(a_map + b_map * (x - xbar_r), add = TRUE)
```



## STOPPED ORIGINAL HERE! (2023-08-20)


### Tidyverse

#### EMPTY: Why are normal distribution normal?

#### EMPTY: A language for describing models

#### Gausian model of heights

##### Plotting the priors

```{r}
#| label: fig-priors-t
#| fig-cap: "Plotting priors of the height model: tidyverse version"
#| attr-source: '#lst-ig-priors-t lst-cap="Plot priors of height model: tidyverse version"'

tibble(x = seq(from = 100, to = 250, by = .1)) %>% 
ggplot(aes(x = x, y = dnorm(x, mean = 178, sd = 20))) +
geom_line() 

tibble(x = seq(from = -10, to = 60, by = .1)) %>%
ggplot(aes(x = x, y = dunif(x, min = 0, max = 50))) +
geom_line()
```


##### Prior predictive simulation

We simulate from both priors at once to get a `r glossary("prior predictive simulation")` of `heights.`

```{r}
#| label: fig-prior-predictive-sim-t
#| fig-cap: "Simulate heights by sampling from the prior: tidyverse version"
#| attr-source: '#lst-fig-prior-predictive-sim-t lst-cap="Heights by sampling from the prior: tidyverse version"'

set.seed(4)

# simulation
  tibble(sample_mu    = rnorm(1e4, mean = 178, sd  = 20),
         sample_sigma = runif(1e4, min = 0, max = 50)) %>% 
  mutate(height = rnorm(1e4, mean = sample_mu, sd = sample_sigma)) |> 
  
# plot
  ggplot(aes(x = height)) +
  geom_density(fill = "deepskyblue")
```

In contrast to @fig-prior-predictive-sim2-a where we have seen negative heights and giants of more than 5 meter, the prior predictive simulation here is better but still -- with our world knowledge - not realistic as there never have been people of about three meter. But [Robert Pershing Wadlow](https://en.wikipedia.org/wiki/Robert_Wadlow) as the largest man ever measured with 2.72 meter is pretty near of the maximum height in @fig-prior-predictive-sim-t.

#### Finding the posterior distribution with brm

```{r}
#| label: fig-post-dist-brm-t
#| fig-cap: "Posterior Distribution of height model"
#| attr-source: '#lst-fig-post-dist-brm-t lst-cap="Posterior Distribution of height model with `brms::brm()`"'

data(package = "rethinking", list = "Howell1")
d_t <- Howell1
d2_t <- d_t[d_r$age >= 18, ]

t4.1 <- 
  brms::brm(data = d2_t, 
      family = gaussian,
      height ~ 1,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/t04.01")

print(t4.1)
```

The `summary()` functions for `brmsfit` objects gives identical results. A slightly different output results from a Stan-like summary with `t4.1$fit`.

To get different intervals than the 95% default values you can use the `prob` argument for the `print()` or `summary()` function, for instance `summary(t4.1, prob = .89)`.


#### Sampling from a brm() fit

Again I will skip some details of the complex and confusing `r glossary("dispersion matrix")` discussion. The important thing in the tidyverse framework is, that we have to put the `r glossary("HMC")` chains result of `brms::brm()` in a data frame. We do that with the important `brms::as_draws_df()` function:


```{r}
#| label: sample-post-dist-t4.1
#| attr-source: '#lst-sample-post-dist-t4.1 lst-cap="Sample posterior distribution from t4.1"'

post_t <- brms::as_draws_df(t4.1)
head(post_t)
```

From here we can compute `r glossary("covariance")` and `r glossary("correlation")`.

```{r}
#| label: cov-t4.1
#| attr-source: '# lst-cov-t4.1 lst-cap="Compute covariance matrix for model t.41"'

post_t |> 
    select(b_Intercept:sigma) |> 
    cov()
```

```{r}
#| label: cor-t4.1
#| attr-source: '# lst-cor-t4.1 lst-cap="Compute correlation matrix for model t.41"'

post_t |> 
    select(b_Intercept:sigma) |> 
    cor()
```

***
::: callout-note
###### TODO attr-source
Check if all code chunk have an `attr-source:` attribute. The last one I did was in @lst-fig-sim-heights-only-with-priors-b.
:::
***

## STOPPED TIDYVERSE HERE! (B: 2023-08-20)

## Practice

Problems are labeled Easy (E), Medium (M), and Hard (H).

### 4E1

In the model definition below, which line is the likelihood?

$$
y_{i} \sim \operatorname{Normal}(\mu, \sigma) \\
\mu \sim \operatorname{Normal}(0, 10) \\
\sigma \sim \operatorname{Exponential}(1)
$$ {#eq-4e1}

**My answer**: $y_{i} \sim \operatorname{Normal}(\mu, \sigma)$, the other lines are priors.

### 4E2

In the model definition just above, how many parameters are in the posterior distribution? 

**My answer**: Just one.

::: callout-warning
#### Wrong Answer

In the definition @eq-4e1 $y_{i}$  is not to be estimated, but represents the data we have at hand and want to understand through parameters. Erroneously I took this for the parameter. The correct answer is $\mu$ and §\sigma$ as both are the parameters which we attempt to estimate. So the correct answer is: Two
:::

### 4E3

Using the model definition above, write down the appropriate form of `r glossary("Bayes’ theorem")` that includes the proper likelihood and priors.


### 4E4

In the model definition below, which line is the linear model?

$$
y_{i} \sim \operatorname{Normal}(\mu, \sigma) \\
\mu_{i} = \alpha + \beta{x_{i}} \\
\alpha \sim \operatorname{Normal}(0, 10) \\
\beta \sim \operatorname{Normal}(0, 1) \\
\sigma \sim \operatorname{Exponential}(1)
$$
**My answer**: $\mu_{i} = \alpha + \beta{x_{i}}$

### 4E5

In the model definition just above, how many parameters are in the posterior distribution?

**My answer**: There are three parameters.

```{r}
#| label: session-info
#| attr-source: '#lst-session-info lst-cap="Session info"'

sessionInfo()
```

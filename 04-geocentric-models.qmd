# Geocentric Models

## File setup {.unnumbered}

```{r}
#| label: setup

library(tidyverse)
library(patchwork)
```

```{r}
#| label: set-glossary

library(glossary)
glossary_path("glossary.yml")
glossary_popup("hover")
```

```{r}
#| label: glossary-style
#| results: asis

glossary_style(color = "#0066cc", 
               text_decoration = "underline double 1px",
               def_bg = "#333",
               def_color = "white")
```

## `ORIGINAL`

### Why normal distributions? {#sec-why-normal-dist-a}

Why are there so many distribution approximately [normal]{.smallcaps},
resulting in a Gaussian curve? Because there will be more combinations
of outcomes that sum up to a "central" value, rather than to some
extreme value.

::: callout-tip
Any process that adds together random values from the same distribution
converges to a normal.
:::

#### Normal by addition

Whatever the average value of the source distribution, each sample from
it can be thought of as a fluctuation from that average value. When we
begin to add these fluctuations together, they also begin to cancel one
another out. A large positive fluctuation will cancel a large negative
one. The more terms in the sum, the more chances for each fluctuation to
be canceled by another, or by a series of smaller ones in the opposite
direction. So eventually the most likely sum, in the sense that there
are the most ways to realize it, will be a sum in which every
fluctuation is canceled by another, a sum of zero (relative to the
mean).

It doesn't matter what shape the underlying distribution possesses. It
could be uniform, like in our example above, or it could be (nearly)
anything else. Depending upon the underlying distribution, the
convergence might be slow, but it will be inevitable.

See the excellent article [Why is normal distribution so
ubiquitous?](https://ekamperi.github.io/mathematics/2021/01/29/why-is-normal-distribution-so-ubiquitous.html)
which also explains the example of random walks from SR2. See also the
scientific paper [Why are normal distribution
normal?](https://www.journals.uchicago.edu/doi/pdf/10.1093/bjps/axs046)
of the The British Journal for the Philosophy of Science.

#### Normal by multiplication

This is not only valid for addition but also for multiplication of small
values: Multiplying small numbers is approximately the same as addition.

#### Normal by log-multipliation

But even the multiplication of large values tend to produce Gaussian
distributions on the log scale.

#### Using Gaussian distributions

The justifications for using the Gaussian distribution fall into two
broad categories:

1.  **Ontological justification**: The world is full of Gaussian
    distributions, approximately. We're never going to experience a
    perfect Gaussian distribution. But it is a widespread pattern,
    appearing again and again at different scales and in different
    domains. Measurement errors, variations in growth, and the
    velocities of molecules all tend towards Gaussian distributions.

There are many other patterns in nature, so make no mistake in assuming
that the Gaussian pattern is universal. In later chapters, we'll see how
other useful and common patterns, like the exponential and gamma and
Poisson, also arise from natural processes. The Gaussian is a member of
a family of fundamental natural distributions known as the **Exponential
family**. All of the members of this family are important for working
science, because they populate our world.

2.  **Epistemological justification**: The Gaussian represents a
    particular state of ignorance. When all we know or are willing to
    say about a distribution of measures (measures are continuous values
    on the real number line) is their mean and variance, then the
    Gaussian distribution arises as the most consistent with our
    assumptions. It is the least surprising and least informative
    assumption to make. --- If you don't think the distribution should
    be Gaussian, then that implies that you know something else that you
    should tell your golem about, something that would improve
    inference.

::: callout-caution
Although the Gaussian distribution is common in nature and has some nice
properties, there are some risks in using it as a default data model.
The Gaussian distribution has some very thin tails---there is very
little probability in them. Instead most of the mass in the Gaussian
lies within one standard deviation of the mean. Many natural (and
unnatural) processes have much heavier tails.
:::

The Gaussian is a continuous distribution, unlike the discrete
distributions of earlier chapters. Probability distributions with only
discrete outcomes, like the binomial, are called *probability mass*
functions and denoted `Pr`. Continuous ones like the Gaussian are called
*probability density* functions, denoted with *`p`* or just plain old
*`f`*, depending upon author and tradition. For mathematical reasons,
probability densities can be greater than 1. Try `dnorm(0,0,0.1)`", for
example, which is the way to make R calculate *`p`*`(0|0, 0.1)`. The
answer, about 4, is no mistake. Probability *density* is the rate of
change in cumulative probability. So where cumulative probability is
increasing rapidly, density can easily exceed 1. But if we calculate the
area under the density function, it will never exceed 1. Such areas are
also called *probability mass*.

### Model describing language

1.  First, we recognize a set of variables to work with. Some of these
    variables are observable. We call these *data.* Others are
    unobservable things like rates and averages. We call these
    *parameters*.
2.  We define each variable either in terms of the other variables or in
    terms of a *probability distribution*.
3.  The combination of variables and their probability distributions
    defines a *joint generative model* that can be used both to simulate
    hypothetical observations as well as analyze real ones.

This outline applies to models in every field, from astronomy to art
history. The biggest difficulty usually lies in the subject
matter---which variables matter and how does theory tell us to connect
them?---not in the mathematics.

The mathy way to summarize models will be something like: (taken from
Kurz's version as it defines the general approach more clearly.)

------------------------------------------------------------------------

::: {#def-summarize-a-model}
How to summarize a model mathematically?

$$
\begin{align*}
\text{criterion}_i & \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i  & = \beta \times \text{predictor}_i \\
\beta &  \sim \operatorname{Normal}(0, 10) \\
\sigma & \sim \operatorname{Exponential}(1) \\
x_i   &  \sim \operatorname{Normal}(0, 1).
\end{align*}
$$ {#eq-summarize-a-model}
:::

------------------------------------------------------------------------

::: callout-tip
The ampersand sign `&` in the code of Kurz' version is used for
horizontal alignment of different parts for case statements. It wouldn't
be necessary in @eq-summarize-a-model because there is no conditional
statement.
:::

`r glossary("Statistical Model", "Models")` are mappings of one set of
variables through a probability distribution onto another set of
variables. Fundamentally, these models define the ways values of some
variables can arise, given values of other variables.

#### Re-describing the globe tossing model

Recall the proportion of the water problem from previous chapters. The
model in that case was always:

------------------------------------------------------------------------

::: {#def-glob-tossing-model}
Describe the globe tossing model from @sec-sampling-the-imaginary

$$
\begin{align*}
W \sim \operatorname{Binomial}(N, p) \space \space (1)\\
p \sim \operatorname{Uniform}(0, 1)  \space \space (2)
\end{align*}
$$ {#eq-globe-tossing-model}

-   `W`: observed count of water
-   `N`: total number of tosses
-   `p`: proportion of water on the globe

Read the above statement as:

1.  **First line**: The count W is distributed binomially with sample
    size `N` and probability `p`.
2.  **Second line**: The prior for `p` is assumed to be uniform between
    zero and one.
:::

------------------------------------------------------------------------

::: callout-important
The first line in these kind of models always defines the likelihood
function used in Bayes' theorem. The other lines define priors.
:::

Both of the lines in the model of @eq-globe-tossing-model are
`r glossary("stochastic")`, as indicated by the `~` symbol. A stochastic
relationship is just a mapping of a variable or parameter onto a
distribution. It is stochastic because no single instance of the
variable on the left is known with certainty. Instead, the mapping is
probabilistic: Some values are more plausible than others, but very many
different values are plausible under any model. Later, we'll have models
with deterministic definitions in them.

##### From model definition to Bayes' theorem

To relate the mathematical format of @eq-globe-tossing-model to Bayes'
theorem, you could use the model definition to define the posterior
distribution:

------------------------------------------------------------------------

::: {#def-from-model-to-bayes-theorem}
From model definition to Bayes' theorem

$$
Pr(p|w,n) = \frac{\operatorname{Binomial(w|n,p)}\operatorname{Uniform(p|0,1)}}{\int\operatorname{Binomial(w|n,p)}\operatorname{Uniform(p|0,1)}dp}
$$ {#eq-from-model-to-bayes-theorem}
:::

------------------------------------------------------------------------

That monstrous denominator is just the average likelihood again. It
standardizes the posterior to sum to 1. The action is in the numerator,
where the posterior probability of any particular value of `p` is seen
again to be proportional to the product of the likelihood and prior. In
R code form, this is the same grid approximation calculation you've been
using all along.

We will write it in a form that is compatible and therefore better
recognizable with the expression in @eq-from-model-to-bayes-theorem:

```{r}
#| label: model-bayes-theorem-a
#| attr-source: '#lst-model-bayes-theorem-a lst-cap="Calculate the posterior distribution in a way that is regognizable with the Bayes theorem"'

## R code 4.6 ##############
w <- 6
n <- 9
p_grid_a <- seq(from = 0, to = 1, length.out = 100)
posterior_num <- dbinom(w, n, p_grid_a) * dunif(p_grid_a, 0, 1)
posterior_a <- posterior_num / sum(posterior_num)

```

Compare to the calculations in earlier chapters, for example with
@lst-grid-approx-a.

### Gaussian model of height {#sec-gaussian-model-of-height-a}

There are an infinite number of possible Gaussian distributions. Some
have small means. Others have large means. Some are wide, with a large
`σ`. Others are narrow. We want our Bayesian machine to consider every
possible distribution, each defined by a combination of `μ` and `σ`, and
rank them by posterior plausibility. Posterior plausibility provides a
measure of the logical compatibility of each possible distribution with
the data and model.

#### The data

The data contained in `data(Howell1)` are partial census data for the
Dobe area !Kung San, compiled from interviews conducted by Nancy Howell
in the late 1960s. Much more raw data is available for download from
https://tspace.library.utoronto.ca/handle/1807/10395.

For the non-anthropologists reading along, the !Kung San are the most
famous foraging population of the twentieth century, largely because of
detailed quantitative studies by people like Howell.

::: callout-caution
Loading data from a package with `data()` is only possible if you have
already loaded the package. In our example:

```{r}
#| label: loading-data-from-package1_a
#| eval: false


## R code 4.7 #######################
library(rethinking)
data(Howell1)
d_a <- Howell1
```

Because of many function name conflicts with {**brms**} I do not want to
load {**rethinking**} and will call the function of these conflicted
packages with `<package name>::<function name>()` Therefore I have to
use another, not so usual loading strategy of the data set:

```{r}
#| label: loading-data-from-package2_a
#| attr-source: '#lst-loading-data-from-package2_a lst-cap="Load data `Howell1` from the {**rethinking**} package without loading the package and assign the data to an object. Rethinking"'

data(package = "rethinking", list = "Howell1")
d_a <- Howell1
```

The advantage of this strategy is that I have not always to detach the
{**rethinking**} package and to make sure {**rethinking**} is detached
before using {**brms**} as it is necessary in the Kurz's {**tidyverse**}
/ {**brms**} version.
:::

##### Show the data

```{r}
#| label: show-howell-data-a
#| attr-source: '#lst-show-howell-data-a lst-cap="Show and inspect the data: rethinking"'

## R code 4.8 ####################
str(d_a)

## R code 4.9 ###################
rethinking::precis(d_a)
```

This data frame contains four columns. Each column has 544 entries, so
there are 544 individuals in these data. Each individual has a recorded
height (centimeters), weight (kilograms), age (years), and "maleness" (0
indicating female and 1 indicating male).

##### Select the height data of adults

We're going to work with just the height column, for the moment. All we
want for now are heights of adults in the sample. The reason to filter
out non-adults for now is that height is strongly correlated with age,
before adulthood.

```{r}
#| label: select-height-adults-a
#| attr-source: '#lst-select-height-adults-a lst-cap="Select the height data of adults (individuals older or equal than 18 years): base R version"'

## R code 4.10 ###################
head(d_a$height)
 
## R code 4.11 ###################
d2_a <- d_a[d_a$age >= 18, ]

```

We'll be working with the data frame d2 now. It should have 352 rows
(individuals) in it. We will check this with `nrow(d2_a)` =
`r nrow(d2_a)`.

#### The model

Our goal is to model the data in `d2_a` using a Gaussian distribution.

##### Plot the distribution of heights

```{r}
#| label: fig-dist-heights-a
#| fig-cap: "The distribution of the heights data,overlaid by an ideal Gaussian distribution: rethinking version"
#| attr-source: '#lst-fig-dist-heights-a lst-cap="Plot the distribution of the heights of adults: rethinking version"'

rethinking::dens(d2_a$height, norm.comp = TRUE)
```

With the option `norm.comp = TRUE` I have overlaid a Gaussian
distribution to see the differences to the actual data. There are some
differences locally, especially on the peak of the distribution. But the
tails looks nice and we can say that the overall impression of the curve
is Gaussian.

::: callout-caution
###### Decisions how to model the data

Gawking at the raw data, to try to decide how to model them, is usually
not a good idea. The data could be, for example, a mixture of different
Gaussian distributions. Furthermore, the empirical distribution need not
be actually Gaussian in order to justify using a Gaussian probability
distribution.
:::

Define the heights as normally distributed with a mean `μ` and standard
deviation `σ`

------------------------------------------------------------------------

::: {#def-height-normal-dist}
Heights normally distributed

$$
h_{i} \sim \operatorname{Normal}(σ, μ) 
$$ {#eq-height-normal-dist}
:::

------------------------------------------------------------------------

The symbol `h` refers to the list of heights, and the subscript `i`
means each individual element of this list. It is conventional to use
`i` because it stands for index. The index `i` takes on row numbers, and
so in this example can take any value from 1 to 352 (the number of
heights in `d2_a$height`). As such, the model above is saying that all
the golem knows about each height measurement is defined by the same
normal distribution, with mean `μ` and standard deviation `σ`.

The short model in @def-height-normal-dist assumes that the values
$h_{i}$ are *independent and identically distributed*, abbreviated
`i.i.d.`, `iid`, or `IID`.

To complete the model, we're going to need some priors. The parameters
to be estimated are both `μ` and `σ`, so we need a prior `Pr(μ, σ)`, the
joint prior probability for all parameters. In most cases, priors are
specified independently for each parameter, which amounts to assuming
$Pr(μ,σ) = Pr(μ)Pr(σ)$.

------------------------------------------------------------------------

::: {#def-prior-height-model}
Priors for heights model

$$
\begin{align*}
h_{i} \sim \operatorname{Normal}(μ, σ) \space \space (1) \\ 
μ \sim \operatorname{Normal}(178, 20)  \space \space (2) \\ 
μ \sim \operatorname{Uniform}(0, 50)   \space \space (3)      
\end{align*}
$$ {#eq-prior-height-model}

1.  First line represents the likelihood.
2.  Second line is the chosen `μ`(mu, mean) prior.
3.  Third line is the chosen `σ` (sigma, standard deviation) prior.
:::

------------------------------------------------------------------------

Let's think about the chosen value for the priors more in detail:

The prior for `μ` is a broad Gaussian prior, centered on 178 cm, with
95% of probability between 178 ± 40 cm.

Why 178 cm? Your author is 178 cm tall. And the range from 138 cm to 218
cm encompasses a huge range of plausible mean heights for human
populations. So domain-specific information has gone into this prior.
Everyone knows something about human height and can set a reasonable and
vague prior of this kind. But in many regression problems, as you'll see
later, using prior information is more subtle, because parameters don't
always have such clear physical meaning.

Whatever the prior, it's a very good idea to plot your priors, so you
have a sense of the assumption they build into the model.

##### Plot the mean prior (mu)

```{r}
#| label: fig-mean-prior-a
#| fig-cap: "Plot of the chosen mean prior: base R version"

## R code 4.12 ###############################
curve(dnorm(x, 178, 20), from = 100, to = 250)
```

You can see that the golem is assuming that the average height (not each
individual height) is almost certainly between 140 cm and 220 cm. So
this prior carries a little information, but not a lot.

##### Plot the prior of the standard deviation (sigma)

A standard deviation like `σ` must be positive, so bounding it at zero
makes sense. How should we pick the upper bound? In this case, a
standard deviation of 50 cm would imply that 95% of individual heights
lie within 100 cm of the average height. That's a very large range.

```{r}
#| label: fig-sd-prior-a
#| fig-cap: "Plot the chosen prior for the standard deviation: base R version"

## R code 4.13 ###########################
curve(dunif(x, 0, 50), from = -10, to = 60)
```

##### Prior predictive simulation

> Once you've chosen priors for *h, μ*, and *σ*, these imply a joint
> prior distribution of individual heights. By simulating from this
> distribution, you can see what your choices imply about observable
> height. This helps you diagnose bad choices.

Okay, so how to do this? You can quickly simulate heights by sampling
from the prior, like you sampled from the posterior back in
@sec-sampling-the-imaginary. Remember, every posterior is also
potentially a prior for a subsequent analysis, so you can process priors
just like posteriors.

```{r}
#| label: fig-prior-predictive-sim-a
#| fig-cap: "Simulate heights by sampling from the prior: rethinking version"

set.seed(4) # to make example reproducible
## R code 4.14 #######################################
sample_mu_a <- rnorm(1e4, 178, 20)
sample_sigma_a <- runif(1e4, 0, 50)
prior_h_a <- rnorm(1e4, sample_mu_a, sample_sigma_a)
rethinking::dens(prior_h_a, norm.comp = TRUE)
```

> It displays a vaguely bell-shaped density with thick tails. It is the
> expected distribution of heights, averaged over the prior. Notice that
> the prior probability distribution of height is not itself Gaussian.
> This is okay. The distribution you see is not an empirical
> expectation, but rather the distribution of relative plausibilities of
> different heights, before seeing the data.

This comment is strange for me as in my point of view the distribution
*is* Gaussian. It is true that the tails are (a little bit?) thicker
than in the standard Gaussian distribution. But in my view
@fig-prior-predictive-sim-a is more Gaussian than @fig-dist-heights-a.
OK, in @fig-dist-heights-a we have just `r nrow(d2_a)` data and in
@fig-prior-predictive-sim-a we sampled 10,000 times. But this is not a
counter argument for @fig-prior-predictive-sim-a not being a a bell
shaped distribution.

##### Simulate heights from priors with large sd

Prior predictive simulation is very useful for assigning sensible
priors, because it can be quite hard to anticipate how priors influence
the observable variables. As an example, consider a much flatter and
less informative prior for `μ`, like $μ \sim Normal(178, 100)$. Priors
with such large standard deviations are quite common in Bayesian models,
but they are hardly ever sensible.

```{r}
#| label: fig-prior-predictive-sim2-a
#| fig-cap: "Simulate heights from priors with a large standard deviation: rethinking version"

set.seed(4) # to make example reproducible
## R code 4.15 ############################
sample_mu2_a <- rnorm(1e4, 178, 100)
prior_h2_a <- rnorm(1e4, sample_mu2_a, sample_sigma_a)
rethinking::dens(prior_h2_a)
```

The results of @fig-prior-predictive-sim2-a contradicts our scientific
knowledge --- but also our common sense --- about possible height values
of humans. Now the model, before seeing the data, expects people to have
negative height. It also expects some giants. One of the tallest people
in recorded history, [Robert Pershing
Wadlow](https://en.wikipedia.org/wiki/Robert_Wadlow) (1918--1940) stood
272 cm tall. In our prior predictive simulation many people are taller
than this.

Does this matter? In this case, we have so much data that the silly
prior is harmless. But that won't always be the case. There are plenty
of inference problems for which the data alone are not sufficient, no
matter how numerous. Bayes lets us proceed in these cases. But only if
we use our scientific knowledge to construct sensible priors. Using
scientific knowledge to build priors is not cheating. The important
thing is that your prior not be based on the values in the data, but
only on what you know about the data before you see it.

#### Grid approximation of the posterior distribution

We are going to map out the posterior distribution through brute force
calculations.

This is not recommended because it is

-   laborious and computationally expensive
-   usually so impractical as to be essentially impossible.

Therefor the grid approximation technique has limited relevance. Later
on we will use the quadratic approximation with `rethinking::quap()`.

```{r}
#| label: grid-approx-posterior-a

## R code 4.16 ##################################

# establish range of μ and σ values, respectively, to calculate over 
# as well as how many points to calculate in-between. 
mu.list_a <- seq(from = 150, to = 160, length.out = 100)
sigma.list_a <- seq(from = 7, to = 9, length.out = 100)

# expands μ & σ values into a matrix of all of the combinations
post_a <- expand.grid(mu_a = mu.list_a, sigma_a = sigma.list_a)

# compute the log-likelihood at each combination of μ and σ
post_a$LL <- sapply(1:nrow(post_a), function(i) {
  sum(
    dnorm(d2_a$height, post_a$mu[i], post_a$sigma[i], log = TRUE)
  )
})

# multiply the prior by the likelihood
# as the priors are on the log scale adding = multiplying
post_a$prod <- post_a$LL + dnorm(post_a$mu_a, 178, 20, TRUE) +
  dunif(post_a$sigma_a, 0, 50, TRUE)

# getting back on the probability scale without rounding error 
post_a$prob <- exp(post_a$prod - max(post_a$prod))

```

> **Comment to the last line**: the obstacle for getting back on the
> probability scale is that rounding error is always a threat when
> moving from log-probability to probability. If you use the obvious
> approach, like `exp( post$prod )`, you'll get a vector full of zeros,
> which isn't very helpful. This is a result of R's rounding very small
> probabilities to zero.

**Plot contour lines**

```{r}
#| label: fig-contour-plot-a
#| fig-cap: "Draw a contour plot: rethinking version"

## R code 4.17 ##################################
rethinking::contour_xyz(post_a$mu_a, post_a$sigma_a, post_a$prob)

```

You can inspect this posterior distribution, now residing in
`post_a$prob`, using a variety of plotting commands.

**Plot heat map**

```{r}
#| label: fig-heat-map-a
#| fig-cap: "Draw a heat map: rethinking version"

## R code 4.18 ##################################
rethinking::image_xyz(post_a$mu_a, post_a$sigma_a, post_a$prob)

```

#### Sampling from the posterior

To study this posterior distribution in more detail, again I'll push the
flexible approach of sampling parameter values from it. This works just
like it did in @sec-sampling-to-summarize, when you sampled values of
`p` from the posterior distribution for the globe tossing example. The
only new trick is that since there are two parameters, and we want to
sample combinations of them, we first randomly sample row numbers in
post in proportion to the values in \`post_a\$prob´. Then we pull out
the parameter values on those randomly sampled rows.

```{r}
#| label: fig-posterior-sample-a
#| fig-cap: "Samples from the posterior distribution for the heights data. The density of points is highest in the center, reflecting the most plausible combinations of μ and σ. There are many more ways for these parameter values to produce the data, conditional on the model. (rethinking version)"

## R code 4.19 ###########################

# randomly sample row numbers in post_a 
# in proportion to the values in post_a$prob. 
sample.rows <- sample(1:nrow(post_a),
  size = 1e4, replace = TRUE,
  prob = post_a$prob
)

# pull out the parameter values
sample.mu_a <- post_a$mu[sample.rows]
sample.sigma_a <- post_a$sigma[sample.rows]

## R code 4.20 ###########################
plot(sample.mu_a, sample.sigma_a, cex = 0.8, pch = 21, col = rethinking::col.alpha(rethinking:::rangi2, 0.1))

```

The function `col.alpha()` is part of the {**rethinking**} R package.
All it does is make colors transparent, which helps the plot in FIGURE
4.4 (here: @fig-posterior-sample-a) more easily show density, where
samples overlap. `rangi2` itself is just the [definition of a hex color
code](https://github.com/rmcelreath/rethinking/blob/2f01a9c5dac4bc6e9a6f95eec7cae268200a8181/R/colors.r#L22)
("#8080FF") specifying the shade of blue.

Adjust the plot to your tastes by playing around with `cex` (character
expansion, the size of the points), `pch` (plot character), and the 0.1
transparency value.

**Marginal Posterior Density**

Now that you have these samples, you can describe the distribution of
confidence in each combination of `μ` and `σ` by summarizing the
samples. Think of them like data and describe them, just like in
@sec-sampling-to-summarize. For example, to characterize the shapes of
the marginal posterior densities of `μ` and `σ`, all we need to do is:

```{r}
#| label: fig-marg-post-density-a
#| fig-cap: "Shapes of the marginal posterior densities of μ and σ: rethinking version"

## R code 4.21 #########################
rethinking::dens(sample.mu_a)
rethinking::dens(sample.sigma_a)

```

The jargon "marginal" here means "averaging over the other parameters."
Execute the above code and inspect the plots. These densities are very
close to being normal distributions. And this is quite typical. As
sample size increases, posterior densities approach the normal
distribution. If you look closely, though, you'll notice that the
density for σ has a longer right-hand tail. I'll exaggerate this
tendency a bit later, to show you that this condition is very common for
standard deviation parameters.

**Posterior Compatibility Intervals (PIs)**

To summarize the widths of these densities with posterior compatibility
intervals we use:

```{r}
#| label: post-comp-intervals-a
#| attr-source: '#lst-post-comp-intervals-a lst-cap="Posterior Compatibility Intervals (PIs): rethinking version"'

## R code 4.22 ####################
rethinking::PI(sample.mu_a)
rethinking::PI(sample.sigma_a)
```

Since these samples are just vectors of numbers, you can compute any
statistic from them that you could from ordinary data: `mean`, `median`,
or `quantile`, for example.

**Sample size and the normality of sigmas posterior**

Before moving on to using quadratic approximation `rethinking::quap()`
as shortcut to all of this inference, it is worth repeating the analysis
of the height data above, but now with only a fraction of the original
data. The reason to do this is to demonstrate that, in principle, the
posterior is not always so Gaussian in shape. There's no trouble with
the mean, `μ`. For a Gaussian likelihood and a Gaussian prior on `μ`,
the posterior distribution is always Gaussian as well, regardless of
sample size. It is the standard deviation `σ` that causes problems. So
if you care about `σ`---often people do not---you do need to be careful
of abusing the quadratic approximation.

The deep reasons for the posterior of `σ` tending to have a long
right-hand tail are complex. But a useful way to conceive of the problem
is that variances must be positive. As a result, there must be more
uncertainty about how big the variance (or standard deviation) is than
about how small it is. For example, if the variance is estimated to be
near zero, then you know for sure that it can't be much smaller. But it
could be a lot bigger.

Let's quickly analyze only 20 of the heights from the height data to
reveal this issue. To sample 20 random heights from the original list:

```{r}
#| label: fig-sample-only-20-a
#| fig-cap: "Sample 20 heights: rethinking version"

## R code 4.23 ######################################
d3_a <- sample(d2_a$height, size = 20)

## R code 4.24 ######################################
mu2_a.list <- seq(from = 150, to = 170, length.out = 200)
sigma2_a.list <- seq(from = 4, to = 20, length.out = 200)
post2_a <- expand.grid(mu = mu2_a.list, sigma = sigma2_a.list)
post2_a$LL <- sapply(1:nrow(post2_a), function(i) {
  sum(dnorm(d3_a,
    mean = post2_a$mu[i], sd = post2_a$sigma[i],
    log = TRUE
  ))
})
post2_a$prod <- post2_a$LL + dnorm(post2_a$mu, 178, 20, TRUE) +
  dunif(post2_a$sigma, 0, 50, TRUE)
post2_a$prob <- exp(post2_a$prod - max(post2_a$prod))
sample2_a.rows <- sample(1:nrow(post2_a),
  size = 1e4, replace = TRUE,
  prob = post2_a$prob
)
sample2_a.mu <- post2_a$mu[sample2_a.rows]
sample2_a.sigma <- post2_a$sigma[sample2_a.rows]
plot(sample2_a.mu, sample2_a.sigma,
  cex = 0.5,
  col = rethinking::col.alpha(rethinking:::rangi2, 0.1),
  xlab = "mu", ylab = "sigma", pch = 16
)

```

you'll see another scatter plot of the samples from the posterior
density, but this time you'll notice a distinctly longer tail at the top
of the cloud of points.

**Marginal Posterior Density with only 20 rows**

You should also inspect the marginal posterior density for σ, averaging
over μ, produced with:

```{r}
#| label: fig-marg-post-density-a2
#| fig-cap: "Marginal posterior density for σ, averaging over μ: rethinking version"

## R code 4.25
rethinking::dens(sample2_a.sigma, norm.comp = TRUE)

```

#### Finding the posterior distribution with quap()

> To build the **quadratic approximation**, we'll use quap, a command in
> the `rethinking` package. The `quap` function works by using the model
> definition you were introduced to earlier in this chapter. Each line
> in the definition has a corresponding definition in the form of R
> code. The engine inside quap then uses these definitions to define the
> posterior probability at each combination of parameter values. Then it
> can climb the posterior distribution and find the peak, its MAP
> (**Maximum A Posteriori** estimate). Finally, it estimates the
> quadratic curvature at the MAP to produce an approximation of the
> posterior distribution. (parenthesis and emphasis are mine)

::: callout-note
The procedure used by `rethinking:quap()` is very similar to what many
non-Bayesian procedures do, just without any priors.
:::

1.  We start with the Howell1 data frame for adults `d2_a` (age \>= 18).
    We will place the R code equivalents into an `alist()` We are going
    to use the @def-prior-height-model. (Code 4.27).
2.  Then we fit the model with `rethinking::quap()` to the data in the
    data frame `d2_a` (Code 4.28) to `m4.1`.
3.  Now we can have a look with `rethinking::precis()` at the posterior
    distribution (Code 4.29).

```{r}
#| label: post-dist-quap-m4-1-a
#| attr-source: '#lst-post-dist-quap-m4-1-a lst-cap="Finding the posterior distribution with rethinking::quap()"'

## R code 4.27 ######################
flist <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(178, 20),
  sigma ~ dunif(0, 50)
)

## R code 4.28 ######################
m4.1 <- rethinking::quap(flist, data = d2_a)

## R code 4.29 ######################
rethinking::precis(m4.1)

```

> These numbers provide Gaussian approximations for each parameter's
> *marginal* distribution. This means the plausibility of each value of
> `_μ_`, after averaging over the plausibilities of each value of `_σ_`,
> is given by a Gaussian distribution with mean 154.6 and standard
> deviation 0.4.
>
> The 5.5% and 94.5% quantiles are percentile interval boundaries,
> corresponding to an 89% compatibility interval. Why 89%? It's just the
> default. It displays a quite wide interval, so it shows a
> high-probability range of parameter values. If you want another
> interval, such as the conventional and mindless 95%, you can use
> `precis(m4.1, prob=0.95)`. But I don't recommend 95% intervals,
> because readers will have a hard time not viewing them as significance
> tests. 89 is also a prime number, so if someone asks you to justify
> it, you can stare at them meaningfully and incant, "Because it is
> prime." That's no worse justification than the conventional
> justification for 95%.

I encourage you to compare these 89% boundaries to the compatibility
intervals from the grid approximation in @lst-post-comp-intervals-a
earlier. You'll find that they are almost identical. When the posterior
is approximately Gaussian, then this is what you should expect.

##### Start values for `rethinking::quap()` {#sec-start-values-rethinking}

Mean and standard deviation are good values to start values for hill
climbing. If you don't specify `rethinking::quap()` will use a random
value.

```{r}
#| label: start-values-quap
#| attr-source: '#start-values-quap lst-cap="Define start values for rethinking::quap()"'

## R code 4.30 ######################
start <- list(
  mu = mean(d2_a$height),
  sigma = sd(d2_a$height)
)
m4.1_2 <- rethinking::quap(flist, data = d2_a, start = start)
rethinking::precis(m4.1_2)

```

::: callout-note
###### list() and alist()

Note that the list of start values is a regular `list`, not an `alist`
like the formula list is. The two functions `alist` and `list` do the
same basic thing: allow you to make a collection of arbitrary R objects.
They differ in one important respect: `list` evaluates the code you
embed inside it, while `alist` does not. So when you define a list of
formulas, you should use `alist`, so the code isn't executed. But when
you define a list of start values for parameters, you should use `list`,
so that code like `mean(d2_a$height)` will be evaluated to a numeric
value.
:::

**Slicing in more information**

> The priors we used before are very weak, both because they are nearly
> flat and because there is so much data. So I'll splice in a more
> informative prior for `*μ*`, so you can see the effect. All I'm going
> to do is change the standard deviation of the prior to 0.1, so it's a
> very narrow prior. I'll also build the formula right into the call to
> `quap` this time.

```{r}
#| label: post-dist-quap-m4.2
#| attr-source: '#lst-post-dist-quap-m4.2 lst-cap="Finding the posterior distribution with a narrower prior rethinking::quap()"'

## R code 4.31 ###########################
m4.2 <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu ~ dnorm(178, 0.1),
    sigma ~ dunif(0, 50)
  ),
  data = d2_a
)
rethinking::precis(m4.2)

```

> Notice that the estimate for `*μ*` has hardly moved off the prior. The
> prior was very concentrated around 178. So this is not surprising. But
> also notice that the estimate for `*σ*` has changed quite a lot, even
> though we didn't change its prior at all. Once the golem is certain
> that the mean is near 178---as the prior insists---then the golem has
> to estimate `*σ*` conditional on that fact. This results in a
> different posterior for `*σ*`, even though all we changed is prior
> information about the other parameter.

::: callout-caution
###### `μ` has hardly moved off the prior

At first I did not understand "that the estimate for `*μ*` has hardly
moved off the prior". I thought this assertion refers to the value of
`*μ*` in both calculation. *μ* has changed considerably from 154.61 to
177.86 and under that assumption the above quote does not make sense.

But in contrast to my wrong assumption the assertion refers to the
difference between the chosen prior (178) and the resulting value of
`*μ*` (177.86).
:::

#### Sampling from a quap

The above explains how to get a quadratic approximation of the
posterior, using `rethinking::quap()`. But how do we then get samples
from the quadratic approximate posterior distribution? --- When R
constructs a quadratic approximation, it calculates not only standard
deviations for all parameters, but also the covariances among all pairs
of parameters. Just like a mean and standard deviation (or its square, a
variance) are sufficient to describe a one-dimensional Gaussian
distribution, a list of means and a matrix of variances and covariances
are sufficient to describe a multi-dimensional Gaussian distribution.

```{r}
#| label: calc-var-cov-m4.1-a
#| attr-source: '#lst-calc-var-cov-m4.1-a lst-cap="Calculation of the variance-covariance matrix: rethinking version"'

## R code 4.32 ###################
rethinking::vcov(m4.1)
```

`vcov()` returns the variance-covariance matrix of the main parameters
of a fitted model object. In the above {**rethinking**} version is uses
the class `map2stan` for a fitted Stan model as `m4.1` is of class
`map`.

::: callout-caution
###### rethinking::vcov

In @lst-calc-var-cov-m4.1-a I am explicitly using the package
{**rethinking**} for the `vcov()` function. The same function is also
available as a base R function with `stats::vcov()`. But this generates
an error because there is no method known for an object of class `map`
from the rethinking package. The help file for `stats::vcov()` only says
that the `vcov` object is an S3 method for classes `lm`, `glm`, `mlm`
and `aov` but not for `map`.

> Error in UseMethod("vcov") : no applicable method for 'vcov' applied
> to an object of class "map"

I could have used only `vcov()`. But this only works when the
{**rethinking**} package is already loaded. In that case R knows because
of the class of the object which `vcov()` version to use. In this case:
class of object = `class(m4.1)` `r class(m4.1)`.
:::

@lst-calc-var-cov-m4.1-a results in a variance-covariance matrix. It is
the multi-dimensional glue of a quadratic approximation, because it
tells us how each parameter relates to every other parameter in the
posterior distribution. A variance-covariance matrix can be factored
into two elements: (1) a vector of variances for the parameters and (2)
a correlation matrix that tells us how changes in any parameter lead to
correlated changes in the others.

```{r}
#| label: vcov-decomp-m4.1-a

## R code 4.33 #######################
base::diag(rethinking::vcov(m4.1))      # <1>
stats::cov2cor(rethinking::vcov(m4.1))  # <2>
```

1.  `base::diag()` extracts the diagonal of the (variance-covariance)
    matrix. The two-element vector in the output is the list of
    variances. If you take the square root of this vector, you get the
    standard deviations that are shown in `rethinking::precis()` output.
2.  `stats::cov2cor()` scales a covariance matrix into the corresponding
    correlation matrix. The two-by-two matrix in the output is this
    correlation matrix. Each entry shows the correlation, bounded
    between −1 and +1, for each pair of parameters. The 1's indicate a
    parameter's correlation with itself. If these values were anything
    except 1, we would be worried. The other entries are typically
    closer to zero, and they are very close to zero in this example.
    This indicates that learning μ tells us nothing about σ and likewise
    that learning σ tells us nothing about μ. This is typical of simple
    Gaussian models of this kind. But it is quite rare more generally,
    as you'll see in later chapters.

Okay, so how do we get samples from this multi-dimensional posterior?
Now instead of sampling single values from a simple Gaussian
distribution, we sample vectors of values from a multi-dimensional
Gaussian distribution.

```{r}
#| label: extract-samples-m4.1-a
#| attr-source: '#lst-extract-samples-m4.1-a lst-cap="Extract sample vectors of values from the multi-dimensional Gaussian distribution of m4.1: rethinking version"'


## R code 4.34 #######################
post3_a <- rethinking::extract.samples(m4.1, n = 1e4)
head(post3_a)
```

You end up with a data frame, post, with 10,000 (1e4) rows and two
columns, one column for `_μ_` and one for `_σ_`. Each value is a sample
from the posterior, so the mean and standard deviation of each column
will be very close to the MAP values from before. You can confirm this
by summarizing the samples:

```{r}
#| label: summary-samples-m4.1-a
#| attr-source: '#lst-summary-samples-m4.1-a lst-cap="Summary the extracted samples: rethinking version"'

## R code 4.35 ##################
rethinking::precis(post3_a)
```

Compare these values to the output from @lst-post-dist-quap-m4-1-a. And
you can use `plot(post)` to see how much they resemble the samples from
the grid approximation in FIGURE 4.4 (here @fig-posterior-sample-a).
These samples also preserve the covariance between `_μ_` and `_σ_`. This
hardly matters right now, because `_μ_` and `_σ_` don't co-vary at all
in this model. But once you add a predictor variable to your model,
covariance will matter a lot.

```{r}
#| label: fig-posterior-sample-vectors-a
#| fig-cap: "Samples from the vectors of values from a multi-dimensional Gaussian distribution. The density of points is highest in the center, reflecting the most plausible combinations of μ and σ. It is very similar to @fig-posterior-sample-a (rethinking version)"

base::plot(post3_a)
```

##### Under the hood with multivariate sampling {#sec-under-the-hood-multivariate-sampling-a}

The function `rethinking::extract.samples()` is for convenience. It is
just running a simple simulation of the sort you conducted near the end
of @sec-sampling-the-imaginary with @lst-sim-pred-samples-a. Here's a
peak at the motor. The work is done by a multi-dimensional version of
`stats::rnorm()`, `MASS::mvrnorm()`. The function `stats::rnorm()`
simulates random Gaussian values, while `MASS::mvrnorm()` simulates
random vectors of multivariate Gaussian values. Here's how to use it the
{**MASS**} function to do what `rethinking::extract.samples()` does:

```{r}
#| label: fig-posterior-sample-vectors-MASS-a
#| fig-cap: "Extract samples from the vectors of values from a multi-dimensional Gaussian distribution using the {**MASS**} package. It is same calculation as in @fig-posterior-sample-a (rethinking version)"


## R code 4.36 ######################
post4_a <- MASS::mvrnorm(n = 1e4, mu = rethinking::coef(m4.1), 
                      Sigma = rethinking::vcov(m4.1))
plot(post4_a)
```

### Linear prediction

What we've done until now is just a Gaussian model of height in a
population of adults. But typically, we are interested in modeling how
an outcome is related to some other variable, a predictor variable. If
the predictor variable has any statistical association with the outcome
variable, then we can use it to predict the outcome. When the predictor
variable is built inside the model in a particular way, we'll have
linear regression.

Let's look at how height in these Kalahari foragers (the outcome
variable) co-varies with weight (the predictor variable).

```{r}
#| label: fig-height-against-weight-a
#| fig-cap: "Adult height and weight against one another"

## R code 4.37 #####################
plot(d2_a$height ~ d2_a$weight)
```

There's obviously a relationship: Knowing a person's weight helps you
predict height. To make this vague observation into a more precise
quantitative model that relates values of `weight` to plausible values
of `height`, we need some more technology. How do we take our Gaussian
model from @sec-gaussian-model-of-height-a and incorporate predictor
variables?

#### The linear model strategy

##### Model definition

Recall @def-prior-height-model for the Gaussian height model. How do we
get `weight` into this model? Let `_x_` be the name for the column of
weight measurements, `d2_a$weight`. Let the average of the `_x_` values
be $\overline{x}$, "ex bar". Now we have a predictor variable `_x_`,
which is a list of measures of the same length as `_h_`. To get weight
into the model, we define the mean `_μ_` as a function of the values in
`_x_`.

------------------------------------------------------------------------

::: {#def-height-weight-linear-model}
Linear model height against weight

$$
\begin{align*}
h_{i} \sim \operatorname{Normal}(μ_{i}, σ) \space \space (1) \\ 
μ_{i} = \alpha + \beta(x_{i}-\overline{x}) \space \space (2) \\
\alpha \sim \operatorname{Normal}(178, 20) \space \space (3)  \\ 
\beta \sim \operatorname{Normal}(0,10) \space \space (4) \\
\sigma \sim \operatorname{Uniform}(0, 50) \space \space (5)      
\end{align*}
$$ {#eq-height-weight-linear-model}

(1) **Likelihood (Probability of the data)**: The first line is nearly
    identical to before, except now there is a little index $i$ on the
    $μ$ as well as the $h$. You can read $h_{i}$ as "each height" and
    $\mu_{i}$ as "each $μ$" The mean $μ$ now depends upon unique values
    on each row $i$. So the little $i$ on $\mu_{i}$ indicates that *the
    mean depends upon the row*.

(2) **Linear model**: The mean $μ$ is no longer a parameter to be
    estimated. Rather, as seen in the second line of the model, $\mu{i}$
    is constructed from other parameters, $\alpha$ and $\beta$, and the
    observed variable $x$. This line is not a stochastic relationship
    ----- there is no `~` in it, but rather an `=` in it ----- because
    the definition of $\mu{i}$ is deterministic. That is to say that,
    once we know $\alpha$ and $\beta$ and $x_{i}$, we know $\mu{i}$ with
    certainty. (More details in @sec-linear-model-a.)

(3) **includes (3),(4) and(5) with** $\alpha, \beta, \sigma$ priors: The
    remaining lines in the model define distributions for the unobserved
    variables. These variables are commonly known as parameters, and
    their distributions as priors. There are three parameters:
    $\alpha, \beta, \sigma$. You've seen priors for $\alpha$ and $\beta$
    before, although $\sigma$ was called $\mu$ back then. (More details
    in @sec-priors-a)
:::

------------------------------------------------------------------------

##### Linear model {#sec-linear-model-a}

The value $x_{i}$ in the second line of @def-height-weight-linear-model
is just the weight value on row $i$. It refers to the same individual as
the height value, $h_{i}$, on the same row. The parameters $\alpha$ and
$\beta$ are more mysterious. Where did they come from? We made them up.
The parameters $\mu$ and $\sigma$ are necessary and sufficient to
describe a Gaussian distribution. But $\alpha$ and $\beta$ are instead
devices we invent for manipulating $\mu$, allowing it to vary
systematically across cases in the data.

You'll be making up all manner of parameters as your skills improve. One
way to understand these made-up parameters is to think of them as
targets of learning. Each parameter is something that must be described
in the posterior distribution. So when you want to know something about
the data, you ask your golem by inventing a parameter for it. This will
make more and more sense as you progress.

What does the second line of @def-height-weight-linear-model? It tells
the regression golem that you are asking two questions about the mean of
the outcome.

1.  What is the expected height when $x_{i} = \overline{x}$? The
    parameter $\alpha$ answers this question, because when
    $x_{i} = \overline{x}$, $\mu_{i} = \alpha$. For this reason,
    $\alpha$ is often called the **intercept**. But we should think not
    in terms of some abstract line, but rather in terms of the meaning
    with respect to the observable variables.
2.  What is the change in expected height, when $x_{i}$ changes by 1
    unit? The parameter $\beta$ answers this question. It is often
    called a **slope**, again because of the abstract line. Better to
    think of it as a rate of change in expectation.

Jointly these two parameters ask the golem to find a line that relates
$x$ to $h$, a line that passes through $\alpha$ when
$x_{i} = \overline{x}$ and has slope $\beta$. That is a task that golems
are very good at. It's up to you, though, to be sure it's a good
question.

##### Priors {#sec-priors-a}

The prior for $\beta$ in @def-height-weight-linear-model deserves
explanation. Why have a Gaussian prior with mean zero? This prior places
just as much probability below zero as it does above zero, and when
$\beta = 0$, weight has no relationship to height. To figure out what
this prior implies, we have to simulate the prior predictive
distribution.

The goal is to simulate heights from the model, using only the priors.
First, let's consider a range of weight values to simulate over. The
range of observed weights will do fine. Then we need to simulate a bunch
of lines, the lines implied by the priors for $\alpha$ and $\beta$.
Here's how to do it, setting a seed so you can reproduce it exactly:

```{r}
#| label: fig-sim-heights-only-with-priors-a
#| fig-cap: "Simulating heights from the model, using only the priors: rethinking version"

## R code 4.38 #####################
set.seed(2971)
N_100_a <- 100 # 100 lines
a <- rnorm(N_100_a, 178, 20)
b <- rnorm(N_100_a, 0, 10)


## R code 4.39 #####################
plot(NULL,
  xlim = range(d2_a$weight), ylim = c(-100, 400),
  xlab = "weight", ylab = "height"
)
abline(h = 0, lty = 2)
abline(h = 272, lty = 1, lwd = 0.5)
mtext("b ~ dnorm(0,10)")
xbar <- mean(d2_a$weight)
for (i in 1:N_100_a) {
  curve(a[i] + b[i] * (x - xbar),
    from = min(d2_a$weight), to = max(d2_a$weight), add = TRUE,
    col = rethinking::col.alpha("black", 0.2)
  )
}

```

For reference, I've added a dashed line at zero---no one is shorter than
zero---and the "Wadlow" line at 272 cm for the world's tallest person.
The pattern doesn't look like any human population at all. It
essentially says that the relationship between weight and height could
be absurdly positive or negative. Before we've even seen the data, this
is a bad model. Can we do better?

We can do better immediately. We know that average height increases with
average weight, at least up to a point. Let's try restricting it to
positive values. The easiest way to do this is to define the prior as
Log-Normal instead. Defining $\beta$ as `Log-Normal(0,1)` means to claim
that the logarithm of $\beta$ has a Normal(0,1) distribution.

------------------------------------------------------------------------

::: {#def-prior-log-normal-dist}
Defining the prior as Log-Normal distribution

$$
\beta \sim \operatorname{Log-Normal}(0,1)
$$ {#eq-prior-log-normal-dist}
:::

------------------------------------------------------------------------

Base R provides the `dlnorm()` and `rlnorm()` densities for working with
log-normal distributions. You can simulate this relationship to see what
this means for $\beta$:

```{r}
#| label: fig-log-normal-a
#| fig-cap: "Log-Normal distribution: rethinking version"


set.seed(4) # to reproduce with tidyverse version

## R code 4.40 ####################
b <- rlnorm(1e4, 0, 1)
rethinking::dens(b, xlim = c(0, 5), adj = 0.1)
```

If the logarithm of $\beta$ is normal, then $\beta$ itself is strictly
positive. The reason is that `exp(x)` is greater than zero for any real
number `x`. This is the reason that Log-Normal priors are commonplace.
They are an easy way to enforce positive relationships.

So what does this earn us? Do the prior predictive simulation again, now
with the Log-Normal prior:

```{r}
#| label: fig-prior-pred-sim-a
#| fig-cap: "Prior predictive simulation again, now with the Log-Normal prior: rethinking version"


## R code 4.41 ###################
set.seed(2971)
N_100_a <- 100 # 100 lines
a <- rnorm(N_100_a, 178, 20)
b <- rlnorm(N_100_a, 0, 1)

## R code 4.39 ###################
plot(NULL,
  xlim = range(d2_a$weight), ylim = c(-100, 400),
  xlab = "weight", ylab = "height"
)
abline(h = 0, lty = 2)
abline(h = 272, lty = 1, lwd = 0.5)
mtext("b ~ dnorm(0,10)")
xbar <- mean(d2_a$weight)
for (i in 1:N_100_a) {
  curve(a[i] + b[i] * (x - xbar),
    from = min(d2_a$weight), to = max(d2_a$weight), add = TRUE,
    col = rethinking::col.alpha("black", 0.2)
  )
}


```

This is much more sensible. There is still a rare impossible
relationship. But nearly all lines in the joint prior for $\alpha$ and
$\beta$ are now within human reason.

::: callout-note
###### What is the correct prior?

There is no more a uniquely correct prior than there is a uniquely
correct likelihood. Statistical models are machines for inference. Many
machines will work, but some work better than others. Priors can be
wrong, but only in the same sense that a kind of hammer can be wrong for
building a table.

Priors encode states of information before seeing data. So priors allow
us to explore the consequences of beginning with different information.
In cases in which we have good prior information that discounts the
plausibility of some parameter values, like negative associations
between height and weight, we can encode that information directly into
priors. When we don't have such information, we still usually know
enough about the plausible range of values. And you can vary the priors
and repeat the analysis in order to study how different states of
initial information influence inference. Frequently, there are many
reasonable choices for a prior, and all of them produce the same
inference.
:::

::: callout-note
###### Prior predictive simulation and p-hacking

When the model is adjusted in light of the observed data, then p-values
no longer retain their original meaning. False results are to be
expected. This is valid for Bayesian and Non-Bayesian statistics. Even
if Bayesian statistics don't pay any attention to p-values, the danger
remains. We could choose our priors conditional on the observed sample,
just to get some desired (wrong) result. It is therefore important to
choose priors conditional on pre-data knowledge of the variables---their
constraints, ranges, and theoretical relationships. We should always
judge our priors against general facts, not the sample.
:::

#### Finding the posterior distribution

The code needed to approximate the posterior is a straightforward
modification of the kind of code you've already seen. All we have to do
is incorporate our new model for the mean into the model specification
inside `rethinking::quap()` and be sure to add a prior for the new
parameter, `β`. Let's repeat the model definition, now with the
corresponding R code as footnotes:

------------------------------------------------------------------------

::: {#def-find-post-dist}
Finding the posterior distribution

$$
\begin{align*}
h_{i} \sim \operatorname{Normal}(μ_{i}, σ) \space \space (1) \\ 
μ_{i} = \alpha + \beta(x_{i}-\overline{x}) \space \space (2) \\
\alpha \sim \operatorname{Normal}(178, 20) \space \space (3)  \\ 
\beta \sim \operatorname{Log-Normal}(0,10) \space \space (4) \\
\sigma \sim \operatorname{Uniform}(0, 50)  \space \space (5) \\    
\end{align*} 
$$ {#eq-find-post-dist-a}

```         
height ~ dnorm(mu, sigma)     # <1>
mu <- a + b * (weight - xbar) # <2>
a ~ dnorm(178, 20)            # <3>
b ~ dlnorm(0, 10)             # <4>
sigma ~ dunif(0, 50)          # <5>
```
:::

------------------------------------------------------------------------

Notice that the linear model, in the R code on the right-hand side, uses
the R assignment operator, `<-` instead of the symbol `=`.

```{r}
#| label: find-post-dist-a
#| attr-source: '#lst-find-post-dist-a lst-cap="Find the posterior distribution of the linear height-weight model: rethinking version"'

## R code 4.42 #############################

# define the average weight, x-bar
xbar <- mean(d2_a$weight)

# fit model
m4.3 <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - xbar),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d2_a
)

# summary result
## R code 4.44 ############################
rethinking::precis(m4.3)
```

You can usefully think of as assigning to $y = log(x)$ the order of
magnitude of $x = exp(y)$. The function is the reverse, turning a
magnitude into a value. These definitions will make a mathematician
shriek. But much of our computational work relies only on these
intuitions.

These definitions allow the Log-Normal prior for `β` to be coded another
way. Instead of defining a parameter `β`, we define a parameter that is
the logarithm of `β` and then assign it a normal distribution. Then we
can reverse the logarithm inside the linear model. It looks like this:

```{r}
#| label: find-post-dist2-a
#| attr-source: '#lst-find-post-dist2-a lst-cap="Find the posterior distribution of the linear height-weight model (log version): rethinking version"'

## R code 4.43 ############################
m4.3b <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + exp(log_b) * (weight - xbar),
    a ~ dnorm(178, 20),
    log_b ~ dnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d2_a
)

rethinking::precis(m4.3b)
```

Note the `exp(log_b)` in the definition of `mu`. This is the same model
as `m4.3`. It will make the same predictions. But instead of `β` in the
posterior distribution, you get `log((β)`. It is easy to translate
between the two, because $\beta = exp(log(\beta))$. In code form:
`b <- exp(log_b)`.

#### Interpretating the posterior distribution

Statistical models are hard to interpret. There are two broad categories
of interpretation: - reading tables - plotting simulation

Plotting posterior distributions and posterior predictions is better
than attempting to understand a table.

##### Table of marginal distributions

I have already included the summary (`precis()`) for the `m4.3` model in
@lst-find-post-dist-a. But I will repeat it to inspect the marginal
posterior distributions of the parameters in detail:

```{r}
#| label: table-summary-m4.3
#| attr-source: '#lst-table-summary-m4.3 lst-cap="Display the marginal posterior distributions of the parameters: rethinking version"'


## R code 4.44 ################
rethinking::precis(m4.3)

```

**Interpreting the result of `rethinking::precis(m4.3)`**

1.  First row: quadratic approximation for $\alpha$
2.  Second row: quadratic approximation for $\beta$
3.  Third row: quadratic approximation for $\sigma$

Let's focus on b ($\beta$), because it's the new parameter. Since
($\beta$) is a slope, the value 0.90 can be read as *a person 1 kg
heavier is expected to be 0.90 cm taller*. 89% of the posterior
probability ($94.5-5.5$) lies between 0.84 and 0.97. That suggests that
($\beta$) values close to zero or greatly above one are highly
incompatible with these data and this model. It is most certainly not
evidence that the relationship between weight and height is linear,
because the model only considered lines. It just says that, if you are
committed to a line, then lines with a slope around 0.9 are plausible
ones.

Remember, the numbers in the default precis output aren't sufficient to
describe the quadratic posterior completely. For that, we also require
the variance-covariance matrix.

```{r}
#| label: var-cov-matrix-m4.3
#| attr-source: '#lst-var-cov-matrix-m4.3 lst-cap="Calculate the variance-covariance matrix for model m4.3"'

## R code 4.45 ######################
round(rethinking::vcov(m4.3), 3)
```

There is very little covariation among the parameters in this case. The
lack of covariance among the parameters results from
`r glossary("centering")`.

```{r}
#| label: fig-marg-post-cov-m4.3
#| fig-cap: "The marginal posteriors and the covariance matrix for model m4.3"
#| attr-source: '#lst-var-cov-matrix-m4.3 lst-cap="Show the marginal posteriors and covariance matrix for model m4.3"'
#| warning: false

rethinking::pairs(m4.3)
```

##### Plotting posterior inference against the data

It's almost always much more useful to plot the posterior inference
against the data. Not only does plotting help in interpreting the
posterior, but it also provides an informal check on model assumptions.

We're going to start with a simple version of that task, superimposing
just the posterior mean values over the height and weight data. Then
we'll slowly add more and more information to the prediction plots,
until we've used the entire posterior distribution.

We'll start with just the raw data and a single line. The code below
plots the raw data, computes the posterior mean values for `a` and `b`,
then draws the implied line:

```{r}
#| label: fig-raw-data-line-m4.3
#| fig-cap: "Height in centimeters (vertical) plotted against weight in kilograms (horizontal), with the line at the posterior mean plotted in black: rethinking version"
#| attr-source: '#lst-fig-raw-data-line-m4.3 lst-cap="Height against weight with linear regression"'

## R code 4.46 ############################################
plot(height ~ weight, data = d2_a, col = rethinking::rangi2)
post_m4.3 <- rethinking::extract.samples(m4.3)
a_map <- mean(post_m4.3$a)
b_map <- mean(post_m4.3$b)
curve(a_map + b_map * (x - xbar), add = TRUE)
```

Each point in this plot is a single individual. The black line is
defined by the mean slope `β` and mean intercept `α`. This is not a bad
line. It certainly looks highly plausible. But there are an infinite
number of other highly plausible lines near it. Let's draw those too.

##### Adding uncertainty around the mean

Plots of the average line, like in @fig-raw-data-line-m4.3, are useful
for getting an impression of the magnitude of the estimated influence of
a variable. But they do a poor job of communicating uncertainty.
Remember, the posterior distribution considers every possible regression
line connecting height to weight. It assigns a relative plausibility to
each. This means that each combination of `α` and `β` has a posterior
probability. It could be that there are many lines with nearly the same
posterior probability as the average line. Or it could be instead that
the posterior distribution is rather narrow near the average line.

So how can we get that uncertainty onto the plot? Together, a
combination of `α` and `β` define a line. And so we could sample a bunch
of lines from the posterior distribution. Then we could display those
lines on the plot, to visualize the uncertainty in the regression
relationship.

To better appreciate how the posterior distribution contains lines, we
work with all of the samples from the model.

```{r}
#| label: collect-post-samples-a

## R code 4.47 ################
# post_m4.3 <- rethinking::extract.samples(m4.3) # already in previous listing
post_m4.3[1:5, ]
```

Each row is a correlated random sample from the joint posterior of all
three parameters, using the covariances provided by
`rethinking::vcov(m4.3)` (@lst-var-cov-matrix-m4.3). The paired values
of `a` and `b` on each row define a line. The average of very many of
these lines is the posterior mean line. But the scatter around that
average is meaningful, because it alters our confidence in the
relationship between the predictor and the outcome.

So now let's display a bunch of these lines, so you can see the scatter.
This lesson will be easier to appreciate, if we use only some of the
data to begin. Then you can see how adding in more data changes the
scatter of the lines. So we'll begin with just the first 10 cases in
`d2_a`. The following code extracts the first 10 cases and re-estimates
the model:

```{r}
#| label: extract-10-re-est-mod-a

## R code 4.48 ##########################
N10_a <- 10
dN10_a <- d2_a[1:N10_a, ]
mN10_a <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - mean(weight)),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = dN10_a
)

```

Now let's plot 20 of these lines for the 10 data points, to see what the
uncertainty looks like.

```{r}
#| label: fig-plot-lines-10-points-a
#| fig-cap: "Samples from the quadratic approximate posterior distribution for the height/weight model, m4.3. 20 lines sampled from 10 data points of the posterior distribution, showing the uncertainty in the regression relationship: rethinking version"


## R code 4.49 ##############################
# extract 20 samples from the posterior
post_20_m4.3 <- rethinking::extract.samples(mN10_a, n = 20)

# display raw data and sample size
plot(dN10_a$weight, dN10_a$height,
  xlim = range(d2_a$weight), ylim = range(d2_a$height),
  col = rethinking::rangi2, xlab = "weight", ylab = "height"
)
mtext(rethinking:::concat("N = ", N10_a))

# plot the lines, with transparency
for (i in 1:20) {
  curve(post_20_m4.3$a[i] + post_20_m4.3$b[i] * (x - mean(dN10_a$weight)),
    col = rethinking::col.alpha("black", 0.3), add = TRUE
  )
}

```

The result is shown in the upper-left plot in FIGURE 4.7. By plotting
multiple regression lines, sampled from the posterior, it is easy to see
both the highly confident aspects of the relationship and the less
confident aspects. The cloud of regression lines displays greater
uncertainty at extreme values for weight.

The other plots in FIGURE 4.7 show the same relationships, but for
increasing amounts of data. Just re-use the code from before, but change
N \<- 10 to some other value. Notice that the cloud of regression lines
grows more compact as the sample size increases. This is a result of the
model growing more confident about the location of the mean.

Plot 20 lines sampled from 50 data points of the posterior distribution

```{r}
#| label: fig-plot-lines-50-points-a
#| fig-cap: "Samples from the quadratic approximate posterior distribution for the height/weight model, m4.3. 20 lines sampled from 50 data points of the posterior distribution, showing the uncertainty in the regression relationship: rethinking version"

## R code 4.48, 4.49 ######################
N50_a <- 50
dN50_a <- d2_a[1:N50_a, ]
mN50_a <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - mean(weight)),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = dN50_a
)

# extract 20 samples from the posterior
post_50_m4.3 <- rethinking::extract.samples(mN50_a, n = 20)

# display raw data and sample size
plot(dN50_a$weight, dN50_a$height,
  xlim = range(d2_a$weight), ylim = range(d2_a$height),
  col = rethinking::rangi2, xlab = "weight", ylab = "height"
)
mtext(rethinking:::concat("N = ", N50_a))

# plot the lines, with transparency
for (i in 1:20) {
  curve(post_50_m4.3$a[i] + post_50_m4.3$b[i] * (x - mean(dN50_a$weight)),
    col = rethinking::col.alpha("black", 0.3), add = TRUE
  )
}

```

Plot 20 lines sampled from 352 data points of the posterior distribution

```{r}
#| label: fig-plot-lines-all-352-points-a
#| fig-cap: "Samples from the quadratic approximate posterior distribution for the height/weight model, m4.3. 20 lines sampled from all 352 data points of the posterior distribution, showing the uncertainty in the regression relationship."

## R code 4.48, 4.49 ###########################
N352_a <- 352
dN352_a <- d2_a[1:N352_a, ]
mN352_a <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - mean(weight)),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = dN352_a
)

# extract 20 samples from the posterior
post_352_m4.3 <- rethinking::extract.samples(mN352_a, n = 20)

# display raw data and sample size
plot(dN352_a$weight, dN352_a$height,
  xlim = range(d2_a$weight), ylim = range(d2_a$height),
  col = rethinking::rangi2, xlab = "weight", ylab = "height"
)
mtext(rethinking:::concat("N = ", N352_a))

# plot the lines, with transparency
for (i in 1:20) {
  curve(post_352_m4.3$a[i] + post_352_m4.3$b[i] * (x - mean(dN352_a$weight)),
    col = rethinking::col.alpha("black", 0.3), add = TRUE
  )
}

```

##### Plotting regression intervals and contours

The cloud of regression lines in @fig-plot-lines-10-points-a,
@fig-plot-lines-50-points-a and @fig-plot-lines-all-352-points-a is an
appealing display, because it communicates uncertainty about the
relationship in a way that many people find intuitive. But it's more
common, and often much clearer, to see the uncertainty displayed by
plotting an interval or contour around the average regression line.

```{r}
#| label: calc-PI-around-regr-line-a
#| attr-source: '#lst-calc-PI-around-regr-line-a lst-cap="Calculate uncertainty around the average regression line"'

## R code 4.50 ##########################

post_m4.3 <- rethinking::extract.samples(m4.3)      # <1>
mu_at_50_a <- post_m4.3$a + post_m4.3$b * (50 - xbar) # <2>
head(mu_at_50_a)                                      # <3>
```

1.  Repeating the code for drawing (extracting and collecting) from the
    fitted model m4.3 (already done in @fig-raw-data-line-m4.3)
2.  The code to the right of the `<-` takes its form from the equation
    for $\mu_{i} = \alpha + \beta(x_{i} - \overline{x})$. The value of
    $x_{i}$ in this case is 50.
3.  The result is a vector of predicted means, one for each random
    sample from the posterior. Since joint `a` and `b` went into
    computing each, the variation across those means incorporates the
    uncertainty in and correlation between both parameters.

It might be helpful at this point to actually plot the density for this
vector of means:

```{r}
#| label: fig-density-vector-mean-50-a
#| fig-cap: "The quadratic approximate posterior distribution of the mean height, μ, when weight is 50 kg. This distribution represents the relative plausibility of different values of the mean. Rehtinking version"

## R code 4.51 ##################
rethinking::dens(mu_at_50_a, col = rethinking::rangi2, lwd = 2, xlab = "mu|weight=50")
```

Since the components of `μ` have distributions, so too does `μ`. And
since the distributions of `α` and `β` are Gaussian, so too is the
distribution of `μ` (adding Gaussian distributions always produces a
Gaussian distribution).

Since the posterior for `μ` is a distribution, you can find intervals
for it, just like for any posterior distribution. To find the 89%
compatibility interval of `μ` at 50 kg, just use the `PI()` command as
usual:

```{r}
#| label: PI-mu-at-50-a
#| attr-source: '#lst-PI-mu-at-50-a lst-cap="89% compatibility interval of μ at 50 kg"'

## R code 4.52 ##############
rethinking::PI(mu_at_50_a, prob = 0.89)

```

What these numbers mean is that the central 89% of the ways for the
model to produce the data place the average height between about 159 cm
and 160 cm (conditional on the model and data), assuming the weight is
50 kg.

That's good so far, but we need to repeat the above calculation for
every weight value on the horizontal axis, not just when it is 50 kg. We
want to draw 89% intervals around the average slope in
@fig-raw-data-line-m4.3.

This is made simple by strategic use of the
`rethinking::`link()`function, a part of the {**rethinking**} package. What`rethinking::link()`will do is take your`rethinking::quap()`approximation, sample from the posterior distribution, and then compute`μ\`
for each case in the data and sample from the posterior distribution.
Here's what it looks like for the data you used to fit the model:

```{r}
#| label: calc-mu-with-link-a
#| attr-source: '#lst-calc-mu-with-link-a lst-cap="Calculate μ for each case in the data and sample from the posterior distribution: Rethinking version"'

## R code 4.53 ##############
mu_a <- rethinking::link(m4.3)
str(mu_a)
```

You end up with a big matrix of values of `μ`. Each row is a sample from
the posterior distribution. The default is 1000 samples, but you can use
as many or as few as you like. Each column is a case (row) in the data.
There are 352 rows in `d2_a`, corresponding to 352 individuals. So there
are 352 columns in the matrix mu above.

The function `rethinking::link()` provides a posterior distribution of
`μ` for each case we feed it. So above we have a distribution of `μ` for
each individual in the original data. We actually want something
slightly different: a distribution of `μ` for each unique weight value
on the horizontal axis. It's only slightly harder to compute that, by
just passing `rethinking::link()` some new data:

```{r}
#| label: calc-dist-mu-unique-with-link-a
#| attr-source: '#lst-calc-dist-mu-unique-with-link-a lst-cap="Calculate a distribution of μ for each unique weight value on the horizontal axis: rethinking version"'

## R code 4.54 ###############################
# define sequence of weights to compute predictions for
# these values will be on the horizontal axis
weight.seq <- seq(from = 25, to = 70, by = 1)

# use link to compute mu
# for each sample from posterior
# and for each weight in weight.seq
mu2_a <- rethinking::link(m4.3, data = data.frame(weight = weight.seq))
str(mu2_a)
```

And now there are only 46 columns in mu, because we fed it 46 different
values for weight. To visualize what you've got here, let's plot the
distribution of `μ` values at each height.

```{r}
#| label: fig-dist-mu-height-100-a
#| fig-cap: "The first 100 values in the distribution of μ at each weight value. Rethinking version"

## R code 4.55
# use type="n" to hide raw data
base::plot(height ~ weight, d2_a, type = "n")

# loop over samples and plot each mu value
for (i in 1:100) {
  graphics::points(weight.seq, mu2_a[i, ], pch = 16, col = rethinking::col.alpha(rethinking::rangi2, 0.1))
}

```

At each weight value in `weight.seq`, a pile of computed `μ` values are
shown. Each of these piles is a Gaussian distribution, like that in
@fig-density-vector-mean-50-a. You can see now that the amount of
uncertainty in `μ` depends upon the value of weight. And this is the
same fact you saw in @fig-plot-lines-10-points-a,
@fig-plot-lines-50-points-a and @fig-plot-lines-all-352-points-a.

The final step is to summarize the distribution for each weight value.
We'll use `base::apply()`, which applies a function of your choice to a
matrix.

```{r}
#| label: sum-dist-weight-a
#| attr-source: '#lst-sum-dist-weight-a lst-cap="Summary of the distribution for each weight value. Rethinking version"'

## R code 4.56 #####################
# summarize the distribution of mu
mu2.mean <- apply(mu2_a, 2, mean)                      # <1>
mu2.PI <- apply(mu2_a, 2, rethinking::PI, prob = 0.89) # <2>
mu2.mean                                               # <3>
mu2.PI                                                 # <4>
```

1.  Read `apply(mu2,2,mean)` as "compute the mean of each column
    (dimension '2') of the matrix mu". Now `mu2.mean` contains the
    average `μ` at each weight value.
2.  `mu2.PI` contains 89% lower and upper bounds for each weight value.
3.  `mu2.mean` and `mu2.PI` are just different kinds of summaries of the
    distributions in `mu2_a`, with each column being for a different
    weight value. These summaries are only summaries. The "estimate" is
    the entire distribution.

```{r}
#| label: fig-summaries-on-data-top-a
#| fig-cap: "Plot of the summaries on top of the !Kung height data again, now with 89% compatibility interval of the mean indicated by the shaded region. Compare this region to the distributions of blue points in @fig-dist-mu-height-100-a."

## R code 4.57 ###########################
# plot raw data
# fading out points to make line and interval more visible
plot(height ~ weight, data = d2_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))

# plot the MAP line, aka the mean mu for each weight
graphics::lines(weight.seq, mu2.mean)

# plot a shaded region for 89% PI
rethinking::shade(mu2.PI, weight.seq)

```

::: callout-caution
There is very little uncertainty about the average height as a function
of average weight. But keep in mind that these inferences are always
conditional on the model. Think of the regression line in
@fig-summaries-on-data-top-a as saying: *Conditional on the assumption
that height and weight are related by a straight line, then this is the
most plausible line, and these are its plausible bounds.*
:::

Using this approach, you can derive and plot posterior prediction means
and intervals for quite complicated models, for any data you choose. As
long as you know the structure of the model ----- how parameters relate
to the data ----- you can use samples from the posterior to describe any
aspect of the model's behavior.

Summarizing the three steps for generating predictions and intervals
from the posterior of a fit model:

1.  Use `rethinking::link()` to generate distributions of posterior
    values for `μ`. The default behavior of `rethinking::link()` is to
    use the original data, so you have to pass it a list of new
    horizontal axis values you want to plot posterior predictions
    across.
2.  Use summary functions like `mean` or `PI` to find averages and lower
    and upper bounds of `μ` for each value of the predictor variable.
3.  Finally, use plotting functions like `graphics::lines()` and
    `rethinking::shade()` to draw the lines and intervals. Or you might
    plot the distributions of the predictions, or do further numerical
    calculations with them. It's really up to you.

::: callout-note
You could write a function that accomplish the same thing as
`rethinking::link()`:

```{r}
#| label: writing-link-function-a
#| attr-source: '#lst-writing-link-function-a lst-cap="Code to perform the same steps as the rethinking::link() function"'
#| 
## R code 4.58 ################################
post_m4.3 <- rethinking::extract.samples(m4.3)
mu.link <- function(weight) post_m4.3$a + post_m4.3$b * (weight - xbar)
weight.seq <- seq(from = 25, to = 70, by = 1)
mu3_a <- sapply(weight.seq, mu.link)
mu3.mean <- apply(mu3_a, 2, mean)
mu3.CI <- apply(mu3_a, 2, rethinking::PI, prob = 0.89)
mu3.mean
mu3.CI
```

And the values in `mu3.mean` and `mu3.CI` should be very similar
(allowing for simulation variance) to what you got the automated way,
using `rethinking::link()` in @lst-sum-dist-weight-a.

Knowing this manual method is useful both for (1) understanding and (2)
sheer power. Whatever the model you find yourself with, this approach
can be used to generate posterior predictions for any component of it.
Automated tools like link save effort, but they are never as flexible as
the code you can write yourself.
:::

##### Prediction intervals

Now let's walk through generating an 89% prediction interval for actual
heights, not just the average height, `μ`. This means we'll incorporate
the standard deviation `σ` and its uncertainty as well. Remember, the
first line of the statistical model in @eq-find-post-dist-a is:

------------------------------------------------------------------------

::: {#def-mod-line-1-a}
Remember the first line of @eq-find-post-dist-a

$$
h_{i} \sim \operatorname{Normal}(μ_{i}, σ)
$$ {#eq-mod-line-1-a}
:::

------------------------------------------------------------------------

What you've done so far is just use samples from the posterior to
visualize the uncertainty in $μ_{i}$, the linear model of the mean. But
actual predictions of heights depend also upon the distribution in the
first line. The Gaussian distribution on the first line tells us that
the model expects observed heights to be distributed around `μ`, not
right on top of it. And the spread around `μ` is governed by `σ`. All of
this suggests we need to incorporate `σ` in the predictions.

Imagine simulating heights. For any unique weight value, you sample from
a Gaussian distribution with the correct mean `μ` for that weight, using
the correct value of σ sampled from the same posterior distribution. If
you do this for every sample from the posterior, for every weight value
of interest, you end up with a collection of simulated heights that
embody the uncertainty in the posterior as well as the uncertainty in
the Gaussian distribution of heights.

There is a tool called `rethinking::sim()` which does this simulation of
the posterior observations:

```{r}
#| label: simulate-post-dist-a
#| attr-source: '#lst-simulate-post-dist-a lst-cap="Simulation of the posterior observations"'

## R code 4.59 #######################
sim.height <- rethinking::sim(m4.3, data = list(weight = weight.seq))
str(sim.height)
```

This matrix is much like the earlier one, `mu`, but it contains
simulated heights, not distributions of plausible average height, `μ`.

We can summarize these simulated heights in the same way we summarized
the distributions of `μ`, by using apply:

```{r}
#| label: sum-sim-heights-a
#| attr-source: '#lst-sum-sim-heights-a lst-cap="Summarize simulted heights"'

## R code 4.60 ###################
height.PI <- apply(sim.height, 2, rethinking::PI, prob = 0.89)
```

Now `height.PI` contains the 89% posterior prediction interval of
observable (according to the model) heights, across the values of weight
in `weight.seq`.

Let's plot everything we've built up: (1) the average line, (2) the
shaded region of 89% plausible `μ`, and (3) the boundaries of the
simulated heights the model expects.

```{r}
#| label: fig-pred-interval-height-a
#| fig-cap: "89% prediction interval for height, as a function of weight. The solid line is the average line for the mean height at each weight. The two shaded regions show different 89% plausible regions. The narrow shaded interval around the line is the distribution of μ. The wider shaded region represents the region within which the model expects to find 89% of actual heights in the population, at each weight."

## R code 4.61 ##################
# plot raw data
plot(height ~ weight, d2_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))

# draw MAP line
graphics::lines(weight.seq, mu3.mean)

# draw HPDI region for line
rethinking::shade(mu3.CI, weight.seq)

# draw PI region for simulated heights
rethinking::shade(height.PI, weight.seq)


```

Notice that the outline for the wide shaded interval is a little rough.
This is the simulation variance in the tails of the sampled Gaussian
values. If it really bothers you, increase the number of samples you
take from the posterior distribution. The optional n parameter for
`sim.height` controls how many samples are used. Try for example 1e4
samples and run the plotting code again. You'll see the shaded boundary
smooth out some.

With extreme percentiles, it can be very hard to get out all of the
roughness. Luckily, it hardly matters, except for aesthetics. Moreover,
it serves to remind us that all statistical inference is approximate.
The fact that we can compute an expected value to the 10th decimal place
does not imply that our inferences are precise to the 10th decimal
place.

```{r}
#| label: fig-pred-interval-height2-a
#| fig-cap: "89% prediction interval for height, as a function of weight. Shaded boundary smoothed out by sampling 1e4 times instead of the standard value of 1e3"

## R code 4.62 (4.59 & 4.60) ###################

## R code 4.59 ################
sim2.height <- rethinking::sim(m4.3, data = list(weight = weight.seq), n = 1e4)
height2.PI <- apply(sim2.height, 2, rethinking::PI, prob = 0.89)

## R code 4.61 ##################
# plot raw data
plot(height ~ weight, d2_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))

# draw MAP line
graphics::lines(weight.seq, mu3.mean)

# draw HPDI region for line
rethinking::shade(mu3.CI, weight.seq)

# draw PI region for simulated heights
rethinking::shade(height2.PI, weight.seq)
```

Just like with `rethinking::link()`, it's useful to know a little about
how `rethinking::sim()` operates. For every distribution like `dnorm`,
there is a companion simulation function. For the Gaussian distribution,
the companion is `rnorm`, and it simulates sampling from a Gaussian
distribution. What we want R to do is simulate a height for each set of
samples, and to do this for each value of weight. The following will do
it:

```{r}
#| label: writing-sim-function-a
#| attr-source: '#lst-writing-sim-function-a lst-cap="Writing you own rethinking:sim() function"'

## R code 4.63 ########################################

post_m4.3 <- rethinking::extract.samples(m4.3)
# post <- extract.samples(m4.3)
# weight.seq <- 25:70
sim3.height <- sapply(weight.seq, function(weight) {
  rnorm(
    n = nrow(post_m4.3),
    mean = post_m4.3$a + post_m4.3$b * (weight - xbar),
    sd = post_m4.3$sigma
  )
})
height3.PI <- apply(sim3.height, 2, rethinking::PI, prob = 0.89)
```

The values in `height.PI` will be practically identical to the ones
computed in the main text and displayed in FIGURE 4.10.

```{r}
#| label: compare-height-PIs-str--a
#| attr-source: '#lst-compare-height-PIs-str-a lst-cap="Compare height3.PI with height.PI using str()"'

str(height.PI)
str(height3.PI)

```

Small differences are the result of the randomized sampling process:

```{r}
#| label: compare-height-PIs-head-a
#| attr-source: '#lst-compare-height-PIs-head-a lst-cap="Compare height3.PI with height.PI using head()"'

head(height.PI)[ , 1:6]
head(height3.PI)[ , 1:6]
```

### Curves from lines

We'll consider two commonplace methods that use linear regression to
build curves. The first is `r glossary("polynomial regression")`. The
second is `r glossary("spline")`. Both approaches work by transforming a
single predictor variable into several synthetic variables. But splines
have some clear advantages.

#### Polynomial regression

Polynomial regression uses powers of a variable -- squares and cubes --
as extra predictors. This is an easy way to build curved associations.
Polynomial regressions are very common, and understanding how they work
will help scaffold later models.

```{r}
#| label: fig-scatterplot-height-weight-a
#| fig-cap: "Height in centimeters (vertical) plotted against weight in kilograms (horizontal). This time with the full data, e.g., the non-adults data."
#| attr-source: '#fig-scatterplot-height-weight-a lst-cap="Height in centimeters (vertical) plotted against weight in kilograms (horizontal): rethinking version"'

plot(height ~ weight, data = d_a, col = rethinking::rangi2)
```

The relationship is visibly curved, now that we've included the
non-adult individuals. (Compare with adult data in
@fig-raw-data-line-m4.3)

The most common polynomial regression is a parabolic model of the mean.
Let `x` be standardized body weight. Then the parabolic equation for the
mean height is:

------------------------------------------------------------------------

::: {#def-parabolic-mean}
Parabolic equation for the mean height

$$
\mu_{i} = \alpha + \beta_{1}x_{i} + \beta_{2}x_{i}^2
$$ {#eq-parabolic-mean}
:::

------------------------------------------------------------------------

@eq-parabolic-mean is a parabolic (second order) polynomial. The
$\alpha + \beta_{1}x_{i}$ part is the same linear function of x in a
linear regression, just with a little "1" subscript added to the
parameter name, so we can tell it apart from the new parameter. The
additional term uses the square of $x_{i}$ to construct a parabola,
rather than a perfectly straight line. The new parameter $\beta_{2}$
measures the curvature of the relationship.

> Fitting these models to data is easy. Interpreting them can be hard.

::: callout-tip
Polynomial parameters are in general very difficult to understand. But
prior predictive simulation does help a lot.
:::

**Fitting a parabolic model of height on weight**

The first thing to do is to
`r glossary("Standardization", "standardize")` the predictor variable.
We've done this in previous examples. But this is especially helpful for
working with polynomial models. When predictor variables have very large
values in them, there are sometimes numerical glitches. Even well-known
statistical software can suffer from these glitches, leading to mistaken
estimates. These problems are very common for polynomial regression,
because the square or cube of a large number can be truly massive.
Standardizing largely resolves this issue. It should be your default
behavior.

> A quadratic function (also called a quadratic, a quadratic polynomial,
> or a polynomial of degree 2) is special type of polynomial function
> where the highest-degree term is second degree. ... The graph of a
> quadratic function is a parabola, a 2-dimensional curve that looks
> like either a cup(∪) or a cap(∩). ([Statistis How
> To](https://www.statisticshowto.com/quadratic-function/))

To define the parabolic model, just modify the definition of $\mu_{i}$.

------------------------------------------------------------------------

::: {#def-parabolic-model}
Fitting a parabolic model of height on weight

$$
\begin{align*}
h_{i} \sim \operatorname{Normal}(μ_{i}, σ) \space \space (1) \\ 
μ_{i} = \alpha + \beta_{1}x_{i} + \beta_{2}x_{i}^2 \space \space (2) \\
\alpha \sim \operatorname{Normal}(178, 20) \space \space (3)  \\ 
\beta_{1} \sim \operatorname{Log-Normal}(0,10) \space \space (4) \\
\beta_{2} \sim \operatorname{Normal}(0,10) \space \space (5) \\
\sigma \sim \operatorname{Uniform}(0, 50) \space \space (6)      
\end{align*}
$$ {#eq-parabolic-model}

```         
height ~ dnorm(mu, sigma)                  # <1>
mu <- a + b1 * weight.s + b2 * weight.s2^2 # <2>
a ~ dnorm(178, 20)                         # <3>
b1 ~ dlnorm(0, 10)                         # <4>
b2 ~ dnorm(0, 10)                          # <5>
sigma ~ dunif(0, 50)                       # <6>
```
:::

------------------------------------------------------------------------

The confusing issue here is assigning a prior for $\beta_{2}$, the
parameter on the squared value of `x`. Unlike $\beta_{1}$, we don't want
a positive constraint.

Approximating the posterior is straightforward. Just modify the
definition of `mu` so that it contains both the linear and quadratic
terms. But in general it is better to pre-process any variable
transformations -- you don't need the computer to recalculate the
transformations on every iteration of the fitting procedure. So I'll
also build the square of `weight_s` as a separate variable:

```{r}
#| label: quap-parabolic
#| attr-source: '#lst-quap-parabolic lst-cap="Finding the posterior distribution of a parabolic model of height on weight with rethinking::quap()"'

## R code 4.65 #######################
d_a$weight_s <- (d_a$weight - mean(d_a$weight)) / sd(d_a$weight)
d_a$weight_s2 <- d_a$weight_s^2
m4.5 <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1 * weight_s + b2 * weight_s2,
    a ~ dnorm(178, 20),
    b1 ~ dlnorm(0, 1),
    b2 ~ dnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d_a
)
```

Now, since the relationship between the outcome height and the predictor
weight depends upon two slopes, b1 and b2, it isn't so easy to read the
relationship off a table of coefficients:

```{r}
#| label: show-result-m4.5
#| attr-source: '#lst-show-result-m4.5 lst-cap="Show table of coefficients for model m4.5"'

## R code 4.66 ################
rethinking::precis(m4.5)
```

The parameter $\alpha$ (a) is still the intercept, so it tells us the
expected value of height when weight is at its mean value. But it is no
longer equal to the mean height in the sample, since there is no
guarantee it should in a polynomial regression. And those $\beta_{1}$
and $\beta_{2}$ parameters are the linear and square components of the
curve. But that doesn't make them transparent.

You have to plot these model fits to understand what they are saying. So
let's do that. We'll calculate the mean relationship and the 89%
intervals of the mean and the predictions, like in the previous section.
Here's the working code:

```{r}
#| label: fig-polynomial-regression-a
#| fig-cap: "Polynomial regressions of height on weight (standardized), for the full !Kung data."
#| attr-source: '#lst-fig-polynomial-regression-a lst-cap="Polynomial regressions of height on weight (standardized), for the full !Kung data."'

## R code 4.67 ################
weight5.seq <- seq(from = -2.2, to = 2, length.out = 30)
pred_dat_a <- list(weight_s = weight5.seq, weight_s2 = weight5.seq^2)
mu5_a <- rethinking::link(m4.5, data = pred_dat_a)
mu5.mean <- apply(mu5_a, 2, mean)
mu5.PI <- apply(mu5_a, 2, rethinking::PI, prob = 0.89)
sim5.height <- rethinking::sim(m4.5, data = pred_dat_a)
height5.PI <- apply(sim5.height, 2, rethinking::PI, prob = 0.89)


## R code 4.68 #################
plot(height ~ weight_s, d_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))
graphics::lines(weight5.seq, mu5.mean)
rethinking::shade(mu5.PI, weight5.seq)
rethinking::shade(height5.PI, weight5.seq)

```

The quadratic regression does a pretty good job. It is much better than
a linear regression for the full Howell1 data set.

```{r}
#| label: fig-find-post-full-data-a
#| fig-cap: "Find the posterior distribution of the linear height-weight model (full data set): rethinking version"
#| attr-source: '#lst-fig-find-post-full-data-a'
#| code-summary: "Posterior distribution of the linear height-weight model: rethinking version"

## R code 4.42 #############################

# define the average weight, x-bar
xbar_full <- mean(d_a$weight)

# fit model
m4.3_full <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * (weight - xbar_full),
    a ~ dnorm(178, 20),
    b ~ dlnorm(0, 1),
    sigma ~ dunif(0, 50)
  ),
  data = d_a
)

## R code 4.46 ############################################
plot(height ~ weight, data = d_a, col = rethinking::rangi2)
post_m4.3_full <- rethinking::extract.samples(m4.3_full)
a_map_full <- mean(post_m4.3_full$a)
b_map_full <- mean(post_m4.3_full$b)
curve(a_map_full + b_map_full * (x - xbar_full), add = TRUE)



## R code 4.58 ################################
mu.link_full <- function(weight) post_m4.3_full$a + post_m4.3_full$b * (weight - xbar_full)
weight.seq_full <- seq(from = 0, to = 70, by = 1)
mu3_a_full <- sapply(weight.seq_full, mu.link_full)
mu3.mean_full <- apply(mu3_a_full, 2, mean)
mu3.CI_full <- apply(mu3_a_full, 2, rethinking::PI, prob = 0.89)

## R code 4.59 #######################
sim.height_full <- rethinking::sim(m4.3_full, data = list(weight = weight.seq_full))

## R code 4.60 ###################
height.PI_full <- apply(sim.height_full, 2, rethinking::PI, prob = 0.89)

## R code 4.61 ##################
# plot raw data
# plot(height ~ weight, d2_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))

# draw MAP line
graphics::lines(weight.seq_full, mu3.mean_full)

# draw HPDI region for line
rethinking::shade(mu3.CI_full, weight.seq_full)

# draw PI region for simulated heights
rethinking::shade(height.PI_full, weight.seq_full)


```

I had `xbar` to recalculate with the new data set. All the other code is
the same as in @lst-find-post-dist-a and @lst-fig-raw-data-line-m4.3.

@fig-find-post-full-data-a shows the familiar linear regression from
earlier in the chapter, but now with the standardized predictor and full
data with both adults and non-adults. The linear model makes some
spectacularly poor predictions, at both very low and middle weights.
Compare this to @fig-polynomial-regression-a, the new quadratic
regression. The curve does a better job of finding a central path
through the data.

Let's see what will happen if we are using a higher-order polynomial
regression, a cubic regression on weight. The model is:

------------------------------------------------------------------------

::: {#def-cubic-regression}
Cubic regression on weigth

$$
\begin{align*}
h_{i} \sim \operatorname{Normal}(μ_{i}, σ) \space \space (1) \\ 
μ_{i} = \alpha + \beta_{1}x_{i} + \beta_{2}x_{i}^2 + \beta_{3}x_{i}^3 \space \space (2) \\
\alpha \sim \operatorname{Normal}(178, 20) \space \space (3)  \\ 
\beta_{1} \sim \operatorname{Log-Normal}(0,10) \space \space (4) \\
\beta_{2} \sim \operatorname{Normal}(0,10) \space \space (5) \\
\beta_{3} \sim \operatorname{Normal}(0,10) \space \space (6) \\
\sigma \sim \operatorname{Uniform}(0, 50) \space \space (7)      
\end{align*}
$$ {#eq-cubic-regression}

```         
height ~ dnorm(mu, sigma)               # <1>
mu <- a + b1 * weight.s + b2 * weight.s2^2 + b3 * weight.s3^3 # <2>
a ~ dnorm(178, 20)                      # <3>
b1 ~ dlnorm(0, 10)                      # <4>
b2 ~ dnorm(0, 10)                       # <5>
b3 ~ dnorm(0, 10)                       # <6>
sigma ~ dunif(0, 50)                    # <7>
```
:::

------------------------------------------------------------------------

Fit the model accordingly. It is only a slight modification of the
parabolic model's code:

```{r}
#| label: fig-cubic-regression-a
#| fig-cap: "Cubic regressions of height on weight (standardized), for the full !Kung data."
#| attr-source: '#lst-fig-cubic-regression-a lst-cap="Cubic regressions of height on weight (standardized), for the full !Kung data."'

## R code 4.69 ####################
d_a$weight_s3 <- d_a$weight_s^3
m4.6 <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1 * weight_s + b2 * weight_s2 + b3 * weight_s3,
    a ~ dnorm(178, 20),
    b1 ~ dlnorm(0, 1),
    b2 ~ dnorm(0, 10),
    b3 ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ),
  data = d_a
)

## R code 4.67 ################
weight6.seq <- seq(from = -2.2, to = 2, length.out = 30)
pred_dat6_a <- list(weight_s = weight6.seq, weight_s2 = weight6.seq^2,
                    weight_s3 = weight6.seq^3)
mu6_a <- rethinking::link(m4.6, data = pred_dat6_a)
mu6.mean <- apply(mu6_a, 2, mean)
mu6.PI <- apply(mu6_a, 2, rethinking::PI, prob = 0.89)
sim5.height <- rethinking::sim(m4.6, data = pred_dat6_a)
height6.PI <- apply(sim5.height, 2, rethinking::PI, prob = 0.89)


## R code 4.68 #################
plot(height ~ weight_s, d_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))
graphics::lines(weight6.seq, mu6.mean)
rethinking::shade(mu6.PI, weight6.seq)
rethinking::shade(height6.PI, weight6.seq)


```

This cubic curve is even more flexible than the parabola, so it fits the
data even better.

But it's not clear that any of these models make a lot of sense. They
are good geocentric descriptions of the sample, yes. But there are two
problems. First, a better fit to the sample might not actually be a
better model. That's the subject of @sec-ulisses-compass. Second, the
model contains no biological information. We aren't learning any causal
relationship between height and weight. We'll deal with this second
problem much later, in @sec-genealized-linear-madness.

**Converting back to natural scale**

The plots @fig-find-post-full-data-a, @fig-polynomial-regression-a and
@fig-cubic-regression-a have standard units on the horizontal axis.
These units are sometimes called `z-scores`. But suppose you fit the
model using standardized variables, but want to plot the estimates on
the original scale. All that's really needed is first to turn off the
horizontal axis when you plot the raw data:

```{r}
#| label: fig-x-axis-turned-off-a
#| attr-source: '#lst-fig-x-axis-turned-off-a lst-cap="Cubic regression with x-axis turned off: Rehtinking version"'

## R code 4.70 ###########
plot(height ~ weight_s, d_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5), xaxt = "n")
```

The `xaxt` at the end there turns off the horizontal axis. Then you
explicitly construct the axis, using the `graphics::axis()` function.

```{r}
#| label: fig-cubic-regression-natural-scale-a
#| attr-source: '#lst-fig-cubic-regression-natural-scale-a lst-cap="Cubic regression with x-axis in natural scale: Rehtinking version"'

## R code 4.71 #############

plot(height ~ weight_s, d_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5), xaxt = "n")

at_a <- c(-2, -1, 0, 1, 2)                           # <1>
labels <- at_a * sd(d_a$weight) + mean(d_a$weight)   # <2>
axis(side = 1, at = at_a, labels = round(labels, 1)) # <3>
```

1.  Defines the location of the labels, in standardized units.
2.  Takes standardized units and converts them back to the original
    scale.
3.  Draws the axis.

Take a look at the help `?axis` for more details.

#### Splines

The second way to introduce a curve is to construct something known as a
`r glossary("B-spline", "spline")`. The word spline originally referred
to a long, thin piece of wood or metal that could be anchored in a few
places in order to aid drafters or designers in drawing curves. In
statistics, a spline is a smooth function built out of smaller,
component functions. There are actually many types of splines. The
`r glossary("B-spline")` we'll look at here is commonplace. The "B"
stands for "basis," which here just means "component." B-splines build
up wiggly functions from simpler less-wiggly components. Those
components are called basis functions. While there are fancier splines,
we want to start B-splines because they force you to make a number of
choices that other types of splines automate. You'll need to understand
B-splines before you can understand fancier splines.

To see how B-splines work, we'll need an example that is much
wigglier---that's a scientific term---than the !Kung stature data.
Cherry trees blossom all over Japan in the spring each year, and the
tradition of flower viewing follows. The timing of the blossoms can vary
a lot by year and century. Let's load a thousand years of blossom dates:

```{r}
#| label: load-cherry-blossoms-data-a
#| attr-source: '#lst-load-cherry-blossoms-data-a lst-cap="Load Cherry Blossoms data and display summary (rethinking version)"'

## R code 4.72 modified ######################
data(package = "rethinking", list = "cherry_blossoms")
d4_a <- cherry_blossoms
rethinking::precis(d4_a)
```

See `?cherry_blossoms` for details and sources. We're going to work with
the historical record of the **d**ay **o**f **y**ear of first bloom,
`doy`, for now. It ranges from 86 (late March) to 124 (early May). The
years with recorded blossom dates run from 812 CE to 2015 CE. You should
go ahead and plot `doy` against year to see. There might be some wiggly
trend in that cloud. It's hard to tell. (For the abbreviation CE see:
[Common Era (CE) and Before Common Era
(BCE)](https://www.timeanddate.com/calendar/ce-bce-what-do-they-mean.html))

```{r}
#| label: fig-scatterplot-cbl-a
#| fig-cap: "Display raw data for `doy` (Day of the year of first blossom) against the year: base R version"

plot(doy ~ year, data = d4_a)
```

There might be some wiggly trend in that cloud. It's hard to tell.

Let's try extracting a trend with a `r glossary("B-spline")`. B-splines
divide the full range of some predictor variable, like `year`, into
parts. Then they assign a parameter to each part. These parameters are
gradually turned on and off in a way that makes their sum into a fancy,
wiggly curve.

::: {#generating-b-splines .callout-important}
###### Procedure for generating B-splines {#sec-procedure-for-generating-b-splines}

1.  Choose the number and distribution of knots
2.  Choose the polynomial degree
3.  Get the parameter weights for each basis function (define the model
    and make it run)
:::

##### Choice of knots

Remember, the knots are just values of year that serve as pivots for our
spline. Where should the knots go? There are different ways to answer
this question. You can, in principle, put the knots wherever you like.
Their locations are part of the model, and you are responsible for them.
Let's do what we did in the simple example above, place the knots at
different evenly-spaced quantiles of the predictor variable. This gives
you more knots where there are more observations. We used only 5 knots
in the first example. Now let's go for 15:

```{r}
#| label: choose-knots-a
#| attr-source: '#lst-choose-knots-a lst-cap="Choose the knots that serve as pivots for the spline: rethinking version"'

## R code 4.73 ####################
d5_a <- d4_a[complete.cases(d4_a$doy), ] # complete cases on doy
num_knots15_a <- 15
(knot_list15_a <- quantile(d5_a$year, 
                           probs = seq(0, 1, length.out = num_knots15_a)))
```

##### Choice of polynomial degree

The next choice is polynomial degree. This determines how basis
functions combine, which determines how the parameters interact to
produce the spline. For degree 1, as in FIGURE 4.12, two basis functions
combine at each point. For degree 2, three functions combine at each
point. For degree 3, four combine. R already has a nice function that
will build basis functions for any list of knots and degree. This code
will construct the necessary basis functions for a degree 3 (cubic)
spline:

```{r}
#| label: compute-b-spline-matrix-a
#| attr-source: '#lst-compute-b-spline-matrix-a lst-cap="Compute the B-spline basis matrix for a cubic spline (degree 3): rethinking version"'

## R code 4.74 #######################
B_a <- splines::bs(d5_a$year,
  knots = knot_list15_a[-c(1, num_knots15_a)],
  degree = 3, intercept = TRUE
)
```

The B-spline basis matrix B_a should have 827 rows and 17 columns. Lets
check it:

-   Number of rows = `r nrow(B_a)`
-   Number of columns = `r ncol(B_a)`

Each row is a year, corresponding to the rows in the d5_a data frame.
Each column is a basis function, one of our synthetic variables defining
a span of years within which a corresponding parameter will influence
prediction.

To display the basis functions, just plot each column against year:

```{r}
#| label: fig-b-spline-matrix-a
#| attr-source: '#lst-fig-b-spline-matrix-a lst-cap="B-spline basis matrix for a cubic spline (degree 3): rethinking version"'

## R code 4.75 #############################
plot(NULL, xlim = range(d5_a$year), ylim = c(0, 1), xlab = "year", ylab = "basis")
for (i in 1:ncol(B_a)) lines(d5_a$year, B_a[, i])

```

The plot shows, just like the top in FIGURE 4.12., the basis functions.
However now more of these functions overlap.

##### Parameter weights for each basic function

Now to get the parameter weights for each basis function, we need to
actually define the model and make it run. The model is just a linear
regression. The synthetic basis functions do all the work. We'll use
each column of the matrix `B_a` as a variable. We'll also have an
intercept to capture the average blossom day. This will make it easier
to define priors on the basis weights, because then we can just conceive
of each as a deviation from the intercept.

This is also the first time we've used an
`r glossary("exponential distribution")` as a prior. Exponential
distributions are useful priors for scale parameters, parameters that
must be positive. The prior for `σ` is exponential with a rate of 1. The
way to read an exponential distribution is to think of it as containing
no more information than an average deviation. That average is the
inverse of the rate. So in this case it is $1/1 = 2$. If the rate were
0.5, the mean would be$1/0.5 = 2$. We'll use exponential priors for the
rest of the book, in place of uniform priors. It is much more common to
have a sense of the average deviation than of the maximum.

To build this model in `rethinking::quap()`, we just need a way to do
that sum. The easiest way is to use matrix multiplication.
@sec-matrix-multiplication has more details about why this works. The
only other trick is to use a start list for the weights to tell
`rethinking::quap()` how many there are.

```{r}
#| label: fit-model-m4.7
#| attr-source: '#lst-fit-model-m4.7 lst-cap="Fit the model of B-spline basis matrix m4.7"'

## R code 4.76 ################################
m4.7 <- rethinking::quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + B %*% w,
    a ~ dnorm(100, 10),
    w ~ dnorm(0, 10),
    sigma ~ dexp(1)
  ),
  data = list(D = d5_a$doy, B = B_a),
  start = list(w = rep(0, ncol(B_a)))
)
```

A summary with `rethinking::precis` won't reveal much.

```{r}
#| label: summarize-model-m4.7
#| attr-source: '#lst-summarize-model-m4.7 lst-cap="Summarize model m4.7"'

rethinking::precis(m4.7,depth = 2)
```

You should see 17 `w` parameters. But you can't tell what the model
thinks from the parameter summaries. Instead we need to plot the
posterior predictions. First, here are the weighted basis functions:

```{r}
#| label: fig-weighted-basis-function
#| fig-cap: "Weighted basis functions"

## R code 4.77 #################
post5_a <- rethinking::extract.samples(m4.7)
w <- apply(post5_a$w, 2, mean)
plot(NULL,
  xlim = range(d5_a$year), ylim = c(-6, 6),
  xlab = "year", ylab = "basis * weight"
)
for (i in 1:ncol(B_a)) lines(d5_a$year, w[i] * B_a[, i])

```

And finally we will plot the 97% posterior interval for `μ`, at each
year:

```{r}
#| label: fig-PI-cbl-a
#| fig-cap: "97% posterior interval for μ, at each year"
#| attr-source: '#lst-fig-PI-cbl-a lst-cap="97% posterior interval for μ, at each year"'

## R code 4.78 ######################
mu_cbl_a <- rethinking::link(m4.7)
mu_PI_cbl_a <- apply(mu_cbl_a, 2, rethinking::PI, 0.97)
plot(d5_a$year, d5_a$doy, 
     col = rethinking::col.alpha(rethinking::rangi2, 0.3), pch = 16)
rethinking::shade(mu_PI_cbl_a, d5_a$year, 
                  col = rethinking::col.alpha("black", 0.5))

```

The spline is much wigglier now. Something happened around 1500, for
example. If you add more knots, you can make this even wigglier. You
might wonder how many knots is correct. We'll be ready to (re)address
that question in a few more chapters.

Distilling the trend across years provides a lot of information. But
year is not really a causal variable, only a proxy for features of each
year. In the practice problems below, you'll compare this trend to the
temperature record, in an attempt to explain those wiggles.

##### Matrix multiplication in the spline model {#sec-matrix-multiplication}

Matrix algebra is a new way to represent ordinary algebra. It is often
much more compact. So to make model m4.7 easier to program, we used a
matrix multiplication of the basis matrix `B_a` by the vector of
parameters w: `B %*% w`. This notation is just linear algebra shorthand
for (1) multiplying each element of the vector `w` by each value in the
corresponding row of `B_a` and then (2) summing up each result. You
could also fit the same model with the following less-elegant code:

```{r}
#| label: fit-model-m4.7alt-a
#| attr-source: '#lst-fit-model-m4.7alt-a lst-cap="Fit model m4.7 with less-elegant code in matrix multiplication"'

## R code 4.79 ################################
m4.7alt <- rethinking::quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + sapply(1:827, function(i) sum(B[i, ] * w)),
    a ~ dnorm(100, 1),
    w ~ dnorm(0, 10),
    sigma ~ dexp(1)
  ),
  data = list(D = d5_a$doy, B = B_a),
  start = list(w = rep(0, ncol(B_a)))
)
rethinking::precis(m4.7alt, depth = 2)

```

```{r}
#| label: fig-PI-cbl2-a
#| fig-cap: "97% posterior interval for μ, at each year: Alternate matrix multiplication"
#| attr-source: '#lst-fig-PI-cbl-a lst-cap="97% posterior interval for μ, at each year with matrix multiplication manually"'

## R code 4.78 ######################
mu_cbl2_a <- rethinking::link(m4.7alt)
mu_PI_cbl2_a <- apply(mu_cbl2_a, 2, rethinking::PI, 0.97)
plot(d5_a$year, d5_a$doy, 
     col = rethinking::col.alpha(rethinking::rangi2, 0.3), pch = 16)
rethinking::shade(mu_PI_cbl2_a, d5_a$year, 
                  col = rethinking::col.alpha("black", 0.5))

```

#### Smooth functions for a rough world

The splines in the previous section are just the beginning. A entire
class of models, GENERALIZED ADDITIVE MODELS (GAMs), focuses on
predicting an outcome variable using smooth functions of some predictor
variables.

::: callout-tip
###### Resources for GAMs

-   Wood, S. N. (2017). Generalized Additive Models: An Introduction
    with R, Second Edition (2nd ed.). Taylor & Francis Inc.
-   SemanticScholar: [Series of paper dedicated to
    GAMs](https://www.semanticscholar.org/paper/Generalized-Additive-Models%3A-An-Introduction-with-R-G%C3%B3mez%E2%80%90Rubio/025f25133a5c1da746eb7e7719bb715b71a7f518)
-   Anish Singh Walia: [Generalized Additive
    Model](https://datascienceplus.com/generalized-additive-models/)
-   Noam Ross: [GAMs in
    R](https://noamross.github.io/gams-in-r-course/): A Free,
    Interactive Course using `mgcv`
-   Michael Clark: [Generalized Additive
    Models](https://m-clark.github.io/generalized-additive-models/)

------------------------------------------------------------------------

-   Adam Shaif: What is Gneralised Additive Model? ([Medium member
    story](https://towardsdatascience.com/generalised-additive-models-6dfbedf1350a))
-   Eugenio Anello: Generalized Additive Models with R ([Medium member
    story](https://pub.towardsai.net/generalized-additive-models-with-r-5f01c8e52089))
:::

## `TIDYVERSE`

### Why normal distributions?

I am not going to replicate the examples of @sec-why-normal-dist-a. They
do not help conceptually about the main issues why normal distributions
are wide-spread. But I will mention even the empty sub-chapter to get a
symmetry in the table of content.

#### Empty: Normal by addition

#### Empty: Normal by multiplication

#### Empty: Normal by log-multiplication

#### Empty: Using Gaussian distributions

### Model describing language

#### From model definition to Bayes' theorem

We can use grid approximation to work through our globe tossing model.

```{r}
#| label: model-bayes-b
#| attr-source: '#lst-model-bayes-b lst-cap="Calculate globe tossing model with syntax from Bayes theorem"'


# how many `p_grid` points would you like?
n_points <- 100

d_bayes_b <-
  tibble(p_grid = seq(from = 0, to = 1, length.out = n_points),
         w      = 6, 
         n      = 9) %>% 
  mutate(prior      = dunif(p_grid, min = 0, max = 1),
         likelihood = dbinom(w, size = n, prob = p_grid)) %>%
  mutate(posterior = likelihood * prior / sum(likelihood * prior))

head(d_bayes_b)
```

In case you were curious, here's what they look like.

```{r}
#| label: fig-prior-likelihood-posterior-b
#| fig-cap: "Prior, likelihood and posterior distribution for globe tossing model calculated with syntax accoridng to Bayes' theorem"

d_bayes_b %>% 
  pivot_longer(prior:posterior) %>% 
  
  # dictate the order in which the panels will appear
  mutate(name = factor(name, levels = c("prior", "likelihood", "posterior"))) %>% 
    
  ggplot(aes(x = p_grid, y = value, fill = name)) +
  geom_area() +
  scale_fill_manual(values = c("blue", "red", "purple")) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(legend.position = "none") +
  facet_wrap(~ name, scales = "free")
```

The posterior is a combination of the prior and the likelihood. When the
prior is flat across the parameter space, the posterior is just the
likelihood re-expressed as a probability. As we go along, you'll see we
almost never use flat priors in practice. Be warned that eschewing flat
priors is a recent development, however. You only have to look at the
literature from a couple decades ago to see mounds and mounds of flat
priors.

### Gaussian model of height

#### The data

##### Show the data

```{r}
#| label: loading-data-from-package_b
#| attr-source: '#lst-loading-data-from-package_b lst-cap="Load data `Howell1` from the {**rethinking**} package without loading the package and assign the data to an object. Tidyverse"'

data(package = "rethinking", list = "Howell1")
d_b <- Howell1
```

```{r}
#| label: show-howell-data1-b

d_b |>
    glimpse()

```

`glimpse()` is the tidyverse analogue for `str()`.

```{r}
#| label: show-howell-data2-b
d_b |> 
    summary()
```

Kurz tells us that the {**brms**} package does not have a function that
works like `rethinking::precis()` for providing numeric and graphical
summaries of variables, as in the second part of
@lst-show-howell-data-a. Kurz suggests `base::summary()` to get some of
the information from `rethinking::precis()`.

```{r}
#| label: show-howell-data3-b
d_b |>            
    skimr::skim() 

```

I think `skimr::skim()` is a better option as an alternative to
`rethinking::precis()` as `base::summary()` because it also has a
graphical summary of the variables. {**skimr**} has many other useful
functions and is very adaptable. I propose to install and to try it out.

##### Select the height data of adults

With {**tidyverse**} we can isolate height values with the
`dplyr::select()` function and we are using the `dplyr::filter()`
function to make an adults-only data frame.

```{r}
#| label: select-height-adults-b
#| attr-source: '#lst-select-height-adults-b lst-cap="Select the height data of adults (individuals older or equal than 18 years): tidyverse version"'

d_b %>%
  select(height) %>% 
  glimpse()

d2_b <- 
  d_b %>%
  filter(age >= 18) 
 
glimpse(d2_b)
```

The two functions of @lst-select-height-adults-b are much more readable
and understandable as the weird base R syntax in
@lst-select-height-adults-a.

#### The model

##### Plot the distribution of heights

The plot of the heights distribution compared with the standard Gaussian
distribution is missing in Kurz's version. I added this plot by using
the last example of [How to Plot a Normal Distribution in
R](https://www.statology.org/plot-normal-distribution-r/).

```{r}
#| label: fig-dist-heights-b
#| fig-cap: "The distribution of the heights data, overlaid by an ideal Gaussian distribution: tidyverse version"
#| attr-source: '#lst-fig-dist-heights-b lst-cap="Plot the distribution of the heights of adults: tidyverse version"'

p0 <- 
    d2_b |> 
    ggplot(aes(height)) +
    geom_density() +

    stat_function(
        fun = dnorm,
        args = with(d2_b, c(mean = mean(height), sd = sd(height)))
        ) +
    scale_x_continuous("Height in cm")

p0
```

##### Plot the mean prior

Here is the shape for the prior $μ \sim Normal(178, 20)$.

```{r}
#| label: fig-mean-prior-b
#| fig-cap: "Plot of the chosen mean prior: tidyverse version"
#| attr-source: '#lst-fig-mean-prior-b lst-cap="Plot of the chosen mean prior: tidyverse version"'

p1 <-
  tibble(x = seq(from = 100, to = 250, by = .1)) %>% 
    
  ggplot(aes(x = x, y = dnorm(x, mean = 178, sd = 20))) +
  geom_line() +
  scale_x_continuous(breaks = seq(from = 100, to = 250, by = 75)) +
  labs(title = "mu ~ dnorm(178, 20)",
       y = "density")

p1
```

##### Plot the prior of the standard deviation (sigma)

And here's the ggplot2 code for our prior for `σ`, a uniform
distribution with a minimum value of 0 and a maximum value of 50. We
don't really need the `y`-axis when looking at the shapes of a density,
so we'll just remove it with `scale_y_continuous()`.

```{r}
#| label: fig-sd-prior-b
#| fig-cap: "Plot the chosen prior for the standard deviation: tidyverse version"

p2 <-
  tibble(x = seq(from = -10, to = 60, by = .1)) %>%
  
  ggplot(aes(x = x, y = dunif(x, min = 0, max = 50))) +
  geom_line() +
  scale_x_continuous(breaks = c(0, 50)) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("sigma ~ dunif(0, 50)")

p2
```

##### Prior predictive simulation

We can simulate from both priors at once to get a prior probability
distribution of `height`.

```{r}
#| label: fig-prior-predictive-sim-b
#| fig-cap: "Simulate heights by sampling from the prior: tidyverse version"

n <- 1e4
set.seed(4)

sim <-
  tibble(sample_mu_b    = rnorm(n, mean = 178, sd  = 20),
         sample_sigma_b = runif(n, min = 0, max = 50)) %>% 
  mutate(height = rnorm(n, mean = sample_mu_b, sd = sample_sigma_b))
  
p3 <- sim %>% 
  ggplot(aes(x = height)) +
  geom_density(fill = "deepskyblue") +
  scale_x_continuous(breaks = c(0, 73, 178, 283)) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("height ~ dnorm(mu, sigma)") +
  theme(panel.grid = element_blank())

p3
```

If you look at the `x`-axis breaks on the plot in McElreath's lower left
panel in Figure 4.3, you'll notice they're intentional. To compute the
mean and 3 standard deviations above and below, you might do this.

```{r}
#| label: compute-mean-3sd-b
sim %>% 
  summarise(ll   = mean(height) - sd(height) * 3,
            mean = mean(height),
            ul   = mean(height) + sd(height) * 3) %>% 
  mutate_all(round, digits = 1)
```

Here's the work to make the lower right panel of Figure 4.3.

```{r}
#| label: fig-reproduce-4.3-low-right
#| fig-cap: "Reproduce lower right panels of Figure 4.3"


# simulate
set.seed(4)

sim <-
  tibble(sample_mu_b    = rnorm(n, mean = 178, sd = 100),
         sample_sigma_b = runif(n, min = 0, max = 50)) %>% 
  mutate(height = rnorm(n, mean = sample_mu_b, sd = sample_sigma_b))

# compute the values we'll use to break on our x axis
breaks <-
  c(mean(sim$height) - 3 * sd(sim$height), 0, mean(sim$height), mean(sim$height) + 3 * sd(sim$height)) %>% 
  round(digits = 0)

# this is just for aesthetics
text <-
  tibble(height = 272 - 25,
         y      = .0013,
         label  = "tallest man",
         angle  = 90)

# plot
p4 <-
  sim %>% 
  ggplot(aes(x = height)) +
  geom_density(fill = "deepskyblue", color = "black") +
  geom_vline(xintercept = 0, color = "black") +
  geom_vline(xintercept = 272, color = "black", linetype = 3) +
  geom_text(data = text,
            aes(y = y, label = label, angle = angle),
            color = "black") +
  scale_x_continuous(breaks = breaks) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("height ~ dnorm(mu, sigma)\nmu ~ dnorm(178, 100)") +
  theme(panel.grid = element_blank())

p4
```

Let's combine the four to make our version of McElreath's Figure 4.3.

```{r}
#| label: fig-reproduce-3.4
#| fig-cap: "Reproduction of Figure 3.4"

library(patchwork)
(p1 + xlab("mu") | p2 + xlab("sigma")) / (p3 | p4)
```

On page 84, McElreath said his prior simulation indicated 4% of the
heights would be below zero. He also drew the break down compared to the
tallest man on record, [Robert Pershing
Wadlow](https://en.wikipedia.org/wiki/Robert_Wadlow) (1918--1940).

```{r}
#| label: calc-breaks-b

sim %>% 
  count(height < 0) %>% 
  mutate(percent = 100 * n / sum(n))

sim %>% 
  count(height < 272) %>% 
  mutate(percent = 100 * n / sum(n))
```

#### Grid approximation of the posterior distribution

With grid approximation we are going to use the brute force method for
the calculation of the posterior distribution. This technique has
limited relevance. Later on we will use the quadratic approximation with
`brms::brm()`.

It is the same technique we have use in
@sec-sampling-from-a-grid-approximate-posterior respectively in the
tidyverse version in @sec-grid-approximation-b. As there is no
conceptually new information to learn, I am not going into the details
of the following code. (It combines several code chunk from Kurz's
version.) But I am going to foreshadow the most important differences in
the tidyverse approach of the grid approximation technique:

Instead of `base::grid_expand()` we will use `tidyr::crossing()` Instead
of `base::sapply()` we will use `purr::map2()`

The produced tibble contains data frames in its cells, so that we have
to use the `tidyr::unnest()` function to expand the list-column
containing data frames into rows and columns.

Referring to the plots:

-   Instead of `rethinking::contour_xyz()` we will use
    `ggplot2::geom_contour()`
-   Instead of `rethinking::image_xyz()` we will use
    `ggplot2::geom_raster()`

```{r}
#| label: grid-approx-posterior-b
#| attr-source: '#lst-grid-approx-posterior-b lst-cap="Grid Approximation of the posterior distribution: tidyverse version"'

n <- 200

d_grid_b <-
  # we'll accomplish with `tidyr::crossing()` what McElreath did with base R `expand.grid()`
  crossing(mu_b    = seq(from = 140, to = 160, length.out = n),
           sigma_b = seq(from = 4, to = 9, length.out = n))

glimpse(d_grid_b)

grid_function <- function(mu, sigma) {
  
  dnorm(d2_b$height, mean = mu, sd = sigma, log = TRUE) %>% 
    sum()
  
}

d_grid2_b <-
  d_grid_b %>% 
  mutate(log_likelihood_b = map2(mu_b, sigma_b, grid_function)) %>%
  unnest(log_likelihood_b) %>% 
  mutate(prior_mu_b    = dnorm(mu_b, mean = 178, sd = 20, log = T),
         prior_sigma_b = dunif(sigma_b, min = 0, max = 50, log = T)) %>% 
  mutate(product_b = log_likelihood_b + prior_mu_b + prior_sigma_b) %>% 
  mutate(probability_b = exp(product_b - max(product_b)))
  
head(d_grid2_b)
```

```{r}
#| label: fig-contour-b
#| fig-cap: "Draw 2D contours of a 3D surface"

d_grid2_b %>% 
  ggplot(aes(x = mu_b, y = sigma_b, z = probability_b)) + 
  geom_contour() +
  labs(x = expression(mu),
       y = expression(sigma)) +
  coord_cartesian(xlim = range(d_grid2_b$mu_b),
                  ylim = range(d_grid2_b$sigma_b)) +
  theme(panel.grid = element_blank())

```

```{r}
#| label: fig-heatmap-b
#| fig-cap: "Draw heat map"

d_grid2_b %>% 
  ggplot(aes(x = mu_b, y = sigma_b, fill = probability_b)) + 
  geom_raster(interpolate = TRUE) +
  scale_fill_viridis_c(option = "B") +
  labs(x = expression(mu),
       y = expression(sigma)) +
  theme(panel.grid = element_blank())
```

#### Sampling from the posterior

We can use `dplyr::sample_n()` to sample rows, with replacement, from
`d_grid2_b`.

```{r}
#| label: fig-posterior-sample-b
#| fig-cap: "Samples from the posterior distribution for the heights data. The density of points is highest in the center, reflecting the most plausible combinations of μ and σ. There are many more ways for these parameter values to produce the data, conditional on the model. (tidyverse version)"


set.seed(4)

d_grid_samples_b <- 
  d_grid2_b %>% 
  sample_n(size = 1e4, replace = T, weight = probability_b)

d_grid_samples_b %>% 
  ggplot(aes(x = mu_b, y = sigma_b)) + 
  geom_point(size = .9, alpha = 1/15) +
  scale_fill_viridis_c() +
  labs(x = expression(mu[samples]),
       y = expression(sigma[samples])) +
  theme(panel.grid = element_blank())
```

We can use `tidyr::pivot_longer()` and then `ggplot2::facet_wrap()` to
plot the densities for both `mu` and `sigma` at once.

```{r}
#| label: fig-densities-mu-sigma
#| fig-cap: "Shapes of the marginal posterior densities of μ and σ: tidyverse version"

d_grid_samples_b %>% 
  pivot_longer(mu_b:sigma_b) %>% 

  ggplot(aes(x = value)) + 
  geom_density(fill = "deepskyblue", color = "black") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ name, scales = "free", labeller = label_parsed)
```

We'll use the {**tidybayes**} package to compute their posterior modes
and 95% HDIs.

::: callout-important
There is a companion package {**ggdist**} which is imported completely
by {**tidybayes**}. Whenever you cannot find the function in
{**tidybayes**} then look at the documentation of {**ggdist**}. This is
also the case for the `tidybayes::mode_hdi()` function. In the help
files of {**tidybayes**} you will just find notes about a deprecated
`tidybayes::mode_hdih()` function but not the arguments of its new
version without the last `h` (for horizontal) `tidybayes::mode_hdi()`.
But you can look up these details in the {**ggdist**} documentation.
This observation is valid for many families of deprecated functions.

There is a division of functionality between {**tidybayes**} and
{**ggdist**}:

-   {**tidybayes**}: Tidy Data and 'Geoms' for Bayesian Models: Compose
    data for and extract, manipulate, and visualize posterior draws from
    Bayesian models in a tidy data format. Functions are provided to
    help extract tidy data frames of draws from Bayesian models and that
    generate point summaries and intervals in a tidy format.
-   {**ggdist**}: Visualizations of Distributions and Uncertainty:
    Provides primitives for visualizing distributions using
    {**ggplot2**} that are particularly tuned for visualizing
    uncertainty in either a frequentist or Bayesian mode. Both
    analytical distributions (such as frequentist confidence
    distributions or Bayesian priors) and distributions represented as
    samples (such as bootstrap distributions or Bayesian posterior
    samples) are easily visualized.
:::

::: callout-todo
###### TODO: ggdist \<-\> tidybayes

Replace `tidyverse::` with `ggdist::` where appropriate. This is
important when you want to find the related help file.
:::

```{r}
#| label: post-mode-hdi95-b

d_grid_samples_b %>% 
  pivot_longer(mu_b:sigma_b) %>% 
  group_by(name) %>% 
  tidybayes::mode_hdi(value) 
```

Let's say you wanted their posterior medians and 50% quantile-based
intervals, instead. Just switch out the last line for
`tidybayes::median_qi(value, .width = .5)`.

```{r}
#| label: post-median-qi90-b

d_grid_samples_b %>% 
  pivot_longer(mu_b:sigma_b) %>% 
  group_by(name) %>% 
  tidybayes::median_qi(value, .width = .5)
```

**Sample size and the normality of σ's posterior**

I will skip this part as there is nothing conceptually new in this
section.

#### Finding the posterior distribution with brm()

> In the text, McElreath indexed his models with names like `m4.1`. I
> will largely follow that convention, but will replace the *m* with a
> *b* to stand for the **`brms`** package.

Here's how to fit the first model for this chapter.

```{r}
#| label: post-dist-brms-b4.1
#| att-source: '#lst-post-dist-brms-b4.1 lst-cap="Finding the posterior distribution with brms::brm()"'

b4.1 <- 
  brms::brm(data = d2_b, 
      family = gaussian,
      height ~ 1,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.01")

brms:::plot.brmsfit(b4.1)
```

If you want detailed diagnostics for the HMC chains, call
`brms::launch_shinystan(b4.1)`. That'll keep you busy for a while. See
the [ShinyStan website](https://mc-stan.org/users/interfaces/shinystan)
for more information.

::: callout-caution
###### Launch of shinystan turned off

I turned off the evaluation of the following chunk. It took some time
and the it referred to a local page `http://127.0.0.1:6367/`\`where I
could inspect many details of the model. But this is at the moment too
complex to me: I do not understand all the parameters and the many
configurable options programmed with a {**shiny**) interface.
:::

```{r}
#| label: detailled-diganostic-chains-brms-b4.1
#| eval: false

brms::launch_shinystan(b4.1)

```

```{r}
#| label: print-summary-brms-b4.1

brms:::print.brmsfit(b4.1)

```

```{r}
#| label: print-stan-like-summary-brms-b4.1

b4.1$fit
```

Whereas rethinking defaults to 89% intervals, using `print()` or
`summary()` with {**brms**} models defaults to 95% intervals.

::: callout-note
As I have learned shortly: `print()` or `summary()` are generic
functions where one can add new printing methods with new classes. In
this case `class(b4.1)` = `r class(b4.1)`. This means I do not need to
add `brms::` to secure that I will get the {**brms**} printing or
summary method as I didn't load the {**brms**} package. Quite the
contrary: Adding `brms::` would result into the message: "Error:
'summary' is not an exported object from 'namespace:brms'".

As I really want to specify explicitly the method these generic
functions should use, I need to use the syntax `brms:::print.brmsfit()`
or `brms:::summary.brmsfit()` respectively.

In this respect I have to learn more about S3 classes. There are many
important web resources about this subject that I have found with the
search string "r what is s3 class". Maybe I should start with the [S3
chapter in Advanced R](https://adv-r.hadley.nz/s3.html).
:::

Unless otherwise specified, Kurz will stick with 95% intervals
throughout. To get those 89% intervals or McElreath approach, one could
use the `prob` argument within `summary()` or `print()`.

```{r}
#| label: summary-interval-.89-brms-b4.1

brms:::summary.brmsfit(b4.1, prob = .89)

```

Here's the `brms::brm()` code for the model with the very narrow `_μ_`
prior corresponding to the `rethinking::quap()` code in
@lst-post-dist-quap-m4.2.

```{r}
#| label: fig-post-dist-brms-b4.2
#| fig-cap: "Finding the posterior distribution with a narrower prior using brms::brm()"
#| att-source: '#lst-post-dist-brms-b4.2 lst-cap="Finding the posterior distribution with a narrower prior using brms::brm()"'

b4.2 <- 
  brms::brm(data = d2_b, 
      family = gaussian,
      height ~ 1,
      prior = c(brms::prior(normal(178, 0.1), class = Intercept),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.02")

brms:::plot.brmsfit(b4.2, widths = c(1, 2))

```

And here's the model `summary()`.

```{r}
#| label: summary-narrow-prior

brms:::summary.brmsfit(b4.2)

```

Subsetting the `summary()` output with `$fixed` provides a convenient
way to compare the Intercept summaries between `b4.1` and `b4.2`.

```{r}
#| label: compare-summaries-b4.1-b4.2

rbind(brms:::summary.brmsfit(b4.1)$fixed,
      brms:::summary.brmsfit(b4.2)$fixed)

```

#### Sampling from a ~~quap()~~ `brm()` fit

{**brms**} doesn't seem to have a convenience function that works the
way `vcov()` does for {**rethinking**}.

```{r}
#| label: calc-var-cov-m4.1-b
#| attr-source: '#lst-calc-var-cov-m4.1-b lst-cap="Calculation of vcov(): tidyverse version."'

brms:::vcov.brmsfit(b4.1)
```

This only returns the first element in the matrix it did for
{**rethinking**}. That is, it appears `brms::vcov()` only returns the
variance/covariance matrix for the single-level `_β_` parameters.

::: callout-caution
###### brms::vcov()

Referring to a similar situation with `rethinking::vcov()` in
@lst-calc-var-cov-m4.1-a I cannot write `brms::vcov()`, but have to use
either `brms:::vcov.brmsfit(b4.1)` or just `vcov(b4.1)`. The weird thing
is that the first time it also works with `brms::vcov()` but only the
first time!
:::

However, if you really wanted this information, you could get it after
putting the Hamilton Monte Carlo (HMC) chains in a data frame. We do
that with the `brms::as_draws_df()` function:

```{r}
#| label: put-hmc-into-df-b
#| attr-source: '#lst-put-hmc-into-df-b lst-cap="Extract the iteration of the Hamiliton Monte Carlo (HMC) chains into a data frame"'

post_b <- brms::as_draws_df(b4.1)

head(post_b)
```

::: callout-tip
###### draws object

The functions of the family `as_draws()` transform `brmsfit` objects to
`draws` objects, a format supported by the {**posterior**} package.
{brms} currently imports the family of `as_draws()`functions from the
{**posterior**} package, a tool for working with posterior
distributions.

@lst-put-hmc-into-df-b produced the {**brms**} version of what McElreath
achieved with `extract.samples()` in @lst-extract-samples-m4.1-a.
However, what happened under the hood was different. Whereas rethinking
used the `mvnorm()` function from the {**MASS**} package, with
{**brms**} we just extracted the iterations of the HMC chains and put
them in a data frame.
:::

Now `select()` the columns containing the draws from the desired
parameters `b_Intercept` and `sigma` and feed them into `cov()`.

```{r}
#| label: calc-cov-post-b
#| attr-source: '#lst-calc-cov-post-b lst-cap="Calculate the vector of variances and correlation matrix for b_Intercept and sigma"'

select(post_b, b_Intercept:sigma) %>% 
  stats::cov()
```

@lst-calc-cov-post-b displays "(1) a vector of variances for the
parameters and (2) a correlation matrix" for them (p. 90). Here are just
the variances (i.e., the diagonal elements) and the correlation matrix.

```{r}
#| label: calc-var-post-b
#| attr-source: '#lst-calc-var-post-b lst-cap="Calculate only variances (the diagonal values)"'

select(post_b, b_Intercept:sigma) %>%
  stats::cov() %>%
  base::diag()

```

```{r}
#| label: calc-corr-matrix-post-b
#| attr-source: '#lst-calc-corr-matrix-post-b lst-cap="Calculate only crrelation"'

# correlation
post_b %>%
  select(b_Intercept, sigma) %>%
  stats::cor()
```

```{r}
#| label: show-data-structure

str(post_b)
```

The `post_b` object is not just a data frame, but also of class
`draws_df`, which means it contains three metadata variables ----
`.chain`, `.iteration`, and `.draw` --- which are often hidden from
view, but are there in the background when needed. As you'll see, we'll
make good use of the `.draw` variable in the future. Notice how our post
data frame also includes a vector named `lp__`. That's the log
posterior.

For details, see: - The [Log-Posterior (function and
gradient)](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html#the-log-posterior-function-and-gradient)
section of the Stan Development Team's (2023) vignette [RStan: the R
interface to
Stan](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html)
and - Stephen Martin's [explanation of the log
posterior](https://discourse.mc-stan.org/t/basic-question-what-is-lp-in-posterior-samples-of-a-brms-regression/17567/2)
on the Stan Forums.

::: callout-caution
###### Summaries of {brms} and {posterior} packages

Kurz claims that `summary()` function doesn't work for {**brms**}
posterior data frames quite the way `rethinking::precis()` does for
posterior data frames from the {\*\*rethinking\*} package. But I this
observation is somewhat misleading.

The posterior data frame `post_b` is not of class `brms`. Let's check
this:

```{r}
#| label: class-post_b-b

class(post_b)
```

The class `draws_df`and `draws` refers to the {**posterior**} and not to
the {**brms**} package. Remember: In @lst-put-hmc-into-df-b we
transformed with the function `as_draws_df` the `brms` object into a
`draws_df` and `draws` object.

Therefore Kurz's claim should be read: The `summary()` function doesn't
work for {**posterior**} posterior data frames quite the way
`rethinking::precis()` does for posterior data frames from the
{**rethinking**} package. Instead of calling `brms:::summary.brmsfit()`
I will use `posterior:::summary.draws()`.

I wouldn't have noticed this difference if I hadn't mentioned explicitly
the name of the packages in front of the function, because in that case
R would have used `base::summary()` as in Kurz's text.
:::

```{r}
#| label: base-summary-samples-b4.1-b
#| attr-source: '#lst-base-summary-samples-b4.1-b lst-cap="Summary the extracted iterations of the HMC chains: base version"' 

base::summary(post_b[, 1:2])

```

```{r}
#| label: posterior-summary-samples-b4.1-b
#| attr-source: '#lst-posterior-summary-samples-b4.1-b lst-cap="Summary the extracted iterations of the HMC chains: posterior version"' 

posterior:::summary.draws(post_b[, 1:2])


```

To get a similar summary with tiny histograms Kurz offers different
solutions:

-   A base R approach by using the transpose of a `stats::quantile()`
    call nested within `base::apply()`
-   A {**tidyverse**} approach
-   A {**brms**} approach by just putting the `brm()` fit object into
    `posterior_summary()`
-   A {**tidybayes**} approach using `tidybayes::mean_hdi()` if you're
    willing to drop the posterior `sd` and
-   Using additionally the [function
    `histospark()`](https://github.com/hadley/precis/blob/master/R/histospark.R)
    (from the unfinished {**precis**} package by Hadley Wickham supposed
    to replace `base::summary()`) to get the tiny histograms and to add
    them into the tidyverse approach.

Additionally I will propose using the {**skimr**} packages:

```{r}
#| label: skim-summary-samples-b4.1-b
#| attr-source: '#lst-skim-summary-samples-b4.1-b lst-cap="Summary the extracted iterations of the HMC chains: skimr version"' 

skimr::skim(post_b[, 1:2])
```

Kurz refers only shortly to both `overthinking` blocks of this section:

-   Start values for `rethinking::quap()` resp. `brms::brm()` (See
    @sec-start-values-rethinking): Within the `brm()` function, you use
    the `init` argument fpr the start values.
-   Under the hood with multivariate sampling (See
    @sec-under-the-hood-multivariate-sampling-a): Again Kurz remarked
    that `brms::as_draws_df()` is not the same as
    `rethinking::extract.samples()`. What this exactly means will
    (hopefully) explained later in @sec-markov-chain-monte-carlo.

### Linear prediction

```{r}
#| label: fig-height-against-weight-b
#| fig-cap: "Adult height and weight against one another"


d2_b |> 
    ggplot(aes(height, weight)) + 
    geom_point()
```

#### The linear model strategy

##### Empty: Model definition

##### Empty: Linear model

##### Priors {#sec-priors-b}

```{r}
#| label: fig-sim-heights-only-with-priors-b
#| fig-cap: "Simulating heights from the model, using only the priors: tidyverse version"

set.seed(2971)
# how many lines would you like?
n_lines <- 100

lines <-
  tibble(n = 1:n_lines,
         a = rnorm(n_lines, mean = 178, sd = 20),
         b = rnorm(n_lines, mean = 0, sd = 10)) %>% 
  expand_grid(weight = range(d2_b$weight)) %>% 
  mutate(height = a + b * (weight - mean(d2_b$weight)))


lines %>% 
  ggplot(aes(x = weight, y = height, group = n)) +
  geom_hline(yintercept = c(0, 272), linetype = 2:1, linewidth = 1/3) +
  geom_line(alpha = 1/10) +
  coord_cartesian(ylim = c(-100, 400)) +
  ggtitle("b ~ dnorm(0, 10)") +
  theme_classic()

```

Using the Log-Normal distribution prohibits negative values. This is an
important constraint for height and weight as these variables cannot be
under 0.

```{r}
#| label: fig-log-normal-b
#| fig-cap: "Log-Normal distribution: tidyverse version"

set.seed(4)

tibble(b = rlnorm(1e4, meanlog = 0, sdlog = 1)) %>% 
  ggplot(aes(x = b)) +
  geom_density(fill = "grey92") +
  coord_cartesian(xlim = c(0, 5)) +
  theme_classic()

```

::: callout-note
Kurz used with `mean`and `sd`an abbreviated version of the argument
names `meanlog` and `sdlog`.
:::

I am not very skilled with the Log-Normal distribution, and so I am
happy that Kurz added some explanations:

> If you're unfamiliar with the log-normal distribution, it is the
> distribution whose logarithm is normally distributed. For example,
> here's what happens when we compare Normal(0,1) with
> log(Log-Normal(0,1)).

```{r}
#| label: fig-normal-log-normal
#| fig-cap: "Compare Normal(0,1) with log(Log-Normal(0,1))"

set.seed(4)

tibble(rnorm           = rnorm(1e5, mean = 0, sd = 1),
       `log(rlognorm)` = log(rlnorm(1e5, meanlog = 0, sdlog = 1))) %>% 
  pivot_longer(everything()) %>% 

  ggplot(aes(x = value)) +
  geom_density(fill = "grey92") +
  coord_cartesian(xlim = c(-3, 3)) +
  theme_classic() +
  facet_wrap(~ name, nrow = 2)
```

> Those values are ~~what~~ the mean and standard deviation of the
> output from the `rlnorm()` function **after** they are log
> transformed. The formulas for the actual mean and standard deviation
> for the log-normal distribution itself are complicated (see
> [Wikipedia](https://en.wikipedia.org/wiki/Log-normal_distribution)).

------------------------------------------------------------------------

::: {#def-mean-sd}
Calculate mean and standard deviation

$$
\begin{align*}
\text{mean}               & = \exp \left (\mu + \frac{\sigma^2}{2} \right) \\
\text{standard deviation} & = \sqrt{[\exp(\sigma ^{2})-1] \; \exp(2\mu +\sigma ^{2})}
\end{align*}
$$ {#eq-formula-mean-sd}
:::

------------------------------------------------------------------------

Let's try our hand at those formulas and compute the mean and standard
deviation for Log-Normal(0,1):

```{r}
#| label: compute-mu-sigma-for-log-normal-manually-b

mu    <- 0
sigma <- 1

# mean
exp(mu + (sigma^2) / 2)

# sd
sqrt((exp(sigma^2) - 1) * exp(2 * mu + sigma^2))
```

Let's confirm with simulated draws from `rlnorm()`.

```{r}
#| label: compute-log-normal-b
#| fig-cap: "Compute mean and standard deviation of the Log-Normal distribution: tidyverse version"

set.seed(4)

tibble(x = rlnorm(1e7, meanlog = 0, sdlog = 1)) %>% 
  summarise(mean = mean(x),
            sd   = sd(x))
```

```{r}
#| label: fig-prior-pred-sim-b
#| fig-cap: "Prior predictive simulation again, now with the Log-Normal prior: tidyverse version"


# make a tibble to annotate the plot
text <-
  tibble(weight = c(34, 43),
         height = c(0 - 25, 272 + 25),
         label  = c("Embryo", "World's tallest person (272 cm)"))

# simulate
set.seed(2971)

tibble(n = 1:n_lines,
       a = rnorm(n_lines, mean = 178, sd = 20),
       b = rlnorm(n_lines, mean = 0, sd = 1)) %>% 
  expand_grid(weight = range(d2_b$weight)) %>% 
  mutate(height = a + b * (weight - mean(d2_b$weight))) %>%
  
  # plot
  ggplot(aes(x = weight, y = height, group = n)) +
  geom_hline(yintercept = c(0, 272), linetype = 2:1, linewidth = 1/3) +
  geom_line(alpha = 1/10) +
  geom_text(data = text,
            aes(label = label),
            size = 3) +
  coord_cartesian(ylim = c(-100, 400)) +
  ggtitle("log(b) ~ dnorm(0, 1)") +
  theme_classic()
```

::: callout-tip
###### Literature reference

The paper by Simmons, Nelson and Simonsohn (2011), [False-positive
psychology: Undisclosed flexibility in data collection and analysis
allows presenting anything as
significant](https://journals.sagepub.com/doi/10.1177/0956797611417632),
is often cited as an introduction to the problem.
:::

#### Finding the posterior distribution

Unlike with McElreath's `rethinking::quap()` formula syntax, Kurz is not
aware that we can just specify something like `weight – xbar` in the
`formula` argument in `brms::brm()`.

However, the alternative is easy: Just make a new variable in the data
that is equivalent to `weight – mean(weight)`. We'll call it `weight_c`.

```{r}
#| label: create-weight-diff-var-b
#| attr-source: '#lst-create-weight-diff-var-b lst-cap="Create a new variable in the data equivalent to weight - mean(height): tidyverse version"'


d2_b <-
  d2_b %>% 
  mutate(weight_c = weight - mean(weight))
```

Unlike with McElreath's {**rethinking**} package, the conventional
`brms::brm()` syntax doesn't mirror the statistical notation. But here
are the analogues to the exposition at the bottom of page 97:

| {**rethinking**} package                                    | {**brms**} package: `brms::brm()`          |
|------------------------------------|------------------------------------|
| $\text{height}_i \sim \operatorname{Normal}(\mu_i, \sigma)$ | `family = gaussian`                        |
| $\mu_i = \alpha + \times (\beta \text{weight}_i)$           | `height \sim 1 + weight_c`                 |
| $\alpha \sim \operatorname{Normal}(178, 20)$                | `prior(normal(178, 20), class = Intercept` |
| $\beta \sim \operatorname{Log-Normal}(0, 1)$                | `prior(lognormal(0, 1), class = b)`        |
| $\sigma \sim \operatorname{Uniform}(0, 50)$                 | `prior(uniform(0, 50), class = sigma)`     |

: Mirror the statistical notation of the rethinking package with the
tidyverse approach using the brms package
{#tbl-mirror-rethinking-tidyverse}

```{r}
#| label: find-and-summarize-post-dist-b
#| attr-source: '#lst-find-and-summarize-post-dist-b lst-cap="Find the posterior distribution of the linear height-weight model: tidyverse version"'

b4.3 <- 
  brms::brm(data = d2_b, 
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(lognormal(0, 1), class = b),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03")

brms:::summary.brmsfit(b4.3)
```

Here are the trace plots.

```{r}
#| label: fig-find-post-dist-b
#| fig-cap: "Find the posterior distribution of the linear height-weight model: tidyverse version"

plot(b4.3, widths = c(1, 2))
```

{**brms**} does not allow users to insert coefficients into functions
like exp() within the conventional `formula` syntax. We can fit a
{**brms**} model like McElreath's `m4.3b` if we adopt what's called the
[non-linear
syntax](https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html).
The non-linear syntax is a lot like the syntax McElreath uses in
{**rethinking**} in that it typically includes both predictor and
variable names in the `formula`. Since this is so early in the book, I
won't go into a full-blown explanation, here. There will be many more
opportunities to practice with the non-linear syntax in the chapters to
come.

```{r}
#| label: find-post-dist2-b
#| attr-source: '#lst-find-post-dist2-b lst-cap="Find the posterior distribution of the linear height-weight model (log version): tidyverse version"'

b4.3b <- 
  brms::brm(data = d2_b, 
      family = gaussian,
      brms::bf(height ~ a + exp(lb) * weight_c,
         a ~ 1,
         lb ~ 1,
         nl = TRUE),
      prior = c(brms::prior(normal(178, 20), class = b, nlpar = a),
                brms::prior(normal(0, 1), class = b, nlpar = lb),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03b")
```

If you execute `summary(b4.3b)`, you'll see the intercept and `σ`
summaries for this model are about the same as those for `b4.3`, above.

```{r}
#| label: summarize-post-dist2-b
#| attr-source: '#lst-summarize-post-dist2-b lst-cap="Summarize the posterior distribution of the linear height-weight model (log version): tidyverse version"'

brms:::summary.brmsfit(b4.3b)
```

The difference is for the β parameter, which we called `lb` in the
`b4.3b` model. If we term that parameter from `b4.3` as $\beta^{b4.3}$
and the one from our new model $\beta^{b4.3b}$, it turns out that
$\beta^{b4.3} = exp(\beta^{b4.3b})$.

```{r}
#| label: extract-fixed-effects-b
#| attr-source: '#lst-extract-foxed-effects-b lst-cap="Extract and compare the population-level (fixed) effects from object b4.3 and b4.3b"'

brms::fixef(b4.3)["weight_c", "Estimate"]
brms::fixef(b4.3b)["lb_Intercept", "Estimate"] %>% exp()
```

They're the same within simulation variance.

#### Interpreting the posterior distribution

##### Tables of marginal distribution

With a little `[]` subsetting we can exclude the log posterior from our
summary for `b4.3`.

```{r}
#| label: table-summary-b4.3
#| attr-source: '#lst-table-summary-b4.3 lst-cap="Display the marginal posterior distributions of the parameters: brms version"'

brms::posterior_summary(b4.3)[1:3, ] %>% 
  round(digits = 2)
```

Without the subsetting we will get 2 more lines:

```{r}
#| label: table-summary2-b4.3
#| attr-source: '#lst-table-summary2-b4.3 lst-cap="Display the marginal posterior distributions of the parameters: brms version"'

brms::posterior_summary(b4.3) %>% 
  round(digits = 2)
```

::: callout-note
###### TODO: Interpret printout

-   `b_Intercept` represents `a` in the rethinking version.
-   `b_weight_c` represents `b` in the rethinking version. But why did
    we have to calculate it different?
-   `sigma` is the quadratic approximation of the standard deviation.
-   `lprior` is -- I assume -- the log prior.
-   `l__` is what??
:::

Looking up `brms::posterior_summary()` I learned that the "function
mainly exists to retain backwards compatibility. It will eventually be
replaced by functions of the {**posterior**} package".

One of the following examples suggests to convert the `brmsfit` object
into a `draws` object and then to use `posterior::summarise_draws()`.
But reading the help file of `posterior::summarise_draws()` it turned
out that it "will convert an object to a draws object if it isn't
already".

```{r}
#| label: table-summary3-b4.3
#| attr-source: '#lst-table-summary3-b4.3 lst-cap="Display the marginal posterior distributions of the parameters: posterior version"'

posterior::summarize_draws(b4.3, posterior::default_summary_measures(), 
                           .num_args = list(sigfig = 2))[1:3, ]


```

@lst-table-summary3-b4.3 is the default call using
`posterior::default_summary_measures()`. This returns the default
measures. But we can adapt this standard summary to get a very similar
result a in @lst-table-summary-m4.3.

```{r}
#| label: table-summary4-b4.3
#| attr-source: '#lst-table-summary4-b4.3 lst-cap="Display the marginal posterior distributions of the parameters: posterior version"'

posterior::summarize_draws(b4.3, "mean", "median", "sd", 
                           ~quantile(., probs = c(0.055, 0.945)),
                           .num_args = list(sigfig = 2))[1:3, ]
```

But don't forget that there exists also `summary()` as an alias either
for `brmsfit` or `draws` objects which can be used for a standardized
output:

```{r}
#| label: table-summary5-b4.3
#| attr-source: '#lst-table-summary5-b4.3 lst-cap="Display and compare the marginal posterior distributions of the parameters of the brms and the posterior version"'

## summary for brmsfit object
summary(b4.3)

## summary for draws object
summary(brms::as_draws_array(b4.3))[1:3, ]

```

If we put our {**brms**} fit into the `brms:::vcov.brmsfit()` function,
we'll get the variance/covariance matrix of the `intercept` and
`weight_c` coefficient.

```{r}
#| label: var-cov-matrix-b4.3
#| attr-source: '#lst-var-cov-matrix-b4.3 lst-cap="Calculate the variance-covariance matrix for model b4.3"'

brms:::vcov.brmsfit(b4.3) %>% 
  round(3)
```

No `σ`, however. To get that, we'll have to extract the posterior draws
and use the `cov()` function, instead.

```{r}
#| label: var-cov-matrix2-b4.3
#| attr-source: '#lst-var-cov-matrix2-b4.3 lst-cap="Calculate the variance-covariance matrix for model b4.3 with {posterior} draws objects"'

brms::as_draws_df(b4.3) %>%
  select(b_Intercept:sigma) %>%
  cov() %>%
  round(digits = 3)
```

The `pairs()` function will work for a brms fit much like it would one
from rethinking. It will show "both the marginal posteriors and the
covariance".

```{r}
#| label: fig-marg-post-cov-b4.3
#| fig-cap: "The marginal posteriors and the covariance matrix for model b4.3"
#| attr-source: '#lst-vfig-marg-post-cov-b4.3 lst-cap="Show the marginal posteriors and covariance matrix for model m4.3"'

brms:::pairs.brmsfit(b4.3)
```

##### Plotting posterior inference around the mean

Here is the code for Figure 4.6.

```{r}
#| label: fig-raw-data-line-b4.3
#| fig-cap: "Height in centimeters (vertical) plotted against weight_c (horizontal), with the line at the posterior mean plotted in black: tidyverse version"


d2_b %>%
  ggplot(aes(x = weight_c, y = height)) +
  geom_abline(intercept = brms:::fixef.brmsfit(b4.3)[1], 
              slope     = brms:::fixef.brmsfit(b4.3)[2]) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  theme_classic()
```

Note how the breaks on our `x`-axis look off. That's because we fit the
model with `weight_c` and we plotted the points in that metric, too.
Since we computed `weight_c` by subtracting the mean of weight from the
data, we can adjust the `x`-axis break point labels by simply adding
that value back.

```{r}
#| label: fig-raw-data-line2-b4.3
#| fig-cap: "Height in centimeters (vertical) plotted against weight in kilograms (horizontal), with the line at the posterior mean plotted in black: tidyverse version"
#| code-summary: "Reproducing Figure 4.6 of SR2"

labels <-
  c(-10, 0, 10) + mean(d2_b$weight) %>% 
  round(digits = 0)

d2_b %>%
  ggplot(aes(x = weight_c, y = height)) +
  geom_abline(intercept = brms:::fixef.brmsfit(b4.3)[[1]], 
              slope     = brms:::fixef.brmsfit(b4.3)[[2]]) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  scale_x_continuous("weight",
                     breaks = c(-10, 0, 10),
                     labels = labels) +
  theme_bw() +
  theme(panel.grid = element_blank())
```

Furthermore note the use of the `brms:::fixef.brmsfit()` function within
`ggplot2::geom_abline()`. The function extracts the population-level
('fixed') effects from a `brmsfit` object. Let's try and see what
`brms::fixef()` produces:

```{r}
#| label: using-fixef-b
#| attr-source: '#lst-using-fixef-b lst-cap="Extract population-level estimates from a brmsfit object"'
#| code-summary: "Extract population-level estimates from a brmsfit object"

brms::fixef(b4.3)
```

So the function `ggplot2::geom_abline()` has used the intercept (`a`)
and the slope (`b`) of the estimate column.

::: callout-note
Instead of using \[1\] and \[2\] I have used \[\[1\]\] and \[\[2\]\].

> `[` selects sub-lists: it always returns a list. If you use it with a
> single positive integer, it returns a list of length one. `[[` selects
> an element within a list. (from [Advanced
> R](https://adv-r.hadley.nz/subsetting.html#subsetting-answers), 2nd
> ed.)
:::

::: {-callout-note}
Remember: `brms::fixef(x)` is equivalent to `brms:::fixef.brmsfit(x)` if
`x` is a `brmsfit` object.
:::

##### Adding uncertainty around the mean

Instead of `rethinking::extract.samples()` the {**brms**} packages
extract all the posterior draws with `brms::as_draws_df()`. We have
already done this with @lst-put-hmc-into-df-b. We just repeat this code
here using `dplyr::slice(1:6)` instead of `utils::head()`

```{r}
#| label: put-hmc-into-df2-b
#| attr-source: '#lst-put-hmc-into-df2-b lst-cap="Extract the iteration of the Hamiliton Monte Carlo (HMC) chains into a data frame"'

post_b <- brms::as_draws_df(b4.3)
post_b %>%
  slice(1:6)
```

Here are the four models leading up to Figure 4.7:

```{r}
#| label: calc-all-four-models-b
#| attr-source: '#lst-calc-all-four-models-b lst-cap="Calculate all four models"'

dN10_b <- 10

b4.3_010 <- 
  brms::brm(data = d2_b %>%
        slice(1:dN10_b),  # note our tricky use of `N` and `slice()`
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(lognormal(0, 1), class = b),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03_010")

dN50_b <- 50

b4.3_050 <- 
  brms::brm(data = d2_b %>%
        slice(1:dN50_b), 
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(lognormal(0, 1), class = b),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03_050")

dN150_b <- 150

b4.3_150 <- 
  brms::brm(data = d2_b %>%
        slice(1:dN150_b), 
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(lognormal(0, 1), class = b),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03_150")

dN352_b <- 352

b4.3_352 <- 
  brms::brm(data = d2_b %>%
        slice(1:dN352_b), 
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(lognormal(0, 1), class = b),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03_352")
```

Here are the trace plots and coefficient summaries from these four
models.

```{r}
#| label: trace-plots-cov-sum-b
#| attr-source: '#lst-trace-plots-cov-sum-b lst-cap="Trace plots and coefficient summaries from all four models"'


plot(b4.3_010)
print(b4.3_010)

plot(b4.3_050)
print(b4.3_050)

plot(b4.3_150)
print(b4.3_150)

plot(b4.3_352)
print(b4.3_352)
```

We'll need to put the chains of each model into data frames.

```{r}
#| label: put-chain-into-model-b
#| #| attr-source: #lst-put-chain-into-model-b lst-cap="Put the chains of each model into data frames"'

post010_b4.3 <- brms::as_draws_df(b4.3_010)
post050_b4.3 <- brms::as_draws_df(b4.3_050)
post150_b4.3 <- brms::as_draws_df(b4.3_150)
post352_b4.3 <- brms::as_draws_df(b4.3_352)
```

Here is the code for the four individual plots:

```{r}
#| label: calc-code-for-plots-b
#| attr-source: '#lst-calc-code-for-plots-b lst-cap="Prepare data for four individual plots"'

p5 <- 
  ggplot(data =  d2_b[1:10, ], 
         aes(x = weight_c, y = height)) +
  geom_abline(data = post010_b4.3 %>% slice(1:20),
              aes(intercept = b_Intercept, slope = b_weight_c),
              linewidth = 1/3, alpha = .3) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  coord_cartesian(xlim = range(d2_b$weight_c),
                  ylim = range(d2_b$height)) +
  labs(subtitle = "N = 10")

p6 <-
  ggplot(data =  d2_b[1:50, ], 
         aes(x = weight_c, y = height)) +
  geom_abline(data = post050_b4.3 %>% slice(1:20),
              aes(intercept = b_Intercept, slope = b_weight_c),
              linewidth = 1/3, alpha = .3) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  coord_cartesian(xlim = range(d2_b$weight_c),
                  ylim = range(d2_b$height)) +
  labs(subtitle = "N = 50")

p7 <-
  ggplot(data =  d2_b[1:150, ], 
         aes(x = weight_c, y = height)) +
  geom_abline(data = post150_b4.3 %>% slice(1:20),
              aes(intercept = b_Intercept, slope = b_weight_c),
              linewidth = 1/3, alpha = .3) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  coord_cartesian(xlim = range(d2_b$weight_c),
                  ylim = range(d2_b$height)) +
  labs(subtitle = "N = 150")

p8 <- 
  ggplot(data =  d2_b[1:352, ], 
         aes(x = weight_c, y = height)) +
  geom_abline(data = post352_b4.3 %>% slice(1:20),
              aes(intercept = b_Intercept, slope = b_weight_c),
              linewidth = 1/3, alpha = .3) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  coord_cartesian(xlim = range(d2_b$weight_c),
                  ylim = range(d2_b$height)) +
  labs(subtitle = "N = 352")
```

Now we can combine the ggplots with patchwork syntax to make the full
version of Figure 4.7.

```{r}
#| label: fig-draw-plots-figure-4.7-b
#| fig-cap: "Samples from the quadratic approximate posterior distribution for the height/weight model, b4.3, with increasing amounts of data. In each plot, 20 lines sampled from the posterior distribution, showing the uncertainty in the regression relationship. Tidyverse version."



library(patchwork)

(p5 + p6 + p7 + p8) &
  scale_x_continuous("weight",
                     breaks = c(-10, 0, 10),
                     labels = labels) &
  theme_classic()
```

##### Plotting regression intervals and contours

Since we used `weight_c` to fit our model, we might first want to
understand what exactly the mean value is for weight.

```{r}
#| label: calc-mean-weight-b
#| attr-source: '#lst-calc-mean-weight-b lst-cap="Calculate mean of weight"'

mean(d2_b$weight)
```

Just a hair under 45. If we're interested in $\mu$ at `weight` = 50,
that implies we're also interested in $\mu$ at `weight_c` + 5.01. Within
the context of our model, we compute this with
$\alpha + \beta \cdot 5.01$. Here's what that looks like with `post_b`.

```{r}
#| label: calc-mean-weight-at-50-b
#| attr-source: '#lst-calc-mean-weight-at-50-b lst-cap="Calculate the mean at weight 50 kg"'

mu_at_50_b <- 
  post_b %>% 
  transmute(mu_at_50_b = b_Intercept + b_weight_c * 5.01)
 
head(mu_at_50_b)
```

And here is a version McElreath's Figure 4.8 density plot.

```{r}
#| label: fig-density-vector-mean-50-b
#| fig-cap: "The quadratic approximate posterior distribution of the mean height, μ, when weight is 50 kg. This distribution represents the relative plausibility of different values of the mean. Tidyverse version"

mu_at_50_b %>%
  ggplot(aes(x = mu_at_50_b)) +
  geom_density(linewidth = 0, fill = "deepskyblue") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(mu["height | weight = 50"])) +
  theme_classic()
```

We'll use `tidybayes::mean_hdi()` to get both 89% and 95% HPDIs along
with the mean.

```{r}
#| label: calc-mean-and-HPDI-b
#| attr-source: '#lst-calc-mean-and-HPDI-b lst-cap="Calculate both 89% and 95% Highest Priority Intensity Intervals (HPDIs) along with the mean."'

tidybayes::mean_hdi(mu_at_50_b[, 1], .width = c(.89, .95))
```

If you wanted to express those sweet 95% HPDIs on your density plot, you
might use `tidybayes::stat_halfeye()`. Since `tidybayes::stat_halfeye()`
also returns a point estimate, we'll throw in the mode.

```{r}
#| label: fig-half-eye-b
#| fig-cap: "Plot of half-eye (density + interval) geometry"
mu_at_50_b %>%
  ggplot(aes(x = mu_at_50_b, y = 0)) +
  tidybayes::stat_halfeye(point_interval = tidybayes::mode_hdi, .width = .95,
               fill = "deepskyblue") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(mu["height | weight = 50"])) +
  theme_classic()
```

With {**brms**}, you would use fitted() to do what McElreath
accomplished with `rethinking::link()`.

::: callout-caution
Kurz applies the function `fitted()` in the code, but in the text he
uses twice `brms::fitted()` which doesn't exist. I used both
`brms:::fitted.brmsfit()` and `stats::fitted()` to get the same results.

The object `b4.3` is of class `brmsfit` but in the help file of
`stats::fitted()` you can read: "`fitted` is a generic function which
extracts fitted values from objects returned by modeling functions.
**All object classes which are returned by model fitting functions
should provide a `fitted` method.** (emphasis is mine)

My interpretation therefore is that `stats::fitted()` is using
`brms:::fitted.brmsfit()`. Thts why the results are identical.
:::

```{r}
#| label: calc-mu-with-fitted-b
#| attr-source: '#lst-calc-mu-with-fitted-b lst-cap="Calculate μ for each case in the data and sample from the posterior distribution: Tidyverse version"'

mu2_b <- brms:::fitted.brmsfit(b4.3, summary = F)
mu2.1_b <- stats::fitted(b4.3, summary = F)
str(mu2_b)
str(mu2.1_b)
```

When you specify `summary = F`, `brms:::fitted.brmsfit()` returns a
matrix of values with as many rows as there were post-warmup draws
across your Hamilton Monte Carlo (HMC) chains and as many columns as
there were cases in your analysis. Because we had 4,000 post-warmup
draws and $n=352$, `brms:::fitted.brmsfit()` returned a matrix of 4,000
rows and 352 vectors. If you omitted the `summary = F` argument, the
default is TRUE and `brms:::fitted.brmsfit()` will return summary
information instead.

Much like `rethinking::link()`, `brms:::fitted.brmsfit()` can
accommodate custom predictor values with its `newdata` argument.

```{r}
#| label: calc-dist-mu-unique-with-fitted.brmsfit-b
#| attr-source: '#lst-calc-dist-mu-unique-with-fitted.brmsfit-b lst-cap="Calculate a distribution of μ for each unique weight value on the horizontal axis: tidyverse version"'

weight_seq <- 
  tibble(weight = 25:70) %>% 
  mutate(weight_c = weight - mean(d2_b$weight))

mu3_b <-
  brms:::fitted.brmsfit(b4.3,
         summary = F,
         newdata = weight_seq) %>%
  data.frame() %>%
  # here we name the columns after the `weight` values from which they were computed
  set_names(25:70) %>% 
  mutate(iter = 1:n())

head(mu3_b)
```

::: callout-caution
The {rethinking} version uses the variable `weight.seq` whereas the
tidyverse version uses `weight_seq`.
:::

Anticipating {**ggplot2**}, we went ahead and converted the output to a
data frame. But we might do a little more data processing with the aid
of `tidyr::pivot_longer()`, which will convert the data from the wide
format to the long format.

::: callout-tip
###### Literature references

If you are new to the distinction between wide and long data, you can
learn more from the [Pivot data from wide to
long](https://tidyr.tidyverse.org/reference/pivot_longer.html) vignette
from the tidyverse team (2020); Simon Ejdemyr's blog post, [Wide & long
data](https://sejdemyr.github.io/r-tutorials/basics/wide-and-long/); or
Karen Grace-Martin's blog post, [The wide and long data format for
repeated measures
data](https://www.theanalysisfactor.com/wide-and-long-data/).
:::

```{r}
#| label: convert-wide-to-long-b
#| attr-source: '#lst-convert-wide-to-long-b lst-cap="Data processing: Convert data from wide to long format: tidyverse version"'

mu4_b <- 
  mu3_b %>%
  pivot_longer(-iter,
               names_to = "weight",
               values_to = "height") %>% 
  # we might reformat `weight` to numerals
  mutate(weight = as.numeric(weight))

head(mu4_b)
```

Now our data processing is done, here we reproduce McElreath's Figure
4.9.a.

```{r}
#| label: fig-dist-mu-height-100-b
#| fig-cap: "The first 100 values in the distribution of μ at each weight value. Tidyverse version"
d2_b %>%
  ggplot(aes(x = weight, y = height)) +
  geom_point(data = mu4_b %>% filter(iter < 101), 
             color = "navyblue", alpha = .05) +
  coord_cartesian(xlim = c(30, 65)) +
  theme(panel.grid = element_blank())
```

With `brms:::fitted.brmsfit()`, it's quite easy to plot a regression
line and its intervals. Just omit the `summary = T` argument.

```{r}
#| label: sum-dist-weight-b
#| attr-source: '#lst-sum-dist-weight-b lst-cap="Summary of the distribution for each weight value. Tidyverse version"'

mu_summary <-
  brms:::fitted.brmsfit(b4.3, 
         newdata = weight_seq) %>%
  data.frame() %>%
  bind_cols(weight_seq)

head(mu_summary)
```

Here it is, our analogue to Figure 4.9.b.

```{r}
#| label: fig-summaries-on-data-top-b
#| fig-cap: "Plot of the summaries on top of the !Kung height data again, now with 89% compatibility interval of the mean indicated by the shaded region. Compare this region to the distributions of blue points in @fig-dist-mu-height-100-b."


d2_b %>%
  ggplot(aes(x = weight, y = height)) +
  geom_smooth(data = mu_summary,
              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              stat = "identity",
              fill = "grey70", color = "black", alpha = 1, linewidth = 1/2) +
  geom_point(color = "navyblue", shape = 1, size = 1.5, alpha = 2/3) +
  coord_cartesian(xlim = range(d2_b$weight)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())
```

If you wanted to use intervals other than the default 95% ones, you'd
include the probs argument like this:
`brms:::fitted.brmsfit(b4.3, newdata = weight.seq, probs = c(.25, .75))`.
The resulting third and fourth vectors from the `fitted()` object would
be named `Q25` and `Q75` instead of the default `Q2.5` and `Q97.5`. The
[Q prefix](https://github.com/paul-buerkner/brms/issues/425) stands for
quantile.

Similar to `rethinking::link()`, `brms:::fitted.brmsfit()` uses the
formula from your model to compute the model expectations for a given
set of predictor values. I use it a lot in this project. If you follow
along, you'll get a good handle on it. But to dive deeper, you can [go
here for the
documentation](https://rdrr.io/cran/brms/man/fitted.brmsfit.html).
Though we won't be using it in this project, {**brms**} users might want
to know that `fitted()` is also an alias for the
`brms::posterior_epred()` function, about which you might [learn more
here](https://rdrr.io/cran/brms/man/posterior_epred.brmsfit.html). Users
can always learn more about them and other functions in the [{**brms**}
reference
manual](https://cran.r-project.org/web/packages/brms/brms.pdf).

##### Prediction intervals

We've only been plotting the `μ` part. In order to bring in the
variability expressed by `σ`, we'll have to switch to the `predict()`
function. Much as `brms:::fitted.brmsfit()` was our analogue to
`rethinking::link()`, `brms:::predict.brmsfit()` is our analogue to
`rethinking::sim()`.

We can reuse our `weight_seq` data from before. The `predict()` code
looks a lot like what we used for `fitted()`. Compare
@lst-calc-dist-mu-unique-with-fitted.brmsfit-b with
@lst-calc-pred-height-with-predict.brmsfit-b.

```{r}
#| label: calc-pred-height-with-predict.brmsfit-b
#| attr-source: '#lst-calc-pred-height-with-predict.brmsfit-b lst-cap="Calculate the prediction of heights: tidyverse version"'

pred_height <-
  brms:::predict.brmsfit(b4.3,
          newdata = weight_seq) %>%
  data.frame() %>%
  bind_cols(weight_seq)
  
pred_height %>%
  slice(1:6)
```

This time the summary information in our data frame is for, as McElreath
put it, "simulated heights, not distributions of plausible average
height, `μ`" (p. 108). Another way of saying that is that these
simulations are the joint consequence of both `μ` and `σ`, unlike the
results of `fitted()`, which only reflect `μ`.

Figure 4.10 shows how you might visualize them:

```{r}
#| label: fig-reproduce-figure-4.10
#| fig-cap: "89% prediction interval for height, as a function of weight. The solid line is the average line for the mean height at each weight. The two shaded regions show different 89% plausible regions. The narrow shaded interval around the line is the distribution of μ. The wider shaded region represents the region within which the model expects to find 89% of actual heights in the population, at each weight. (tidyverse version)"

d2_b %>%
  ggplot(aes(x = weight)) +
  geom_ribbon(data = pred_height, 
              aes(ymin = Q2.5, ymax = Q97.5),
              fill = "grey83") +
  geom_smooth(data = mu_summary,
              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              stat = "identity",
              fill = "grey70", color = "black", alpha = 1, linewidth = 1/2) +
  geom_point(aes(y = height),
             color = "navyblue", shape = 1, size = 1.5, alpha = 2/3) +
  coord_cartesian(xlim = range(d2_b$weight),
                  ylim = range(d2_b$height)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())
```

To smooth out the rough shaded interval we would have in the {**brms**}
model fitting approach to refit `b4.3` after specifying a larger number
of post-warmup iterations with alterations to the `iter` and `warmup`
parameters.

::: callout-note
###### TODO: Smooth boundary

I should try to smooth out the grey boundary, because it would give me
more experiences how to use the different parameters to fit models with
the {**brms**} approach.

This experiment would also an occasion to change the intervals from the
default 95% ones to the 89% McElreath's is using. See his hint in the
paragraph before displaying Figure 10 how to change it in the
{**rethinking**} version.

In {**brms**} I would have to change the `probs` argument to
brms:::fitted.brmsfit(b4.3, newdata = weight.seq, probs = c(0.055,
0.945)). The resulting third and fourth vectors from the fitted() object
would be named Q5.5 and Q94.5 instead of the default Q2.5 and Q97.5. The
Q prefix stands for quantile. See [Rename summary columns of predict()
and related methods](https://github.com/paul-buerkner/brms/issues/425).
:::

Next we follow McElreath's example and do our model-based predictions by
hand. Instead of relying on base R `apply()` and `sapply()`, the main
action in the tidyverse approach is in `expand_grid()`, the second
`mutate()` line and the `group_by()` + `summarise()` combination.

```{r}
#| label: fig-predict-manually-b
#| attr-source: '#lst-fig-predict-manually-b lst-cap="Model-based predictions without {brms} and pedict(): mean with quantiles of 0.25 and .975"'

set.seed(4)

post_b %>% 
  tidyr::expand_grid(weight = 25:70) %>% 
  mutate(weight_c = weight - mean(d2_b$weight)) %>% 
  mutate(sim_height = rnorm(n(),
                            mean = b_Intercept + b_weight_c * weight_c,
                            sd   = sigma)) %>% 
  group_by(weight) %>% 
  summarise(mean = mean(sim_height),
            ll   = quantile(sim_height, prob = .025),
            ul   = quantile(sim_height, prob = .975)) %>% 
  
  # plot
  ggplot(aes(x = weight)) +
  geom_smooth(aes(y = mean, ymin = ll, ymax = ul),
              stat = "identity",
              fill = "grey83", color = "black", alpha = 1, linewidth = 1/2) +
  geom_point(data = d2_b,
             aes(y = height),
             color = "navyblue", shape = 1, size = 1.5, alpha = 2/3) +
  coord_cartesian(xlim = range(d2_b$weight),
                  ylim = range(d2_b$height)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())
```

We specifically left out the `fitted()` intervals to make it more
apparent what we were simulating. You might also note that we could have
easily replaced that three-line summarize() code with a single line of
`tidybayes::mean_qi(sim_height)`, or whatever combination of central
tendency and interval type you wanted (e.g.,
`tidybayes::mode_hdi(sim_height, .width = .89)`)

Let's try this out:

```{r}
#| label: fig-predict-manually2-b
#| attr-source: '#lst-fig-predict-manually2-b lst-cap="Model-based predictions without {brms} and pedict(): mean with width .89"'

set.seed(4)

post_b %>% 
  tidyr::expand_grid(weight = 25:70) %>% 
  mutate(weight_c = weight - mean(d2_b$weight)) %>% 
  mutate(sim_height = rnorm(n(),
                            mean = b_Intercept + b_weight_c * weight_c,
                            sd   = sigma)) %>% 
  group_by(weight) %>% 
  tidybayes::mean_qi(sim_height, .width = .89) %>% 
  
  # plot
  ggplot(aes(x = weight)) +
  geom_smooth(aes(y = .point, ymin = .lower, ymax = .upper),
              stat = "identity",
              fill = "grey83", color = "black", alpha = 1, linewidth = 1/2) +
  geom_point(data = d2_b,
             aes(y = height),
             color = "navyblue", shape = 1, size = 1.5, alpha = 2/3) +
  coord_cartesian(xlim = range(d2_b$weight),
                  ylim = range(d2_b$height)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())
```

```{r}
#| label: fig-predict-manually3-b
#| attr-source: '#lst-fig-predict-manually3-b lst-cap="Model-based predictions without {brms} and pedict(): mode with width .89"'

set.seed(4)

post_b %>% 
  tidyr::expand_grid(weight = 25:70) %>% 
  mutate(weight_c = weight - mean(d2_b$weight)) %>% 
  mutate(sim_height = rnorm(n(),
                            mean = b_Intercept + b_weight_c * weight_c,
                            sd   = sigma)) %>% 
  group_by(weight) %>% 
  tidybayes::mode_hdi(sim_height, .width = .89) %>% 
  
  # plot
  ggplot(aes(x = weight)) +
  geom_smooth(aes(y = .point, ymin = .lower, ymax = .upper),
              stat = "identity",
              fill = "grey83", color = "black", alpha = 1, linewidth = 1/2) +
  geom_point(data = d2_b,
             aes(y = height),
             color = "navyblue", shape = 1, size = 1.5, alpha = 2/3) +
  coord_cartesian(xlim = range(d2_b$weight),
                  ylim = range(d2_b$height)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())
```

::: \###### TODO: .width vs. ll/ul

Revise @sec-sampling-to-summarize to understand better the difference
between width (= defined probability mass, for example .89) and ll/ul (=
defined boundaries, for example .025 and .975).

What is the equivalent of width .89 (defined in probability mass) in
defined boundaries under the assumption of a Gaussian distribution?
Solution: .055 and.945!

### Curves from lines

#### Polynomial regression

To see an application for fitting curves instead of lines we are going
to use the `Howell1` data, but this time the full data set. The reason
is that in the non-adult years there is a steeper slope than in the
adult years.

```{r}
#| label: fig-scatterplot-height-weight-b
#| fig-cap: "Height in centimeters (vertical) plotted against weight in kilograms (horizontal). This time with the full data, e.g., the non-adults data."
#| attr-source: '#fig-scatterplot-height-weight-b lst-cap="Height in centimeters (vertical) plotted against weight in kilograms (horizontal): tidyverse version"'

d_b %>% 
  ggplot(aes(x = weight, y = height)) +
  geom_point(color = "navyblue", shape = 1, size = 1.5, alpha = 2/3) +
  annotate(geom = "text",
           x = 42, y = 115,
           label = "This relation is\nvisibly curved.",
           family = "Times") +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())
```

Standardizing will help `brms::brm()` fit the model. We might
standardize our weight variable like so:

```{r}
#| label: standardize-weight-b
#| attr-source: '#lst-standardize-weight-b lst-cap="Stadardize the weight variable"'

d3_b <-
  d_b %>%
  mutate(weight_s = (weight - mean(weight)) / sd(weight)) %>% 
  mutate(weight_s2 = weight_s^2)
```

We fit the quadratic model (se @eq-parabolic-model) with {**brms**}:

```{r}
#| label: brm-parabolic
#| attr-source: '#lst-brm-parabolic lst-cap="Finding the posterior distribution of a parabolic model of height on weight with brms::brm()"'

b4.5 <- 
  brms::brm(data = d3_b, 
      family = gaussian,
      height ~ 1 + weight_s + weight_s2,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(lognormal(0, 1), class = b, coef = "weight_s"),
                brms::prior(normal(0, 1), class = b, coef = "weight_s2"),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.05")
```

Note our use of the coef argument within our prior statements. Since β1
and β2 are both parameters of `class = b` within the {**brms**} set-up,
we need to use the `coef` argument when we want their priors to differ.

```{r}
#| label: fig-brm-parabolic
#| attr-source: '#lst-fig-brm-parabolic lst-cap="Plot the posterior distribution of a parabolic model of height on weight calculated with brms::brm()"'

plot(b4.5, widths = c(1, 2))
```

::: callout-note
###### TODO: Interpret graphic
:::

```{r}
#| label: print-brm-parabolic
#| attr-source: '#lst-print-brm-parabolic lst-cap="Pint the result of the posterior distribution of a parabolic model of height on weight calculated with brms::brm()"'
#| 
print(b4.5)
```

::: callout-note
###### TODO: Interpret printout
:::

Our quadratic plot requires new `fitted()`- and `predict()`-oriented
data wrangling.

```{r}
#| label: wrangling-for-parabolic-model-b
#| attr-source: '#lst-wrangling-for-parabolic-model-b lst-cap="Data wrangling as a preparation for the parabolic (quadratic) model"'

weight_seq_full <- 
  tibble(weight_s = seq(from = -2.5, to = 2.5, length.out = 30)) %>% 
  mutate(weight_s2 = weight_s^2)

fitd_quad <-
  fitted(b4.5, 
         newdata = weight_seq_full) %>%
  data.frame() %>%
  bind_cols(weight_seq_full)

pred_quad <-
  predict(b4.5, 
          newdata = weight_seq_full) %>%
  data.frame() %>%
  bind_cols(weight_seq_full)  
```

Replicating Figure 4.11.b:

```{r}
#| label: fig-replicate-4.11.b
#| fig-cap: "Replicate Figure 4.11.b: Polynomial regressions of height on weight (standardized), for the full !Kung data. The raw data are shown by the circles. The solid curves show the path of μ in each model, and the shaded regions show the 95% interval of the mean (close to the solid curve) and the 95% interval of predictions (wider). (Note: This is slightly different to the original version with a width of .89 and qunatiles at .055 and .945 resp. Q5.5 and Q94.5)"

p10 <-
  ggplot(data = d3_b, 
         aes(x = weight_s)) +
  geom_ribbon(data = pred_quad, 
              aes(ymin = Q2.5, ymax = Q97.5),
              fill = "grey83") +
  geom_smooth(data = fitd_quad,
              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              stat = "identity",
              fill = "grey70", color = "black", alpha = 1, linewidth = 1/2) +
  geom_point(aes(y = height),
             color = "navyblue", shape = 1, size = 1.5, alpha = 1/3) +
  labs(subtitle = "quadratic",
       y = "height") +
  coord_cartesian(xlim = range(d3_b$weight_s),
                  ylim = range(d3_b$height)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())

p10
```

From a formula perspective, the cubic model is a simple extension of the
quadratic (compare @eq-parabolic-model with @eq-cubic-regression). --
Before we fit the model, we need to wrangle the data again.

```{r}
#| label: wrangling-for-cubic-model-b
#| attr-source: '#lst-wrangling-for-cubic-model-b lst-cap="Data wrangling as a preparation for the cubic model."'

d3_b <-
  d3_b %>% 
  mutate(weight_s3 = weight_s^3)
```

Now fit the model:

```{r}
#| label: fit-cubic-regression-model-b
#| attr-source: '#lst-fit-cubic-regression-model-b lst-cap="Fit a cubic regression model of height on weight (standardized), for the full !Kung data."'

b4.6 <- 
  brms::brm(data = d3_b, 
      family = gaussian,
      height ~ 1 + weight_s + weight_s2 + weight_s3,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(lognormal(0, 1), class = b, coef = "weight_s"),
                brms::prior(normal(0, 1), class = b, coef = "weight_s2"),
                brms::prior(normal(0, 1), class = b, coef = "weight_s3"),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.06")
```

Here's the `fitted()`, `predict()`, and {**ggplot2**} code for Figure
4.11.c, the cubic model.

```{r}
#| label: fig-cubic-regression-model-b
#| fig-cap: "Fit a cubic regression model of height on weight (standardized), for the full !Kung data"
#| attr-source: '#lst-fig-cubic-regression-model-b lst-cap="Fit a cubic regression model of height on weight (standardized), for the full !Kung data: tidyverse version"'

weight_seq_full <- 
  weight_seq_full %>% 
  mutate(weight_s3 = weight_s^3)

fitd_cub <-
  fitted(b4.6, 
         newdata = weight_seq_full) %>%
  as_tibble() %>%
  bind_cols(weight_seq_full)

pred_cub <-
  predict(b4.6, 
          newdata = weight_seq_full) %>%
  as_tibble() %>%
  bind_cols(weight_seq_full) 

p11 <-
  ggplot(data = d3_b, 
       aes(x = weight_s)) +
  geom_ribbon(data = pred_cub, 
              aes(ymin = Q2.5, ymax = Q97.5),
              fill = "grey83") +
  geom_smooth(data = fitd_cub,
              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              stat = "identity",
              fill = "grey70", color = "black", alpha = 1, linewidth = 1/4) +
  geom_point(aes(y = height),
             color = "navyblue", shape = 1, size = 1.5, alpha = 1/3) +
  labs(subtitle = "cubic",
       y = "height") +
  coord_cartesian(xlim = range(d3_b$weight_s),
                  ylim = range(d3_b$height)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())

p11
```

And now we'll fit the good old linear model.

```{r}
#| label: fit-linear-regression-model-b
#| attr-source: '#lst-fit-linear-regression-model-b lst-cap="Fit a linear regression model of height on weight (standardized), for the full !Kung data."'

b4.7 <- 
  brms::brm(data = d3_b, 
      family = gaussian,
      height ~ 1 + weight_s,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(lognormal(0, 1), class = b, coef = "weight_s"),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.07")
```

And here's the `fitted()`, `predict()`, and {**ggplot2**} code for
Figure 4.11.a, the linear model.

```{r}
#| label: fig-linear-regression-model-b
#| fig-cap: "Fit a linear regression model of height on weight (standardized), for the full !Kung data"
#| attr-source: '#lst-fig-linear-regression-model-b lst-cap="Fit a linear regression model of height on weight (standardized), for the full !Kung data: tidyverse version"'

fitd_line <-
  fitted(b4.7, 
         newdata = weight_seq_full) %>%
  as_tibble() %>%
  bind_cols(weight_seq_full)

pred_line <-
  predict(b4.7, 
          newdata = weight_seq_full) %>%
  as_tibble() %>%
  bind_cols(weight_seq_full) 

p9 <-
  ggplot(data = d3_b, 
       aes(x = weight_s)) +
  geom_ribbon(data = pred_line, 
              aes(ymin = Q2.5, ymax = Q97.5),
              fill = "grey83") +
  geom_smooth(data = fitd_line,
              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),
              stat = "identity",
              fill = "grey70", color = "black", alpha = 1, linewidth = 1/4) +
  geom_point(aes(y = height),
             color = "navyblue", shape = 1, size = 1.5, alpha = 1/3) +
  labs(subtitle = "linear",
       y = "height") +
  coord_cartesian(xlim = range(d3_b$weight_s),
                  ylim = range(d3_b$height)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank())

p9
```

Did you notice how we labeled each of the past three plots as `p1`,
`p2`, and `p3`? Here we use those names to plot them all together with
{**patchwork**} syntax.

```{r}
#| label: fig-plot-all-together-b
#| fig-cap: "Plot all three models (linear, quadratic and cubic) together"
#| attr-source: '#lst-fig-plot-all-together-b lst-cap="Plot all three models (linear, quadratic and cubic) together, using the {**patchwork**} syntax"'

library(patchwork)
p9 | p10 | p11
```

**Converting back to natural scale** You can apply McElreath's
conversion trick within the {**ggplot2**} environment, too. Here it is
with the cubic model.

```{r}
#| label: fig-cubic-regression-natural-scale-b
#| attr-source: '#lst-fig-cubic-regression-natural-scale-b lst-cap="Cubic regression with x-axis in natural scale: tidyverse version"'

at_b <- c(-2, -1, 0, 1, 2)

ggplot(data = d3_b, 
       aes(x = weight_s)) +
  geom_ribbon(data = pred_cub, 
              aes(ymin = Q2.5, ymax = Q97.5),
              fill = "grey83") +
  geom_point(aes(y = height),
             color = "navyblue", shape = 1, size = 1.5, alpha = 1/3) +
  coord_cartesian(xlim = range(d3_b$weight_s)) +
  theme(text = element_text(family = "Times"),
        panel.grid = element_blank()) +
  
  # here it is!
  scale_x_continuous("standardized weight converted back",
                     breaks = at_a,
                     labels = round(at_b*sd(d3_b$weight) + mean(d3_b$weight), 1))
```

#### Splines

```{r}
#| label: load-cherry-blossoms-data-b
#| attr-source: '#lst-load-cherry-blossoms-data-b lst-cap="Load Cherry Blossoms data and display summary (tidyverse version)"'

## R code 4.72 modified ######################
data(package = "rethinking", list = "cherry_blossoms")
d4_b <- cherry_blossoms


# ground-up tidyverse way to summarize
(
d4.2_b <- 
    d4_b %>% 
      gather() %>% 
      group_by(key) %>% 
      summarise(mean = mean(value, na.rm = T),
                sd   = sd(value, na.rm = T),
                ll   = quantile(value, prob = .055, na.rm = T),
                ul   = quantile(value, prob = .945, na.rm = T)) %>% 
      mutate_if(is.double, round, digits = 2) 
)

d4.2_b |> 
  skimr::skim()



```

Kurz's version does not have the mini histograms. I added another
summary with `skimr::skim()` to add tiny graphics.

```{r}
#| label: fig-scatterplot-cbl-b
#| fig-cap: "Display raw data for `doy` (Day of the year of first blossom) against the year: tidyverse version"
#| warning: true

d4_b %>% 
  ggplot(aes(x = year, y = doy)) +
  # color from here: https://www.colorhexa.com/ffb7c5
  geom_point(color = "#ffb7c5", alpha = 1/2) +
  theme_bw() +
  theme(panel.grid = element_blank(),
        # color from here: https://www.colordic.org/w/, inspired by https://chichacha.netlify.com/2018/11/29/plotting-traditional-colours-of-japan/
        panel.background = element_rect(fill = "#4f455c"))

```

By default {**ggplots**} removes missing data records with a warning. It
turns out there are cases with missing data for the `doy` variable.

```{r}
#| label: doy-missing-data
#| attr-source: '#lst-doy-missing-data lst-cap="Display missing data of the day of the year variable `doy`"'

d4_b %>% 
  count(is.na(doy)) %>% 
  mutate(percent = 100 * n / sum(n))
```

Let's follow McElreath and make a subset of the data that excludes cases
with missing data in `doy.` Within the tidyverse, we might do so with
the `tidyr::drop_na()` function. -- This is a much easier way than my
approach using `dplyr::filter()` (See
[StackOverflow](https://stackoverflow.com/a/70848085/7322615):

```{r}
#| label: remove-missing-data-records
#| attr-source: '#lst-remove-missing-data-records lst-cap="Remove records of missing data for the two interesting variable `doy` and `year`"'

d5_b <- 
    d4_b %>% 
        filter(if_all(.col = c(year, doy), .fns = Negate(is.na)))
d5_b
```

But it worked and resulted in the same 827 records.

##### Choice of knots

Now we start with the spline procedure as mentioned in
@sec-procedure-for-generating-b-splines with choosing number and
location of knots:

```{r}
#| label: choose-knots-b
#| attr-source: '#lst-choose-knots-b lst-cap="Choose the knots that serve as pivots for the spline: tiyverse version"'

num_knots15_b <- 15
knot_list15_b <- quantile(d5_b$year, 
          probs = seq(from = 0, to = 1, length.out = num_knots15_b))
knot_list15_b
```

```{r}
#| label: fig-chosen-knots-b
#| fig-cap: "Show scatterplot with chosen knots: tiyverse version"
#| attr-source: '#lst-fig-chosen-knots-b lst-cap="Scatterplot with chosen knots: tiyverse version"'

d5_b %>% 
  ggplot(aes(x = year, y = doy)) +
  geom_vline(xintercept = knot_list15_b, 
             color = "white", alpha = 1/2) +
  geom_point(color = "#ffb7c5", alpha = 1/2) +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank())
```

##### Choice of polynomial degree

```{r}
#| label: compute-b-spline-matrix-b
#| attr-source: '#lst-compute-b-spline-matrix-b lst-cap="Compute the B-spline basis matrix for a cubic spline (degree 3): tidyverse version"'

B_b <- splines::bs(d5_b$year,
        knots = knot_list15_b[-c(1, num_knots15_b)], 
        degree = 3, 
        intercept = TRUE)
```

Look closely at McElreath's tricky `knot_list[-c(1, num_knots)]` code.
Whereas `knot_list` contains 15 ordered `year` values, McElreath shaved
off the first and last `year` values with `knot_list[-c(1, num_knots)]`,
leaving 13. This is because, by default, the `bs()` function places
knots at the boundaries. Since the first and 15^th^ values in
`knot_list15_b` were boundary values for `year`, we removed them to avoid
redundancies. We can confirm this with the code, below.

```{r}
#| label: show-data-structure-b
#| attr-source: '#lst-show-data-structure-b lst-cap="Show data structure to verify that `splines::bs()` adds the boundary knots"'

B_b %>% str()
```

Look at the second to last line,
`- attr(*, "Boundary.knots")= int [1:2] 812 2015`. Those default
`"Boundary.knots"` are the same as `knot_list15_b[c(1, num_knots)]`.
Let's confirm.

```{r}
#| label: boundary-knots
#| attr-source: '#lst-boundary-knots lst-cap="Boundary knots"'
knot_list15_b[c(1, num_knots15_b)]
```

By the `degree = 3` argument, we indicated we wanted a cubic spline.
McElreath used `degree = 1` for Figure 4.12. For reasons I'm not
prepared to get into, here, {**splines**} don't always include intercept
parameters. Indeed, the `splines::bs()` default is `intercept = FALSE`.
McElreath's code indicated he wanted to fit a B-spline that included an
intercept. Thus: `intercept = TRUE`.

Here's how we might make our version of the top panel of Figure 4.13.

```{r}
#| label: fig-basic-functions-b
#| fig-cap: "Basic functions of a cubic spline with 15 knots"
#| attr-source: '#lst-fig-basic-functions-b lst-cap="Basic functions of a cubic spline with 15 knots"'

# wrangle a bit
b_b <-
  B_b %>% 
  data.frame() %>% 
  set_names(str_c(0, 1:9), 10:17) %>%  
  bind_cols(select(d5_b, year)) %>% 
  pivot_longer(-year,
               names_to = "bias_function",
               values_to = "bias")

# plot
b_b %>% 
  ggplot(aes(x = year, y = bias, group = bias_function)) +
  geom_vline(xintercept = knot_list15_b, color = "white", alpha = 1/2) +
  geom_line(color = "#ffb7c5", alpha = 1/2, linewidth = 1.5) +
  ylab("bias value") +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank())
```

To elucidate what's going on in that plot, we might break it up with
`ggplot2::facet_wrap()`.

```{r}
#| label: fig-facet-basic-functions-b
#| fig-cap: "Basic functions of a cubic spline with 15 knots broken up in different facets"
#| fig-height: 10
#| attr-source: '#lst-fig-facet-basic-functions-b lst-cap="Basic functions of a cubic spline with 15 knots broken up in different facets"'

b_b %>% 
  mutate(bias_function = str_c("bias function ", 
                               bias_function)) %>% 
  ggplot(aes(x = year, y = bias)) +
  geom_vline(xintercept = knot_list15_b, 
             color = "white", alpha = 1/2) +
  geom_line(color = "#ffb7c5", linewidth = 1.5) +
  ylab("bias value") +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank(),
        strip.background = element_rect(fill = scales::alpha("#ffb7c5", .25), color = "transparent"),
        strip.text = element_text(size = 8, margin = margin(0.1, 0, 0.1, 0, "cm"))) +
  facet_wrap(~ bias_function, ncol = 1)
```

##### Parameter weights for each basic function

To get the parameter weights for each basis function, we need to
actually define the model and make it run. The model is just a linear
regression. The synthetic basis functions do all the work. We'll use
each column of the matrix `B2_b` as a variable. We'll also have an
intercept to capture the average blossom day. This will make it easier
to define priors on the basis weights, because then we can just conceive
of each as a deviation from the intercept.

Our model will follow the form:

------------------------------------------------------------------------

::: {#def-spines-blossom-model}

$$
\begin{align*}
\text{day\_in\_year}_i \sim \operatorname{Normal}(\mu_i, \sigma) \\
\mu_i  = \alpha + {\sum_{k=1}^K w_k B_{k, i}} \\
\alpha \sim \operatorname{Normal}(100, 10) \\
w_j \sim \operatorname{Normal}(0, 10) \\
\sigma \sim \operatorname{Exponential}(1)

\end{align*}
$$ {#eq-spines-blossom-model-b}

where $\alpha$ is the intercept, $B_{k, i}$ is the value of the
$k^\text{th}$ bias function on the $i^\text{th}$ row of the data, and
$w_k$ is the estimated regression weight for the corresponding
$k^\text{th}$ bias function.

As for the new parameter type for $\sigma$, the exponential distribution
is controlled by a single parameter, $\lambda$, which is also called the
*rate*. As it turns out, the mean of the exponential distribution is the
inverse of the rate, $1 / \lambda$.

:::

------------------------------------------------------------------------

We are going to use the `dexp()` function to get a sense of what that
prior looks like.

```{r}
#| label: fig-exponential-prior-b
#| fig-cap: "Using the density of the exponential distribution `dexp()` to show the prior"
#| fig-width: 4
#| fig-height: 2.5
#| attr-source: '#lst-fig-exponential-prior-b lst-cap="Using the density of the exponential distribution to show the prior"'

tibble(x = seq(from = 0, to = 10, by = 0.1)) %>% 
  mutate(d = dexp(x, rate = 1)) %>% 
  
  ggplot(aes(x = x, y = d)) +
  geom_area(fill = "#ffb7c5") +
  scale_y_continuous(NULL, breaks = NULL) +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank())
```

> We'll use exponential priors for the rest of the book, in place of
> uniform priors. It is much more common to have a sense of the average
> deviation than of the maximum. (McElreath)

Before fitting this model in {**brms**}, we will take a minor detour on the data
structure. In his R code 4.76 (@lst-fit-model-m4.7), McElreath defined
his data in a list, `list( D=d5_a$doy , B = B_a)`. Our approach will be
a little different. Here, we'll add the `B_b` matrix to our `d5_b` data
frame and name the results as `d6_b`.

```{r}
#| label: add-matrix-to-df-b
#| attr-source: '#lst-add-matrix-to-df-b lst-cap="Add B-splines matrix for 15 knots to the data frame by creating a new data frame"'

d6_b <-
  d5_b %>% 
  mutate(B = B_b) 

# take a look at the structure of the new data frame d6_b
d6_b %>% glimpse()
```

In the `d6_b` data, columns `year` through `temp_lower` are all standard
data columns. The `B` column is a *matrix column*, which contains the
same number of rows as the others, but also smuggled in 17 columns
*within* that column. Each of those 17 columns corresponds to one of our
synthetic $B_{k}$ variables. The advantage of such a data structure
is we can simply define our `formula` argument as $doy \sim 1 + B$, where
`B` is a stand-in for `B.1 + B.2 + ... + B.17`.

Here's how we fit the model:

```{r}
#| label: fit-model-b4.8
#| attr-source: '#lst-fit-model-b4.8 lst-cap="Fit model b4.8"'

b4.8 <- 
  brms::brm(data = d6_b,
      family = gaussian,
      doy ~ 1 + B,
      prior = c(brms::prior(normal(100, 10), class = Intercept),
                brms::prior(normal(0, 10), class = b),
                brms::prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.08")
```

Here is the model summary:

```{r}
#| label: summarize-model-b4.8
#| attr-source: '#lst-summarize-model-b4.8 lst-cap="Summarize model b4.8"'

brms:::print.brmsfit(b4.8)
```

In @lst-summarize-model-b4.8 I have used this time the full call for the generic function of `print()`. Just using `print()` would have been enough, but I wanted to check if I have understood the concept of a generic S3 method. The help file pf `print()` says:

>  It is a generic function which means that new printing methods can be easily added for new classes.

::: {.callout-note}
TODO: Read [S3 Chapter](https://adv-r.hadley.nz/s3.html) of Advanced R
:::

Look at that. Each of the 17 columns in our `B` matrix was assigned its own parameter. If you fit this model using McElreath’s rethinking code, you’ll see the results are very similar. Anyway, McElreath’s comments are in line with the general consensus on spline modes: the parameter estimates are very difficult to interpret directly. It’s often easier to just plot the results. 

First we’ll use `brms::as_draws_df()` to transform `d6_b` to a `draw` object so that it can processed easier by the {**posterior**} package. (??)

```{r}
post6_b <- brms::as_draws_df(b4.8)

glimpse(post6_b)
```

With a little wrangling, we can use summary information from `post6_b` to make our version of the middle panel of Figure 4.13.

```{r}
#| label: fig-basis-fun-weighted-b
#| fig-cap: "Each basis function weighted by its corresponding parameter"
#| attr-source: '#lst-fig-basis-fun-weighted-b lst-cap="Weight each basis function by its corresponding parameter"'

post6_b %>% 
  select(b_B1:b_B17) %>% 
  set_names(c(str_c(0, 1:9), 10:17)) %>% 
  pivot_longer(everything(), names_to = "bias_function") %>% 
  group_by(bias_function) %>% 
  summarise(weight = mean(value)) %>% 
  full_join(b_b, by = "bias_function") %>% 
  
  # plot
  ggplot(aes(x = year, y = bias * weight, group = bias_function)) +
  geom_vline(xintercept = knot_list15_b, color = "white", alpha = 1/2) +
  geom_line(color = "#ffb7c5", alpha = 1/2, linewidth = 1.5) +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank()) 
```

In case you missed it, the main action in the {**ggplot2**} code was `y = bias * weight`, where we defined the $y$-axis as the product of `bias` and `weight`. This is fulfillment of the $w_k B_{k, i}$ parts of the model. 

Now here's how we might use `brms:::fitted.brmsfit()` to get the *expected values of the posterior predictive distribution* to make the lower plot of Figure 4.13.


```{r}
f <- brms:::fitted.brmsfit(b4.8)

f %>% 
  data.frame() %>% 
  bind_cols(d6_b) %>% 
  
  ggplot(aes(x = year, y = doy, ymin = Q2.5, ymax = Q97.5)) + 
  geom_vline(xintercept = knot_list15_b, color = "white", alpha = 1/2) +
  geom_hline(yintercept = brms::fixef(b4.8)[1, 1], color = "white", linetype = 2) +
  geom_point(color = "#ffb7c5", alpha = 1/2) +
  geom_ribbon(fill = "white", alpha = 2/3) +
  labs(x = "year",
       y = "day in year") +
  theme_bw() +
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank())
```


If it wasn’t clear, the dashed horizontal line intersecting a little above 100 on the $y$-axis is the posterior mean for the intercept.


##### 5 knot model

Now let’s use our skills to remake the simpler model expressed in Figure 4.12. This model, recall, is based on 5 knots.

```{r}
# redo the `B` splines
num_knots5_b <- 5
knot_list5_b <- quantile(d5_b$year, 
             probs = seq(from = 0, to = 1, length.out = num_knots5_b))

B5_b <- splines::bs(d5_b$year,
        knots = knot_list5_b[-c(1, num_knots5_b)], 
        # this makes the splines liner rater than cubic
        degree = 1, 
        intercept = TRUE)

# define a new data frame (d7_b)
d7_b <- 
  d5_b %>% 
  mutate(B = B5_b)

b4.9 <- 
  brms::brm(data = d7_b,
      family = gaussian,
      formula = doy ~ 1 + B,
      prior = c(brms::prior(normal(100, 10), class = Intercept),
                brms::prior(normal(0, 10), class = b),
                brms::prior(exponential(1), class = sigma)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.09")
```

Review the new model summary.

```{r}
print(b4.9)
```

Here we do all the work in bulk to make and save the three subplots for Figure 4.12.

```{r}
## top
## wrangle a bit

b5_b <-
  invoke(data.frame, d7_b) %>% 
  pivot_longer(starts_with("B"),
               names_to = "bias_function",
               values_to = "bias")
  
# plot
p10 <- 
  b5_b %>% 
  ggplot(aes(x = year, y = bias, group = bias_function)) +
  geom_vline(xintercept = knot_list5_b, color = "white", alpha = 1/2) +
  geom_line(color = "#ffb7c5", alpha = 1/2, linewidth = 1.5) +
  scale_x_continuous(NULL, breaks = NULL) +
  ylab("bias value")

## middle
# wrangle
p11 <-
  brms::as_draws_df(b4.9) %>% 
  select(b_B1:b_B5) %>% 
  set_names(str_c("B.", 1:5)) %>% 
  pivot_longer(everything(), names_to = "bias_function") %>% 
  group_by(bias_function) %>% 
  summarise(weight = mean(value)) %>% 
  full_join(b5_b, by = "bias_function") %>% 
  
  # plot
  ggplot(aes(x = year, y = bias * weight, group = bias_function)) +
  geom_vline(xintercept = knot_list5_b, color = "white", alpha = 1/2) +
  geom_line(color = "#ffb7c5", alpha = 1/2, linewidth = 1.5) +
  scale_x_continuous(NULL, breaks = NULL)

## bottom
# wrangle
f2_b <- fitted(b4.9)

p12 <-
  f2_b %>% 
  data.frame() %>% 
  bind_cols(d7_b) %>% 
  
  # plot
  ggplot(aes(x = year, y = doy, ymin = Q2.5, ymax = Q97.5)) + 
  geom_vline(xintercept = knot_list5_b, color = "white", alpha = 1/2) +
  geom_hline(yintercept = brms::fixef(b4.9)[1, 1], color = "white", linetype = 2) +
  geom_point(color = "#ffb7c5", alpha = 1/2) +
  geom_ribbon(fill = "white", alpha = 2/3) +
  labs(x = "year",
       y = "day in year")
```

Now combine the subplots with {**patchwork**} syntax and behold their glory.

```{r}
library(patchwork)
(p10 / p11 / p12) &
  theme_bw() &
  theme(panel.background = element_rect(fill = "#4f455c"),
        panel.grid = element_blank())
```


## STOPPED HERE! (B: 2023-08-16)

#### Smooth functions for a rough world

B-splines invent a series of entirely new, synthetic predictor variables. Each of these synthetic variables exists only to gradually turn a specific parameter on and off within a specific range of the real predictor variable. Each of the synthetic variables is called a basis function.

Basis functions are synthetic variables created by B-splines. Each of these synthetic variables exists only to gradually turn a specific parameter on and off within a specific range of the real predictor variable.

B-splines of order $n$ are basis functions for spline functions of the same order defined over the same knots, meaning that all possible spline functions can be built from a linear combination of B-splines, and there is only one unique combination for each spline function.

B-splines play the role of basis functions for the spline function space, hence the name. This property follows from the fact that all pieces have the same continuity properties, within their individual range of support, at the knots.


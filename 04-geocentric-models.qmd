# Geocentric Models

```{r}
#| label: setup

library(tidyverse)
```

## Why normal distributions are normal?

Why are there so many distribution approximately normal, resulting in a
Gaussian curve? Because there will be more combinations of outcomes that
sum up to a "central" value, rather than to some extreme value.

::: callout-tip
Any process that adds together random values from the same distribution
converges to a normal.
:::

### Normal by addition

Whatever the average value of the source distribution, each sample from
it can be thought of as a fluctuation from that average value. When we
begin to add these fluctuations together, they also begin to cancel one
another out. A large positive fluctuation will cancel a large negative
one. The more terms in the sum, the more chances for each fluctuation to
be canceled by another, or by a series of smaller ones in the opposite
direction. So eventually the most likely sum, in the sense that there
are the most ways to realize it, will be a sum in which every
fluctuation is canceled by another, a sum of zero (relative to the
mean).

It doesn't matter what shape the underlying distribution possesses. It
could be uniform, like in our example above, or it could be (nearly)
anything else. Depending upon the underlying distribution, the
convergence might be slow, but it will be inevitable.

See the excellent article [Why is normal distribution so
ubiquitous?](https://ekamperi.github.io/mathematics/2021/01/29/why-is-normal-distribution-so-ubiquitous.html)
which also explains the example of random walks from SR2. See also the
scientific paper [Why are normal distribution
normal?](https://www.journals.uchicago.edu/doi/pdf/10.1093/bjps/axs046)
of the The British Journal for the Philosophy of Science.

### Normal by multiplication

This is not only valid for addition but also for multiplication of small
values: Multiplying small numbers is approximately the same as addition.

### Normal by log-multipliation

But even the multiplication of large values tend to produce Gaussian
distributions on the log scale.

### Using Gaussian distribution

The justifications for using the Gaussian distribution fall into two
broad categories:

1.  **Ontological justification**: The world is full of Gaussian
    distributions, approximately. We're never going to experience a
    perfect Gaussian distribution. But it is a widespread pattern,
    appearing again and again at different scales and in different
    domains. Measurement errors, variations in growth, and the
    velocities of molecules all tend towards Gaussian distributions.

There are many other patterns in nature, so make no mistake in assuming
that the Gaussian pattern is universal. In later chapters, we'll see how
other useful and common patterns, like the exponential and gamma and
Poisson, also arise from natural processes. The Gaussian is a member of
a family of fundamental natural distributions known as the **Exponential
family**. All of the members of this family are important for working
science, because they populate our world.

2.  **Epistemological justification**: The Gaussian represents a
    particular state of ignorance. When all we know or are willing to
    say about a distribution of measures (measures are continuous values
    on the real number line) is their mean and variance, then the
    Gaussian distribution arises as the most consistent with our
    assumptions. It is the least surprising and least informative
    assumption to make. --- If you don't think the distribution should
    be Gaussian, then that implies that you know something else that you
    should tell your golem about, something that would improve
    inference.

::: callout-caution
Although the Gaussian distribution is common in nature and has some nice
properties, there are some risks in using it as a default data model.
The Gaussian distribution has some very thin tails---there is very
little probability in them. Instead most of the mass in the Gaussian
lies within one standard deviation of the mean. Many natural (and
unnatural) processes have much heavier tails.
:::

The Gaussian is a continuous distribution, unlike the discrete
distributions of earlier chapters. Probability distributions with only
discrete outcomes, like the binomial, are called *probability mass*
functions and denoted `Pr`. Continuous ones like the Gaussian are called
*probability density* functions, denoted with *`p`* or just plain old
*`f`*, depending upon author and tradition. For mathematical reasons,
probability densities can be greater than 1. Try `dnorm(0,0,0.1)`", for
example, which is the way to make R calculate *`p`*`(0|0, 0.1)`. The
answer, about 4, is no mistake. Probability *density* is the rate of
change in cumulative probability. So where cumulative probability is
increasing rapidly, density can easily exceed 1. But if we calculate the
area under the density function, it will never exceed 1. Such areas are
also called *probability mass*.

## A language describing models

1.  First, we recognize a set of variables to work with. Some of these
    variables are observable. We call these data. Others are
    unobservable things like rates and averages. We call these
    parameters.
2.  We define each variable either in terms of the other variables or in
    terms of a probability distribution.
3.  The combination of variables and their probability distributions
    defines a joint generative model that can be used both to simulate
    hypothetical observations as well as analyze real ones.

Models are mappings of one set of variables through a probability
distribution onto another set of variables. Fundamentally, these models
define the ways values of some variables can arise, given values of
other variables.

### Re-describing the globe tossing model

::: {#def-glob-tossing-model}
Recall the proportion of water problem from previous chapters. The model
in that case was always:

$$
\begin{align*}
W \sim Binomial(N, p) \\
p \sim Uniform(0, 1)
\end{align*}
$$

-   `W`: observed count of water
-   `N`: total number of tosses
-   `p`: proportion of water on the globe

Read the above statement as:

1.  **First line**: The count W is distributed binomially with sample
    size `N` and probability `p`.
2.  **Second line**: The prior for `p` is assumed to be uniform between
    zero and one.

The first line defines the likelihood function used in Bayes' theorem.
The other lines define priors. Both of the lines in this model are
**stochastic**, as indicated by the `~` symbol. A stochastic
relationship is just a mapping of a variable or parameter onto a
distribution. It is stochastic because no single instance of the
variable on the left is known with certainty. Instead, the mapping is
probabilistic: Some values are more plausible than others, but very many
different values are plausible under any model. Later, we'll have models
with deterministic definitions in them.
:::

## Gaussian model of height {#sec-gaussian-model-of-height}

There are an infinite number of possible Gaussian distributions. Some
have small means. Others have large means. Some are wide, with a large
`σ`. Others are narrow. We want our Bayesian machine to consider every
possible distribution, each defined by a combination of `μ` and `σ`, and
rank them by posterior plausibility. Posterior plausibility provides a
measure of the logical compatibility of each possible distribution with
the data and model.

### The data

#### Original

The data contained in `data(Howell1)` are partial census data for the
Dobe area !Kung San, compiled from interviews conducted by Nancy Howell
in the late 1960s. Much more raw data is available for download from
https://tspace.library.utoronto.ca/handle/1807/10395.

For the non-anthropologists reading along, the !Kung San are the most
famous foraging population of the twentieth century, largely because of
detailed quantitative studies by people like Howell.

::: callout-caution
Loading data from a package with `data()` is only possible if you have
already loaded the package. In our example:

```{r}
#| label: loading-data-from-package1_a
#| eval: false


## R code 4.7 #######################
library(rethinking)
data(Howell1)
d_a <- Howell1
```

Because of many function name conflicts with {**brms**} I do not want to
load {**rethinking**} and will call the function of these conflicted
packages with `<package name>::<function name>()` Therefore I have to
use another, not so usual loading strategy of the data set:

```{r}
#| label: loading-data-from-package2_a
#| attr-source: '#lst-loading-data-from-package2_a lst-cap="Load data `Howell1` from the {**rethinking**} package without loading the package and assign the data to an object. Rethinking"'

data(package = "rethinking", list = "Howell1")
d_a <- Howell1
```

The advantage of this strategy is that I have not always to detach the
{**rethinking**} package and to make sure {**rethinking**} is detached
before using {**brms**} as it is necessary in the Kurz's {**tidyverse**}
/ {**brms**} version.
:::

##### Show the data

```{r}
#| label: show-howell-data-a
#| attr-source: '#lst-show-howell-data-a lst-cap="Show and inspect the data: rethinking"'

## R code 4.8 ####################
str(d_a)

## R code 4.9 ###################
rethinking::precis(d_a)
```

This data frame contains four columns. Each column has 544 entries, so
there are 544 individuals in these data. Each individual has a recorded
height (centimeters), weight (kilograms), age (years), and "maleness" (0
indicating female and 1 indicating male).

##### Select the height data of adults

We're going to work with just the height column, for the moment. All we
want for now are heights of adults in the sample. The reason to filter
out non-adults for now is that height is strongly correlated with age,
before adulthood.

```{r}
#| label: select-height-adults-a
#| attr-source: '#lst-select-height-adults-a lst-cap="Select the height data of adults (individuals older or equal than 18 years): base R version"'

## R code 4.10 ###################
head(d_a$height)
 
## R code 4.11 ###################
d2_a <- d_a[d_a$age >= 18, ]

```

We'll be working with the data frame d2 now. It should have 352 rows
(individuals) in it. We will check this with `nrow(d2_a)` =
`r nrow(d2_a)`.

#### Tidyverse

##### Show the data

```{r}
#| label: loading-data-from-package_b
#| attr-source: '#lst-loading-data-from-package_b lst-cap="Load data `Howell1` from the {**rethinking**} package without loading the package and assign the data to an object. Tidyverse"'

data(package = "rethinking", list = "Howell1")
d_b <- Howell1
```

```{r}
#| label: show-howell-data1-b

d_b |>
    glimpse()

```

`glimpse()` is the tidyverse analogue for `str()`.

```{r}
#| label: show-howell-data2-b
d_b |> 
    summary()
```

Kurz tells us that the {**brms**} package does not have a function that
works like `rethinking::precis()` for providing numeric and graphical
summaries of variables, as in the second part of
@lst-show-howell-data-a. Kurz suggests `base::summary()` to get some of
the information from `rethinking::precis()`.

```{r}
#| label: show-howell-data3-b
d_b |>            
    skimr::skim() 

```

I think `skimr::skim()` is a better option as an alternative to
`rethinking::precis()` as `base::summary()` because it also has a
graphical summary of the variables. {**skimr**} has many other useful
functions and is very adaptable. I propose to install and to try it out.

##### Select the height data of adults

With {**tidyverse**} we can isolate height values with the
`dplyr::select()` function and we are using the `dplyr::filter()`
function to make an adults-only data frame.

```{r}
#| label: select-height-adults-b
#| attr-source: '#lst-select-height-adults-b lst-cap="Select the height data of adults (individuals older or equal than 18 years): tidyverse version"'

d_b %>%
  select(height) %>% 
  glimpse()

d2_b <- 
  d_b %>%
  filter(age >= 18) 
 
glimpse(d2_b)
```

The two functions of @lst-select-height-adults-b are much more readable
and understandable as the weird base R syntax in
@lst-select-height-adults-a.

### The model

#### Original

Our goal is to model the data in `d2_a` using a Gaussian distribution.

Plot the distribution of heights

```{r}
#| label: fig-dist-heights-a
#| fig-cap: "The distribution of the heights data,overlaid by an ideal Gaussian distribution: rethinking version"
#| attr-source: '#lst-fig-dist-heights-a lst-cap="Plot the distribution of the heights of adults: rethinking version"'

rethinking::dens(d2_a$height, norm.comp = TRUE)
```

With the option `norm.comp = TRUE` I have overlaid a Gaussian
distribution to see the differences to the actual data. There are some
differences locally, especially on the peak of the distribution. But the
tails looks nice and we can say that the overall impression of the curve
is Gaussian.

::: callout-caution
###### Decisions how to model the data

Gawking at the raw data, to try to decide how to model them, is usually
not a good idea. The data could be, for example, a mixture of different
Gaussian distributions. Furthermore, the empirical distribution need not
be actually Gaussian in order to justify using a Gaussian probability
distribution.
:::

::: {#def-heights-normal}
Define the heights as normally distributed with a mean `μ` and standard
deviation `σ`

$$
h_{i} \sim Normal(σ, μ) 
$$
:::

The symbol `h` refers to the list of heights, and the subscript `i`
means each individual element of this list. It is conventional to use
`i` because it stands for index. The index `i` takes on row numbers, and
so in this example can take any value from 1 to 352 (the number of
heights in `d2_a$height`). As such, the model above is saying that all
the golem knows about each height measurement is defined by the same
normal distribution, with mean `μ` and standard deviation `σ`.

The short model in @def-heights-normal assumes that the values $h_{i}$
are *independent and identically distributed*, abbreviated `i.i.d.`,
`iid`, or `IID`.

To complete the model, we're going to need some priors. The parameters
to be estimated are both `μ` and `σ`, so we need a prior `Pr(μ, σ)`, the
joint prior probability for all parameters. In most cases, priors are
specified independently for each parameter, which amounts to assuming
$Pr(μ,σ) = Pr(μ)Pr(σ)$.

------------------------------------------------------------------------

::: {#def-height-priors}
Priors for heights model

$$
\begin{align*}
h_{i} \sim Normal(μ, σ)  \\ 
μ \sim Normal(178, 20)   \\ 
μ \sim Uniform(0, 50)       
\end{align*}
$$

1.  First line represents the likelihood.
2.  Second line is the chosen `μ`(mu, mean) prior.
3.  Third line is the chosen `σ` (sigma, standard deviation) prior.
:::

------------------------------------------------------------------------

Let's think about the chosen value for the priors more in detail:

The prior for `μ` is a broad Gaussian prior, centered on 178 cm, with
95% of probability between 178 ± 40 cm.

Why 178 cm? Your author is 178 cm tall. And the range from 138 cm to 218
cm encompasses a huge range of plausible mean heights for human
populations. So domain-specific information has gone into this prior.
Everyone knows something about human height and can set a reasonable and
vague prior of this kind. But in many regression problems, as you'll see
later, using prior information is more subtle, because parameters don't
always have such clear physical meaning.

Whatever the prior, it's a very good idea to plot your priors, so you
have a sense of the assumption they build into the model.

**Plot the mu prior (mean)**

```{r}
#| label: fig-mean-prior-a
#| fig-cap: "Plot of the chosen mean prior: base R version"

## R code 4.12 ###############################
curve(dnorm(x, 178, 20), from = 100, to = 250)
```

You can see that the golem is assuming that the average height (not each
individual height) is almost certainly between 140 cm and 220 cm. So
this prior carries a little information, but not a lot.

**Plot the sigma prior (standard deviation)**

A standard deviation like `σ` must be positive, so bounding it at zero
makes sense. How should we pick the upper bound? In this case, a
standard deviation of 50 cm would imply that 95% of individual heights
lie within 100 cm of the average height. That's a very large range.

```{r}
#| label: fig-sd-prior-a
#| fig-cap: "Plot the chosen prior for the standard deviation: base R version"

## R code 4.13 ###########################
curve(dunif(x, 0, 50), from = -10, to = 60)
```

**Prior predictive simulation**

> Once you've chosen priors for *h, μ*, and *σ*, these imply a joint
> prior distribution of individual heights. By simulating from this
> distribution, you can see what your choices imply about observable
> height. This helps you diagnose bad choices.

Okay, so how to do this? You can quickly simulate heights by sampling
from the prior, like you sampled from the posterior back in
@sec-sampling-the-imaginary. Remember, every posterior is also
potentially a prior for a subsequent analysis, so you can process priors
just like posteriors.

```{r}
#| label: fig-prior-predictive-sim-a
#| fig-cap: "Simulate heights by sampling from the prior: rethinking version"

set.seed(4) # to make example reproducible
## R code 4.14 #######################################
sample_mu_a <- rnorm(1e4, 178, 20)
sample_sigma_a <- runif(1e4, 0, 50)
prior_h_a <- rnorm(1e4, sample_mu_a, sample_sigma_a)
rethinking::dens(prior_h_a, norm.comp = TRUE)
```

> It displays a vaguely bell-shaped density with thick tails. It is the
> expected distribution of heights, averaged over the prior. Notice that
> the prior probability distribution of height is not itself Gaussian.
> This is okay. The distribution you see is not an empirical
> expectation, but rather the distribution of relative plausibilities of
> different heights, before seeing the data.

This comment is strange for me as in my point of view the distribution
*is* Gaussian. It is true that the tails are (a little bit?) thicker
than in the standard Gaussian distribution. But in my view
@fig-prior-predictive-sim-a is more Gaussian than @fig-dist-heights-a.
OK, in @fig-dist-heights-a we have just `r nrow(d2_a)` data and in
@fig-prior-predictive-sim-a we sampled 10,000 times. But this is not a
counter argument for @fig-prior-predictive-sim-a not being a a bell
shaped distribution.

**Simulate heights from priors with large sd**

Prior predictive simulation is very useful for assigning sensible
priors, because it can be quite hard to anticipate how priors influence
the observable variables. As an example, consider a much flatter and
less informative prior for `μ`, like $μ \sim Normal(178, 100)$. Priors
with such large standard deviations are quite common in Bayesian models,
but they are hardly ever sensible.

```{r}
#| label: fig-prior-predictive-sim2-a
#| fig-cap: "Simulate heights from priors with a large standard deviation: rethinking version"

set.seed(4) # to make example reproducible
## R code 4.15 ############################
sample_mu2_a <- rnorm(1e4, 178, 100)
prior_h2_a <- rnorm(1e4, sample_mu2_a, sample_sigma_a)
rethinking::dens(prior_h2_a)
```

The results of @fig-prior-predictive-sim2-a contradicts our scientific
knowledge --- but also our common sense --- about possible height values
of humans. Now the model, before seeing the data, expects people to have
negative height. It also expects some giants. One of the tallest people
in recorded history, [Robert Pershing
Wadlow](https://en.wikipedia.org/wiki/Robert_Wadlow) (1918--1940) stood
272 cm tall. In our prior predictive simulation many people are taller
than this.

Does this matter? In this case, we have so much data that the silly
prior is harmless. But that won't always be the case. There are plenty
of inference problems for which the data alone are not sufficient, no
matter how numerous. Bayes lets us proceed in these cases. But only if
we use our scientific knowledge to construct sensible priors. Using
scientific knowledge to build priors is not cheating. The important
thing is that your prior not be based on the values in the data, but
only on what you know about the data before you see it.

#### Tidyverse

The plot of the heights distribution compared with the standard Gaussian
distribution is missing in Kurz's version. I added this plot by using
the last example of [How to Plot a Normal Distribution in
R](https://www.statology.org/plot-normal-distribution-r/).

```{r}
#| label: fig-dist-heights-b
#| fig-cap: "The distribution of the heights data, overlaid by an ideal Gaussian distribution: tidyverse version"
#| attr-source: '#lst-fig-dist-heights-b lst-cap="Plot the distribution of the heights of adults: tidyverse version"'

p0 <- 
    d2_b |> 
    ggplot(aes(height)) +
    geom_density() +

    stat_function(
        fun = dnorm,
        args = with(d2_b, c(mean = mean(height), sd = sd(height)))
        ) +
    scale_x_continuous("Height in cm")

p0
```

Here is the shape for the prior $μ \sim Normal(178, 20)$.

```{r}
#| label: fig-mean-prior-b
#| fig-cap: "Plot of the chosen mean prior: tidyverse version"
#| attr-source: '#lst-fig-mean-prior-b lst-cap="Plot of the chosen mean prior: tidyverse version"'

p1 <-
  tibble(x = seq(from = 100, to = 250, by = .1)) %>% 
    
  ggplot(aes(x = x, y = dnorm(x, mean = 178, sd = 20))) +
  geom_line() +
  scale_x_continuous(breaks = seq(from = 100, to = 250, by = 75)) +
  labs(title = "mu ~ dnorm(178, 20)",
       y = "density")

p1
```

And here's the ggplot2 code for our prior for `σ`, a uniform
distribution with a minimum value of 0 and a maximum value of 50. We
don't really need the `y`-axis when looking at the shapes of a density,
so we'll just remove it with `scale_y_continuous()`.

```{r}
#| label: fig-sd-prior-b
#| fig-cap: "Plot the chosen prior for the standard deviation: tidyverse version"

p2 <-
  tibble(x = seq(from = -10, to = 60, by = .1)) %>%
  
  ggplot(aes(x = x, y = dunif(x, min = 0, max = 50))) +
  geom_line() +
  scale_x_continuous(breaks = c(0, 50)) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("sigma ~ dunif(0, 50)")

p2
```

We can simulate from both priors at once to get a prior probability
distribution of `height`.

```{r}
#| label: fig-prior-predictive-sim-b
#| fig-cap: "Simulate heights by sampling from the prior: tidyverse version"

n <- 1e4
set.seed(4)

sim <-
  tibble(sample_mu_b    = rnorm(n, mean = 178, sd  = 20),
         sample_sigma_b = runif(n, min = 0, max = 50)) %>% 
  mutate(height = rnorm(n, mean = sample_mu_b, sd = sample_sigma_b))
  
p3 <- sim %>% 
  ggplot(aes(x = height)) +
  geom_density(fill = "deepskyblue") +
  scale_x_continuous(breaks = c(0, 73, 178, 283)) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("height ~ dnorm(mu, sigma)") +
  theme(panel.grid = element_blank())

p3
```

If you look at the `x`-axis breaks on the plot in McElreath's lower left
panel in Figure 4.3, you'll notice they're intentional. To compute the
mean and 3 standard deviations above and below, you might do this.

```{r}
#| label: compute-mean-3sd-b
sim %>% 
  summarise(ll   = mean(height) - sd(height) * 3,
            mean = mean(height),
            ul   = mean(height) + sd(height) * 3) %>% 
  mutate_all(round, digits = 1)
```

Here's the work to make the lower right panel of Figure 4.3.

```{r}
#| label: fig-reproduce-4.3-low-right
#| fig-cap: "Reproduce lower right panels of Figure 4.3"


# simulate
set.seed(4)

sim <-
  tibble(sample_mu_b    = rnorm(n, mean = 178, sd = 100),
         sample_sigma_b = runif(n, min = 0, max = 50)) %>% 
  mutate(height = rnorm(n, mean = sample_mu_b, sd = sample_sigma_b))

# compute the values we'll use to break on our x axis
breaks <-
  c(mean(sim$height) - 3 * sd(sim$height), 0, mean(sim$height), mean(sim$height) + 3 * sd(sim$height)) %>% 
  round(digits = 0)

# this is just for aesthetics
text <-
  tibble(height = 272 - 25,
         y      = .0013,
         label  = "tallest man",
         angle  = 90)

# plot
p4 <-
  sim %>% 
  ggplot(aes(x = height)) +
  geom_density(fill = "deepskyblue", color = "black") +
  geom_vline(xintercept = 0, color = "black") +
  geom_vline(xintercept = 272, color = "black", linetype = 3) +
  geom_text(data = text,
            aes(y = y, label = label, angle = angle),
            color = "black") +
  scale_x_continuous(breaks = breaks) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("height ~ dnorm(mu, sigma)\nmu ~ dnorm(178, 100)") +
  theme(panel.grid = element_blank())

p4
```

Let's combine the four to make our version of McElreath's Figure 4.3.

```{r}
#| label: fig-reproduce-3.4
#| fig-cap: "Reproduction of Figure 3.4"

library(patchwork)
(p1 + xlab("mu") | p2 + xlab("sigma")) / (p3 | p4)
```

On page 84, McElreath said his prior simulation indicated 4% of the
heights would be below zero. He also drew the break down compared to the
tallest man on record, [Robert Pershing
Wadlow](https://en.wikipedia.org/wiki/Robert_Wadlow) (1918--1940).

```{r}
#| label: calc-breaks-b

sim %>% 
  count(height < 0) %>% 
  mutate(percent = 100 * n / sum(n))

sim %>% 
  count(height < 272) %>% 
  mutate(percent = 100 * n / sum(n))
```

### Grid approximation of the posterior distribution

#### Original

We are going to map out the posterior distribution through brute force
calculations.

This is not recommended because it is

-   laborious and computationally expensive
-   usually so impractical as to be essentially impossible.

Therefor the grid approximation technique has limited relevance. Later
on we will use the quadratic approximation with `rethinking::quap()`.

```{r}
#| label: grid-approx-posterior-a

## R code 4.16 ##################################

# establish range of μ and σ values, respectively, to calculate over 
# as well as how many points to calculate in-between. 
mu.list_a <- seq(from = 150, to = 160, length.out = 100)
sigma.list_a <- seq(from = 7, to = 9, length.out = 100)

# expands μ & σ values into a matrix of all of the combinations
post_a <- expand.grid(mu_a = mu.list_a, sigma_a = sigma.list_a)

# compute the log-likelihood at each combination of μ and σ
post_a$LL <- sapply(1:nrow(post_a), function(i) {
  sum(
    dnorm(d2_a$height, post_a$mu[i], post_a$sigma[i], log = TRUE)
  )
})

# multiply the prior by the likelihood
# as the priors are on the log scale adding = multiplying
post_a$prod <- post_a$LL + dnorm(post_a$mu_a, 178, 20, TRUE) +
  dunif(post_a$sigma_a, 0, 50, TRUE)

# getting back on the probability scale without rounding error 
post_a$prob <- exp(post_a$prod - max(post_a$prod))

```

> **Comment to the last line**: the obstacle for getting back on the
> probability scale is that rounding error is always a threat when
> moving from log-probability to probability. If you use the obvious
> approach, like `exp( post$prod )`, you'll get a vector full of zeros,
> which isn't very helpful. This is a result of R's rounding very small
> probabilities to zero.

**Plot contour lines**

```{r}
#| label: fig-contour-plot-a
#| fig-cap: "Draw a contour plot: rethinking version"

## R code 4.17 ##################################
rethinking::contour_xyz(post_a$mu_a, post_a$sigma_a, post_a$prob)

```

You can inspect this posterior distribution, now residing in
`post_a$prob`, using a variety of plotting commands.

**Plot heat map**

```{r}
#| label: fig-heat-map-a
#| fig-cap: "Draw a heat map: rethinking version"

## R code 4.18 ##################################
rethinking::image_xyz(post_a$mu_a, post_a$sigma_a, post_a$prob)

```

#### Tidyverse

With grid approximation we are going to use the brute force method for
the calculation of the posterior distribution. This technique has
limited relevance. Later on we will use the quadratic approximation with
`brms::brm()`.

It is the same technique we have use in
@sec-sampling-from-a-grid-approximate-posterior respectively in the
tidyverse version in @sec-grid-approximation-b. As there is no
conceptually new information to learn, I am not going into the details
of the following code. (It combines several code chunk from Kurz's
version.) But I am going to foreshadow the most important differences in
the tidyverse approach of the grid approximation technique:

Instead of `base::grid_expand()` we will use `tidyr::crossing()` Instead
of `base::sapply()` we will use `purr::map2()`

The produced tibble contains data frames in its cells, so that we have
to use the `tidyr::unnest()` function to expand the list-column
containing data frames into rows and columns.

Referring to the plots:

-   Instead of `rethinking::contour_xyz()` we will use
    `ggplot2::geom_contour()`
-   Instead of `rethinking::image_xyz()` we will use
    `ggplot2::geom_raster()`

```{r}
#| label: grid-approx-posterior-b
#| attr-source: '#lst-grid-approx-posterior-b lst-cap="Grid Approximation of the posterior distribution: tidyverse version"'

n <- 200

d_grid_b <-
  # we'll accomplish with `tidyr::crossing()` what McElreath did with base R `expand.grid()`
  crossing(mu_b    = seq(from = 140, to = 160, length.out = n),
           sigma_b = seq(from = 4, to = 9, length.out = n))

glimpse(d_grid_b)

grid_function <- function(mu, sigma) {
  
  dnorm(d2_b$height, mean = mu, sd = sigma, log = TRUE) %>% 
    sum()
  
}

d_grid2_b <-
  d_grid_b %>% 
  mutate(log_likelihood_b = map2(mu_b, sigma_b, grid_function)) %>%
  unnest(log_likelihood_b) %>% 
  mutate(prior_mu_b    = dnorm(mu_b, mean = 178, sd = 20, log = T),
         prior_sigma_b = dunif(sigma_b, min = 0, max = 50, log = T)) %>% 
  mutate(product_b = log_likelihood_b + prior_mu_b + prior_sigma_b) %>% 
  mutate(probability_b = exp(product_b - max(product_b)))
  
head(d_grid2_b)
```

```{r}
#| label: fig-contour-b
#| fig-cap: "Draw 2D contours of a 3D surface"

d_grid2_b %>% 
  ggplot(aes(x = mu_b, y = sigma_b, z = probability_b)) + 
  geom_contour() +
  labs(x = expression(mu),
       y = expression(sigma)) +
  coord_cartesian(xlim = range(d_grid2_b$mu_b),
                  ylim = range(d_grid2_b$sigma_b)) +
  theme(panel.grid = element_blank())

```

```{r}
#| label: fig-heatmap-b
#| fig-cap: "Draw heat map"

d_grid2_b %>% 
  ggplot(aes(x = mu_b, y = sigma_b, fill = probability_b)) + 
  geom_raster(interpolate = TRUE) +
  scale_fill_viridis_c(option = "B") +
  labs(x = expression(mu),
       y = expression(sigma)) +
  theme(panel.grid = element_blank())
```

### Sampling from the posterior

#### Original

To study this posterior distribution in more detail, again I'll push the
flexible approach of sampling parameter values from it. This works just
like it did in @sec-sampling-to-summarize, when you sampled values of
`p` from the posterior distribution for the globe tossing example. The
only new trick is that since there are two parameters, and we want to
sample combinations of them, we first randomly sample row numbers in
post in proportion to the values in \`post_a\$prob´. Then we pull out
the parameter values on those randomly sampled rows.

```{r}
#| label: fig-posterior-sample-a
#| fig-cap: "Samples from the posterior distribution for the heights data. The density of points is highest in the center, reflecting the most plausible combinations of μ and σ. There are many more ways for these parameter values to produce the data, conditional on the model. (rethinking version)"

## R code 4.19 ###########################

# randomly sample row numbers in post_a 
# in proportion to the values in post_a$prob. 
sample.rows <- sample(1:nrow(post_a),
  size = 1e4, replace = TRUE,
  prob = post_a$prob
)

# pull out the parameter values
sample.mu_a <- post_a$mu[sample.rows]
sample.sigma_a <- post_a$sigma[sample.rows]

## R code 4.20 ###########################
plot(sample.mu_a, sample.sigma_a, cex = 0.8, pch = 21, col = rethinking::col.alpha(rethinking:::rangi2, 0.1))

```

The function `col.alpha()` is part of the {**rethinking**} R package.
All it does is make colors transparent, which helps the plot in FIGURE
4.4 (here: @fig-posterior-sample-a) more easily show density, where
samples overlap. Adjust the plot to your tastes by playing around with
`cex` (character expansion, the size of the points), `pch` (plot
character), and the 0.1 transparency value.

**Marginal Posterior Density**

Now that you have these samples, you can describe the distribution of
confidence in each combination of `μ` and `σ` by summarizing the
samples. Think of them like data and describe them, just like in
@sec-sampling-to-summarize. For example, to characterize the shapes of
the marginal posterior densities of `μ` and `σ`, all we need to do is:

```{r}
#| label: fig-marg-post-density-a
#| fig-cap: "Shapes of the marginal posterior densities of μ and σ: rethinking version"

## R code 4.21 #########################
rethinking::dens(sample.mu_a)
rethinking::dens(sample.sigma_a)

```

The jargon "marginal" here means "averaging over the other parameters."
Execute the above code and inspect the plots. These densities are very
close to being normal distributions. And this is quite typical. As
sample size increases, posterior densities approach the normal
distribution. If you look closely, though, you'll notice that the
density for σ has a longer right-hand tail. I'll exaggerate this
tendency a bit later, to show you that this condition is very common for
standard deviation parameters.

**Posterior Compatibility Intervals (PIs)**

To summarize the widths of these densities with posterior compatibility
intervals we use:

```{r}
#| label: post-comp-intervals-a
#| attr-source: '#lst-post-comp-intervals-a lst-cap="Posterior Compatibility Intervals (PIs): rethinking version"'

## R code 4.22 ####################
rethinking::PI(sample.mu_a)
rethinking::PI(sample.sigma_a)
```

Since these samples are just vectors of numbers, you can compute any
statistic from them that you could from ordinary data: `mean`, `median`,
or `quantile`, for example.

**Sample size and the normality of sigmas posterior**

Before moving on to using quadratic approximation `rethinking::quap()`
as shortcut to all of this inference, it is worth repeating the analysis
of the height data above, but now with only a fraction of the original
data. The reason to do this is to demonstrate that, in principle, the
posterior is not always so Gaussian in shape. There's no trouble with
the mean, `μ`. For a Gaussian likelihood and a Gaussian prior on `μ`,
the posterior distribution is always Gaussian as well, regardless of
sample size. It is the standard deviation `σ` that causes problems. So
if you care about `σ`---often people do not---you do need to be careful
of abusing the quadratic approximation.

The deep reasons for the posterior of `σ` tending to have a long
right-hand tail are complex. But a useful way to conceive of the problem
is that variances must be positive. As a result, there must be more
uncertainty about how big the variance (or standard deviation) is than
about how small it is. For example, if the variance is estimated to be
near zero, then you know for sure that it can't be much smaller. But it
could be a lot bigger.

Let's quickly analyze only 20 of the heights from the height data to
reveal this issue. To sample 20 random heights from the original list:

```{r}
#| label: fig-sample-only-20-a
#| fig-cap: "Sample 20 heights: rethinking version"

## R code 4.23 ######################################
d3_a <- sample(d2_a$height, size = 20)

## R code 4.24 ######################################
mu2_a.list <- seq(from = 150, to = 170, length.out = 200)
sigma2_a.list <- seq(from = 4, to = 20, length.out = 200)
post2_a <- expand.grid(mu = mu2_a.list, sigma = sigma2_a.list)
post2_a$LL <- sapply(1:nrow(post2_a), function(i) {
  sum(dnorm(d3_a,
    mean = post2_a$mu[i], sd = post2_a$sigma[i],
    log = TRUE
  ))
})
post2_a$prod <- post2_a$LL + dnorm(post2_a$mu, 178, 20, TRUE) +
  dunif(post2_a$sigma, 0, 50, TRUE)
post2_a$prob <- exp(post2_a$prod - max(post2_a$prod))
sample2_a.rows <- sample(1:nrow(post2_a),
  size = 1e4, replace = TRUE,
  prob = post2_a$prob
)
sample2_a.mu <- post2_a$mu[sample2_a.rows]
sample2_a.sigma <- post2_a$sigma[sample2_a.rows]
plot(sample2_a.mu, sample2_a.sigma,
  cex = 0.5,
  col = rethinking::col.alpha(rethinking:::rangi2, 0.1),
  xlab = "mu", ylab = "sigma", pch = 16
)

```

you'll see another scatter plot of the samples from the posterior
density, but this time you'll notice a distinctly longer tail at the top
of the cloud of points.

**Marginal Posterior Density with only 20 rows**

You should also inspect the marginal posterior density for σ, averaging
over μ, produced with:

```{r}
#| label: fig-marg-post-density-a2
#| fig-cap: "Marginal posterior density for σ, averaging over μ: rethinking version"

## R code 4.25
rethinking::dens(sample2_a.sigma, norm.comp = TRUE)

```

#### Tidyverse

We can use `dplyr::sample_n()` to sample rows, with replacement, from
`d_grid2_b`.

```{r}
#| label: fig-posterior-sample-b
#| fig-cap: "Samples from the posterior distribution for the heights data. The density of points is highest in the center, reflecting the most plausible combinations of μ and σ. There are many more ways for these parameter values to produce the data, conditional on the model. (tidyverse version)"


set.seed(4)

d_grid_samples_b <- 
  d_grid2_b %>% 
  sample_n(size = 1e4, replace = T, weight = probability_b)

d_grid_samples_b %>% 
  ggplot(aes(x = mu_b, y = sigma_b)) + 
  geom_point(size = .9, alpha = 1/15) +
  scale_fill_viridis_c() +
  labs(x = expression(mu[samples]),
       y = expression(sigma[samples])) +
  theme(panel.grid = element_blank())
```

We can use `tidyr::pivot_longer()` and then `ggplot2::facet_wrap()` to
plot the densities for both `mu` and `sigma` at once.

```{r}
#| label: fig-densities-mu-sigma
#| fig-cap: "Shapes of the marginal posterior densities of μ and σ: tidyverse version"

d_grid_samples_b %>% 
  pivot_longer(mu_b:sigma_b) %>% 

  ggplot(aes(x = value)) + 
  geom_density(fill = "deepskyblue", color = "black") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ name, scales = "free", labeller = label_parsed)
```

We'll use the {**tidybayes**} package to compute their posterior modes
and 95% HDIs.

```{r}
#| label: post-mode-hdi95-b

d_grid_samples_b %>% 
  pivot_longer(mu_b:sigma_b) %>% 
  group_by(name) %>% 
  tidybayes::mode_hdi(value) 
```

Let's say you wanted their posterior medians and 50% quantile-based
intervals, instead. Just switch out the last line for
`tidybayes::median_qi(value, .width = .5)`.

```{r}
#| label: post-median-qi90-b

d_grid_samples_b %>% 
  pivot_longer(mu_b:sigma_b) %>% 
  group_by(name) %>% 
  tidybayes::median_qi(value, .width = .5)
```

**Sample size and the normality of σ's posterior**

I will skip this part as there is nothing conceptually new in this
section.

### Finding the posterior distribution with `quap()` resp. `brms()`

#### Original

> To build the **quadratic approximation**, we'll use quap, a command in
> the `rethinking` package. The `quap` function works by using the model
> definition you were introduced to earlier in this chapter. Each line
> in the definition has a corresponding definition in the form of R
> code. The engine inside quap then uses these definitions to define the
> posterior probability at each combination of parameter values. Then it
> can climb the posterior distribution and find the peak, its MAP
> (**Maximum A Posteriori** estimate). Finally, it estimates the
> quadratic curvature at the MAP to produce an approximation of the
> posterior distribution. (parenthesis and emphasis are mine)

::: callout-note
The procedure used by `rethinking:quap()` is very similar to what many
non-Bayesian procedures do, just without any priors.
:::

1.  We start with the Howell1 data frame for adults `d2_a` (age \>= 18).
    We will place the R code equivalents into an `alist()` We are going
    to use the @def-height-priors. (Code 4.27).
2.  Then we fit the model with `rethinking::quap()` to the data in the
    data frame `d2_a` (Code 4.28) to `m4.1`.
3.  Now we can have a look with `rethinking::precis()` at the posterior
    distribution (Code 4.29).

```{r}
#| label: post-dist-quap-m4-1-a
#| attr-source: '#lst-post-dist-quap-m4-1-a lst-cap="Finding the posterior distribution with rethinking::quap()"'

## R code 4.27 ######################
flist <- alist(
  height ~ dnorm(mu, sigma),
  mu ~ dnorm(178, 20),
  sigma ~ dunif(0, 50)
)

## R code 4.28 ######################
m4.1 <- rethinking::quap(flist, data = d2_a)

## R code 4.29 ######################
rethinking::precis(m4.1)

```

> These numbers provide Gaussian approximations for each parameter's
> *marginal* distribution. This means the plausibility of each value of
> `_μ_`, after averaging over the plausibilities of each value of `_σ_`,
> is given by a Gaussian distribution with mean 154.6 and standard
> deviation 0.4.
>
> The 5.5% and 94.5% quantiles are percentile interval boundaries,
> corresponding to an 89% compatibility interval. Why 89%? It's just the
> default. It displays a quite wide interval, so it shows a
> high-probability range of parameter values. If you want another
> interval, such as the conventional and mindless 95%, you can use
> `precis(m4.1, prob=0.95)`. But I don't recommend 95% intervals,
> because readers will have a hard time not viewing them as significance
> tests. 89 is also a prime number, so if someone asks you to justify
> it, you can stare at them meaningfully and incant, "Because it is
> prime." That's no worse justification than the conventional
> justification for 95%.

I encourage you to compare these 89% boundaries to the compatibility
intervals from the grid approximation in @lst-post-comp-intervals-a
earlier. You'll find that they are almost identical. When the posterior
is approximately Gaussian, then this is what you should expect.

##### Start values for `rethinking::quap()` {#sec-start-values-rethinking}

Mean and standard deviation are good values to start values for hill
climbing. If you don't specify `rethinking::quap()` will use a random
value.

```{r}
#| label: start-values-quap
#| attr-source: '#start-values-quap lst-cap="Define start values for rethinking::quap()"'

## R code 4.30 ######################
start <- list(
  mu = mean(d2_a$height),
  sigma = sd(d2_a$height)
)
m4.1_2 <- rethinking::quap(flist, data = d2_a, start = start)
rethinking::precis(m4.1_2)

```

::: callout-note
###### list() and alist()

Note that the list of start values is a regular `list`, not an `alist`
like the formula list is. The two functions `alist` and `list` do the
same basic thing: allow you to make a collection of arbitrary R objects.
They differ in one important respect: `list` evaluates the code you
embed inside it, while `alist` does not. So when you define a list of
formulas, you should use `alist`, so the code isn't executed. But when
you define a list of start values for parameters, you should use `list`,
so that code like `mean(d2_a$height)` will be evaluated to a numeric
value.
:::

**Slicing in more information**

> The priors we used before are very weak, both because they are nearly
> flat and because there is so much data. So I'll splice in a more
> informative prior for `*μ*`, so you can see the effect. All I'm going
> to do is change the standard deviation of the prior to 0.1, so it's a
> very narrow prior. I'll also build the formula right into the call to
> `quap` this time.

```{r}
#| label: post-dist-quap-m4.2
#| attr-source: '#lst-post-dist-quap-m4.2 lst-cap="Finding the posterior distribution with a narrower prior rethinking::quap()"'

## R code 4.31 ###########################
m4.2 <- rethinking::quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu ~ dnorm(178, 0.1),
    sigma ~ dunif(0, 50)
  ),
  data = d2_a
)
rethinking::precis(m4.2)

```

> Notice that the estimate for `*μ*` has hardly moved off the prior. The
> prior was very concentrated around 178. So this is not surprising. But
> also notice that the estimate for `*σ*` has changed quite a lot, even
> though we didn't change its prior at all. Once the golem is certain
> that the mean is near 178---as the prior insists---then the golem has
> to estimate `*σ*` conditional on that fact. This results in a
> different posterior for `*σ*`, even though all we changed is prior
> information about the other parameter.

::: callout-caution
###### `μ` has hardly moved off the prior

At first I did not understand "that the estimate for `*μ*` has hardly
moved off the prior". I thought this assertion refers to the value of
`*μ*` in both calculation. *μ* has changed considerably from 154.61 to
177.86 and under that assumption the above quote does not make sense.

But in contrast to my wrong assumption the assertion refers to the
difference between the chosen prior (178) and the resulting value of
`*μ*` (177.86).
:::

#### Tidyverse

> In the text, McElreath indexed his models with names like `m4.1`. I
> will largely follow that convention, but will replace the *m* with a
> *b* to stand for the **`brms`** package.

Here's how to fit the first model for this chapter.

```{r}
#| label: post-dist-brms-b4.1
#| att-source: '#lst-post-dist-brms-b4.1 lst-cap="Finding the posterior distribution with brms::brm()"'

b4.1 <- 
  brms::brm(data = d2_b, 
      family = gaussian,
      height ~ 1,
      prior = c(brms::prior(normal(178, 20), class = Intercept),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.01")

brms:::plot.brmsfit(b4.1)
```

If you want detailed diagnostics for the HMC chains, call
`brms::launch_shinystan(b4.1)`. That'll keep you busy for a while. See
the [ShinyStan website](https://mc-stan.org/users/interfaces/shinystan)
for more information.

::: callout-caution
###### Launch of shinystan turned off

I turned off the evaluation of the following chunk. It took some time
and the it referred to a local page `http://127.0.0.1:6367/`\`where I
could inspect many details of the model. But this is at the moment too
complex to me: I do not understand all the parameters and the many
configurable options programmed with a {**shiny**) interface.
:::

```{r}
#| label: detailled-diganostic-chains-brms-b4.1
#| eval: false

brms::launch_shinystan(b4.1)

```

```{r}
#| label: print-summary-brms-b4.1

brms:::print.brmsfit(b4.1)

```

```{r}
#| label: print-stan-like-summary-brms-b4.1

b4.1$fit
```

Whereas rethinking defaults to 89% intervals, using `print()` or
`summary()` with {**brms**} models defaults to 95% intervals.

::: callout-note
As I have learned shortly: `print()` or `summary()` are generic
functions where one can add new printing methods with new classes. In
this case `class(b4.1)` = `r class(b4.1)`. This means I do not need to
add `brms::` to secure that I will get the {**brms**} printing or
summary method as I didn't load the {**brms**} package. Quite the
contrary: Adding `brms::` would result into the message: "Error:
'summary' is not an exported object from 'namespace:brms'".

As I really want to specify explicitly the method these generic
functions should use, I need to use the syntax `brms:::print.brmsfit()`
or `brms:::summary.brmsfit()` respectively.

In this respect I have to learn more about S3 classes. There are many
important web resources about this subject that I have found with the
search string "r what is s3 class". Maybe I should start with the [S3
chapter in Advanced R](https://adv-r.hadley.nz/s3.html).
:::

Unless otherwise specified, Kurz will stick with 95% intervals
throughout. To get those 89% intervals or McElreath approach, one could
use the `prob` argument within `summary()` or `print()`.

```{r}
#| label: summary-interval-.89-brms-b4.1

brms:::summary.brmsfit(b4.1, prob = .89)

```

Here's the `brms::brm()` code for the model with the very narrow `_μ_`
prior corresponding to the `rethinking::quap()` code in
@lst-post-dist-quap-m4.2.

```{r}
#| label: fig-post-dist-brms-b4.2
#| fig-cap: "Finding the posterior distribution with a narrower prior using brms::brm()"
#| att-source: '#lst-post-dist-brms-b4.2 lst-cap="Finding the posterior distribution with a narrower prior using brms::brm()"'

b4.2 <- 
  brms::brm(data = d2_b, 
      family = gaussian,
      height ~ 1,
      prior = c(brms::prior(normal(178, 0.1), class = Intercept),
                brms::prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.02")

brms:::plot.brmsfit(b4.2, widths = c(1, 2))

```

And here's the model `summary()`.

```{r}
#| label: summary-narrow-prior

brms:::summary.brmsfit(b4.2)

```

Subsetting the `summary()` output with `$fixed` provides a convenient
way to compare the Intercept summaries between `b4.1` and `b4.2`.

```{r}
#| label: compare-summaries-b4.1-b4.2

rbind(brms:::summary.brmsfit(b4.1)$fixed,
      brms:::summary.brmsfit(b4.2)$fixed)

```

### Sampling from a quap

#### Original

The above explains how to get a quadratic approximation of the
posterior, using `rethinking::quap()`. But how do we then get samples
from the quadratic approximate posterior distribution? --- When R
constructs a quadratic approximation, it calculates not only standard
deviations for all parameters, but also the covariances among all pairs
of parameters. Just like a mean and standard deviation (or its square, a
variance) are sufficient to describe a one-dimensional Gaussian
distribution, a list of means and a matrix of variances and covariances
are sufficient to describe a multi-dimensional Gaussian distribution.

```{r}
#| label: calc-var-cov-m4.1-a
#| attr-source: '#lst-calc-var-cov-m4.1-a lst-cap="Calculation of the variance-covariance matrix: rethinking version"'

## R code 4.32 ###################
rethinking::vcov(m4.1)
```

`vcov()` returns the variance-covariance matrix of the main parameters
of a fitted model object. In the above {**rethinking**} version is uses
the class `map2stan` for a fitted Stan model as `m4.1` is of class
`map`.

::: callout-caution
###### rethinking::vcov

In @lst-calc-var-cov-m4.1-a I am explicitly using the package
{**rethinking**} for the `vcov()` function. The same function is also
available as a base R function with `stats::vcov()`. But this generates
an error because there is no method known for an object of class `map`
from the rethinking package. The help file for `stats::vcov()` only says
that the `vcov` object is an S3 method for classes `lm`, `glm`, `mlm`
and `aov` but not for `map`.

> Error in UseMethod("vcov") : no applicable method for 'vcov' applied
> to an object of class "map"

I could have used only `vcov()`. But this only works when the
{**rethinking**} package is already loaded. In that case R knows because
of the class of the object which `vcov()` version to use. In this case:
class of object = `class(m4.1)` `r class(m4.1)`.
:::

@lst-calc-var-cov-m4.1-a results in a variance-covariance matrix. It is
the multi-dimensional glue of a quadratic approximation, because it
tells us how each parameter relates to every other parameter in the
posterior distribution. A variance-covariance matrix can be factored
into two elements: (1) a vector of variances for the parameters and (2)
a correlation matrix that tells us how changes in any parameter lead to
correlated changes in the others.

```{r}
#| label: vcov-decomp-m4.1-a

## R code 4.33 #######################
base::diag(rethinking::vcov(m4.1))      # <1>
stats::cov2cor(rethinking::vcov(m4.1))  # <2>
```

1.  `base::diag()` extracts the diagonal of the (variance-covariance)
    matrix. The two-element vector in the output is the list of
    variances. If you take the square root of this vector, you get the
    standard deviations that are shown in `rethinking::precis()` output.
2.  `stats::cov2cor()` scales a covariance matrix into the corresponding
    correlation matrix. The two-by-two matrix in the output is this
    correlation matrix. Each entry shows the correlation, bounded
    between −1 and +1, for each pair of parameters. The 1's indicate a
    parameter's correlation with itself. If these values were anything
    except 1, we would be worried. The other entries are typically
    closer to zero, and they are very close to zero in this example.
    This indicates that learning μ tells us nothing about σ and likewise
    that learning σ tells us nothing about μ. This is typical of simple
    Gaussian models of this kind. But it is quite rare more generally,
    as you'll see in later chapters.

Okay, so how do we get samples from this multi-dimensional posterior?
Now instead of sampling single values from a simple Gaussian
distribution, we sample vectors of values from a multi-dimensional
Gaussian distribution.

```{r}
#| label: extract-samples-m4.1-a
#| attr-source: '#lst-extract-samples-m4.1-a lst-cap="Extract sample vectors of values from the multi-dimensional Gaussian distribution of m4.1: rethinking version"'


## R code 4.34 #######################
post3_a <- rethinking::extract.samples(m4.1, n = 1e4)
head(post3_a)
```

You end up with a data frame, post, with 10,000 (1e4) rows and two
columns, one column for `_μ_` and one for `_σ_`. Each value is a sample
from the posterior, so the mean and standard deviation of each column
will be very close to the MAP values from before. You can confirm this
by summarizing the samples:

```{r}
#| label: summary-samples-m4.1-a
#| attr-source: '#lst-summary-samples-m4.1-a lst-cap="Summary the extracted samples: rethinking version"'

## R code 4.35 ##################
rethinking::precis(post3_a)
```

Compare these values to the output from @lst-post-dist-quap-m4-1-a. And
you can use `plot(post)` to see how much they resemble the samples from
the grid approximation in FIGURE 4.4 (here @fig-posterior-sample-a).
These samples also preserve the covariance between `_μ_` and `_σ_`. This
hardly matters right now, because `_μ_` and `_σ_` don't co-vary at all
in this model. But once you add a predictor variable to your model,
covariance will matter a lot.

```{r}
#| label: fig-posterior-sample-vectors-a
#| fig-cap: "Samples from the vectors of values from a multi-dimensional Gaussian distribution. The density of points is highest in the center, reflecting the most plausible combinations of μ and σ. It is very similar to @fig-posterior-sample-a (rethinking version)"

base::plot(post3_a)
```

##### Under the hood with multivariate sampling {#sec-under-the-hood-multivariate-sampling}

The function `rethinking::extract.samples()` is for convenience. It is
just running a simple simulation of the sort you conducted near the end
of @sec-sampling-the-imaginary with @lst-sim-pred-samples-a. Here's a
peak at the motor. The work is done by a multi-dimensional version of
`stats::rnorm()`, `MASS::mvrnorm()`. The function `stats::rnorm()`
simulates random Gaussian values, while `MASS::mvrnorm()` simulates
random vectors of multivariate Gaussian values. Here's how to use it the
{**MASS**} function to do what `rethinking::extract.samples()` does:

```{r}
#| label: fig-posterior-sample-vectors-MASS-a
#| fig-cap: "Extract samples from the vectors of values from a multi-dimensional Gaussian distribution using the {**MASS**} package. It is same calculation as in @fig-posterior-sample-a (rethinking version)"


## R code 4.36 ######################
post4_a <- MASS::mvrnorm(n = 1e4, mu = rethinking::coef(m4.1), 
                      Sigma = rethinking::vcov(m4.1))
plot(post4_a)
```

#### Tidyverse

{**brms**} doesn't seem to have a convenience function that works the
way `vcov()` does for {**rethinking**}.

```{r}
#|label: calc-var-cov-m4.1-b
#|attr-source: '#lst-calc-var-cov-m4.1-b lst-cap="Calculation of vcov(): tidyverse version."'

brms:::vcov.brmsfit(b4.1)
```

This only returns the first element in the matrix it did for
{**rethinking**}. That is, it appears `brms::vcov()` only returns the
variance/covariance matrix for the single-level `_β_` parameters.

::: callout-caution
###### brms::vcov()

Referring to a similar situation with `rethinking::vcov()` in
@lst-calc-var-cov-m4.1-a I cannot write `brms::vcov()`, but have to use
either `brms:::vcov.brmsfit(b4.1)` or just `vcov(b4.1)`. The weird thing
is that the first time it also works with `brms::vcov()` but only the
first time!
:::

However, if you really wanted this information, you could get it after
putting the Hamilton Monte Carlo (HMC) chains in a data frame. We do
that with the `brms::as_draws_df()` function:

```{r}
#| label: put-hmc-into-df-b
#| attr-source: '#lst-put-hmc-into-df-b lst-cap="Extract the iteration of the Hamiliton Monte Carlo (HMC) chains into a data frame"'

post_b <- brms::as_draws_df(b4.1)

head(post_b)
```

::: callout-tip
###### draws object

The functions of the family `as_draws()` transform `brmsfit` objects to
`draws` objects, a format supported by the {**posterior**} package.
{brms} currently imports the family of `as_draws()`functions from the
{**posterior**} package, a tool for working with posterior
distributions.

@lst-put-hmc-into-df-b produced the {**brms**} version of what McElreath
achieved with `extract.samples()` in @lst-extract-samples-m4.1-a.
However, what happened under the hood was different. Whereas rethinking
used the `mvnorm()` function from the {**MASS**} package, with
{**brms**} we just extracted the iterations of the HMC chains and put
them in a data frame.
:::

Now `select()` the columns containing the draws from the desired
parameters `b_Intercept` and `sigma` and feed them into `cov()`.

```{r}
#| label: calc-cov-post-b
#| attr-source: '#lst-calc-cov-post-b lst-cap="Calculate the vector of variances and correlation matrix for b_Intercept and sigma"'

select(post_b, b_Intercept:sigma) %>% 
  stats::cov()
```

@lst-calc-cov-post-b displays "(1) a vector of variances for the
parameters and (2) a correlation matrix" for them (p. 90). Here are just
the variances (i.e., the diagonal elements) and the correlation matrix.

```{r}
#| label: calc-var-post-b
#| attr-source: '#lst-calc-var-post-b lst-cap="Calculate only variances (the diagonal values)"'

select(post_b, b_Intercept:sigma) %>%
  stats::cov() %>%
  base::diag()

```

```{r}
#| label: calc-corr-matrix-post-b
#| attr-source: '#lst-calc-corr-matrix-post-b lst-cap="Calculate only crrelation"'

# correlation
post_b %>%
  select(b_Intercept, sigma) %>%
  stats::cor()
```

```{r}
#| label: show-data-structure

str(post_b)
```

The `post_b` object is not just a data frame, but also of class
`draws_df`, which means it contains three metadata variables ----
`.chain`, `.iteration`, and `.draw` --- which are often hidden from
view, but are there in the background when needed. As you'll see, we'll
make good use of the `.draw` variable in the future. Notice how our post
data frame also includes a vector named `lp__`. That's the log
posterior.

For details, see: - The [Log-Posterior (function and
gradient)](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html#the-log-posterior-function-and-gradient)
section of the Stan Development Team's (2023) vignette [RStan: the R
interface to
Stan](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html)
and - Stephen Martin's [explanation of the log
posterior](https://discourse.mc-stan.org/t/basic-question-what-is-lp-in-posterior-samples-of-a-brms-regression/17567/2)
on the Stan Forums.

::: callout-caution
###### Summaries of {brms} and {posterior} packages

Kurz claims that `summary()` function doesn't work for {**brms**}
posterior data frames quite the way `rethinking::precis()` does for
posterior data frames from the {\*\*rethinking\*} package. But I this
observation is somewhat misleading.

The posterior data frame `post_b` is not of class `brms`. Let's check
this:

```{r}
#| label: class-post_b-b

class(post_b)
```

The class `draws_df`and `draws` refers to the {**posterior**} and not to
the {**brms**} package. Remember: In @lst-put-hmc-into-df-b we
transformed with the function `as_draws_df` the `brms` object into a
`draws_df` and `draws` object.

Therefore Kurz's claim should be read: The `summary()` function doesn't
work for {**posterior**} posterior data frames quite the way
`rethinking::precis()` does for posterior data frames from the
{**rethinking**} package. Instead of calling `brms:::summary.brmsfit()`
I will use `posterior:::summary.draws()`.

I wouldn't have noticed this difference if I hadn't mentioned explicitly
the name of the packages in front of the function, because in that case
R would have used `base::summary()` as in Kurz's text.
:::

```{r}
#| label: base-summary-samples-b4.1-b
#| attr-source: '#lst-base-summary-samples-b4.1-b lst-cap="Summary the extracted iterations of the HMC chains: base version"' 

base::summary(post_b[, 1:2])

```

```{r}
#| label: posterior-summary-samples-b4.1-b
#| attr-source: '#lst-posterior-summary-samples-b4.1-b lst-cap="Summary the extracted iterations of the HMC chains: posterior version"' 

posterior:::summary.draws(post_b[, 1:2])


```

To get a similar summary with tiny histograms Kurz offers different
solutions:

-   A base R approach by using the transpose of a `stats::quantile()`
    call nested within `base::apply()`
-   A {**tidyverse**} approach
-   A {**brms**} approach by just putting the `brm()` fit object into
    `posterior_summary()`
-   A {**tidybayes**} approach using `tidybayes::mean_hdi()` if you're
    willing to drop the posterior `sd` and
-   Using additionally the [function
    `histospark()`](https://github.com/hadley/precis/blob/master/R/histospark.R)
    (from the unfinished {**precis**} package by Hadley Wickham supposed
    to replace `base::summary()`) to get the tiny histograms and to add
    them into the tidyverse approach.

Additionally I will propose using the {**skimr**} packages:

```{r}
#| label: skim-summary-samples-b4.1-b
#| attr-source: '#lst-skim-summary-samples-b4.1-b lst-cap="Summary the extracted iterations of the HMC chains: skimr version"' 

skimr::skim(post_b[, 1:2])
```

Kurz refers only shortly to both `overthinking` blocks of this section:

-   Start values for `rethinking::quap()` resp. `brms::brm()` (See
    @sec-start-values-rethinking): Within the `brm()` function, you use
    the `init` argument fpr the start values.
-   Under the hood with multivariate sampling (See
    @sec-under-the-hood-multivariate-sampling): Again Kurz remarked that
    `brms::as_draws_df()` is not the same as
    `rethinking::extract.samples()`. What this exactly means will
    (hopefully) explained later in @sec-markov-chain-monte-carlo.

## Linear prediction

### Introduction

#### Original

What we've done until now is just a Gaussian model of height in a
population of adults. But typically, we are interested in modeling how
an outcome is related to some other variable, a predictor variable. If
the predictor variable has any statistical association with the outcome
variable, then we can use it to predict the outcome. When the predictor
variable is built inside the model in a particular way, we'll have
linear regression.

Let's look at how height in these Kalahari foragers (the outcome
variable) co-varies with weight (the predictor variable).

```{r}
#| label: fig-height-against-weight-a
#| fig-cap: "Adult height and weight against one another"

## R code 4.37 #####################
plot(d2_a$height ~ d2_a$weight)
```

There's obviously a relationship: Knowing a person's weight helps you
predict height. To make this vague observation into a more precise
quantitative model that relates values of `weight` to plausible values
of `height`, we need some more technology. How do we take our Gaussian
model from @sec-gaussian-model-of-height and incorporate predictor
variables?

#### Tidyverse

```{r}
#| label: fig-height-against-weight-b
#| fig-cap: "Adult height and weight against one another"


d2_b |> 
    ggplot(aes(height, weight)) + 
    geom_point()
```

### The linear model strategy

#### Original

##### Model definition

Recall @def-height-priors for the Gaussian height model. How do we get
`weight` into this model? Let `_x_` be the name for the column of weight
measurements, `d2_a$weight`. Let the average of the `_x_` values be
$\overline{x}$, "ex bar". Now we have a predictor variable `_x_`, which
is a list of measures of the same length as `_h_`. To get weight into
the model, we define the mean `_μ_` as a function of the values in
`_x_`.

------------------------------------------------------------------------

::: {#def-linear-model}
Linear model height against weight

$$
\begin{align*}
h_{i} \sim Normal(μ_{i}, σ) \space \space (1) \\ 
μ_{i} = \alpha + \beta(x_{i}-\overline{x}) \space \space (2) \\
\alpha \sim Normal(178, 20) \space \space (3)  \\ 
\beta \sim Normal(0,10) \space \space (4) \\
\sigma \sim Uniform(0, 50) \space \space (5)      
\end{align*}
$$

(1) **Likelihood (Probability of the data)**: The first line is nearly
    identical to before, except now there is a little index $i$ on the
    $μ$ as well as the $h$. You can read $h_{i}$ as "each height" and
    $\mu_{i}$ as "each $μ$" The mean $μ$ now depends upon unique values
    on each row $i$. So the little $i$ on $\mu_{i}$ indicates that *the
    mean depends upon the row*.

(2) **Linear model**: The mean $μ$ is no longer a parameter to be
    estimated. Rather, as seen in the second line of the model, $\mu{i}$
    is constructed from other parameters, $\alpha$ and $\beta$, and the
    observed variable $x$. This line is not a stochastic relationship
    ----- there is no `~` in it, but rather an `=` in it ----- because
    the definition of $\mu{i}$ is deterministic. That is to say that,
    once we know $\alpha$ and $\beta$ and $x_{i}$, we know $\mu{i}$ with
    certainty. (More details in @sec-linear-model-a.)

(3) **includes (3),(4) and(5) with** $\alpha, \beta, \sigma$ priors: The
    remaining lines in the model define distributions for the unobserved
    variables. These variables are commonly known as parameters, and
    their distributions as priors. There are three parameters:
    $\alpha, \beta, \sigma$. You've seen priors for $\alpha$ and $\beta$
    before, although $\sigma$ was called $\mu$ back then. (More details
    in @sec-priors-a)
:::

------------------------------------------------------------------------

##### Linear model {#sec-linear-model-a}

The value $x_{i}$ in the second line of @def-linear-model is just the
weight value on row $i$. It refers to the same individual as the height
value, $h_{i}$, on the same row. The parameters $\alpha$ and $\beta$ are
more mysterious. Where did they come from? We made them up. The
parameters $\mu$ and $\sigma$ are necessary and sufficient to describe a
Gaussian distribution. But $\alpha$ and $\beta$ are instead devices we
invent for manipulating $\mu$, allowing it to vary systematically across
cases in the data.

You'll be making up all manner of parameters as your skills improve. One
way to understand these made-up parameters is to think of them as
targets of learning. Each parameter is something that must be described
in the posterior distribution. So when you want to know something about
the data, you ask your golem by inventing a parameter for it. This will
make more and more sense as you progress.

What does the second line of @def-linear-model? It tells the regression
golem that you are asking two questions about the mean of the outcome.

1.  What is the expected height when $x_{i} = \overline{x}$? The
    parameter $\alpha$ answers this question, because when
    $x_{i} = \overline{x}$, $\mu_{i} = \alpha$. For this reason,
    $\alpha$ is often called the **intercept**. But we should think not
    in terms of some abstract line, but rather in terms of the meaning
    with respect to the observable variables.
2.  What is the change in expected height, when $x_{i}$ changes by 1
    unit? The parameter $\beta$ answers this question. It is often
    called a **slope**, again because of the abstract line. Better to
    think of it as a rate of change in expectation.

Jointly these two parameters ask the golem to find a line that relates
$x$ to $h$, a line that passes through $\alpha$ when
$x_{i} = \overline{x}$ and has slope $\beta$. That is a task that golems
are very good at. It's up to you, though, to be sure it's a good
question.

##### Priors {#sec-priors-a}

The prior for $\beta$ in @def-linear-model deserves explanation. Why
have a Gaussian prior with mean zero? This prior places just as much
probability below zero as it does above zero, and when $\beta = 0$,
weight has no relationship to height. To figure out what this prior
implies, we have to simulate the prior predictive distribution.

The goal is to simulate heights from the model, using only the priors.
First, let's consider a range of weight values to simulate over. The
range of observed weights will do fine. Then we need to simulate a bunch
of lines, the lines implied by the priors for $\alpha$ and $\beta$.
Here's how to do it, setting a seed so you can reproduce it exactly:

```{r}
#| label: fig-sim-heights-only-with-priors-a
#| fig-cap: "Simulating heights from the model, using only the priors: rethinking version"

## R code 4.38 #####################
set.seed(2971)
N <- 100 # 100 lines
a <- rnorm(N, 178, 20)
b <- rnorm(N, 0, 10)


## R code 4.39 #####################
plot(NULL,
  xlim = range(d2_a$weight), ylim = c(-100, 400),
  xlab = "weight", ylab = "height"
)
abline(h = 0, lty = 2)
abline(h = 272, lty = 1, lwd = 0.5)
mtext("b ~ dnorm(0,10)")
xbar <- mean(d2_a$weight)
for (i in 1:N) {
  curve(a[i] + b[i] * (x - xbar),
    from = min(d2_a$weight), to = max(d2_a$weight), add = TRUE,
    col = rethinking::col.alpha("black", 0.2)
  )
}

```

For reference, I've added a dashed line at zero---no one is shorter than
zero---and the "Wadlow" line at 272 cm for the world's tallest person.
The pattern doesn't look like any human population at all. It
essentially says that the relationship between weight and height could
be absurdly positive or negative. Before we've even seen the data, this
is a bad model. Can we do better?

We can do better immediately. We know that average height increases with
average weight, at least up to a point. Let's try restricting it to
positive values. The easiest way to do this is to define the prior as
Log-Normal instead. Defining $\beta$ as `Log-Normal(0,1)` means to claim
that the logarithm of $\beta$ has a Normal(0,1) distribution.

------------------------------------------------------------------------

::: {#def-log-normal}
Defining the prior as Log-Normal distribution

$$
\beta \sim LogNormal(0,1)
$$
:::

------------------------------------------------------------------------

Base R provides the `dlnorm()` and `rlnorm()` densities for working with
log-normal distributions. You can simulate this relationship to see what
this means for $\beta$:

```{r}
#| label: fig-log-normal-a
#| fig-cap: "Log-Normal distribution: rethinking version"


set.seed(4) # to reproduce with tidyverse version

## R code 4.40 ####################
b <- rlnorm(1e4, 0, 1)
rethinking::dens(b, xlim = c(0, 5), adj = 0.1)
```

If the logarithm of $\beta$ is normal, then $\beta$ itself is strictly
positive. The reason is that `exp(x)` is greater than zero for any real
number `x`. This is the reason that Log-Normal priors are commonplace.
They are an easy way to enforce positive relationships.

So what does this earn us? Do the prior predictive simulation again, now
with the Log-Normal prior:

```{r}
#| label: fig-prior-pred-sim-a
#| fig-cap: "Prior predictive simulation again, now with the Log-Normal prior: rethinking version"


## R code 4.41 ###################
set.seed(2971)
N <- 100 # 100 lines
a <- rnorm(N, 178, 20)
b <- rlnorm(N, 0, 1)

## R code 4.39 ###################
plot(NULL,
  xlim = range(d2_a$weight), ylim = c(-100, 400),
  xlab = "weight", ylab = "height"
)
abline(h = 0, lty = 2)
abline(h = 272, lty = 1, lwd = 0.5)
mtext("b ~ dnorm(0,10)")
xbar <- mean(d2_a$weight)
for (i in 1:N) {
  curve(a[i] + b[i] * (x - xbar),
    from = min(d2_a$weight), to = max(d2_a$weight), add = TRUE,
    col = rethinking::col.alpha("black", 0.2)
  )
}


```

This is much more sensible. There is still a rare impossible
relationship. But nearly all lines in the joint prior for $\alpha$ and
$\beta$ are now within human reason.

::: callout-note
###### What is the correct prior?

There is no more a uniquely correct prior than there is a uniquely
correct likelihood. Statistical models are machines for inference. Many
machines will work, but some work better than others. Priors can be
wrong, but only in the same sense that a kind of hammer can be wrong for
building a table.

Priors encode states of information before seeing data. So priors allow
us to explore the consequences of beginning with different information.
In cases in which we have good prior information that discounts the
plausibility of some parameter values, like negative associations
between height and weight, we can encode that information directly into
priors. When we don't have such information, we still usually know
enough about the plausible range of values. And you can vary the priors
and repeat the analysis in order to study how different states of
initial information influence inference. Frequently, there are many
reasonable choices for a prior, and all of them produce the same
inference.
:::

::: callout-note
###### Prior predictive simulation and p-hacking

When the model is adjusted in light of the observed data, then p-values
no longer retain their original meaning. False results are to be
expected. This is valid for Bayesian and Non-Bayesian statistics. Even
if Bayesian statistics don't pay any attention to p-values, the danger
remains. We could choose our priors conditional on the observed sample,
just to get some desired (wrong) result. It is therefore important to
choose priors conditional on pre-data knowledge of the variables---their
constraints, ranges, and theoretical relationships. We should always
judge our priors against general facts, not the sample.
:::

#### Tidyverse

##### Model definition (empty)

Nothing to add. Remains empty.

##### Linear model (empty)

Nothing to add. Remains empty.

##### Priors {#sec-priors-b}

```{r}
#| label: fig-sim-heights-only-with-priors-b
#| fig-cap: "Simulating heights from the model, using only the priors: tidyverse version"

set.seed(2971)
# how many lines would you like?
n_lines <- 100

lines <-
  tibble(n = 1:n_lines,
         a = rnorm(n_lines, mean = 178, sd = 20),
         b = rnorm(n_lines, mean = 0, sd = 10)) %>% 
  expand_grid(weight = range(d2_b$weight)) %>% 
  mutate(height = a + b * (weight - mean(d2_b$weight)))


lines %>% 
  ggplot(aes(x = weight, y = height, group = n)) +
  geom_hline(yintercept = c(0, 272), linetype = 2:1, linewidth = 1/3) +
  geom_line(alpha = 1/10) +
  coord_cartesian(ylim = c(-100, 400)) +
  ggtitle("b ~ dnorm(0, 10)") +
  theme_classic()

```

Using the Log-Normal distribution prohibits negative values. This is an
important constraint for height and weight as these variables cannot be
under 0.

```{r}
#| label: fig-log-normal-b
#| fig-cap: "Log-Normal distribution: tidyverse version"

set.seed(4)

tibble(b = rlnorm(1e4, meanlog = 0, sdlog = 1)) %>% 
  ggplot(aes(x = b)) +
  geom_density(fill = "grey92") +
  coord_cartesian(xlim = c(0, 5)) +
  theme_classic()

```

::: callout-note
Kurz used with `mean`and `sd`an abbreviated version of the argument names `meanlog` and `sdlog`.

:::


I am not very skilled with the Log-Normal distribution, and so I am
happy that Kurz added some explanations:

> If you're unfamiliar with the log-normal distribution, it is the
> distribution whose logarithm is normally distributed. For example,
> here's what happens when we compare Normal(0,1) with
> log(Log-Normal(0,1)).

```{r}
#| label: fig-normal-log-normal
#| fig-cap: "Compare Normal(0,1) with log(Log-Normal(0,1))"

set.seed(4)

tibble(rnorm           = rnorm(1e5, mean = 0, sd = 1),
       `log(rlognorm)` = log(rlnorm(1e5, meanlog = 0, sdlog = 1))) %>% 
  pivot_longer(everything()) %>% 

  ggplot(aes(x = value)) +
  geom_density(fill = "grey92") +
  coord_cartesian(xlim = c(-3, 3)) +
  theme_classic() +
  facet_wrap(~ name, nrow = 2)
```


> Those values are ~~what~~ the mean and standard deviation of the
> output from the `rlnorm()` function **after** they are log
> transformed. The formulas for the actual mean and standard deviation
> for the log-normal distribution itself are complicated (see
> [Wikipedia](https://en.wikipedia.org/wiki/Log-normal_distribution)).

$$
mean = exp(\mu + \frac{\sigma^2}{2} \\
standard\space deviation = \sqrt{[exp(\sigma^2)-1]exp(2\sigma^2+\sigma^2)}
$$ {#eq-log-normal}\

Let's try our hand at those formulas and compute the mean and standard
deviation for Log-Normal(0,1):

```{r}
#| label: compute-mu-sigma-for-log-normal-manually-b

mu    <- 0
sigma <- 1

# mean
exp(mu + (sigma^2) / 2)

# sd
sqrt((exp(sigma^2) - 1) * exp(2 * mu + sigma^2))
```

Let's confirm with simulated draws from `rlnorm()`.

```{r}
#| label: compute-log-normal-b
#| fig-cap: "Compute mean and standard deviation of the Log-Normal distribution: tidyverse version"

set.seed(4)

tibble(x = rlnorm(1e7, meanlog = 0, sdlog = 1)) %>% 
  summarise(mean = mean(x),
            sd   = sd(x))
```

```{r}
#| label: fig-prior-pred-sim-b
#| fig-cap: "Prior predictive simulation again, now with the Log-Normal prior: tidyverse version"


# make a tibble to annotate the plot
text <-
  tibble(weight = c(34, 43),
         height = c(0 - 25, 272 + 25),
         label  = c("Embryo", "World's tallest person (272 cm)"))

# simulate
set.seed(2971)

tibble(n = 1:n_lines,
       a = rnorm(n_lines, mean = 178, sd = 20),
       b = rlnorm(n_lines, mean = 0, sd = 1)) %>% 
  expand_grid(weight = range(d2_b$weight)) %>% 
  mutate(height = a + b * (weight - mean(d2_b$weight))) %>%
  
  # plot
  ggplot(aes(x = weight, y = height, group = n)) +
  geom_hline(yintercept = c(0, 272), linetype = 2:1, linewidth = 1/3) +
  geom_line(alpha = 1/10) +
  geom_text(data = text,
            aes(label = label),
            size = 3) +
  coord_cartesian(ylim = c(-100, 400)) +
  ggtitle("log(b) ~ dnorm(0, 1)") +
  theme_classic()
```

::: callout-tip
###### Literature reference

The paper by Simmons, Nelson and Simonsohn (2011), [False-positive
psychology: Undisclosed flexibility in data collection and analysis
allows presenting anything as
significant](https://journals.sagepub.com/doi/10.1177/0956797611417632),
is often cited as an introduction to the problem.
:::

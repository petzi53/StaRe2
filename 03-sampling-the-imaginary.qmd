# Sampling the Imaginary {#sec-sampling-the-imaginary}

```{r}
#| label: setup

library(tidyverse)
```

## Vampire Example {.unnumbered}

### Probability notation {.unnumbered}

To understand the interpretation of the following formulae in this
section I need some [primer of probability
notation](https://www.mathsisfun.com/data/probability-events-conditional.html):

-   $P(A)$ means "**Probability Of Event A**".

    *Example*: What is the probability of $2$ after throwing a dice?
    $P(A) = \frac{1}{6}$

-   $P(B \mid A)$ means "**Probability of Event B given A**". This is a
    conditional probability. After event $A$ has happened, what is the
    probability of $B$. If the events are independent from each other
    than the changes do not influence each other. *Example*: The
    probability to throw $2$ in a second dice throw are still
    $P(B \mid A) = \frac{1}{6}$. If the events are dependent of each
    other then "Probability of event A and event B equals the
    probability of event A times the probability of event B given event
    A."

$$
P(A \space and\space B) = P(A) \times P(B \mid A)
$$

*Example*: Taking a red marble from a urn of $5$ red and $5$ blue
marbles without replacement. Here the probability of a red marble (in
the second draw) given the probability of a red marble (in the first
draw) is

$$
\frac{5}{10} \times \frac{4}{9} = \frac{2}{9}
$$

### Original {.unnumbered}

#### a) Medical Test Scenario with Bayes theorem {.unnumbered}

1.  Suppose there is a blood test that correctly detects vampirism 95%
    of the time.$Pr(positivetest|vampire) = 0.95$.
2.  It's a very accurate test, nearly always catching real vampires. It
    also make mistakes, though, in the form of false positives. One
    percent of the time, it incorrectly diagnoses normal people as
    vampires, $Pr(positive test result| mortal) = 0.01$.
3.  The final bit of information we are told is that vampires are rather
    rare, being only 0.1% of the population, implying
    $Pr(vampire) = 0.001$.

Suppose now that someone tests positive for vampirism. What's the
probability that he or she is a bloodsucking immortal?

The correct approach is just to use Bayes' theorem to invert the
probability, to compute $Pr(vampire | positive)$.

$$
\Pr(\text{vampire}|\text{positive}) = \frac{\Pr(\text{positive}|\text{vampire})\Pr(\text{vampire})}{\Pr(\text{positive})}.
$$

where $Pr(positive)$ is the average probability of a positive test
result, that is,

$$
Pr(positive) = Pr(positive|vampire)Pr(vampire) + Pr(positive|mortal)Pr(1-vampire)
$$

```{r}
#| label: scenario-bayes-theorem-a
#| attr-source: '#lst-bayes-scenario lst-cap="Calculated with the Bayes formula"'

## R code 3.1
Pr_Positive_Vampire_a <- 0.95
Pr_Positive_Mortal_a <- 0.01
Pr_Vampire_a <- 0.001
Pr_Positive_a <- Pr_Positive_Vampire_a * Pr_Vampire_a +
  Pr_Positive_Mortal_a * (1 - Pr_Vampire_a)
(Pr_Vampire_Positive_a <- Pr_Positive_Vampire_a * Pr_Vampire_a / Pr_Positive_a)
```

There is only an 8.7% chance that the suspect is actually a vampire.

> Most people find this result counterintuitive. And it's a very
> important result, because it mimics the structure of many realistic
> testing contexts, such as HIV and DNA testing, criminal profiling, and
> even statistical significance testing ... . Whenever the condition of
> interest is very rare, having a test that finds all the true cases is
> still no guarantee that a positive result carries much information at
> all. The reason is that most positive results are false positives,
> even when all the true positives are detected correctly.

#### b) Medical test scenario with natural frequencies {.unnumbered}

```         
(1)  In a population of 100,000 people, 100 of them are vampires.
(2)  Of the 100 who are vampires, 95 of them will test positive for vampirism.
(3)  Of the 99,900 mortals, 999 of them will test positive for vampirism.
```

There are 999 + 95 = `r 999 + 95` people tested positive. But from these
people only 95 / (999 + 95) = `r (95 / (999 + 95)) * 100` % are actually
vampires.

Or with a slightly different wording it is still easier to
understand: 1. We can just count up the number of people who test
positive: $95 + 999 = 1094$. 2. Out of these $1094$ positive tests, $95$
of them are real vampires, so that implies:

$$
PR(positive|vampire) = \frac{95}{1094}
$$

```{r}
#| label: scenario-common-sense-a
#| attr-source: '#lst-common-sense-scenario lst-cap="Calculated with natural figures instead propabilities"'

95/1094
```

The second presentation of the problem, using counts rather than
probabilities, is often called the *frequency format* or *natural
frequencies*. It is easier for people to understand because are
confronted with count in everyday life. Nobody has ever seen a
probability.

::: callout-note
###### Meta remark: Study guide

This chapter teaches the basic skills for working with samples from the
posterior distribution. We'll begin to use samples to summarize and
simulate model output. The skills learned here will apply to every
problem in the remainder of the book, even though the details of the
models and how the samples are produced will vary.

The chapter exploits the fact that people are better in counts than in
probabilities. We will take the probability distributions from the
previous chapter and sampling from them to produce counts.
:::

> The posterior distribution is a probability distribution. And like all
> probability distributions, we can imagine drawing *samples* from it.
> The sampled events in this case are parameter values. Most parameters
> have no exact empirical realization. The Bayesian formalism treats
> parameter distributions as relative plausibility, not as any physical
> random process. In any event, randomness is always a property of
> information, never of the real world. But inside the computer,
> parameters are just as empirical as the outcome of a coin flip or a
> die toss or an agricultural experiment. The posterior defines the
> expected frequency that different parameter values will appear, once
> we start plucking parameters out of it.

There are two reasons more to use samples:

1.  First, many scientists are uncomfortable with integral calculus,
    even though they have strong and valid intuitions about how to
    summarize data. Working with samples transforms a problem in
    calculus into a problem in data summary, into a frequency format
    problem. An integral in a typical Bayesian context is just the total
    probability in some interval.
2.  Second, some of the most capable methods of computing the posterior
    produce nothing but samples. Many of these methods are variants of
    Markov chain Monte Carlo techniques (MCMC).

Drawing samples from the very simple posterior distribution of the
globe-tossing model might seem as overkill but using this technique from
the start has educational impact: When you inevitably must fit a model
to data using MCMC, you will already know how to make sense of the
output.

### Tidyverse {.unnumbered}

#### a) Medical Test Scenario with Bayes theorem {.unnumbered}

If you would like to know the probability someone is a vampire given
they test positive to the blood-based vampire test, you compute

$$
Pr(vampire|positive) = \frac{Pr(positive|vampire) \times Pr(vampire)} {Pr(positive)}
$$ This is Bayes theorem.

```{r}
#| label: scenario-bayes-theorem-b

tibble(pr_positive_vampire_b = .95,
       pr_positive_mortal_b  = .01,
       pr_vampire_b          = .001) %>% 
  mutate(pr_positive_b = pr_positive_vampire_b * pr_vampire_b + pr_positive_mortal_b * (1 - pr_vampire_b)) %>% 
  mutate(pr_vampire_positive_b = pr_positive_vampire_b * pr_vampire_b / pr_positive_b) %>% 
  glimpse()
```

#### b) Medical test scenario with natural frequencies {.unnumbered}

```{r}
#| label: scenario-natural-frequencies

tibble(pr_vampire_b2          = 100 / 100000,
       pr_positive_vampire_b2 = 95 / 100,
       pr_positive_mortal_b2  = 999 / 99900) %>% 
  mutate(pr_positive_b2 = 95 + 999) %>% 
  mutate(pr_vampire_positive_b2 = pr_positive_vampire_b2 * 100 / pr_positive_b2) %>% 
  glimpse()

```

## Sampling from a grid-approximate posterior {#sec-sampling-from-a-grid-approximate-posterior}

### Original

#### Grid approximation {#sec-grid-approximation-a}

Before we are going to draw samples from the posterior distribution we
need to compute the distribution. Again we are using grid approximation.

```{r}
#| label: grid-approx-a
#| attr-source: '#lst-grid-approx-a lst-cap="Generate the posterior distribution form the globe-tossing example"' 

### R code 3.2 ##########################
# change prob_b to prior
# change prob_data to likelihood
# added variables: n, n_success, n_trials

n_grid_a <- 1000L
n_success_a <- 6L
n_trials_a <-  9L


p_grid_a <- seq(from = 0, to = 1, length.out = n_grid_a)
prior_a <- rep(1, n_grid_a) # = prior, assumed as uniform distribution
likelihood_a <- dbinom(n_success_a, size = n_trials_a, prob = p_grid_a) # = likelihood
posterior_a <- likelihood_a * prior_a
posterior_a <- posterior_a / sum(posterior_a)
```

> Now we wish to draw 10,000 samples from this posterior. Imagine the
> posterior is a bucket full of parameter values, numbers such as 0.1,
> 0.7, 0.5, 1, etc. Within the bucket, each value exists in proportion
> to its posterior probability, such that values near the peak are much
> more common than those in the tails. We're going to scoop out 10,000
> values from the bucket. Provided the bucket is well mixed, the
> resulting samples will have the same proportions as the exact
> posterior density. Therefore the individual values of *p* will appear
> in our samples in proportion to the posterior plausibility of each
> value.

#### Drawing samples

```{r}
#| label: draw-samples-a
#| attr-source: '#lst-draw-samples-a lst-cap="Draw 1000 Samples from the posterior distribution, using `base::set.seed(3)`"'
#| echo: fenced

n_samples_a <- 1e4

set.seed(3) # <1>

## R code 3.3 ##########################################
samples_a <- sample(p_grid_a, prob = posterior_a, 
                    size = n_samples_a, replace = TRUE) # <2>
```

1.  I have included the `base::set.seed()` command myself to provide
    reproducibility. With this code line you will get the same results
    in your sampling process because it is not really a random procedure
    but the outcome of a complex algorithm that can be configured by the
    `set.seed()` function.
2.  The workhorse here is `base::sample`, which randomly pulls values
    from a vector. The vector in this case is `p_grid_a`, the grid of
    1000 (1e3) parameter values. The probability of each value is given
    by `posterior_a`, which we computed with @lst-grid-approx-a.

To compare the calculated values with variant b (the tidyverse version),
I bound the three vectors with `base::cbind()` together into a matrix
and displayed the first six lines with `utils::head()`. Additionally I
also displayed the first 10 values of `samples_a` vector.

```{r}
#| label: display-grid_result_a

# display grid results to compare with variant b
d_a <- cbind(p_grid_a, prior_a, likelihood_a, posterior_a) 
head(d_a, 10)
```

Furthermore I also displayed the first 10 values of `samples_a` vector.

```{r}
#| label: display-samples_result_a

# display sample results to compare with variant b
head(samples_a, 10)
```

#### Plot samples distribution

```{r}
#| label: fig-scatterplot-samples-a
#| fig-cap: "Scatterplot of the drawn samples (Version a)"

## R code 3.4 #########
plot(samples_a)
```

In @fig-scatterplot-samples-a, it is as if you are flying over the
posterior distribution, looking down on it. There are many more samples
from the dense region near 0.6 and very few samples below 0.25.

```{r}
#| label: fig-density-samples-a
#| fig-cap: "Density estimate of the drawn samples (Version a)"

## R code 3.5 #############
rethinking::dens(samples_a)
```

Plot @fig-density-samples-a shows the density estimate computed from our
sampling process. The estimated density is very similar to ideal
posterior you computed via grid approximation. If you draw even more
samples, maybe 1e5 or 1e6, the density estimate will get more and more
similar to the ideal. This is shown in the tidyverse version
@fig-density-samples-b2.

### Tidyverse

#### Grid approximation {#sec-grid-approximation-b}

```{r}
#| label: grid-approx-b
#| attr-source: '#lst-grid-approx-b lst-cap="Grid aproximation: tidyverse"'

# how many grid points would you like?
n_grid_b <- 1000L
n_success_b <- 6L
n_trials_b  <- 9L

(
  d_b <-
  tibble(p_grid_b = seq(from = 0, to = 1, length.out = n_grid_b),
         # note we're still using a flat uniform prior
         prior_b  = 1) %>% 
  mutate(likelihood_b = dbinom(n_success_b, size = n_trials_b, prob = p_grid_b)) %>% 
  mutate(posterior_b = (likelihood_b * prior_b) / sum(likelihood_b * prior_b))
)

```

#### Drawing samples

::: callout-caution
###### Changing variable names

We've renamed McElreath's `prob_p` and `prob_data` as `prior_b` and
`likelihood_b`, respectively. Now we'll use the `dplyr::slice_sample()`
function to sample rows from `d_b`, saving them as `samples_b`.

To get the same variable name of the sample results in version a and b I
will change in the following code chunk `p_grid_b` to `samples_b`. So I
can compare the vector `samples_a` with the column `samples_b`of the
tibble with the same name (`samples_b`).

Additionally: To see the difference between grid and samples I will add
"\_sample" to all the other variable names. For reasons of consistence I
will also change the name of "prior_b"
:::

```{r}
#| label: draw-samples-b

# how many samples would you like?
n_samples_b <- 1e4

# make it reproducible
set.seed(3)

samples_b <-
  d_b %>% 
    slice_sample(n = n_samples_b, weight_by = posterior_b, replace = T)


( 
    samples_b <- samples_b |>
        rename(samples_b = p_grid_b,
               likelihood_samples_b = likelihood_b,
               prior_samples_prior_b = prior_b,
               posterior_samples_b = posterior_b)
)
```

The column `samples_b` of the tibble with the same name (watch out not
to confuse these two objects) is identical with the vector `samples_a`.
This `base::identical()` results in different sampling processes was the
work of `set.seed(3)`.

```{r}
#| label: test-if-samples-identical

identical(samples_a, samples_b$samples_b)
```

With the `utils::str()` function you will get a result with shorter
figures that is better adapted to a small screen.

```{r}
#| label: str-samples-b

str(samples_b)
```

An alternative of the tidyverse approach is `dplyr::glimpse()`.

```{r}
#| label: glimpse-samples-b

glimpse(samples_b)
```

Now we can plot the left panel of Figure 3.1 with
`ggplot2::geom_point()`. But before we do, we'll need to add a variable
numbering the samples.

#### Plot samples distribution

```{r}
#| label: fig-scatterplot-samples-b
#| fig-cap: "Scatterplot of the drawn samples (Version b)"

samples_b %>% 
  mutate(sample_number = 1:n()) %>% 
  
  ggplot(aes(x = sample_number, y = samples_b)) +
  geom_point(alpha = 1/10) +
  scale_y_continuous("proportion of water (p)", limits = c(0, 1)) +
  xlab("sample number")

```

We'll make the density in the right panel with
`ggplot2::geom_density()`.

```{r}
#| label: fig-density-samples-b
#| fig-cap: "Density estimate of the drawn samples (Version b)"

samples_b %>% 
  ggplot(aes(x = samples_b)) +
  geom_density(fill = "grey") +
  scale_x_continuous("proportion of water (p)", limits = c(0, 1))

```

Compare this somewhat smoother @fig-density-samples-b with
@fig-density-samples-a.

If we keep increasing the number of samples we will get a better
approximation to the ideal posterior distribution we have computed via
grid approximation. Here's what it looks like with `1e6`.

```{r}
#| label: fig-density-samples-b2
#| fig-cap: "Density estimate of 1e6 drawn samples (Version b)"

set.seed(3)

d_b %>% 
  slice_sample(n = 1e6, weight_by = posterior_b, replace = T) %>% 
  ggplot(aes(x = p_grid_b)) +
  geom_density(fill = "grey") +
  scale_x_continuous("proportion of water (p)", limits = c(0, 1))

```

## Sampling to Summarize {#sec-sampling-to-summarize}

### Three questions asked {.unnumbered}

All we have done so far is crudely replicate the posterior density we
had already computed in the previous chapter. Now it is time to use
these samples to describe and understand the posterior.

The first step in understanding the posterior distribution is to
summarize it. Exactly how it is summarized depends upon your purpose.
But common questions include:

-   How much posterior probability lies below some parameter value?
-   How much posterior probability lies between two parameter values?
-   Which parameter value marks the lower 5% of the posterior
    probability?
-   Which range of parameter values contains 90% of the posterior
    probability?
-   Which parameter value has highest posterior probability?

The above list of questions can be divided into three inquiries:

1.  Questions about intervals of defined boundaries.
2.  Questions about intervals of defined probability mass.
3.  Questions about point estimates.

### Intervals of Defined Boundaries

#### Original

##### Grid approach

For instance: What is the probability that the proportion of water is
less than 0.5?

```{r}
#| label: grid-boundaries-a

## R code 3.6 ##############################
# add up posterior probability where p < 0.5
sum(posterior_a[p_grid_a < 0.5])
```

About 17% of the posterior probability is below 0.5. Couldn't be easier.

##### Sampling approach

But this easy calculation based on grid approximation is often no
practical when there are more parameters. So let's try the sampling
approach:

To use the samples from the posterior you have to add up all of the
samples below 0.5, but also divide the resulting count by the total
number of samples. In other words, find the frequency of parameter
values below 0.5:

```{r}
#| label: sample-boundaries-0.5-a

## R code 3.7 #############################
(p_boundary_a <- sum(samples_a < 0.5) / 1e4)
```

::: callout-caution
###### Attention: Different values

In comparison with the value in the original book (0.1726) our value of
0.1629 is different. 17%.

The reason for the difference is that you can't get the same values in
sampling processes. This is the nature of randomness. And McElreath did
not include the set.sedd() function for (exact) reproducibility. What
additionally happened is that our `set.seed()`\` value of 3 results in a
somewhat untypical sampling distribution. I will demonstrate this with
additional three samples with different `set.seed()`\` values.
:::

##### Different sampling distributions

```{r}
#| label: different-sample-distributions

# four examples with different seeds
set.seed(42)
samples_a2 <- sample(p_grid_a, prob = posterior_a, size = 1e4, replace = TRUE)
set.seed(123)
samples_a3 <- sample(p_grid_a, prob = posterior_a, size = 1e4, replace = TRUE)
set.seed(1000)
samples_a4 <- sample(p_grid_a, prob = posterior_a, size = 1e4, replace = TRUE)
set.seed(33)
samples_a5 <- sample(p_grid_a, prob = posterior_a, size = 1e4, replace = TRUE)

(p_boundary_a2 <- sum(samples_a2 < 0.5) / 1e4)
(p_boundary_a3 <- sum(samples_a3 < 0.5) / 1e4)
(p_boundary_a4 <- sum(samples_a4 < 0.5) / 1e4)
(p_boundary_a5 <- sum(samples_a5 < 0.5) / 1e4)

# without setting a seed (not reproducible)
set.seed(0)
samples_a6 <- sample(p_grid_a, prob = posterior_a, size = 1e4, replace = TRUE)
samples_a7 <- sample(p_grid_a, prob = posterior_a, size = 1e4, replace = TRUE)
(p_boundary_a6 <- sum(samples_a6 < 0.5) / 1e4)
(p_boundary_a7 <- sum(samples_a7 < 0.5) / 1e4)
```

Although the second figure is near our "bad" result, all sampling
procedures with the chosen different `base::set.seed() values` return
better values (nearer the true value of 0.1718746) as our sampling
procedure fixed with `base::set.seed(3)`. The last two values are done
without setting a `set.seed()` value so my values should differ than
yours.

Using the same approach, you can ask how much posterior probability lies
between 0.5 and 0.75:

```{r}
#| label: sample-boundaries-0.5-0.75-a
#| attr-source: '#sample-boundaries2-a lst-cap="Find the frequency of parameter values below 0.5 with `base::set.seed(3)`"'
#| 
## R code 3.8 #########################################
(p_boundary_a8 <- sum(samples_a > 0.5 & samples_a < 0.75) / 1e4)
```

#### Tidyverse

##### Grid approach

> To get the proportion of water less than some value of `p_grid_b`
> within the {**tidyverse}**, you might first `filter()` by that value
> and then take the `sum()` within `summarise()`.

```{r}
#| label: grid-boundaries-b

# add up posterior probability where p < 0.5
d_b |> filter(p_grid_b < 0.5) |> 
    summarize(sum = sum(posterior_b))

```

##### Sampling approach

If what you want a frequency based on filtering by `samples_b`, then you
might use `n()` within `summarise()`.

```{r}
#| label: samples-boundaries-b

# add up all posterior probabilities of samples under .5
samples_b |> 
    filter(samples_b < .5) |> 
    summarize(sum = n() / n_samples_b)
```

A more explicit approach for the same computation is to follow up
`count()` with `mutate()`.

```{r}
#| label: samples-boundaries-b2

samples_b |> 
    count(samples_b < .5) |> 
    mutate(probability = n / sum(n))

```

An even trickier approach for the same is to insert the logical
statement `p_grid < .5` within the `mean()` function.

```{r}
#| label: samples-boundaries-b3

samples_b |> 
    summarize(sum = mean(samples_b < .5))

```

To determine the posterior probability between 0.5 and 0.75, you can use
`&` within `filter()`. Just multiply that result by 100 to get the value
in percent.

```{r}
#| label: samples-boundaries-b4

samples_b |> 
    filter(samples_b > .5 & samples_b < .75) |> 
    summarize(sum = n() / n_samples_b)

```

And, of course, you can do that with our `mean()` trick, too.

```{r}
#| label: samples-boundaries-b5

samples_b %>%
  summarise(percent = 100 * mean(samples_b > .5 & samples_b < .75))
```

##### Plot interval of defined boundaries

To produce Figure 3.2 of the book we apply following code lines:

```{r}
#| label: fig-upper-part-3.2
#| fig-cap: "Upper part of SR2 Figure 3.2: Posterior distribution produced with {**tidyverse**} tools: Left: The blue area is the posterior probability below a parameter value of 0.5. Right: The posterior probability between 0.5 and 0.75."

# upper left panel
p1 <-
  samples_b  |>  
  ggplot(aes(x = samples_b, y = posterior_samples_b)) +
  geom_line() +
  geom_area(data = samples_b  |>  filter(samples_b < .5), fill = "deepskyblue") +
  labs(x = "proportion of water (p)",
       y = "density")

# upper right panel
p2 <- 
  samples_b  |>  
  ggplot(aes(x = samples_b, y = posterior_samples_b)) +
  geom_line() +
  geom_area(data = samples_b  |>  filter(samples_b > .5 & samples_b < .75), fill = "deepskyblue") +
  labs(x = "proportion of water (p)",
       y = "density")

library(patchwork)
p1 + p2
```

### Intervals of Defined Probability Mass

#### Original

##### Quantiles

> Reporting an interval of defined mass is usually known as a
> **CONFIDENCE INTERVAL**. An interval of posterior probability, such as
> the ones we are working with, may instead be called a **CREDIBLE
> INTERVAL**.

> We're going to call it a **COMPATIBILITY INTERVAL** instead, in order
> to avoid the unwarranted implications of "confidence" and
> "credibility." What the interval indicates is a range of parameter
> values compatible with the model and data. The model and data
> themselves may not inspire confidence, in which case the interval will
> not either.

> For this type of interval, it is easier to find the answer by using
> samples from the posterior than by using a grid approximation. Suppose
> for example you want to know the boundaries of the lower 80% posterior
> probability. You know this interval starts at $p = 0$. To find out
> where it stops, think of the samples as data and ask where the 80th
> percentile lies:

```{r}
#| label: samples-quantile-a

## R code 3.9 #######################
quantile(samples_a, 0.8)
```

Similarly, the middle 80% interval lies between the 10th percentile and
the 90th percentile. These boundaries are found using the same approach:

```{r}
#| label: sample-quantile-a2

## R code 3.10 ###################
quantile(samples_a, c(0.1, 0.9))
```

Intervals of this sort, which assign equal probability mass to each
tail, are very common in the scientific literature. We'll call them
**PERCENTILE INTERVALS** (PI). These intervals do a good job of
communicating the shape of a distribution, as long as the distribution
isn't too asymmetrical. But in terms of describing the shape of the
posterior distribution---which is really all these intervals are asked
to do---the percentile interval can be misleading.

Consider the posterior distribution and different intervals in
@fig-skewed-dist-a. This posterior is consistent with observing three
waters in three tosses and a uniform (flat) prior. It is highly skewed,
having its maximum value at the boundary, $p = 1$. You can compute it,
via grid approximation.

##### Skewed distribution

```{r}
#| label: fig-skewed-dist-a
#| fig-cap: "Skewed posterior distribution observing three waters in three tosses and a uniform (flat) prior. It is highly skewed, having its maximum value at the boundary where p equals 1."

## R code 3.11 #####################
p_grid_skewed_a <- seq(from = 0, to = 1, length.out = 1000)
prior_skewed_a <- rep(1, 1000)
likelihood_skewed_a <- dbinom(3, size = 3, prob = p_grid_skewed_a)
posterior_skewed_a <- likelihood_skewed_a * prior_skewed_a
posterior_skewed_a <- posterior_skewed_a / sum(posterior_skewed_a)

set.seed(3) # added to make sampling distribution reproducible (pb)
samples_skewed_a <- sample(p_grid_skewed_a, size = 1e4, replace = TRUE, prob = posterior_skewed_a)



# added to show the skewed posterior distribution (pb)
rethinking::dens(samples_skewed_a)
```

##### Percentile (compatibility) Intervall (PI)

```{r}
#| label: rethinking-PI-a

## R code 3.12 ############################
rethinking::PI(samples_skewed_a, prob = 0.5)
```

The Percentile compatibility Interval (PI) of `prob = 0.5` assigns 25%
of the probability mass above and below the interval. So it provides the
central 50% probability.

It is just a shorthand for the base R `stats::quantile()` function:

```{r}
#| label: quantiles-PI-a

quantile(samples_skewed_a, prob = c(.25, .75))
```

> This \[percentile compability\] interval assigns 25% of the
> probability mass above and below the interval. So it provides the
> central 50% probability. But in this example, it ends up excluding the
> most probable parameter values, near $p = 1$. So in terms of
> describing the shape of the posterior distribution---which is really
> all these intervals are asked to do---the percentile interval can be
> misleading.

To see how it works we make another PI of `prob = 0.6`. We divide always
the percentage by 2 and subtract it from 50% respectively add this value
to 50%. The result is the probability mass between 20-80%.

```{r}
#| label: rethinking-PI-a2

rethinking::PI(samples_skewed_a, prob = 0.6)
```

But in these examples, we end up excluding the most probable parameter
values, near $p = 1$. So in terms of describing the shape of the
posterior distribution---which is really all these intervals are asked
to do---the percentile interval can be misleading.

##### Highest Posterior Density Interval (HPDI)

> \[In contrast, to PI the HPDI (HIGHEST POSTERIOR DENSITY INTERVAL)\]
> displays the narrowest interval containing the specified probability
> mass. If you think about it, there must be an infinite number of
> posterior intervals with the same mass. But if you want an interval
> that best represents the parameter values most consistent with the
> data, then you want the densest of these intervals. That's what the
> HPDI is. Compute it from the samples with HPDI (also part of
> rethinking):

```{r}
#| label: rethinking-HPDI-a
#| attr-source: '#lst-rethinking-HPDI-a lst-cap="Compute the HPDI from the samples distribution"'

## R code 3.13 ###############################
rethinking::HPDI(samples_skewed_a, prob = 0.5)
```

This interval captures the parameters with highest posterior
probability, as well as being noticeably narrower: 0.18 in width rather
than 0.28 for the percentile interval resp. 0.16 and 0.23 in the
original book version.

So the HPDI has some advantages over the PI. But in most cases, these
two types of interval are very similar. They only look so different in
this case because the posterior distribution is highly skewed. If we
instead used samples from the posterior distribution for six waters in
nine tosses, these intervals would be nearly identical. Try it for
yourself, using different probability masses, such as prob=0.8 and
prob=0.95.

```{r}
#| label: PI-HPDI-in-sym-dist

rethinking::PI(samples_a, prob = 0.8)
rethinking::HPDI(samples_a, prob = 0.8)

rethinking::PI(samples_a, prob = 0.95)
rethinking::HPDI(samples_a, prob = 0.95)
```

##### Difference between PI and HDPI

> When the posterior is bell shaped, it hardly matters which type of
> interval you use. Remember, we're not launching rockets or calibrating
> atom smashers, so fetishizing precision to the 5th decimal place will
> not improve your science.

> The HPDI also has some disadvantages. HPDI is more computationally
> intensive than PI and suffers from greater simulation variance, which
> is a fancy way of saying that it is sensitive to how many samples you
> draw from the posterior. It is also harder to understand and many
> scientific audiences will not appreciate its features, while they will
> immediately understand a percentile interval, as ordinary non-Bayesian
> intervals are typically interpreted (incorrectly) as percentile
> intervals (although see the Rethinking box below).

> Overall, if the choice of interval type makes a big difference, then
> you shouldn't be using intervals to summarize the posterior. Remember,
> the entire posterior distribution is the Bayesian "estimate." It
> summarizes the relative plausibilities of each possible value of the
> parameter. Intervals of the distribution are just helpful for
> summarizing it. If choice of interval leads to different inferences,
> then you'd be better off just plotting the entire posterior
> distribution.

##### Intervall mass of 95?

> The most common interval mass in the natural and social sciences is
> the 95% interval. This interval leaves 5% of the probability outside,
> corresponding to a 5% chance of the parameter not lying within the
> interval (although see below). This customary interval also reflects
> the customary threshold for statistical significance, which is 5% or p
> \< 0.05.

It is just a convention, there are no analytical reasons why you should
choose exactly this interval. But convenience is not a serious
criterion. So what to do instead?

> If you are trying to say that an interval doesn't include some value,
> then you might use the widest interval that excludes the value. Often,
> all compatibility intervals do is communicate the shape of a
> distribution. In that case, a series of nested intervals may be more
> useful than any one interval. For example, why not present 67%, 89%,
> and 97% intervals, along with the median? Why these values? No reason.
> They are prime numbers, which makes them easy to remember. But all
> that matters is they be spaced enough to illustrate the shape of the
> posterior. And these values avoid 95%, since conventional 95%
> intervals encourage many readers to conduct unconscious hypothesis
> tests.

##### Defined boundaries and probabilty mass

The difference between intervals of defined boundaries and intervals of
defined probability mass is that in the first case we ask for a
**probability of frequencies** whereas in the second case we calculate a
specified **amount of posterior probability**. As result from the first
question we get the percentage of the probability whereas the result of
the second question is the probability value of the percentage of
frequencies looked for.

The boundary intervals are grounded on the prob values ($0-1$) and
results in the percentage of the probability whereas the probability
mass intervals focus on the percentage of probabilities ($0-100$%) and
results in probability values.

#### Tidyverse

##### Quantiles

Since we saved our `samples_b` samples within the well-named `samples_b`
tibble, we'll have to index with `$` within `stats::quantile()`.

```{r}
#| label: sample-quantile-b

(q80 <- quantile(samples_b$samples_b, probs = .8))

```

For an alternative approach, we could `dplyr::select()` the `samples_b`
vector, extract it from the tibble with `dplyr::pull()`, and then pump
it into `stats::quantile()`.

> `pull()` is similar to `$`. It's mostly useful because it looks a
> little nicer in pipes, it also works with remote data frames, and it
> can optionally name the output.

```{r}
#| label: sample-quantile-b2

samples_b |> 
    pull(samples_b) |> 
    quantile(probs = .8)
    

```

We might also use `stats::quantile()` within `dplyr::summarise()`.

```{r}
#| label: sample-quantile-b3

samples_b |> 
    summarize(q80_2 = quantile(samples_b, probs = .8))

```

Here's the `summarise()` approach with two probabilities.

```{r}
#| label: sample-quantile-b4

samples_b |> 
    summarize(q10 = quantile(samples_b, probs = .1),
              q90 = quantile(samples_b, probs = .9))
    

```

You can also use the vector feature of R to summarize different
quantiles with one line. But Kurz's version is in the meanwhile
deprecated:

```{r}
#| label: sample-quantile-b5

samples_b |> 
    summarize(q10_90 = quantile(samples_b, probs = c(.1, .9)))

```

```{r}
#| label: sample-quantile-b6

samples_b |> 
    reframe(q10_90 = quantile(samples_b, probs = c(.1, .9)))
```

From the help file of `dplyr::reframe()`:

> While `summarise()` requires that each argument returns a single
> value, and `mutate()` requires that each argument returns the same
> number of rows as the input, `reframe()` is a more general workhorse
> with no requirements on the number of rows returned per group.
>
> `reframe()` creates a new data frame by applying functions to columns
> of an existing data frame. It is most similar to `summarise()`, with
> two big differences:
>
> -   `reframe()` can return an arbitrary number of rows per group,
>     while `summarise()` reduces each group down to a single row.
> -   `reframe()` always returns an ungrouped data frame, while
>     `summarise()` might return a grouped or rowwise data frame,
>     depending on the scenario.
>
> We expect that you'll use `summarise()` much more often than
> `reframe()`, but `reframe()` can be particularly helpful when you need
> to apply a complex function that doesn't return a single summary
> value.

See also the appropriate [section in the blog
post](https://www.tidyverse.org/blog/2023/02/dplyr-1-1-0-pick-reframe-arrange/#reframe)
about changes in {**dplyr**} 1.1.0. The name `reframe()` is in
accordance with `tibble::enframe()` and `tibble::deframe()`:

-   `enframe()`: Takes a vector, returns a data frame
-   `deframe()`: Takes a data frame, returns a vector
-   `reframe()`: Takes a data frame, returns a data frame

> The functions of the tidyverse approach typically returns a data
> frame. But sometimes you just want your values in a numeric vector for
> the sake of quick indexing. In that case, base R `stats::quantile()`
> shines:

```{r}
#| label: sample-quantile-b7

(q10_q90 = quantile(samples_b$samples_b, probs = c(.1, .9)))

```

##### Plot intervals of defined mass

Now we have our cutoff values saved as `q80`, respectively `q10` and
`q90`, we're ready to make the bottom panels of Figure 3.2.

```{r}
#| label: figure-3-2-lower-part

p1 <-
  samples_b %>% 
  ggplot(aes(x = samples_b, y = posterior_samples_b)) +
  geom_line() +
  geom_area(data = samples_b %>% filter(samples_b < q80), fill = "deepskyblue") +
  annotate(geom = "text",
           x = .25, y = .0025,
           label = "lower 80%") +
  labs(x = "proportion of water (p)",
       y = "density")

# upper right panel
p2 <- 
  samples_b %>% 
  ggplot(aes(x = samples_b, y = posterior_samples_b)) +
  geom_line() +
  geom_area(data = samples_b %>% filter(samples_b > q10_q90[[1]] & samples_b < q10_q90[[2]]), fill = "deepskyblue") +
  annotate(geom = "text",
           x = .25, y = .0025,
           label = "middle 80%") +
  labs(x = "proportion of water (p)",
       y = "density")

library(patchwork)
p1 + p2

```

##### Skewed distribution

Again we will demonstrate the misleading character of Pecentile
Intervals (PIs) with a very skewed distribution.

We've already defined `p_grid_b` and `prior_b` within `d_b`, above. Here
we'll reuse them and create a new tibble by updating all the columns
with the skewed parameters.

```{r}
#| label: samples-skewed-dist

n_samples_skewed_b <- 1e4
n_success_skewed_b <- 3
n_trials_skewed_b  <- 3

d_skewed_b <-
tibble(p_grid_skewed_b = seq(from = 0, to = 1, length.out = n_samples_skewed_b),
     # note we're still using a flat uniform prior
     prior_skewed_b  = 1) %>% 
mutate(likelihood_skewed_b = dbinom(n_success_skewed_b, size = n_trials_skewed_b, prob = p_grid_skewed_b)) %>% 
mutate(posterior_skewed_b = (likelihood_skewed_b * prior_skewed_b) / sum(likelihood_skewed_b * prior_skewed_b))

# make the next part reproducible
set.seed(3)

# here's our new samples tibble
(
  samples_skewed_b <-
    d_skewed_b %>% 
    slice_sample(n = n_samples_skewed_b, weight_by = posterior_skewed_b, replace = T)
)
```

```{r}
#| label: skewed-dist-b


# here we update the `dbinom()` parameters 
# for values for a skewed distribution
# assuming three trials results in 3 W (Water)
n_success_skewed_b <- 3
n_trials_skewed_b  <- 3

# update `d_b` to d_skewed_b
d_skewed_b <-
  d_b %>% 
  mutate(likelihood_skewed_b = dbinom(n_success_skewed_b, size = n_trials_skewed_b, prob = p_grid_b)) %>% 
  mutate(posterior_skewed_b  = (likelihood_skewed_b * prior_b) / sum(likelihood_skewed_b * prior_b))

# make the next part reproducible
set.seed(3)

# here's our new samples tibble
(
    samples_skewed_b <- 
        d_skewed_b %>% 
        slice_sample(n = n_samples_skewed_b, weight_by = posterior_skewed_b, replace = T) |> 
        rename(p_skewed_b = p_grid_b,
               prior_skewed_b = prior_b)
)

# added to see the skewed distribution
samples_skewed_b %>% 
  ggplot(aes(x = p_skewed_b, y = posterior_skewed_b)) +
  geom_line()
```

##### Reconsideration

To see the difference how the skewed distribution is different to the
Figure 3.2 lower part, I will draw the appropriate figure here myself.

```{r}
#| label: figure-3.2-skewed-lower-part

p1 <-
  samples_skewed_b %>% 
  ggplot(aes(x = p_skewed_b, y = posterior_skewed_b)) +
  geom_line() +
  geom_area(data = samples_skewed_b %>% filter(p_skewed_b < q80), fill = "deepskyblue") +
  annotate(geom = "text",
           x = .25, y = .0025,
           label = "lower 80%") +
  labs(x = "proportion of water (p)",
       y = "density")

# upper right panel
p2 <- 
  samples_skewed_b %>% 
  ggplot(aes(x = p_skewed_b, y = posterior_skewed_b)) +
  geom_line() +
  geom_area(data = samples_skewed_b %>% filter(p_skewed_b > q10_q90[[1]] & p_skewed_b < q10_q90[[2]]), fill = "deepskyblue") +
  annotate(geom = "text",
           x = .25, y = .0025,
           label = "middle 80%") +
  labs(x = "proportion of water (p)",
       y = "density")

library(patchwork)
p1 + p2
```

##### Introducing tidybayes

The [{**tidybayes**}](https://mjskay.github.io/tidybayes/) package
offers an array of convenience functions for summarizing Bayesian
models. For an introduction using {**tidybayes**} with {**brms**} see
[Extracting and visualizing tidy draws from {**brms**}
models](https://mjskay.github.io/tidybayes/articles/tidy-brms.html).

> {**tidybayes**} is an R package that aims to make it easy to integrate
> popular Bayesian modeling methods into a tidy data + ggplot workflow.
> It builds on top of (and re-exports) several functions for visualizing
> uncertainty from its sister package,
> [{**ggdist**}](https://mjskay.github.io/ggdist/).

I had difficulties to use Kurz's functions because there was an
[overhaul in the naming
scheme](https://mjskay.github.io/tidybayes/reference/tidybayes-deprecated.html)
of {**tidybayes**} version 1.0 and a deprecation of horizontal shortcut
geoms and stats in {**tidybayes**} 2.1. Because {**tidybayes**}
integrates function of the sister package {**ggdist**} the [function
descriptions and references of
{**ggdist**}](https://mjskay.github.io/ggdist/reference/index.html) are
also important to consult.

For the following parts the section on [Point Summaries and
Intervals](https://mjskay.github.io/tidybayes/articles/tidy-brms.html#point-summaries-and-intervals)
and the reference on [Point and interval summaries for tidy data frames
of draws from
distributions](https://mjskay.github.io/ggdist/reference/point_interval.html)
are especially important.

The {**tidybayes**} package contains a [family of
functions](https://mjskay.github.io/tidybayes/articles/tidy-brms.html#point-summaries-and-intervals)
that make it easy to summarize a distribution with a measure of central
tendency accompanied by intervals. With `tidybayes::median_qi()`, we
will ask for the median and quantile-based intervals --- just like we've
been doing with `stats::quantile()`.

::: callout-caution
Although Kurz uses the `samples` data frame I cannot reproduce it with
`samples_b` data frame, because has changed it values recently to the
skewed sampling version. To get the same results as Kurz I have to use
in my naming scheme the `skewed` version.
:::

```{r}
#| label: median-quantile-interval

tidybayes::median_qi(samples_skewed_b$p_skewed_b, .width = .5)
```

Note how the `.width` argument within `tidybayes::median_qi()` worked
the same way the `prob` argument did within `rethinking::PI()`. With
`.width = .5`, we indicated we wanted a quantile-based 50% interval,
which was returned in the `ymin` and `ymax` columns.

The {**tidybayes**} framework makes it easy to request multiple types of
intervals. In the following code chunk we'll request 50%, 80%, and 99%
intervals.

```{r}
#| label: multiple-intervals

tidybayes::median_qi(samples_skewed_b$p_skewed_b, .width = c(.5, .8, .99))

```

> The .width column in the output indexed which line presented which
> interval. The value in the y column remained constant across rows.
> That's because that column listed the measure of central tendency, the
> median in this case.

> Now let's use the `rethinking::HPDI()` function to return 50% highest
> posterior density intervals (HPDIs).

```{r}
#| label: rethinking-HPDI-b


rethinking::HPDI(samples_skewed_b$p_skewed_b, prob = .5)
```

::: callout-note
My results (0.8428428 and 1.0000000) are slightly different from the
output in Kurz's version (0.8418418, 0.9989990). I assume that these
differences are rounding errors. This happened although I had used the
`set.seed(3)` value for the Original and Tidyverse variants. TODO: CHECK
THIS OUT IN MORE DETAILS.
:::

> The reason I introduce {**tidybayes**} now is that the functions of
> the {**brms**} package only support percentile-based intervals of the
> type we computed with `quantile()` and `median_qi()`. But
> {**tidybayes**} also supports HPDIs.

::: callout-warning
The line `mode_hdi(samples$p_grid, .width = .5)` in Kurz's version is
not correct.

The correct code line is: `mode_hdci(samples$p_grid, .width = .5)`

> `hdi` yields the highest-density interval(s) (also known as the
> highest posterior density interval). Note: If the distribution is
> multimodal, `hdi` may return multiple intervals for each probability
> level (these will be spread over rows). You may wish to use `hdci` ...
> instead if you want a single highest-density interval, with the caveat
> that when the distribution is multimodal `hdci` is not a
> highest-density interval. (See [Point and interval summaries for tidy
> data frames of draws from
> distributions](https://mjskay.github.io/ggdist/reference/point_interval.html))

I have therefore changed my version from `tidybayes::mode_hci` to
`tidybayes::mode_hdci` .
:::

```{r}
#| label: tidyverse-HPDI-b

tidybayes::mode_hdci(samples_skewed_b$p_skewed_b, .width = .5)


```

This time we used the mode as the measure of central tendency. With this
family of {**tidybayes**} functions, you specify the measure of central
tendency in the prefix (i.e., mean, median, or mode) and then the type
of interval you'd like (i.e., `qi()` or `hdci()`).

If all you want are the intervals without the measure of central
tendency or all that other technical information, {**tidybayes**} also
offers the handy `qi()` and `hdi()` functions.

```{r}
#| label: tidybayes-qi-skewed-dist

tidybayes::qi(samples_skewed_b$p_skewed_b, .width = .5)
```

The `qi()` function worked for me and results in the same values as in
the Kurz's version. But with `hdi()` I get an error message in the
skewed version. (Tt worked in the normal version.) completely different
result:

```{r}
#| label: tidybayes-hdi-skewed-dist-error
#| error: true

# skewed version: 3 toss -> 3 Water (Success)
tidybayes::hdi(samples_skewed_b$p_skewed_b, na.rm = TRUE, .width = .5)

# original version: 9 toss -> 6 Water (Success)
tidybayes::hdi(samples_b$samples_b, na.rm = TRUE, .width = .5)
```

To work correctly I am calling always `hdci()` instead of `hdi()`:

```{r}
#| label: tidybayes-hdci-skewed-dist

tidybayes::hdci(samples_skewed_b$p_skewed_b, .width = .5)
```

These are the same values as in Kurz's version.

::: callout-warning
###### Error with `tidybayes::hdi()`

In the skewed version `tidybayes::hdi()` does not work for me in the
skewed version. I do not know why this is the case. But if I used
`tidybayes::hdci()` I've got the same result as Kurz.
:::

```{r}
#| label: figure-3.3

# left panel
p1 <-
  samples_skewed_b %>% 
  ggplot(aes(x = p_skewed_b, y = posterior_skewed_b)) +
  # check out our sweet `qi()` indexing
  geom_area(data = samples_skewed_b %>% 
              filter(p_skewed_b >
                tidybayes::qi(samples_skewed_b$p_skewed_b, .width = .5)[1] & 
                     p_skewed_b <
                tidybayes::qi(samples_skewed_b$p_skewed_b, .width = .5)[2]),
                fill = "deepskyblue") +
  geom_line() +
  labs(subtitle = "50% Percentile Interval",
       x = "proportion of water (p)",
       y = "density")

# right panel
p2 <-
  samples_skewed_b %>% 
  ggplot(aes(x = p_skewed_b, y = posterior_skewed_b)) +
  geom_area(data = . %>% 
              filter(p_skewed_b > 
                 tidybayes::hdci(samples_skewed_b$p_skewed_b, .width = .5)[1] & 
                       p_skewed_b < 
                 tidybayes::hdci(samples_skewed_b$p_skewed_b, .width = .5)[2]),
                 fill = "deepskyblue") +
  geom_line() +
  labs(subtitle = "50% HPDI",
       x = "proportion of water (p)",
       y = "density")

# combine!
library(patchwork)
p1 | p2

```

Again I had to change Kurz's version for the correct result: Instead of
`tidybayes::hci()` one must use `tidybayes::hdci()` for the right hand
plot.

Comparing the two panels of the plot you can see that in contrast to the
50% HPDI the 50% of PI does not include the highest probability value.

##### Dots and the Pipe

> In the geom_area() line for the HPDI plot, did you notice how we
> replaced `data = samples_skewed_b` with `data = .`? When using the
> pipe (i.e., `%>%`), you can use the `.` as a placeholder for the
> original data object. It's an odd and handy trick to know about.

(We could have made this change in both parts (p1 and p2) of the graph.
The only condition is that you have used the full name of the data frame
allready in the same piped statement.)

Learn more of the [pipe function %\>% of the {**magrittr**}
package](https://magrittr.tidyverse.org/reference/pipe.html) and about
the [base R native forward pipe
operator](https://stat.ethz.ch/R-manual/R-devel/library/base/html/pipeOp.html).

The native pipe is available starting with R 4.1.0. It is constructed
with `|` followed by `>` resulting in the symbol `|>` to differentiate
it from the {**magrittr**} pipe (%\>%). To understand the details of the
differences of `%>%` and the native R pipe `|>` read this elaborated
[blog article by Isabella
Velásquez](https://ivelasq.rbind.io/blog/understanding-the-r-pipe/index.html),
an employee of [Posit](https://posit.co/) (formerly RStudio).

##### Difference between PI and HPDI

PI and HPDI are only very different if you have a very skewed
distribution. Otherwise they are pretty similar. Let's check this
assertion:

::: callout-note
In this file we have named the variables in the version of the data
frame of 6`W` with 9 trials different than the variables of the version
of 3`W` with 3 trials. Therefore we do not need (re)calculate (= update)
the simulation as in Kurz's version.

Furthermore besides `tidybayes::mean_qi()` I will use
`tidybayes::mean_hdi()` and `tidybayes::mean_hdci()`.
:::

Try out my own variable names:

a)  Results rounded

```{r}
#| label: intervals-6-9-not-skewed-rounded

bind_rows(tidybayes::mean_hdci(samples_b$samples_b, .width = c(.8, .95)),
          tidybayes::mean_hdi(samples_b$samples_b, .width = c(.8, .95)),
          tidybayes::mean_qi(samples_b$samples_b,  .width = c(.8, .95))) %>% 
  select(.width, .interval, ymin:ymax) %>% 
  arrange(.width) %>% 
  mutate_if(is.double, round, digits = 2)
```

Sampling from a somewhat Gaussian distribution shows that there are
absolut no differences between `tidybayes::mean_hdci()` and
`tidybayes::mean_hdi()`.

There are two differences to Kurz's version: 1. `ymin` of 0.80 `hdi` is
in my version 0.48 and not 0.49. 2. `ymax` of 0.95 `hdi` is in my
version 0.90 and not 0.89.

I believe that this (very small) differences are rounding errors. But
not rounding errors in the result values but during the complex `hdi`
calculations as the following not rounded display of values demonstrate:

b)  Results not rounded

```{r}
#| label: intervals-6-9-not-skewed-not-rounded

bind_rows(tidybayes::mean_hdci(samples_b$samples_b, .width = c(.8, .95)),
          tidybayes::mean_hdi(samples_b$samples_b, .width = c(.8, .95)),
          tidybayes::mean_qi(samples_b$samples_b,  .width = c(.8, .95))) %>% 
  select(.width, .interval, ymin:ymax) %>% 
  arrange(.width)
```

But these small differences are not important. In McElreath words: \>
Remember, we're not launching rockets or calibrating atom smashers, so
fetishizing precision to the 5th decimal place will not improve your
science.

The same is true of the small differences between `qi` and `hdi`. They
difference is only 2 resp. 3 percent.

```{r}
#| label: intervals-3-9-skewed-version

bind_rows(tidybayes::mean_hdci(samples_skewed_b$p_skewed_b, .width = c(.8, .95)),
          tidybayes::mean_qi(samples_skewed_b$p_skewed_b,  .width = c(.8, .95))) %>% 
  select(.width, .interval, ymin:ymax) %>% 
  arrange(.width) %>% 
  mutate_if(is.double, round, digits = 2)
```

The skewed version shows bigger differences of 10% resp. 8% between
`mean_hdci()` and `mean_qi()`. (This calculation is missing in Kurz's
version.)

Because of the disadvantages of HPDI (more computationally intensive,
greater simulation variance and harder to understand) we'll primarily
stick to the PI-based intervals.

### Point Estimates

#### Original

> The third and final common summary task for the posterior is to
> produce point estimates of some kind. Given the entire posterior
> distribution, what value should you report? This seems like an
> innocent question, but it is difficult to answer. **The Bayesian
> parameter estimate is precisely the entire posterior distribution,
> which is not a single number, but instead a function that maps each
> unique parameter value onto a plausibility value.** So really the most
> important thing to note is that you don't have to choose a point
> estimate. It's hardly ever necessary and often harmful. It discards
> information. (emphasis is mine, pb)

But if you must do it ... we will take again the globe tossing
experiment in which we observe 3 waters out of 3 tosses, e.g. the very
skewed distribution.

##### Calcualte MAP

> First, it is very common for scientists to report the parameter value
> with highest posterior probability, a *maximum a posteriori* (MAP)
> estimate.

```{r}
#| label: calculate-skewed-MAP-grid-a

### actually the grid is not skewed. I should change the names accordingly.

## R code 3.14 ###############
p_grid_skewed_a[which.max(posterior_skewed_a)]
```

Or if you instead have samples from the posterior, you can still
approximate the same point:

```{r}
#| label: calculate-skewed-MAP-samples-a

## R code 3.15 ################
rethinking::chainmode(samples_skewed_a, adj = 0.01)
```

##### Calculate measures of central tendency

But why is the MAP, the mode, so interesting? Why not report the
posterior mean or median?

```{r}
#| label: mean-median-skewed-samples-a
#| attr-source: '#lst-mean-median-skewed-samples lst-cap="Posterior mean and median of the skewed distribution (all three draws are `W`)"'

## R code 3.16 #############
mean(samples_skewed_a)
median(samples_skewed_a)
```

##### Plot postponend

The graphical representation as shown in Figure 3.4 will be calculated
in the tidyverse version of this section. See: @fig-minimum-loss-b for
the left panel and @fig-minimum-loss2-b for the right panel of Figure
3.4.

> These are also point estimates, and they also summarize the posterior.
> But all three---the mode (MAP), mean, and median---are different in
> this case. How can we choose?

##### Loss function

One principled way to go beyond using the entire posterior as the
estimate is to choose a **LOSS FUNCTION**. A loss function is a rule
that tells you the cost associated with using any particular point
estimate. While statisticians and game theorists have long been
interested in loss functions, and how Bayesian inference supports them,
scientists hardly ever use them explicitly. The key insight is that
*different loss functions imply different point estimates*.

> Calculating expected loss for any given decision means using the
> posterior to average over our uncertainty in the true value. Of course
> we don't know the true value, in most cases. But if we are going to
> use our model's information about the parameter, that means using the
> entire posterior distribution. So suppose we decide $p = 0.5$ will be
> our decision. Then the expected loss will be:

```{r}
#| label: loss-expected-0.5-a

## R code 3.17 ##################
sum(posterior_skewed_a * abs(0.5 - p_grid_skewed_a))

```

> All the code above does is compute the weighted average loss, where
> each loss is weighted by its corresponding posterior probability.
> There's a trick for repeating this calculation for every possible
> decision, using the function sapply.

```{r}
#| label: loss-function-a

## R code 3.18 #############################
loss_a <- sapply(p_grid_skewed_a, function(d) sum(posterior_skewed_a * abs(d - p_grid_skewed_a)))
head(loss_a)
```

> Now the symbol `loss_a` contains a list of loss values, one for each
> possible decision, corresponding the values in `p_grid_skewed_a`. From
> here, it's easy to find the parameter value that minimizes the loss:

```{r}
#| label: minimize-loss-a

## R code 3.19 #############################
p_grid_skewed_a[which.min(loss_a)]
```

> And this is actually the posterior median, the parameter value that
> splits the posterior density such that half of the mass is above it
> and half below it.

We have already calculated the posterior median in
@lst-mean-median-skewed-samples. Because of sampling variation it is not
identical but pretty close (0.8428428 vs. 0.8408408).

**Learnings**:

> In order to decide upon a point estimate, a single-value summary of
> the posterior distribution, we need to pick a loss function. Different
> loss functions nominate different point estimates. The two most common
> examples are the absolute loss as above, which leads to the median as
> the point estimate, and the quadratic loss $(d - p)^{2}$, which leads
> to the posterior mean `(mean(samples))` as the point estimate.

> When the posterior distribution is symmetrical and normal-looking,
> then the median and mean converge to the same point, which relaxes
> some anxiety we might have about choosing a loss function. For the
> original globe tossing data (6 waters in 9 tosses), for example, the
> mean and median are barely different.

**Comparison of mean & median** with globe tossing data (6 waters in 9
tosses) versus (3 waters in 3 tosses):

6/9: Mean = `r mean(samples_a)` Median = `r median(samples_a)` 3/3: Mean
= `r mean(samples_skewed_a)` Median = `r median(samples_skewed_a)`

::: callout-important
Instead of deciding what parameter to report for summarizing the
posterior distribution it is usually better to communicate as much as
you can about the posterior distribution, as well as the data and the
model itself, so that others can build upon your work.

This advice is also valid if you just want to accept or not to accept an
hypothesis. Because the challenge then is to say what the relevant costs
and benefits would be, in terms of the knowledge gained or lost.
:::

#### Tidyverse

##### Calculate MAP

To get the MAP of the skewed version (three tosses with thre `W`) we
have to `arrange()` our `d_b` tibble in descending order by posterior.
Then we will see the corresponding p_grid_b value for its MAP estimate.

```{r}
#| label: calculate-skewed-MAP-grid-b

samples_skewed_b %>% 
  arrange(desc(posterior_skewed_b))
```

To emphasize it, we can use slice() to select the top row. The MAP value
is the column `posterior_skewed_b`.

```{r}
#| label: calculate-skewed-MAP-grid2-b

samples_skewed_b %>% 
  arrange(desc(posterior_skewed_b)) |> 
  slice(1)
```

##### Calculate measures of central tendency

We can get the mode with `mode_hdci()` or `mode_qi()`

```{r}
#| label: tidybayes-mode-qi-and-hdci

samples_skewed_b %>% tidybayes::mode_qi(p_skewed_b)
samples_skewed_b %>% tidybayes::mode_hdci(p_skewed_b)
```

Those returned a lot of output in addition to the mode. If all you want
is the mode itself, you can just use `tidybayes::Mode()`.

```{r}
#| label: tidybayes-mode

tidybayes::Mode(samples_skewed_b$p_skewed_b)
```

Medians and means are typical measures of central tendency, too.

```{r}
#| label: tidybayes-mean-and-median

samples_skewed_b %>% 
  summarise(mean   = mean(p_skewed_b),
            median = median(p_skewed_b))
```

##### Plot left panel of figure 3.4

1.  Bundle the three types of estimates into a tibble

```{r}
#| label: bundle-skewed-point-estimates

(
  point_estimates_b <-
  bind_rows(samples_skewed_b %>% tidybayes::mean_qi(p_skewed_b),
            samples_skewed_b %>% tidybayes::median_qi(p_skewed_b),
            samples_skewed_b %>% tidybayes::mode_qi(p_skewed_b)) %>% 
  select(p_skewed_b, .point) %>% 
  # these last two columns will help us annotate  
  mutate(x = p_skewed_b + c(-.03, .03, -.03),
         y = c(.0005, .0012, .002))
)
```

Plotting the results:

```{r}
#| label: fig-skewed-point-estimates_b
#| fig-cap: "Posterior distribution (blue) after observing 3 water in 3 tosses of the globe. Vertical lines show the locations of the mode, median, and mean. Each point implies a different loss function."

samples_skewed_b %>% 
  ggplot(aes(x = p_skewed_b)) +
  geom_area(aes(y = posterior_skewed_b),
            fill = "deepskyblue") +
  geom_vline(xintercept = point_estimates_b$p_skewed_b) +
  geom_text(data = point_estimates_b,
            aes(x = x, y = y, label = .point),
            angle = 90) +
  labs(x = "proportion of water (p)",
       y = "density") +
  theme(panel.grid = element_blank())
```

In contrast the other distribution with 6 successes by 9 trials
(tosses):

```{r}
#| label: fig-sym-point-estimates_b
#| fig-cap: "Point estimates in the almost symmetrical distribution of 6 successes (`W`) in 9 tosses."

(
    point_estimates_b <-
      bind_rows(samples_b %>% tidybayes::mean_qi(samples_b),
                samples_b %>% tidybayes::median_qi(samples_b),
                samples_b %>% tidybayes::mode_qi(samples_b)) %>% 
      select(samples_b, .point) %>% 
      # these last two columns will help us annotate  
      mutate(x = samples_b + c(-.03, .03, -.03),
             y = c(.0005, .0012, .002))
)

samples_b %>% 
  ggplot(aes(x = samples_b)) +
  geom_area(aes(y = posterior_samples_b),
            fill = "deepskyblue") +
  geom_vline(xintercept = point_estimates_b$samples_b) +
  geom_text(data = point_estimates_b,
            aes(x = x, y = y, label = .point),
            angle = 90) +
  labs(x = "proportion of water (p)",
       y = "density") +
  theme(panel.grid = element_blank())
```

##### Loss function

Let $p$ be the proportion of the Earth covered by water and $d$ be our
guess. If McElreath pays us \$100 if we guess exactly right but
subtracts money from the prize proportional to how far off we are, then
our loss is proportional to $d - p$. If we decide $d = .5$, we can
compute our expected loss.

```{r}
#| label: loss-epected-0.5-b

d_skewed_b |> 
    summarise(`expected loss` = sum(posterior_skewed_b * abs(0.5 - p_grid_b)))
```

The `map()` family of the [{**purrr**}
package](https://purrr.tidyverse.org/) is the tidyverse alternative to
the family of `apply()` functions from the base R framework. You can
learn more about how to use the `map()` family on different places:

-   [Purrr reference](https://purrr.tidyverse.org/reference/map.html):
    Apply a function to each element of a vector
-   [Purrr
    tutorial](https://jennybc.github.io/purrr-tutorial/ls01_map-name-position-shortcuts.html):
    Introduction to map(): extract elements
-   [University of Virginia
    Library](https://data.library.virginia.edu/getting-started-with-the-purrr-package-in-r/):
    Getting Started with the purrr Package in R

Calculate loss function

```{r}
#| label: loss-function-b

make_loss_b <- function(our_d) {
  d_skewed_b %>% 
    mutate(loss_b = posterior_skewed_b * abs(our_d - p_grid_b)) %>% 
    summarise(weighted_average_loss_b = sum(loss_b))
}

(
  l_b <-
  d_skewed_b %>% 
  select(p_grid_b) %>% 
  rename(decision_b = p_grid_b) %>% 
  mutate(weighted_average_loss_b = purrr::map(decision_b, make_loss_b)) %>% 
  unnest(weighted_average_loss_b) 
)
```

Calculate the minimum loss

```{r}
#| label: minimize-loss-b

# this will help us find the x and y coordinates for the minimum value
(
    min_loss_b <-
      l_b %>% 
      filter(weighted_average_loss_b == min(weighted_average_loss_b)) %>% 
      as.numeric()
)
```

Now we're ready for the right panel of Figure 3.4.

```{r}
#| label: fig-minimum-loss-b
#| fig-cap: "Expected loss under the rule that loss is proportional to absolute distance of decision (horizontal axis) from the true value. The point marks the value of `p` that minimizes the expected loss, the posterior median."

l_b %>%   
  ggplot(aes(x = decision_b, y = weighted_average_loss_b)) +
  geom_area(fill = "deepskyblue") +
  geom_vline(xintercept = min_loss_b[1], color = "black", linetype = 3) +
  geom_hline(yintercept = min_loss_b[2], color = "black", linetype = 3) +
  ylab("expected proportional loss") +
  theme(panel.grid = element_blank())
```

We saved the exact minimum value as `min_loss_b[1]`, which is
`r min_loss_b[1]`. Within sampling error, this is the posterior median
as depicted by our samples.

```{r}
#| label: posterior-skewed-median_b

samples_skewed_b %>% 
  summarise(posterior_median_b = median(p_skewed_b))
```

The quadratic loss $(d−p)^{2}$ suggests we should use the mean instead.
Let's investigate.

```{r}
#| label: fig-minimum-loss2-b
#| fig-cap: "Expected loss under the rule that loss is quadratic to the distance of decision (horizontal axis) from the true value. The point marks the value of `p` that minimizes the expected loss, the posterior mean"

# amend our loss function

make_loss2_b <- function(our_d2) {
  d_skewed_b %>% 
    mutate(loss2_b = posterior_skewed_b * (our_d2 - p_grid_b)^2) %>% 
    summarise(weighted_average_loss2_b = sum(loss2_b))
}


# remake our `l` data
l2_b <-
  d_skewed_b %>% 
  select(p_grid_b) %>% 
  rename(decision2_b = p_grid_b) %>% 
  mutate(weighted_average_loss2_b = purrr::map(decision2_b, make_loss2_b)) %>% 
  unnest(weighted_average_loss2_b)

# update to the new minimum loss coordinates

(
    min_loss2_b <-
      l2_b %>% 
      filter(weighted_average_loss2_b == min(weighted_average_loss2_b)) %>% 
      as.numeric()
)

# update the plot
l2_b %>%   
  ggplot(aes(x = decision2_b, y = weighted_average_loss2_b)) +
  geom_area(fill = "deepskyblue") +
  geom_vline(xintercept = min_loss2_b[1], color = "black", linetype = 3) +
  geom_hline(yintercept = min_loss2_b[2], color = "black", linetype = 3) +
  ylab("expected proportional loss") +
  theme(panel.grid = element_blank())
```

Based on quadratic loss $(d−p)^{2}$, the exact minimum value is
`r min_loss2_b[1]`. Within sampling error, this is the posterior mean of
our samples.

```{r}
#| label: posterior-skewed-mean_b

samples_skewed_b %>% 
  summarise(posterior_mean_b = mean(p_skewed_b))
```

## Sampling to simulate prediction

### Original

To generate implied observations from a model is useful for at least
five reasons:

1.  **Model design**: We can sample not only from the posterior, but
    also from the prior. Seeing what the model expects, before the data
    arrive, is the best way to understand the implications of the prior.
    We'll do a lot of this in later chapters, where there will be
    multiple parameters and so their joint implications are not always
    very clear.
2.  **Model checking**: After a model is updated using data, it is worth
    simulating implied observations, to check both whether the fit
    worked correctly and to investigate model behavior.
3.  **Software validation**: In order to be sure that our model fitting
    software is working, it helps to simulate observations under a known
    model and then attempt to recover the values of the parameters the
    data were simulated under.
4.  **Research design**: If you can simulate observations from your
    hypothesis, then you can evaluate whether the research design can be
    effective. In a narrow sense, this means doing power analysis, but
    the possibilities are much broader.
5.  **Forecasting**: Estimates can be used to simulate new predictions,
    for new cases and future observations. These forecasts can be useful
    as applied prediction, but also for model criticism and revision.

#### Dummy data

> Now note that these assumptions not only allow us to infer the
> plausibility of each possible value of *p*, after observation. That's
> what you did in the previous chapter. These assumptions also allow us
> to simulate the observations that the model implies. They allow this,
> because likelihood functions work in both directions. Given a realized
> observation, the likelihood function says how plausible the
> observation is. And given only the parameters, the likelihood defines
> a distribution of possible observations that we can sample from, to
> simulate observation. In this way, Bayesian models are always
> *generative*, capable of simulating predictions. Many non-Bayesian
> models are also generative, but many are not.
>
> We will call such simulated data **DUMMY DATA**, to indicate that it
> is a stand-in for actual data.

##### Probability of each globe toss

> Suppose $N = 2$, two tosses of the globe. Then there are only three
> possible observations: 0 water, 1 water, 2 water. You can quickly
> compute the probability of each, for any given value of *p*. Let's use
> $p = 0.7$, which is just about the true proportion of water on the
> Earth:

```{r}
#| label: dummy-data-a

## R code 3.20 #############################
dbinom(0:2, size = 2, prob = 0.7)
```

##### Simulation of globe tosses

> Now we're going to simulate observations, using these probabilities.
> This is done by sampling from the distribution just described above.
> You could use `sample()` to do this, but R provides convenient
> sampling functions for all the ordinary probability distributions,
> like the binomial.

```{r}
#| label: simulate-1-obs-a

set.seed(3) # for reproducibility

## R code 3.21 #############################
rbinom(1, size = 2, prob = 0.7)
```

(As the outcome results from a random process the above value differs
from the SR2 version. But with `set.seed(3)`\` you will get the same
value of $2$.)

> That $1$ means "2 water in 2 tosses." The "`r`" in `rbinom` stands for
> "random." It can also generate more than one simulation at a time. A
> set of 10 simulations can be made by:

```{r}
#| label: simulate-10-obs-a

set.seed(3)
## R code 3.22 #############################
rbinom(10, size = 2, prob = 0.7)
```

> Let's generate 100,000 dummy observations, just to verify that each
> value (0, 1, or 2) appears in proportion to its likelihood:

```{r}
#| label: simulate-1e5-obs-a

set.seed(3)
## R code 3.23 #############################
dummy_w_a <- rbinom(1e5, size = 2, prob = 0.7)
table(dummy_w_a) / 1e5
```

##### Plot simulation of globe tosses

We could use either the base R `graphics::hist()` or --- as in the SR2
book --- the `rethinking::simplehist()` function.

```{r}
#| label: fig-plot-hist-figure-3.5-a
#| fig-cap: "Distribution of simulated sample observations from 9 tosses of the globe. These samples assume the proportion of water is 0.7. The plot uses the base R `hist()` function"

set.seed(3)
dummy_w_a <- rbinom(1e5, size = 9, prob = 0.7)
hist(dummy_w_a, xlab = "dummy water count")
```

```{r}
#| label: fig-plot-simplehist-figure-3.5-a
#| fig-cap: "Distribution of simulated sample observations from 9 tosses of the globe. These samples assume the proportion of water is 0.7. the plot uses the rethinking::simplehist() function"

set.seed(3)
## R code 3.24 #############################
dummy_w_a <- rbinom(1e5, size = 9, prob = 0.7)
rethinking::simplehist(dummy_w_a, xlab = "dummy water count")
```

> Notice that most of the time the expected observation does not contain
> water in its true proportion, 0.7. That's the nature of observation:
> There is a one-to-many relationship between data and data-generating
> processes. You should experiment with sample size, the `size` input in
> the code above, as well as the prob, to see how the distribution of
> simulated samples changes shape and location.

> Many readers will already have seen simulated observations. **SAMPLING
> DISTRIBUTIONS** are the foundation of common non-Bayesian statistical
> traditions. In those approaches, inference about parameters is made
> through the sampling distribution. In this book, inference about
> parameters is never done directly through a sampling distribution. The
> posterior distribution is not sampled, but deduced logically. Then
> samples can be drawn from the posterior, as earlier in this chapter,
> to aid in inference. In neither case is "sampling" a physical act. In
> both cases, it's just a mathematical device and produces only *small
> world* (@sec-small-and-large-worlds) numbers.

#### Model checking

> MODEL CHECKING means (1) ensuring the model fitting worked correctly
> and (2) evaluating the adequacy of a model for some purpose. Since
> Bayesian models are always *generative*, able to simulate observations
> as well as estimate parameters from observations, once you condition a
> model on data, you can simulate to examine the model's empirical
> expectations.

> We'd like to *propagate* the parameter uncertainty---carry it
> forward---as we evaluate the implied predictions. All that is required
> is averaging over the posterior density for `p`, while computing the
> predictions. For each possible value of the parameter `p`, there is an
> implied distribution of outcomes. So if you were to compute the
> sampling distribution of outcomes at each value of `p`, then you could
> average all of these prediction distributions together, using the
> posterior probabilities of each value of `p`, to get a POSTERIOR
> PREDICTIVE DISTRIBUTION.

The reproduction of FIGURE 3.6 that illustrates this averaging is shown
in ref###.

> we need to learn how to combine sampling of simulated observations, as
> in the previous section, with sampling parameters from the posterior
> distribution. We expect to do better when we use the entire posterior
> distribution, not just some point estimate derived from it.

So how do you actually do the calculations?

```{r}
#| label: sim-pred-values-a

set.seed(3)
## R code 3.25 #############################
w_a <- rbinom(1e4, size = 9, prob = 0.6)
rethinking::simplehist(w_a)
```

> This generates 10,000 (1e4) simulated predictions of 9 globe tosses
> (size=9), assuming $p = 6$. The predictions are stored as counts of
> water, so the theoretical minimum is zero and the theoretical maximum
> is nine.

We used `rethinking::`simplehist(w_a)\` to get a clean histogram of the
simulated outcomes.

> All you need to propagate parameter uncertainty into these predictions
> is replace the value 0.6 with samples from the posterior:

```{r}
#| label: fig-sim-pred-samples-a
#| fig-cap: "Random binomial samples to simulate predicted observations for $p = 0.6$"
#| attr-source: '#lst-sim-pred-samples-a lst-cap="Generate 1e4 random binomial samples to simulate predicted observations for $p = 0.6$"'

set.seed(3)
## R code 3.26 #############################
w2_a <- rbinom(1e4, size = 9, prob = samples_a)
rethinking::simplehist(w2_a)
```

The symbol `samples_a` above is the same list of random samples from the
posterior distribution that we have calculated in @lst-draw-samples-a
and used in previous sections.

> For each sampled value, a random binomial observation is generated.
> Since the sampled values appear in proportion to their posterior
> probabilities, the resulting simulated observations are averaged over
> the posterior. You can manipulate these simulated observations just
> like you manipulate samples from the posterior---you can compute
> intervals and point statistics using the same procedures.

> The simulated model predictions are quite consistent with the observed
> data in this case---the actual count of 6 lies right in the middle of
> the simulated distribution. ... So far, we've only viewed the data
> just as the model views it: Each toss of the globe is completely
> independent of the others. This assumption is questionable.

> So with the goal of seeking out aspects of prediction in which the
> model fails, let's look at the data in two different ways. Recall that
> the sequence of nine tosses was `W L W W W L W L W`. First, consider
> the length of the longest run of either water or land. This will
> provide a crude measure of correlation between tosses. So in the
> observed data, the longest run is 3 W's. Second, consider the number
> of times in the data that the sample switches from water to land or
> from land to water. This is another measure of correlation between
> samples. In the observed data, the number of switches is 6. There is
> nothing special about these two new ways of describing the data. They
> just serve to inspect the data in new ways. In your own modeling,
> you'll have to imagine aspects of the data that are relevant in your
> context, for your purposes.

FIGURE 3.7 showing the simulated predictions, viewed in these two new
ways, is reproduced in ref###. I have also postponed the interpretation
of Figure 3.7 to ref###, because it is more understandable viewing the
plots.

### Tidyverse

#### Dummy data

> Dummy data for the globe tossing model arise from the binomial
> likelihood.

##### Probability of each globe toss

Suppose $N = 2$, two tosses of the globe.

```{r}
#| label: dummy-data-b

tibble(n      = 2,
       `p(w)` = .7,
       w      = 0:n) %>% 
  mutate(density = dbinom(w, size = n, prob = `p(w)`))
```

##### Simulation of globe tosses

Simulate one globe toss:

```{r}
#| label: simulate-1-obs-b

set.seed(3)
rbinom(1, size = 2, prob = .7)
```

Simulate 10 globe tosses.

```{r}
#| label: simulate-10-obs-b

set.seed(3)
rbinom(10, size = 2, prob = .7)
```

Now generate 100,000 (i.e., 1e5) reproducible dummy observations.

```{r}
#| label: simulate-1e5-obs-b

n_draws_b <- 1e5

set.seed(3)
dummy_w_b <- tibble(draws = rbinom(n_draws_b, size = 2, prob = .7)) 
    
    dummy_w_b |> 
        count(draws) |> 
        mutate(proportion = n / nrow(dummy_w_b))
```

##### Plot simulation of globe tosses

The simulation updated to $n=9$ and plotting the tidyverse version of
Figure 3.5.

```{r}
#| label: fig-plot-ggplot2-figure-3.5-b
#| fig-cap: "Distribution of simulated sample observations from 9 tosses of the globe. These samples assume the proportion of water is 0.7. The plot uses the {**ggplot2**} functions. The left panel is Kurz's original, the right one is my version slightly changed."

n_draws_b <- 1e5

set.seed(3)
dummy_w2_b <- tibble(draws = rbinom(n_draws_b, size = 9, prob = .7))

p1 <- dummy_w2_b |> 
    ggplot(aes(x = draws)) + 
    geom_histogram(binwidth = 1, center = 0,
                 fill = "deepskyblue", color = "black", 
                 linewidth = 1/10) +
    # breaks = 0:10 * 2 = equivalent in Kurz's versions:  breaks = 0:4 * 2
    scale_x_continuous("dummy water count", breaks = 0:10 * 2) +
    ylab("frequency") +
    coord_cartesian(xlim = c(0, 9)) +
    theme(panel.grid = element_blank())

p2 <- dummy_w2_b |> 
    ggplot(aes(x = draws)) + 
    geom_histogram(binwidth = 1, center = 0,
                 fill = "deepskyblue", color = "black", 
                 linewidth = 1/10) +
    ## breaks = 0:10 * 2 = equivalent in Kurz's versions:  breaks = 0:4 * 2
    ## I decided to set a break at each of the draws: breaks = 0:9 * 1
    scale_x_continuous("dummy water count", breaks = 0:9 * 1) +
    ylab("frequency") +
    ## I did not zoom into the graph because doesn't look so nice
    ## for instance the last line is not visible
    # coord_cartesian(xlim = c(0, 9)) +
    theme(panel.grid = element_blank())

library(patchwork)
p1 + p2
```

##### Simulating and plotting 9 conditions

McElreath suggested we play around with different values of `size` and
`prob`. The next block of code simulates nine conditions.

```{r}
#| label: fig-simulate-9-conditions
#| fig-cap: "Distribution of 9 simulated sample observations, using different size (number of tosses: 3, 6, 9) and different probabilities (prob: 0.3, 0.6, 0.9)."

n_draws <- 1e5

simulate_binom <- function(n, probability) {
  set.seed(3)
  rbinom(n_draws, size = n, prob = probability) 
}

d9_b <-
  crossing(n9_b           = c(3, 6, 9),
           probability9_b = c(.3, .6, .9)) %>% 
  mutate(draws9_b = map2(n9_b, probability9_b, simulate_binom)) %>% 
  ungroup() %>% 
  mutate(n           = str_c("n = ", n9_b),
         probability = str_c("p = ", probability9_b)) %>% 
  unnest(draws9_b)

d9_b |> 
    slice_sample(n = 10) |> 
    arrange(n9_b)

```

I am still not very experienced with `tidyr::crossing()` and
`tidyr::unnest()`:

-   **`crossing()`** is a wrapper around `expand_grid()` and therefore
    creates a tibble from all combination of inputs. In addition to
    `expand_grid()` it de-duplicates and sorts its input.
-   **`unnest()`** expands a list-column containing data frames into row
    and columns. In the above case `map2()` returns a list and stores
    the data in `draws9_b`.

Instead of `head()` I used `dplyr::slice_sample()` and ordered the
result by the first column. I think this will get a better glimpse on
the data as just the first 6 rows.

```{r}
#| label: fig-sim-plot-9-cond-b

d9_b %>% 
  ggplot(aes(x = draws9_b)) +
  geom_histogram(binwidth = 1, center = 0,
                 color = "deepskyblue", linewidth = 1/10) +
  scale_x_continuous("dummy water count", breaks = 0:4 * 2) +
  ylab("frequency") +
  coord_cartesian(xlim = c(0, 9)) +
  theme(panel.grid = element_blank()) +
  facet_grid(n9_b ~ probability9_b)
```

#### Model checking

On software checking Kurz refers to some material, that is too special
for me. So I do not include it here. Maybe I will come later here again
when I have more experiences with Bayesian statistics and the necessary
tools.

##### Reproduction of Figure 3.6

At first I thought I do not need to refresh the original grid
approximation from @lst-grid-approx-b as I have it stored it with the
unique name `d_b`. But it turned out that the above code with
`n_grid_b = 1000L` does not work, because it draws no vertical lines by
the posterior density. Instead one has to sample 1001 times.

Actually I do not know why this (small) difference is necessary, but I
noticed that with most sample numbers the plot does not work correctly.
It worked with 1071. The sequence 1011, 1021, 1031, 1041, 1051, 1061
misses just one vertical line at .7 Ab exception is 1041, which misses
.6.

So I have to crate the data with 1001 samples again before I produce the
plot.

```{r}
#| label: fig-repr-figure-3.6-top-b
#| fig-cap: "Reproduction of the top part of Figure 3.6"

n2_b <- 1001L
n_success <- 6L
n_trials  <- 9L

(
  d2_b <-
  tibble(p_grid2_b = seq(from = 0, to = 1, length.out = n2_b),
         # note we're still using a flat uniform prior
         prior2_b  = 1) %>% 
  mutate(likelihood2_b = dbinom(n_success, size = n_trials, prob = p_grid2_b)) %>% 
  mutate(posterior2_b = (likelihood2_b * prior2_b) / sum(likelihood2_b * prior2_b))
)

d2_b %>% 
  ggplot(aes(x = p_grid2_b, y = posterior2_b)) +
  geom_area(color = "deepskyblue", fill = "deepskyblue") +
  geom_segment(data = . %>% 
                 filter(p_grid2_b %in% c(seq(from = .1, to = .9, by = .1), 3 / 10)),
               aes(xend = p_grid2_b, yend = 0, linewidth = posterior2_b),
               color = "black", show.legend = F) +
  geom_point(data = . %>%
               filter(p_grid2_b %in% c(seq(from = .1, to = .9, by = .1), 3 / 10))) +
  annotate(geom = "text", 
           x = .08, y = .0025,
           label = "Posterior probability") +
  scale_linewidth_continuous(range = c(0, 1)) +
  scale_x_continuous("probability of water", breaks = 0:10 / 10) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank())
```

We'll need to do a bit of wrangling before we're ready to make the plot
in the middle panel of Figure 3.6.

GAP HERE! (2023-07-31) THERE IS A GAP BECAUSE THE FOLLOWING CODE CHUNKS
OF CHAP 3.3 (MOSTLY DRAWINGS) ARE FOR MY PURPOSE (LEARNING BAYESIAN
STATISTICS) NOT RELEVANT.

## Synopsis

The third chapter teaches the basic skills for working with samples from
the posterior distribution. The posterior distribution is a probability
distribution. And like all probability distributions, we can imagine
drawing samples from it.

### Sampling from a grid-approximate posterior

Before beginning to work with samples, we need to generate them with
@lst-grid-approx-base-demo respectively with
@lst-grid-approx-tidyverse-demo. Then we can draw - for instance -
10,000 samples from the posterior. To make the random draws reproducible
we will use `base::set.seed()` with a see of 3.

#### Base R

```{r}
#| label: draw-samples-base-demo
#| attr-source: '#lst-draw-samples-base-demo lst-cap="Drawing 10,000 sample from the posterior using `base::sample()`"'

## R code 3.2 ###########################
## compute the posterior for the globe tossing model, using grid approximation
p_grid_r <- seq(from = 0, to = 1, length.out = 1000)
prob_p <- rep(1, 1000)
prob_data <- dbinom(6, size = 9, prob = p_grid_r)
posterior <- prob_data * prob_p
posterior_r <- posterior / sum(posterior)

set.seed(3)

## R code 3.3 ###########################
## draw 10,000 samples from the posterior.
samples_r <- sample(p_grid_r, prob = posterior_r, size = 1e4, replace = TRUE)
```

#### Tidyverse

```{r}
#| label: draw-samples-tidyverse-demo
#| attr-source: '#lst-draw-samples-tidyverse-demo lst-cap="Drawing 10,000 sample from the posterior using `dplyr::slice_sample()`"'

# how many grid points would you like?
n <- 1000
n_success <- 6
n_trials  <- 9

  d_t <-
  tibble(p_grid_t = seq(from = 0, to = 1, length.out = n),
         # note we're still using a flat uniform prior
         prior_t  = 1) %>% 
  mutate(likelihood_t = dbinom(n_success, size = n_trials, prob = p_grid_t)) %>% 
  mutate(posterior_t = (likelihood_t * prior_t) / sum(likelihood_t * prior_t))

set.seed(3)
samples_t <-
  d_t %>% 
    slice_sample(n = 1e4, weight_by = posterior_t, replace = T)
```

### Sampling to summarize

The next step after sampling is to summarize and interpret the posterior
distribution. The type of summary depends upon your purpose. But common
questions include:

-   How much posterior probability lies below some parameter value?
-   How much posterior probability lies between two parameter values?
-   Which parameter value marks the lower 5% of the posterior
    probability?
-   Which range of parameter values contains 90% of the posterior
    probability?
-   Which parameter value has highest posterior probability?

These questions can be divided into :

1.  Questions about intervals of *defined boundaries*
    (\@sec-intervals-of-defined-boundaries)
2.  Qustions about Intervals of *defined probability mass*
    (\@sec-intervals-of-defined-probability-mass) and
3.  Questions about *point estimates* (\@sec-point-estimates)

#### Intervals of defined boundaries {#sec-intervals-of-defined-boundaries}

For instance I will ask for the posterior probability that the
proportion of water is less than 0.5. We could use the grid-approximate
posterior to just add up all all of the probabilities, where the
corresponding parameter value is less than 0.5

But since grid approximation isn't practical in general, it won't always
be so easy. Once there is more than one parameter in the posterior
distribution, even this simple sum is no longer very simple. So wee need
to perform the same calculation, using samples from the posterior.

##### Base R: Define boundary

Using the grid-approximate posterior:

```{r}
#| label: define-boundary-from-grid-base-demo
#| attr-source: '#lst-define-boundary-from-grid-base-demo lst-cap="Define boundary by using the grid-approximate posterior with base R functions"'

## R code 3.6 #############################
# add up posterior probability where p < 0.5
sum(posterior_r[p_grid_r < 0.5])
```

Using the samples from the posterior:

```{r}
#| label: define-boundary-from-samples-base-demo
#| attr-source: '#lst-define-boundary-from-samples-base-demo lst-cap="Define boundary by summing up the samples from the posterior with base R functions"'

## R code 3.7 #############################
## sum samples of posterior probability where p < 0.5
sum(samples_r < 0.5) / 1e4


## R code 3.8 #############################
## find boundaries samples of posterior probability 
## where p lies between 0.5 and 0.75
sum(samples_r > 0.5 & samples_r < 0.75) / 1e4

```

##### Tidyverse: Define boundary

```{r}
#| label: define-boundary-from-grid-tidyverse-demo
#| attr-source: '#lst-define-boundary-from-grid-tidyverse-demo lst-cap="Define boundary by using the grid-approximate posterior with {tidyverse} functions"'

## add up posterior probability where p < 0.5
d_t %>% 
  filter(p_grid_t < .5) %>% 
  summarise(sum = sum(posterior_t))
```

Using the samples from the posterior:

```{r}
#| label: define-boundary-from-samples-tidyverse-demo
#| attr-source: '#lst-define-boundary-from-samples-tidyverse-demo lst-cap="Define boundary by summing up the samples from the posterior with {tidyverse} function `filter()` and `summarize()`"'

## filter and summarize samples of posterior probability where p < 0.5
samples_t %>%
  filter(p_grid_t < .5) %>% 
  summarise(sum = n() / 1e4)

## filter and summarize samples of posterior probability 
## where p lies between 0.5 and 0.75
samples_t %>% 
  filter(p_grid_t > .5 & p_grid_t < .75) %>% 
  summarise(sum = n() / 1e4)
```

A more explicit approach for the same computation is to follow up
`dplyr::count()` with `dplyr::mutate()`.

```{r}
#| label: define-boundary-from-samples-tidyverse2-demo
#| attr-source: '#lst-define-boundary-from-samples-tidyverse2-demo lst-cap="Define boundary by summing up the samples from the posterior with {tidyverse} functions `count()` and `mutate()`"'
#| eval: false

## count and sum samples of posterior probability where p < 0.5
samples_t %>% 
  count(p_grid_t < .5) %>% 
  mutate(probability = n / sum(n))

## count and sum samples of posterior probability 
## where p lies between .5 and .75
samples_t %>% 
  count(p_grid_t > .5 & p_grid_t < .75) %>% 
  mutate(probability = n / sum(n))
```

#### Intervals of defined probability mass {#sec-intervals-of-defined-probability-mass}

In scientific journals it is usual to report an interval of defined
mass, usually known as a confidence interval. What the interval
indicates is a range of parameter values compatible with the model and
data. The model and data themselves may not inspire confidence, in which
case the interval will not either. McElreath therefore call these areas
compatibility intervals.

These posterior intervals report two parameter values that contain
between them a specified amount of posterior probability, a probability
mass. For this type of interval, it is easier to find the answer by
using samples from the posterior than by using a grid approximation.

##### Base R: Define probability mass

```{r}
#| label: PI-base-demo
#| attr-source: '#lst-PI-base-demo lst-cap="Boundaries of the lower 80% and middle 80% posterior probability: Base R version"'

## R code 3.9 #######################
## find boundaries of the lower 80% posterior probability
quantile(samples_r, 0.8)

## R code 3.10 ######################
## find the interval of the middle 80% posterior probability
quantile(samples_r, c(0.1, 0.9))
```

Intervals of this sort, which assign equal probability mass to each
tail, are very common in the scientific literature. We'll call them
**percentile intervals (PI)**. These intervals do a good job of
communicating the shape of a distribution, as long as the distribution
isn't too asymmetrical.

But in highly skewed distribution they are --- in terms of supporting
inferences about which parameters are consistent with the data --- not
perfect. It could be the case that they do not include the most probable
parameter variable (the mode or MAP, Maximum A Posterior). In this case
you should use the **highest posterior density interval (HPDI)**.

```{r}
#| label: HPDI-base-demo
#| attr-source: '#lst-HPDI-base-demo lst-cap="Boundaries of the lower 80% and middle 80% posterior probability: Base R version"'

## R code 3.13 ##################################
## find the narrowest region with 50% of the posterior probability
rethinking::HPDI(samples_r, prob = 0.5)
```

##### Tidyverse: Define probability mass

As in the base R version the tidyverse variant is also using the
`quantile()`\` function. The only difference is the call of the correct
vector. Since `p_grid` samples are saved in the `samples` tibble, we'll
have to index with `$` within `quantile()`.

```{r}
#| label: PI-tidyverse-demo
#| attr-source: '#lst-PI-tidyverse-demo lst-cap="Boundaries of the lower 80% and middle 80% posterior probability: tidyverse version"'

## find boundaries of the lower 80% posterior probability
quantile(samples_t$p_grid_t, prob = .8)

## find the interval of the middle 80% posterior probability
quantile(samples_t$p_grid_t, prob = c(.1, .9))
```

For calculating the HPDI we will use the {**tidybayes**} package. which
offers an array of convenience functions for summarizing Bayesian
models.

```{r}
#| label: HPDI-tidyverse-demo
#| attr-source: '#lst-HPDI-tidyverse-demo lst-cap="Boundaries of the lower 80% and middle 80% posterior probability: Base R version"'

## find the narrowest region with 50% of the posterior probability
tidybayes::mode_hdci(samples_t$p_grid_t, .width = .5)
```

#### Point Estimates {#sec-point-estimates}

Point estimates are of limited value: The Bayesian parameter estimate is
precisely the entire posterior distribution, which is not a single
number, but instead a function that maps each unique parameter value
onto a plausibility value. So really the most important thing to note is
that you don't have to choose a point estimate. It's hardly ever
necessary and often harmful. It discards information.

##### Base R

###### Find the mode (MAP) from the grid approximation

```{r}
#| label: MAP-grid-base-demo
#| attr-source: '#lst-MAP-grid-base-demo lst-cap="Find value with the highest posterior probability, also called maximum a posteriori (MAP) estimate, from the grid approximation: Base R version"'

## R code 3.14 ######################
## find highest posterior probability (MAP) from grid approx
p_grid_r[which.max(posterior_r)]
```

###### Find the MAP (mode) from the samples from the posterior

```{r}
#| label: MAP-posterior-rethinking-demo
#| attr-source: '#lst-MAP-posterior-rethinking-demo lst-cap="Find value with the highest posterior probability, also called maximum a posteriori (MAP) estimate, from the posterior: rethinking version"'

## R code 3.15 ######################
## find highest posterior probability (MAP) from the posterior
rethinking::chainmode(samples_r, adj = 0.01)
```

###### Loss function

But why is this point, the mode, interesting? Why not report the
posterior mean or median? These are also point estimates, and they also
summarize the posterior. But all three---the mode (MAP), mean, and
median---are different in this case. How can we choose?

One principled way to go beyond the both extremes (choosing just a point
estimate or using the entire posterior as the estimate) is to choose a
**loss function**. A loss function is a rule that tells you the cost
associated with using any particular point estimate. *Different loss
functions imply different point estimates!*

So suppose we decide $p = 0.5$ will be our decision:

```{r}
#| label: loss-0.5-base-demo
#| attr-source: '#lst-loss-0.5-base-demo lst-cap="Find loss for the decision 0.5: base version"'

## R code 3.17
## loss with decision .05
sum(posterior_r * abs(0.5 - p_grid_r))
```

Calculating the loss function is repeating the calculation of
@lst-loss-0.5-base-demo for every possible decision. Then you can
determine the value for the decision with minimum loss.

```{r}
#| label: loss-minimum-base-demo
#| attr-source: '#lst-loss-minimum-base-demo lst-cap="Find decision for minimum loss: base version"'

## R code 3.18
loss_r <- sapply(p_grid_r, function(d) sum(posterior_r * abs(d - p_grid_r)))

## R code 3.19
p_grid_r[which.min(loss_r)]

```

And this is actually the posterior median, the parameter value that
splits the posterior density such that half of the mass is above it and
half below it.

##### Tidyverse

###### Find the mode (MAP) from the posterior samples

Three different ways to find the mode (MAP) with {**tidybayes**}:

```{r}
#| label: MAP-tidyverse-demo
#| attr-source: '#lst-MAP-tidyverse-demo lst-cap="Find value with the highest posterior probability, also called maximum a posteriori (MAP) estimate, with three different functions: tidybayes version"'
#| eval: false

samples_t |> 
    tidybayes::mode_qi(p_grid_t)

samples_t |> 
    tidybayes::mode_hdci(p_grid_t)

## just the mode
tidybayes::Mode(samples_t$p_grid_t)
```

###### Loss function

Find loss with the decision $p = 0.5$.

```{r}
#| label: loss-0.5-tidyverse-demo
#| attr-source: '#lst-loss-0.5-tidyverse-demo lst-cap="Find loss for the decision 0.5: tidyverse version"'

## find loss with the decision of .5
d_t %>% 
  summarise(`expected loss` = sum(posterior_t * abs(0.5 - p_grid_t)))
```

Find minimum loss: The tidyverse version uses for the loop instead of
`base::sapply()` the `purrr::map()` function from the {**purrr}**
package, a member of the {**tidyverse}** packages.

```{r}
#| label: loss-minimum-tidyverse-demo
#| attr-source: '#lst-loss-minimum-tidyverse-demo lst-cap="Find decision for minimum loss: tidyverse version"'

make_loss_t <- function(our_d) {
  d_t %>% 
    mutate(loss_t = posterior_t * abs(our_d - p_grid_t)) %>% 
    summarise(weighted_average_loss_t = sum(loss_t))
}

loss_t <- 
    d_t %>% 
        select(p_grid_t) %>% 
        rename(decision_t = p_grid_t) %>% 
            mutate(weighted_average_loss_t = purrr::map(decision_t, make_loss_t)) %>%
        unnest(weighted_average_loss_t) 
  
min_loss_t <-
  loss_t %>% 
    filter(weighted_average_loss_t == min(weighted_average_loss_t)) %>%
    as.numeric()
```

### Sampling to simulate prediction

Another common job for samples is to ease simulation of the model's
implied observations. This is useful for 1. Model design 2. Model
checking 3. Software validation 4. Research design 5. Forecasting

#### Dummy data

Instead of using `dbinom()` to compute the probability of events, we
will use `rbinom()` to generate dummy data to simulate observations.

We will generate 100,000 dummy observations to verify that each value of
the globe tossing model (6 Waters with 9 tosses) appears in proportion
to its likelihood:

##### Base R

```{r}
#| label: fig-generate-dummy-obs-base-demo
#| fig-cap: "Dsitribution of 100,000 dummy observations of the globe tossing model resulting in 6 Water with 9 tosses: base R version" 
#| attr-source: '#lst-generate-dummy-obs-base-demo lst-cap="Generating dummy observation to simulate the probability of each value of the globe tossing model: base R version"'

set.seed(3)
## R code 3.24 ###############################
dummy_data_r <- rbinom(1e5, size = 9, prob = 0.7)
## replace `rethinking::simplehist()` with base R `graphics::hist()`
hist(dummy_data_r, xlab = "dummy water count") 

```

##### Tidyverse

```{r}
#| label: generate-dummy-obs-tidyverse-demo
#| fig-cap: "Dsitribution of 100,000 dummy observations of the globe tossing model resulting in 6 Water with 9 tosses: tidyverse version" 
#| attr-source: '#lst-generate-dummy-obs-tidyverse-demo lst-cap="Generating dummy observation to simulate the probability of each value of the globe tossing model: tidyverse version"'

set.seed(3)
d2_t <- tibble(draws_t = rbinom(1e5, size = 9, prob = .7))

# the histogram
d2_t %>% 
  ggplot(aes(x = draws_t)) +
  geom_histogram(binwidth = 1, center = 0,
                 fill = "deepskyblue", color = "black", linewidth = 1/10) +
  scale_x_continuous("dummy water count", breaks = 0:4 * 2) +
  ylab("frequency") +
  theme(panel.grid = element_blank())
```

#### Model checking

Since Bayesian models are always generative, able to simulate
observations as well as estimate parameters from observations, once you
condition a model on data, you can simulate to examine the model's
empirical expectations.

All that is required is averaging over the posterior density for `p`,
while computing the predictions. For each possible value of the
parameter `p`, there is an implied distribution of outcomes. So if you
were to compute the sampling distribution of outcomes at each value of
`p`, then you could average all of these prediction distributions
together, using the posterior probabilities of each value of `p`, to get
a **posterior predictive distribution**.

```{r}
#| label: post-pred-dist-r
#| attr-source: '#lst-post-pred-dist-r lst-cap="Using the posterior probabilities of each value of p, to get a posterior predictive distribution: rethinking version"'

set.seed(3)
## R code 3.26 #############################
w_predicted_r <- rbinom(1e4, size = 9, prob = samples_r)
hist(w_predicted_r)
```

##### Tidyverse

```{r}
#| label: post-pred-dist-t
#| attr-source: '#lst-post-pred-dist-t lst-cap="Using the posterior probabilities of each value of p, to get a posterior predictive distribution: tidyverse version"'

set.seed(3)

samples2_t <-
  d_t %>% 
  slice_sample(n = 1e4, weight_by = posterior_t, replace = T) %>% 
  mutate(w_predicted_t = purrr::map_dbl(p_grid_t, rbinom, n = 1, size = 9))

## plot histogram
samples2_t %>% 
  ggplot(aes(x = w_predicted_t)) +
  geom_histogram(binwidth = 1, center = 0, color = "black",
                 fill = "deepskyblue", linewidth = 1/10) +
  scale_x_continuous("number of water samples",
                     breaks = 0:3 * 3) +
  scale_y_continuous("frequency") +
  ggtitle("Posterior predictive distribution") +
  coord_cartesian(ylim = c(0, 2000)) +
  theme(panel.grid = element_blank())
```

## I STOPPED HERE! (2023-08-01) TO BE CONTINUED {.unnumbered}

## Practice

## Practice with brms

### Tidyverse (and {brms})

With {**brms**}, we'll fit the primary model of $w=6$ and $n=9$.

#### Fit model with the {brms} package

```{r}
#| label: brms-model-fit-b
 

b3.1 <-
  brms::brm(data = list(w = 6), 
      family = binomial(link = "identity"),
      w | trials(9) ~ 0 + Intercept,
      # this is a flat prior
      brms::prior(beta(1, 1), class = b, lb = 0, ub = 1),
      iter = 5000, warmup = 1000,
      seed = 3,
      file = "fits/b03.01")
```

We'll learn more about the beta distribution in Chapter 12. But for now,
here's the posterior summary for `b_Intercept`, the probability of a
"w".

```{r}
#| label: posterior-summary-b

brms::posterior_summary(b3.1)["b_Intercept", ] %>% 
  round(digits = 2)
```

#### Simulate probability values

I had problems with the following code chunk. See [my postings at Kurz's
repo](https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse_2_ed/issues/49).

```{r}
#| label: sim-prob-values

f <-
  brms:::fitted.brmsfit(b3.1, 
         summary = F,
         scale = "linear") %>% 
  data.frame() %>% 
  rlang::set_names("p")

glimpse(f)
```

```{r}
#| label: fig-brms-model-density
#| fig-cap: "Density plot of the fittet {brms} model"

f %>% 
  ggplot(aes(x = p)) +
  geom_density(fill = "deepskyblue", color = "deepskyblue") +
  annotate(geom = "text", x = .08, y = 2.5,
           label = "Posterior probability") +
  scale_x_continuous("probability of water",
                     breaks = c(0, .5, 1),
                     limits = 0:1) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank())
```

The graphic should look like the top part of Figure 3.6, reproduced as
@fig-repr-figure-3.6-top-b. (I am not sure if the differences are
important: I have a smaller apex and my curve is more irregular.)

> Much like we did with samples, we can use this distribution of
> probabilities to predict histograms of $w$ counts. With those in hand,
> we can make an analogue to the histogram in the bottom panel of Figure
> 3.6.

```{r}
#| label: fig-sim-pred-values-b
#| fig-cap: "Simulation to predict posterior distribution" 

set.seed(3)
f <-
  f %>% 
  mutate(w2 = rbinom(n(), size = 9,  prob = p))

# the plot
f %>% 
  ggplot(aes(x = w2)) +
  geom_histogram(binwidth = 1, center = 0, color = "black",
                 fill = "deepskyblue", linewidth = 1/10) +
  scale_x_continuous("number of water samples", breaks = 0:3 * 3) +
  scale_y_continuous(NULL, breaks = NULL, limits = c(0, 5000)) +
  ggtitle("Posterior predictive distribution") +
  coord_cartesian(xlim = c(0, 9)) +
  theme(panel.grid = element_blank())
```

## Session info

```{r}
#| label: session-info

sessionInfo()
```

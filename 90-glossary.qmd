# Glossary {.unnumbered}

Bayesianism (1)

**Bayesian Updating**: A Bayesian model begins with one set of
plausibilities assigned to each of these possibilities. These are the
prior plausibilities. Then it updates them in light of the data, to
produce the posterior plausibilities. This updating process is a kind of
learning. (Chap.2)

**Bayes Theorem**: This is the theorem that gives Bayesian data analysis
its name. But the theorem itself is a trivial implication of probability
theory. The mathematical definition of the posterior distribution arises
from Bayes' Theorem. The key lesson is that the posterior is
proportional to the product of the prior and the probability of the
data. (Chap.2)

**Compatibility Interval**: In scientific journals it is usual to report
an interval of defined mass, usually known as a confidence interval.
What the interval indicates is a range of parameter values compatible
with the model and data. The model and data themselves may not inspire
confidence, in which case the interval will not either. McElreath
therefore call these areas compatibility intervals. These posterior
intervals report two parameter values that contain between them a
specified amount of posterior probability, a probability mass. (Chap.3)

**Confidence Interval**: See Compatibility Interval (Chap.3)

**Credibility Interval**: See Compatibility Interval (Chap.3)

Cross Validation (1)

Directed Acyclic Graph (DAG) (2)

Exponential Family (1)

**Frequentist Approach**: The frequentist approach requires that all
probabilities be defined by connection to the frequencies of events in
very large samples.^[21](javascript:void(0))^ This leads to frequentist
uncertainty being premised on imaginary resampling of data---if we were
to repeat the measurement many many times, we would end up collecting a
list of values that will have some pattern to it. It means also that
parameters and models cannot have probability distributions, only
measurements can. (Chap.1)

Graphical Causal Model (2)

**Grid Approximation**: This numerical technique to approximate a
posterior distribution can be used whenever your model has very few
parameters. We can achieve an excellent approximation of the continuous
posterior distribution by considering only a finite grid of parameter
values. At any particular value of a parameter, \`p\`, it's a simple
matter to compute the posterior probability: just multiply the prior
probability of \`p\` by the likelihood at \`p\`. Repeating this
procedure for each value in the grid generates an approximate picture of
the exact posterior distribution. (Chap.2)

Highest Priority Intensity Interval (HPDI) (3)

**Hessian**: Named after mathematician Ludwig Otto Hesse (1811--1874).
it is a square matrix of second derivatives to find the standard
deviation for a quadratic approximation. (Chap.2)

Identification (1)

Information Criteria (1)

Information Entropy (2)

**Likelihood**: The relative number of ways that a value *p* can produce
the data. It is derived by enumerating all the possible data sequences
that could have happened and then eliminating those sequences
inconsistent with the data. This results graphically and more generally
in a *distribution of variables*, in contrast to conventional
statistics, where a *distribution function assigned to an observed
variable* is usually called a likelihood. (Chap.2)

Loss Function (3)

**Markov Chain Monte Carlo (MCMC)**: There are lots of important model
types, like multilevel (mixed-effects) models, for which neither grid
approximation nor quadratic approximation is always satisfactory. ... As
a result, various counterintuitive model fitting techniques have arisen.
The most popular of these is MCMC, which is a family of conditioning
engines capable of handling highly complex models. (Chap.1, **2**)

Maximum A Posteriori (MAP) (3)

Maximum Entropy (1,2)

Maximum Likehood Estimate (MLE) (2)

Metropolis Algorithm (2)

Model Checking (3)

Multilevel Model (1)

**Overfitting**: In the context of data modeling, overfitting occurs
when the model fits perfectly (or, almost perfectly) the data
[sample](https://www.statista.com/statistics-glossary/definition/371/sample/) on
which it is trained but might not have very good predictive power when
used on
[observations](https://www.statista.com/statistics-glossary/definition/184/observation/)outside
this training sample. Overfitting often occurs when
the [data](https://www.statista.com/statistics-glossary/definition/194/data/)
in the training sample is too noisy, the sample is too small, or the
model is overly complex.
([statista.com](https://www.statista.com/statistics-glossary/definition/1502/overfitting/))
See also [Wikipedia](https://en.wikipedia.org/wiki/Overfitting) and
[IBM](https://www.ibm.com/topics/overfitting). (Chap. 1)

**Parameter**: A conjectured proportion, *p*. It\'s just a way of
indexing possible explanations of the data. Unobserved variables are
also usually called parameters. Data are measured and known; parameters
are unknown and must be estimated from data. (Chap.2)

Partial Pooling (1)

Percentil Interval (PI) (3)

**Posterior Distribution**: Once you have named all the variables and
chosen definitions for each, a Bayesian model can update all of the
prior distributions to their purely logical consequences: the posterior
distribution. For every unique combination of data, likelihood,
parameters, and prior, there is a unique posterior distribution. This
distribution contains the relative plausibility of different parameter
values, conditional on the data and model. The posterior distribution
takes the form of the probability of the parameters, conditional on the
data. The mathematical definition of the posterior distribution arises
from Bayes' Theorem. (Chap. 2)

Posterior Predictive Distribution (3)

**Posterior Probability**: The new, updated plausibility of any specific
*p*. (Chap.2)

**Principle of Indifference**: When there is no reason to say that one
conjecture is more plausible than another, weigh all of the conjectures
equally. (Chap.2)

**Prior Probability**: The prior plausibility of any specific *p*.
(Chap.2)

Process Models (1)

Standard Error: (2)

**Quadratic Approximation**: Under quite general conditions, the region
near the peak of the posterior distribution will be nearly Gaussian---or
\"normal\"---in shape. This means the posterior distribution can be
usefully approximated by a Gaussian distribution. A Gaussian
distribution is convenient, because it can be completely described by
only two numbers: the location of its center (mean) and its spread
(variance). (Chap.2)

Sampling Distribution (1,**3**)

Subjective Bayesian (2)

**Variables**: Variables are just symbols that can take on different
values. In a scientific context, variables include things we wish to
infer, such as proportions and rates, as well as things we might
observe, the data. (Chap.2)

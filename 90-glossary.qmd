# Glossary {.unnumbered}

Bayesianism (1)

**Bayesian Updating**: A Bayesian model begins with one set of
plausibilities assigned to each of these possibilities. These are the
prior plausibilities. Then it updates them in light of the data, to
produce the posterior plausibilities. This updating process is a kind of
learning. (Chap.2)

**Bayes Theorem**: This is the theorem that gives Bayesian data analysis
its name. But the theorem itself is a trivial implication of probability
theory. The mathematical definition of the posterior distribution arises
from Bayes' Theorem. The key lesson is that the posterior is
proportional to the product of the prior and the probability of the
data. (Chap.2)

Centering: (Chap.4)

**Compatibility Interval**: In scientific journals it is usual to report
an interval of defined mass, usually known as a confidence interval.
What the interval indicates is a range of parameter values compatible
with the model and data. The model and data themselves may not inspire
confidence, in which case the interval will not either. McElreath
therefore call these areas compatibility intervals. These posterior
intervals report two parameter values that contain between them a
specified amount of posterior probability, a probability mass. (Chap.3)

**Confidence Interval**: See Compatibility Interval (Chap.3)

**Credibility Interval**: See Compatibility Interval (Chap.3)

Cross Validation (1)

**Data**: Observed variables. See parameter. (Chap.2, 4)

Directed Acyclic Graph (DAG) (2)

**Exchangeable**: There is a mathematical result known as *de Finetti's
theorem* that says values which are exchangeable can be approximated by
mixtures of *independent and identically distributed* (i.i.d.),
distributions. Colloquially, exchangeable values can be reordered.
(Chap.4)

Exponential Family (1, **4**)

**Frequentist Approach**: The frequentist approach requires that all
probabilities be defined by connection to the frequencies of events in
very large samples.^[21](javascript:void(0))^ This leads to frequentist
uncertainty being premised on imaginary resampling of data---if we were
to repeat the measurement many many times, we would end up collecting a
list of values that will have some pattern to it. It means also that
parameters and models cannot have probability distributions, only
measurements can. (Chap.1)

Generalized Linear Models (GLMs): (4)

Graphical Causal Model (2)

**Grid Approximation**: This numerical technique to approximate a
posterior distribution can be used whenever your model has very few
parameters. We can achieve an excellent approximation of the continuous
posterior distribution by considering only a finite grid of parameter
values. At any particular value of a parameter, \`p\`, it's a simple
matter to compute the posterior probability: just multiply the prior
probability of \`p\` by the likelihood at \`p\`. Repeating this
procedure for each value in the grid generates an approximate picture of
the exact posterior distribution. (Chap.2)

Highest Priority Intensity Interval (HPDI) (3)

**Hessian**: Named after mathematician Ludwig Otto Hesse (1811--1874).
it is a square matrix of second derivatives to find the standard
deviation for a quadratic approximation. (Chap.2)

Identification (1)

Information Criteria (1)

Information Entropy (2)

**Joint Generative Model**: It is defined by a combination of variables
and their probability distributions. It can be used both to simulate
hypothetical observations as well as to analyze real ones. (Chap.4)

Large World: References the broader context in which one deploys a
model, the 'real' worlds. See [Small World]{.smallcaps}. (Chap.2)

**Likelihood**: The relative number of ways that a value *p* can produce
the data. It is derived by enumerating all the possible data sequences
that could have happened and then eliminating those sequences
inconsistent with the data. This results graphically and more generally
in a *distribution of variables*, in contrast to conventional
statistics, where a *distribution function assigned to an observed
variable* is usually called a likelihood. (Chap.2)

**Linear Model**: It is strategy to make the parameter for the mean of a
Gaussian distribution, *`µ`*, into a linear function of the predictor
variable and other, new parameters that we invent. The linear model
strategy instructs the golem to assume that the predictor variable has a
constant and additive relationship to the mean of the outcome. The golem
then computes the posterior distribution of this constant relationship.
(Chap.4)

**Linear Regression**: In linear regression, the relationships are
modeled using linear predictor functions whose unknown model parameters
are estimated from the data. Such models are called linear models.
(Wikipedia, Chap.4)

**Loss Function**: A loss function is a rule that tells you the cost
associated with using any particular point estimate. *Different loss
functions imply different point estimates*. (Chap.3)

**Marginal Distribution**: The jargon "marginal" here means "averaging
over the other parameters." (Chap.4)

**Markov Chain Monte Carlo (MCMC)**: There are lots of important model
types, like multilevel (mixed-effects) models, for which neither grid
approximation nor quadratic approximation is always satisfactory. ... As
a result, various counterintuitive model fitting techniques have arisen.
The most popular of these is MCMC, which is a family of conditioning
engines capable of handling highly complex models. (Chap.1, **2**)

**Maximum A Posteriori (MAP)**: The estimate where the posterior's peak
will lie at the posterior distribution. (Chap.3, **4**)

Maximum Entropy (1,2)

Maximum Likehood Estimate (MLE) (2)

Metropolis Algorithm (2)

**Model Checking**: It means (1) ensuring the model fitting worked
correctly and (2) evaluating the adequacy of a model for some purpose.
Since Bayesian models are always *generative*, able to simulate
observations as well as estimate parameters from observations, once you
condition a model on data, you can simulate to examine the model's
empirical expectations. (Chap.3)

Multilevel Model (1)

**Overfitting**: In the context of data modeling, overfitting occurs
when the model fits perfectly (or, almost perfectly) the data
[sample](https://www.statista.com/statistics-glossary/definition/371/sample/) on
which it is trained but might not have very good predictive power when
used on
[observations](https://www.statista.com/statistics-glossary/definition/184/observation/)outside
this training sample. Overfitting often occurs when
the [data](https://www.statista.com/statistics-glossary/definition/194/data/)
in the training sample is too noisy, the sample is too small, or the
model is overly complex.
([statista.com](https://www.statista.com/statistics-glossary/definition/1502/overfitting/))
See also [Wikipedia](https://en.wikipedia.org/wiki/Overfitting) and
[IBM](https://www.ibm.com/topics/overfitting). (Chap. 1)

**p-hacking**: It is the practice of adjusting the model and the data to
achieve a desired result. The desired result is usually a *p*-value less
then 5%. The problem is that when the model is adjusted in light of the
observed data, then *p*-values no longer retain their original meaning.
False results are to be expected. This is valid for Bayesian and
Non-Bayesian statistics. Even if Bayesian statistics don't pay any
attention to p-values, the danger remains. We could choose our priors
conditional on the observed sample, just to get some desired (wrong)
result. It is therefore important to choose priors conditional on
pre-data knowledge of the variables---their constraints, ranges, and
theoretical relationships. We should always judge our priors against
general facts, not the sample.

**Parameter**: Variables that cannot be observed. A conjectured
proportion, *p*. It's just a way of indexing possible explanations of
the data. Data are measured and known; parameters are unknown and must
be estimated from data. Example for parameters are rates, proportions,
and averages. See Data. (Chap.2, 4)

Partial Pooling (1)

**Percentile Interval (PI)**: Intervals that assign equal probability
mass to each tail. are very common in the scientific literature. These
intervals do a good job of communicating the shape of a distribution, as
long as the distribution isn't too asymmetrical. (Chap.3)

**Posterior Distribution**: Once you have named all the variables and
chosen definitions for each, a Bayesian model can update all of the
prior distributions to their purely logical consequences: the posterior
distribution. For every unique combination of data, likelihood,
parameters, and prior, there is a unique posterior distribution. This
distribution contains the relative plausibility of different parameter
values, conditional on the data and model. The posterior distribution
takes the form of the probability of the parameters, conditional on the
data. The mathematical definition of the posterior distribution arises
from Bayes' Theorem. (Chap. 2)

**Posterior Predictive Distribution**: After we compute the sampling
distribution of outcomes at each value of *`p`*, we can average all of
these prediction distributions together, using the posterior
probabilities of each value of *`p`* to get a posterior predictive
distribution. (Chap.3)

**Posterior Probability**: The new, updated plausibility of any specific
*`p`*. (Chap.2)

**Predictor Variable**: Predictor variable, also known sometimes as the
independent variable, is used to make a prediction for dependent
variables. The predictor variable is the counterpart to the dependent
variable, often directly informed or affected by the predictor variable.
([DeepAI](https://deepai.org/machine-learning-glossary-and-terms/predictor-variable))
(Chap.4)

**Principle of Indifference**: When there is no reason to say that one
conjecture is more plausible than another, weigh all of the conjectures
equally. (Chap.2)

**Prior Predictive Simulation**: It is an essential part of your
modeling. Once you've chosen priors for *`h`, `μ`*, and *`σ`*, these
imply a joint prior distribution of individual heights. By simulating
from this distribution, you can see what your choices imply about
observable height. This helps you diagnose bad choices. Lots of
conventional choices are indeed bad ones, and we'll be able to see this
through prior predictive simulations.

**Prior Probability**: The prior plausibility of any specific *`p`*.
(Chap.2)

**Probability Density**: Continuous ones like the Gaussian are called
*probability density* functions, denoted with *`p`* or just plain old
*`f`*, depending upon author and tradition. For mathematical reasons,
probability densities can be greater than 1. Probability *density* is
the rate of change in cumulative probability. So where cumulative
probability is increasing rapidly, density can easily exceed 1. But if
we calculate the area under the density function, it will never
exceed 1. Such areas are also called *probability mass*. See also
Probability Mass. (Chap.4)

**Probability Mass**: Probability distributions with only discrete
outcomes, like the binomial, are called *probability mass* functions and
denoted `Pr`\`. If we calculate the area under the density function, it
will never exceed 1. Such areas are also called *probability mass*. See
also Probability Density. (Chap.4)

Process Models (1)

**Regression**: The term has come to mean using one or more predictor
variables to model the distribution of one or more outcome variables.
The original use of term, however, arose from anthropologist Francis
Galton's (1822--1911) observation that the sons of tall and short men
tended to be more similar to the population mean, hence *regression to
the mean*. (Chap.4)

[**Small World**]{.smallcaps}: Reference to the self-contained logical
world of the model. (Chap.2) See [Large World.]{.smallcaps}

Standard Error: (2)

**Stochastic**: A stochastic relationship is just a mapping of a
variable or parameter onto a distribution. It is *stochastic* because no
single instance of the variable on the left is known with certainty.
Instead, the mapping is probabilistic: Some values are more plausible
than others, but very many different values are plausible under any
model. (Chap.4)

**Quadratic Approximation**: Under quite general conditions, the region
near the peak of the posterior distribution will be nearly Gaussian---or
"normal"---in shape. This means the posterior distribution can be
usefully approximated by a Gaussian distribution. A Gaussian
distribution is convenient, because it can be completely described by
only two numbers: the location of its center (mean) and its spread
(variance). (Chap.2, 4)

**Sampling Distribution**: They are the foundation of common
non-Bayesian statistical traditions. In those approaches, inference
about parameters is made through the sampling distribution. In Bayesian
statistics, inference about parameters is never done directly through a
sampling distribution. The posterior distribution is not sampled, but
deduced logically. Then samples can be drawn from the posterior to aid
in inference. In neither case is "sampling" a physical act. In both
cases, it's just a mathematical device and produces only *small world*
numbers. (1,**3**)

Subjective Bayesian (2)

**Variables**: Variables are just symbols that can take on different
values. In a scientific context, variables include things we wish to
infer, such as proportions and rates, as well as things we might
observe, the data. (Chap.2)

**Variance-Covariance Matrix**: It is the multi-dimensional glue of a
quadratic approximation, because it tells us how each parameter relates
to every other parameter in the posterior distribution. A
variance-covariance matrix can be factored into two elements: (1) a
vector of variances for the parameters and (2) a correlation matrix that
tells us how changes in any parameter lead to correlated changes in the
others. (Chap.4)

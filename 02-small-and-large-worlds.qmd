# Small and Large Worlds {#sec-small-and-large-worlds}

```{r}
#| label: setup

library(tidyverse)
```

## Original {.unnumbered}

-   The **small world** is the self-contained logical world of the
    model.
-   The **large world** is the broader context in which one deploys a
    model.

**Meta Remark (Study Guide)**

> This chapter focuses on the small world. It explains probability
> theory in its essential form: counting the ways things can happen.
> Bayesian inference arises automatically from this perspective. Then
> the chapter presents the stylized components of a Bayesian statistical
> model, a model for learning from data. Then it shows you how to
> animate the model, to produce estimates.
>
> All this work provides a foundation for the next chapter, in which
> you'll learn to summarize Bayesian estimates, as well as begin to
> consider large world obligations.

## Tidyverse {.unnumbered}

**Meta Remark (My Replication Strategy)**

The work by Solomon Kurz has many references to R specifics, so that
people new to R can follow the course. Most of these references are not
new to me, so I will not include them in my personal notes. There are
also very important references to other relevant articles I do not know.
But I will put these kind of references for now aside and will me mostly
concentrate on the replication and understanding of the code examples.

One challenge for the author (Kurz) was to replicate all the graphics of
the original version, even if they were produced just for understanding
without underlying code. I will use only code lines that are essential
to display Bayesian results. Therefore I will not replicate the very
extensive explication how to produce with tidyverse means the graphic of
the garden of forking data.

## The Garden of Forking Data

### Original

::: callout-tip
###### Bayesian inference is counting of possibilities

Bayesian inference is really just counting and comparing of
possibilities. ... In order to make good inference about what actually
happened, it helps to consider everything that could have happened. A
Bayesian analysis is a garden of forking data, in which alternative
sequences of events are cultivated. As we learn about what did happen,
some of these alternative sequences are pruned. In the end, what remains
is only what is logically consistent with our knowledge.
:::

#### Counting possibilities

> Suppose there's a bag, and it contains four marbles. These marbles
> come in two colors: blue and white. We know there are four marbles in
> the bag, but we don't know how many are of each color. We do know that
> there are five possibilities: (1) \[⚪⚪⚪⚪\], (2) \[⚫⚪⚪⚪\], (3)
> \[⚫⚫⚪⚪\], (4) \[⚫⚫⚫⚪\], (5) \[⚫⚫⚫⚫\]. These are the only
> possibilities consistent with what we know about the contents of the
> bag. **Call these five possibilities the *conjectures*.**
>
> Our goal is to figure out which of these conjectures is most
> plausible, given some evidence about the contents of the bag. We do
> have some evidence: A sequence of three marbles is pulled from the
> bag, one at a time, replacing the marble each time and shaking the bag
> before drawing another marble. **The sequence that emerges is: ⚫⚪⚫,
> in that order. These are the data.** (bold emphasis pb)
>
> So now let's plant the garden and see how to use the data to infer
> what's in the bag. Let's begin by considering just the single
> conjecture, \[⚫⚪⚪⚪\], that the bag contains one blue and three
> white marbles. ...
>
> ...
>
> Notice that even though the three white marbles look the same from a
> data perspective---we just record the color of the marbles, after
> all---they are really different events. This is important, because it
> means that there are three more ways to see ⚪ than to see ⚫.

|              |                        |
|--------------|------------------------|
| Conjecture   | Ways to produce ⚫⚪⚫ |
| \[⚪⚪⚪⚪\] | 0 × 4 × 0 = 0          |
| \[⚫⚪⚪⚪\] | 1 × 3 × 1 = 3          |
| \[⚫⚫⚪⚪\] | 2 × 2 × 2 = 8          |
| \[⚫⚫⚫⚪\] | 3 × 1 × 3 = 9          |
| \[⚫⚫⚫⚫\] | 4 × 0 × 4 = 0          |

I have bypassed the counting procedure related with the step-by-step
visualization of the garden of forking data. The multiplication in the
above table is still a summarized counting:

::: callout-important
> Multiplication is just a shortcut to enumerating and counting up all
> of the paths through the garden that could produce all the
> observations.
:::

> Notice that the number of ways to produce the data, for each
> conjecture, can be computed by first counting the number of paths in
> each "ring" of the garden and then by multiplying these counts
> together. ... The fact that numbers are multiplied during calculation
> doesn't change the fact that this is still just counting of logically
> possible paths. This point will come up again, when you meet a formal
> representation of Bayesian inference.

The multiplication in the above table has to be interpreted the
following way:

1.  The possibility of the conjecture that the bag contains four white
    marbles is zero because the result shows also black marbles. This is
    the other way around for the last conjecture of four blue/black
    marbles.
2.  The possibility of the conjecture that the bag contains one black
    and three white marbles is calculated the following way: The first
    marble of the result is black and --- according to our conjecture
    --- there is only one way (=1) to produce this black marble. The
    next marble we have drawn is white. This is consistent with three
    (=3) different ways( marbles) of our conjecture. The last drawn
    marble is again black which corresponds again with just one way
    (possibility) following our conjecture. So we get as result of the
    garden of forking data: `1 x 3 x 1`.
3.  The calculation of the other conjectures follows the same pattern.

#### Combining Other Information

> We may have additional information about the relative plausibility of
> each conjecture. This information could arise from knowledge of how
> the contents of the bag were generated. It could also arise from
> previous data. Whatever the source, it would help to have a way to
> combine different sources of information to update the plausibilities.
> Luckily there is a natural solution: Just multiply the counts.
>
> To grasp this solution, suppose we're willing to say each conjecture
> is equally plausible at the start. So we just compare the counts of
> ways in which each conjecture is compatible with the observed data.
> This comparison suggests that \[⚫⚫⚫⚪\] is slightly more plausible
> than \[⚫⚫⚪⚪\], and both are about three times more plausible than
> \[⚫⚪⚪⚪\]. **Since these are our initial counts, and we are going
> to update them next, let's label them *prior*.** (bold emphasis pb)

::: callout-tip
###### Principle of Indifference

As we will see later: the choice of the prior is not relevant for the
final result. This is called the *Principle of Indifference*.

The only difference between a good or bad choice is the time (the number
of steps) the updating process needs to produce the final result.

Before seeing any data the most common solution is to assign an equal
number of ways that each conjecture could be correct.
:::

> Which assumption should we use, when there is no previous information
> about the conjectures? The most common solution is to assign an equal
> number of ways that each conjecture could be correct, before seeing
> any data. This is sometimes known as the **PRINCIPLE OF
> INDIFFERENCE**: When there is no reason to say that one conjecture is
> more plausible than another, weigh all of the conjectures equally. ...
>
> For the sort of problems we examine in this book, the principle of
> indifference results in inferences very comparable to mainstream
> non-Bayesian approaches, most of which contain implicit equal
> weighting of possibilities. For example a typical non-Bayesian
> confidence interval weighs equally all of the possible values a
> parameter could take, regardless of how implausible some of them are.
> In addition, many non-Bayesian procedures have moved away from equal
> weighting, through the use of penalized likelihood and other methods.

**Bayesian Updating Process**

Here's how we do the updating:

1.  First we count the numbers of ways each conjecture could produce the
    new observation, ⚫.
2.  Then we multiply each of these new counts by the prior numbers of
    ways for each conjecture.

|     |              |     |                    |     |              |     |            |     |
|--------|--------|--------|--------|--------|--------|--------|--------|--------|
|     |              |     |                    |     |              |     |            |     |
|     | Conjecture   |     | Ways to produce ⚫ |     | Prior counts |     | New count  |     |
|     | \[⚪⚪⚪⚪\] |     | 0                  |     | 0            |     | 0 × 0 = 0  |     |
|     | \[⚫⚪⚪⚪\] |     | 1                  |     | 3            |     | 3 × 1 = 3  |     |
|     | \[⚫⚫⚪⚪\] |     | 2                  |     | 8            |     | 8 × 2 = 16 |     |
|     | \[⚫⚫⚫⚪\] |     | 3                  |     | 9            |     | 9 × 3 = 27 |     |
|     | \[⚫⚫⚫⚫\] |     | 4                  |     | 0            |     | 0 × 4 = 0  |     |

::: {.callout-note style="color: blue;"}
###### Typo

In the book the table header "Ways to produce" includes ⚪ instead of
--- as I think is correct --- ⚫.
:::

#### From Counts to Probability

::: {.callout-tip style="color: green"}
###### Principle of honest ignorance

*When we don't know what caused the data, potential causes that may
produce the data in more ways are more plausible*.
:::

Two reasons for using probabilities instead of counts:

1.  Only relative value matters.
2.  Counts will very fast grow very large and difficult to manipulate.

![](img/plausibility-formula-1-min.png){fig-align="center"}

```{=html}
<center>
  That little <span style="font-size: 40px;">∝</span> means <em>proportional to</em>.
</center>
```
**Standardizing the plausibility**

![](img/plausibility-formula-2-min.png){fig-align="center"}

```{r}
#| label: compute-plausibilities

## R code 2.1
ways <- c(0, 3, 8, 9, 0)
ways / sum(ways)
```

I understand that in the above code we assume as the very first prior
plausibility the ways the results can be produced by the assumed
conjecture proportion of blue marbles. It corresponds to the parameter
value. This conclusion is somewhat hidden as our very first calculation
already takes three drawn marbles (⚫⚪⚫) into account. From another
perspective this could also be seen as the first draw (⚫) followed by
two Bayesian updates (⚪⚫). This corresponds to the following counting:
(In the second and third draw the first factor of the multiplication is
always the prior.)

1.  First draw ⚫: 0,1,2,3,4 ways corresponding to the 4 chosen
    conjectures.
2.  Second draw ⚪: 0 x 4 = 0, 1 x 3 = 3, 2 x 2 = 4, 3 x 1 = 3, 4 x 0 =
    0
3.  Third draw ⚫: 0 x 0 = 0, 3 x 1 = 3, 4 x 2 = 8, 3 x 3 = 9, 0 x 4 = 0

This demonstration shows that the book example of ⚫⚪⚫ already
contains three priors: The first is identical with the conjectured
proportion of blue marbles (0,1,2,3,4) and is equivalent to the
parameter value for each conjecture. The second and third marbles
already uses Bayesian updating.

**Names of the different parts of the formula**

Data = ⚫⚪⚫.

-   A conjectured proportion of blue marbles, *p*, is usually called a
    **PARAMETER** value. It's just a way of indexing possible
    explanations of the data. For instance one conjectured proportion of
    one blue marble could be: ⚫⚪⚪⚪ (`p = 1`). The others are:
    ⚪⚪⚪⚪ (`p = 0`), ⚫⚫⚪⚪ (`p = 2` , ⚫⚫⚫⚪ (`p = 3`), and
    ⚫⚫⚫⚫ (`p = 4` ways).
-   The relative number of ways that a value *p* can produce the data is
    usually called a **LIKELIHOOD**. It is derived by enumerating all
    the possible data sequences that could have happened and then
    eliminating those sequences inconsistent with the data. For
    instance: `0.00, 0.15, 0.40, 0.45, 0.00`
-   The prior plausibility of any specific *p* is usually called the
    **PRIOR PROBABILITY**. For instance: `0, 3, 8, 9, 0`
-   The new, updated plausibility of any specific *p* is usually called
    the **POSTERIOR PROBABILITY**. For instance: `0, 3, 16, 27, 0`

### Tidyverse

#### Counting possibilities

> If we're willing to code the marbles as 0 = "white" 1 = "blue", we can
> arrange the possibility data in a tibble as follows.

::: {.callout-note style="color: blue;"}
###### Changed code slightly

I changed `rep()` to `rep.int()` and added `L` to the value of p1 resp.
p5 to get integers (instead of doubles).
:::

```{r}
#| label: create-marble-data

d <-
  tibble::tibble(p1 = 0L,
         p2 = rep.int(1:0, times = c(1, 3)),
         p3 = rep.int(1:0, times = c(2, 2)),
         p4 = rep.int(1:0, times = c(3, 1)),
         p5 = 1L)

head(d)
```

> You might depict the possibility data in a plot.

```{r}
#| label: fig-show-marble-data
#| fig-cap: "Marble Data"
#| attr-source: '#lst-marble-data lst-cap="Code listing to show the marble data as graph"'
#| code-summary: "Show the marble data as graph"
#| echo: true

d %>% 
  set_names(1:5) %>%    # <1>
  mutate(x = 1:4) %>%  # <2> 
  pivot_longer(-x, names_to = "possibility") %>%  # <3>
  mutate(value = value %>% as.character()) %>%    # <4>
  
  ggplot(aes(x = x, y = possibility, fill = value)) + # <5>
  geom_point(shape = 21, size = 5) + # <5>
  scale_fill_manual(values = c("white", "navy")) + # <5>
  scale_x_discrete(NULL, breaks = NULL) + # <5>
  theme(legend.position = "none") # <5>
 
```

1.  Change the column names from p\<number\> to \<number\>.
2.  Add a new column with the values 1 to 4 for each row.
3.  Convert data frame from wide to long, excluding the `x` column.
4.  Change the variable type of `value` column from double to character.
5.  Plot the results.

At first I could not understand the code lines. I had to execute line by
line to see what happens:

##### Annotation (1)

```{r}
#| label: using-set-names

set_names(d, 1:5)
```

`rlang::set_names()` comes from {**rlang**} package which contains
function for base types and core tidyverse features. It is exported to
{**purrr**}, a package with tools for functional programming. It is
equivalent to `stats::setNames()` but has more features and stricter
argument checking. I does nothing more as to change the column names
from p\<number\> to \<number\>. If one had used just numbers for the
probability columns this line wouldn't have been necessary as it is
shown in the next chunk. (I omitted the `utils::head()` argument of the
last line as it is not necessary.)

```{r}
#| label: create-marble-data-2

df <-
  tibble(`1` = 0,
         `2` = rep(1:0, times = c(1, 3)),
         `3` = rep(1:0, times = c(2, 2)),
         `4` = rep(1:0, times = c(3, 1)),
         `5` = 1)

df
```

It is interesting to see that the first and last column are doubles and
not integers. I believe that the reason is that these two columns do not
have variations, e.g. do not contain both variable values, so that R
assumes the more general class of `numeric` and type of `double`.

-   `class(5L)` and `typeof(5L`) both results to *integer* .
-   Whereas `class(5)` is *integer* but `typeof(5)` is *double*.

```{r}
#| label: number-types

class(5L)
typeof(5L)
class(5)
typeof(5)
```

##### Annotation (2)

After understanding what `stats::set_names()` does the next line with
`dplyr::mutate()` is easy. It adds a new column `x` with the values 1 to
4 for each row.

```{r}
#| label: add-x-column

df <- df |> 
    set_names(1:5) |> 
    mutate(x = 1:4)
df
```

##### Annotation (3)

I understood that the data frame is converted from a wide to a long
structure. But together with the pipe and not naming the first parameter
`-x` I did not catch the essence of the command.

A first understanding comes from the fact, that it is wrong to covert
all columns to the long format:

```{r}
#| label: wrong-long-conversion

pivot_longer(data = df, cols = everything(), names_to = "possibility") |> 
    print(n = 24)
```

This (wrong) example shows that it is mandatory to exclude `x` from the
conversion. Otherwise it would be included and integrated into the
`possibility` column.

```{r}
#| label: correct-long-conversion

(d <- 
    pivot_longer(data = df, cols = -x, names_to = "possibility"))

```

The above code line for the conversion from wide to long is equivalent
with naming explicitly all column names to convert:

```{r}
#| label: test-if-identical

d1 <- pivot_longer(data = df, cols = -x, names_to = "possibility")
d2 <- pivot_longer(data = df, 
                   cols = c(`1`, `2`, `3`, `4`, `5`), 
                   names_to = "possibility")
identical(d1, d2)
```

The identity demonstrates: The `-x` parameter excludes the `x` column
from the wide to long transformation. It is a shorthand for naming all 5
columns that should be transformed.

Instead of the `base::identical()` function you could also use
`base::all.equal()`. Comparing data frames is a rather complicated
action summarized by a blog post from Sharla Gelfand. But keep in mind
that the publication date is 2020-02-17 and that therefore some commands
are outdated. Most important: use `base::all.equal()` instead of
`dplyr::all_equal()`.

::: {.callout-warning style="color: orange;"}
> `dplyr::all_equal()` allows you to compare data frames, optionally
> ignoring row and column names. It is deprecated as of dplyr 1.1.0,
> because it makes it too easy to ignore important differences.

The current version of the {**dplyr**} packages is 1.1.2.
:::

##### Annotation (4)

```{r}
#| label: change-column-type
(d <- d |> 
    mutate(value = value %>% as.character()))
```

The `value` column is of type `double` as can be seen in the result of
Annotation (3). For the plot it has to be changed to the type of
`character`. Otherwise it could not be used with the `fill` option.

##### Annotation (5)

The plot uses code from the {**ggplot2**} package, which I do understand
and will therefore not explain here.

```{r}
#| label: plot-possibilities-conjectures

d |> 
  ggplot(aes(x = x, y = possibility, fill = value)) + 
  geom_point(shape = 21, size = 5) + 
  scale_fill_manual(values = c("white", "navy")) +
  scale_x_discrete(NULL, breaks = NULL) + 
  theme(legend.position = "none")
```

##### Summarize the Possiblity Structure

> Here's the basic structure of the possibilities per marble draw.

```{r}
#| label: tbl-basic-prob-struct
#| tbl-cap: possibility-structure

tibble(draw    = 1:3,
       marbles = 4) %>% 
  mutate(possibilities = marbles ^ draw) %>% 
  kableExtra::kbl() |> 
  kableExtra::kable_classic(full_width = F)
```

::: {.callout-note style="color: blue"}
###### Table Packages Used

Kurz employed the {**flextable**} package to print tables. As I have no
experience with this package, I will apply {**kableExtra**} in this
document.\
\
Until now I had used most of the time kableExtra and sometimes DT. For a
short compilation of available table packages see the section on [Other
packages for creating
tables](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html)
in the R Markdown Cookbook. The following excursion on tables follows
the blog article [Top 7 Packages for Making Beautiful Tables in
R](https://towardsdatascience.com/top-7-packages-for-making-beautiful-tables-in-r-7683d054e541)
by Devashree Madhugiri.
:::

#### Excursion: Tables

-   [{**gt**}](https://gt.rstudio.com/): The gt package offers a
    different and easy-to-use set of functions that helps us build
    display tables from tabular data. The gt philosophy states that a
    comprehensive collection of table parts can be used to create a
    broad range of functional tables. These are the table body, the
    table footer, the spanner column labels, the column labels, and the
    table header. (I should look into the {**gt**} package in more
    detail as it is developed by the RStudio/Posit team, that stands not
    only for high quality but also for tidyverse compatibility.)

    ![](https://gt.rstudio.com/reference/figures/gt_parts_of_a_table.svg)

-   [{**formattable**}](https://renkun-ken.github.io/formattable/):
    Formattable data frames are data frames that will be displayed in
    HTML tables using formatter functions. This package includes
    techniques to produce data structures with predefined formatting
    rules, such that the objects maintain the original data but are
    formatted. The package consists of several standard formattable
    objects, including percent, comma, currency, accounting, and
    scientific.

-   [{**kableExtra**}](https://haozhu233.github.io/kableExtra/): It
    extends the basic functionality of `knitr::kable()` tables. Although
    `knitr::kable()` is simple by design, it has many features missing
    which are usually available in other packages. {**kableExtra**} has
    filled the gap nicely. One of the best thing about {**kableExtra**}
    is that most of its table capabilities work for both HTML and PDF
    formats.

-   [{**DT**}](https://rstudio.github.io/DT/): dt is an abbreviation of
    'DataTables.' Data objects in R can be rendered as HTML tables using
    the JavaScript library 'DataTables' (typically via R Markdown or
    Shiny).

-   [{**flextable**}](https://davidgohel.github.io/flextable/): This
    package helps you to create reporting table from a data frame
    easily. You can merge cells, add headers, add footers, change
    formatting, and set how data in cells is displayed. Table content
    can also contain mixed types of text and image content. Tables can
    be embedded from R Markdown documents into HTML, PDF, Word, and
    PowerPoint documents and can be embedded using Package Officer for
    Microsoft Word or PowerPoint documents. Tables can also be exported
    as R plots or graphic files, e.g., png, pdf, and jpeg.

-   [{**reactable**}](https://glin.github.io/reactable/): `reactable()`
    creates a data table from tabular data with sorting and pagination
    by default. The data table is an HTML widget that can be used in R
    Markdown documents and Shiny applications or viewed from an R
    console. It is based on the React Table library and made with
    reactR. Features are:

    -   It creates a data table with sorting, filtering, and pagination
    -   It has built-in column formatting
    -   It supports custom rendering via R or JavaScript
    -   It works seamlessly within R Markdown documents and the Shiny
        app

-   [{**ractablefmtr**}](https://kcuilla.github.io/reactablefmtr/index.html):
    The package improves the appearance and formatting of tables created
    using the reactable R library. It includes many conditional
    formatters that are highly customizable and easy to use.

I sure there are other packages as well. But the above seven packages
are a first starting point to learn creating and displaying
sophisticated data tables in R.

> The authors of R Markdown Cookbook (Yihui Xie, Christophe Dervieux,
> Emily Riederer) mention also several other table packages in the
> section [Other packages for creating
> tables](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html):
>
> -   **rhandsontable** ([Owen
>     2021](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html#ref-R-rhandsontable)):
>     Also similar to **DT**, and has an Excel feel (e.g., you can edit
>     data directly in the table). Visit
>     <https://jrowen.github.io/rhandsontable/> to learn more about it.
>
> -   **pixiedust** ([Nutter
>     2021](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html#ref-R-pixiedust)):
>     Features creating tables for models (such as linear models)
>     converted through the **broom** package ([Robinson, Hayes, and
>     Couch
>     2023](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html#ref-R-broom)).
>     It supports Markdown, HTML, and LaTeX output formats. Its
>     repository is at <https://github.com/nutterb/pixiedust>.
>
> -   **stargazer** ([Hlavac
>     2022](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html#ref-R-stargazer)):
>     Features formatting regression models and summary statistics
>     tables. The package is available on CRAN at
>     <https://cran.r-project.org/package=stargazer>.
>
> -   **xtable** ([Dahl et al.
>     2019](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html#ref-R-xtable)):
>     Perhaps the oldest package for creating tables---the first release
>     was made in 2000. It supports both LaTeX and HTML formats. The
>     package is available on CRAN at
>     <https://cran.r-project.org/package=xtable>.
>
> I'm not going to introduce the rest of packages, but will just list
> them here: **tables** ([Murdoch
> 2023](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html#ref-R-tables)),
> **pander** ([Daróczi and Tsegelskyi
> 2022](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html#ref-R-pander)),
> **tangram** ([S. Garbett
> 2023](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html#ref-R-tangram)),
> **ztable** ([Moon
> 2021](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html#ref-R-ztable)),
> and **condformat** ([Oller Moreno
> 2022](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html#ref-R-condformat)).

## Building a Model

### Original

**Globe-tossing**

We are going to use a toy example, but it has the same structure as a
typical statistical analyses. The first nine samples produce the
following data:

`W L W W W L W L W` (W indicates water and L indicates land.)

Designing a simple Bayesian model benefits from a design loop with three
steps.

1.  **Data story**: Motivate the model by narrating how the data might
    arise.
2.  **Update**: Educate your model by feeding it the data.
3.  **Evaluate**: All statistical models require supervision, leading to
    model revision.

#### A Data Story

> Bayesian data analysis usually means producing a story for how the
> data came to be. This story may be *descriptive*, specifying
> associations that can be used to predict outcomes, given observations.
> Or it may be *causal*, a theory of how some events produce other
> events.

All data stories have to be complete in the sense that they are
sufficient for specifying an algorithm for simulating new data.

> You can motivate your data story by trying to explain how each piece
> of data is born. This usually means describing aspects of the
> underlying reality as well as the sampling process. The data story in
> ... \[our toy case of globe-tossing\] is simply a restatement of the
> sampling process:
>
> 1.  The true proportion of water covering the globe is *`p`*.
> 2.  A single toss of the globe has a probability *`p`* of producing a
>     water (W) observation. It has a probability `1 – p` of producing a
>     land (L) observation.
> 3.  Each toss of the globe is independent of the others.

#### Bayesian Updating

> A Bayesian model begins with one set of plausibilities assigned to
> each of these possibilities. These are the prior plausibilities. Then
> it updates them in light of the data, to produce the posterior
> plausibilities. This updating process is a kind of learning, called
> **BAYESIAN UPDATING**.
>
> ...
>
> For the sake of the example only, let's program our Bayesian machine
> to initially assign the same plausibility to every proportion of
> water, every value of *p*. We'll do better than this later.
>
> ...
>
> Every time a "W" is seen, the peak of the plausibility curve moves to
> the right, towards larger values of *p*. Every time an "L" is seen, it
> moves the other direction. The maximum height of the curve increases
> with each sample, meaning that fewer values of *p* amass more
> plausibility as the amount of evidence increases. As each new
> observation is added, the curve is updated consistent with all
> previous observations.

To see the results of the different (updating) steps I am going to
reproduce Figure 2.5 (How a Bayesian model learns) of the book. In the
tidyverse-version we will learn how to write R code to reproduce the
graphic.

![Copy of Figure 2.5: **How a Bayesian model learns**. In each plot,
previous plausibilities (dashed curve) are updated in light of the
latest observation to produce a new set of plausibilities (solid
curve).](img/bayesian_model_learns_step_by_step-min.png){#fig-2-5-book-copy
fig-alt="Nine small diagrams to show the relationship between plausibility against proportion of water after each sample."}

#### Evaluate

Keep in mind two cautious principles:

1.  <div>

    > -   First, **the model's certainty is no guarantee that the model
    >     is a good one**. ... \[M\]odels of all sorts---Bayesian or
    >     not---can be very confident about an inference, even when the
    >     model is seriously misleading. This is because the inferences
    >     are conditional on the model. What your model is telling you
    >     is that, given a commitment to this particular model, it can
    >     be very sure that the plausible values are in a narrow range.
    >     Under a different model, things might look differently.
    >
    > -   Second, **it is important to supervise and critique your
    >     model's work**. ... When something is irrelevant to the
    >     machine, it won't affect the inference directly. But it may
    >     affect it indirectly ... So it is important to check the
    >     model's inferences in light of aspects of the data it does not
    >     know about. Such checks are an inherently creative enterprise,
    >     left to the analyst and the scientific community. Golems are
    >     very bad at it. (emphasis pb)

    </div>

Further keep in mind that

> the goal is not to test the truth value of the model's assumptions. We
> know the model's assumptions are never exactly right, in the sense of
> matching the true data generating process. ... Moreover, models do not
> need to be exactly true in order to produce highly precise and useful
> inferences.
>
> Instead, the objective is to check the model's adequacy for some
> purpose. This usually means asking and answering additional questions,
> beyond those that originally constructed the model. Both the questions
> and answers will depend upon the scientific context. So it's hard to
> provide general advice.

### Video

#### Bayesian Probability of the Water Proportion

[R Code snippet
2.1](https://speakerdeck.com/rmcelreath/statistical-rethinking-2023-lecture-02?slide=50):
Statistical Rethinking 2023 - Lecture 02, Slide 50.

```{r}
#| label: probability-water

sample <- c("W", "L", "W", "W", "W", "L", "W", "L", "W")
W <- sum(sample == "W") # number of W observed
L <- sum(sample == "L") # number of L observed
p <- c(0, 0.25, 0.5, 0.75, 1) # proportions W

# using vectorized version instead of sapply()
# see: https://github.com/rmcelreath/stat_rethinking_2023/issues/6
get_ways <- function(q) (q * 4)^W * ((1 - q) * 4)^L
ways <- get_ways(p)
prob <- ways / sum(ways)
cbind(p, ways, prob)

```

`prob` is called the **posterior distribution** because it's posterior
to the sample to the updating we did in light of the data.

This estimator is optimal you cannot do better than this if your model
is correct and the model here doesn't mean the particular value of P -
it means the generative hypothesis about how the garden is drawn given a
particular value of P.

#### Test Before You Est(imate)

In the planned third version of the book McElreath wants to include from
the beginning the evaluation part of the process. It is mentioned in the
book already in this chapter 2 but without practical implementation and
code examples. But we can find some remarks in his [Statistical
Rethinking Videos
2023](https://www.youtube.com/playlist?list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus).

> We want to worry about the correctness of code in scientific data
> analysis as well because scientific data analysis is in the vast
> majority of fields a kind of amateur software development. There is
> scripting and we want to document our code and we need to worry about
> errors and want to have a reliable workflow that does some quality
> assurance. So we've coded a generative simulation and we've coded an
> estimator and now we'd like to test our estimator with our generative
> simulation.
> ([37:40](https://www.youtube.com/watch?v=R1vcdhPBlXA&list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus&index=2&t=37m40s))

1.  Code a generative simulation
2.  Code an estimator
3.  Test (2) with (1)

##### Simulating the Globe Tossing

[![R Code 2.3, Statistical Rethinking 2023 - Lecture 02, \[Slide
54\](https://speakerdeck.com/rmcelreath/statistical-rethinking-2023-lecture-02?slide=54).](img/code-sim-globe-tossing-min.png){fig-alt="code snippet for a simulation of globe tossing"}](https://speakerdeck.com/rmcelreath/statistical-rethinking-2023-lecture-02?slide=54)

```{r}
#| label: sim-globe-tossing

# function to toss a globe covered p by water N times
sim_globe <- function(p = 0.7, N = 9) {
  sample(c("W", "L"), size = N, prob = c(p, 1 - p), replace = TRUE)
}
sim_globe()
```

You can simulate the experiment arbitrary times for any particular
proportion of water you like. This is a way to explore the design of an
experiment as well as debug the code.
([39:55](https://www.youtube.com/watch?v=R1vcdhPBlXA&list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus&index=2&t=39m55s)).

```{r}
#| label: replicate-sim
#| lst-cap: "R code snippet 2.4: Replicate simulation"

# replicate simulation 10 times with different p values (here: p = 0.5)
replicate(sim_globe(p = 0.5, N = 9), n = 10) 
```

::: callout-warning
###### Quarto cannot list & execute code in the same chunk

2023-05-08: `lst-label` and `lst-cap` are only working in display code
snippets but not in snippets to execute. See [lst-cap and lst-label in
Quarto?](https://community.rstudio.com/t/lst-cap-and-lst-label-in-quarto/157173)
and [lst-label and lst-cap do not produce listing caption and
reference](https://github.com/quarto-dev/quarto-cli/issues/1580).
:::

2023-07-23: In the meanwhile I found a work around:

```{r}
#| label: replicate-sim2
#| attr-source: '#lst-replicate-sim2 lst-cap="R code snippet 2.4: Replicate simulation"'

replicate(sim_globe(p = 0.5, N = 9), n = 10) 
```

##### Test the simulation at extreme values

R code snippets 2.5 and 2.6: [Slide
57](https://speakerdeck.com/rmcelreath/statistical-rethinking-2023-lecture-02?slide=57).

```{r}
#| label: test-sim-extrem-values
sim_globe(p = 1, N = 10) # test, when p = 1

sum(sim_globe(p = 0.5, N = 1e4) == "W") / 1e4 # test 1e4 (10.000) times
```

##### Code an estimator and test it with the simulation

In the following compute_posterior() function I could not manage to get
working the part with the bars. As far as I understand it comes from the
`animint` or `animint2` package but I did not know how to fill in the
second required parameter `x.name`.

```{r}
#| label: test-est-with-sim

# function to compute posterior distribution 
compute_posterior <- function(the_sample, poss = c(0, 0.25, 0.5, 0.75, 1)) {
  W <- sum(the_sample == "W") # number of W observed
  L <- sum(the_sample == "L") # number of L observed
  get_ways <- function(q) (q * 4)^W * ((1 - q) * 4)^L
  ways <- get_ways(poss)
  # ways <- sapply(poss, function(q) (q * 4)^W * ((1 - q) * 4)^L)
  post <- ways / sum(ways)
  # cannot find second parameter of function make_bar()
  # bars <- sapply(post, function(q) animint2::make_bar(q)) 
  data.frame(poss, ways, post = round(post, 3))
}

compute_posterior(sim_globe())
```

##### Summary

1.  Test the estimator where the answer is known
2.  Explore different sampling designs
3.  Develop intuition for sampling and estimation

### Tidyverse

Let's save our globe-tossing data `W L W W W L W L W` in a tibble:

```{r}
#| label: globe-tossing-data

(d <- tibble(toss = c("w", "l", "w", "w", "w", "l", "w", "l", "w")))
```

#### A Data Story

#### Bayesian Updating {#sec-expand_grid}

For the updating process we need to add to the data the cumulative
number of trials, `n_trials`, and the cumulative number of successes,
`n_successes` (i.e., `toss == "w"`).

```{r}
#| label: bayesian-updating-start

(
  d <-
  d %>% 
  mutate(n_trials  = 1:9,
         n_success = cumsum(toss == "w"))
  )

```

The program code for reproducing the Figure 2.5 of the book (here in in
this document it is @fig-2-5-book-copy) is pretty complex. Again I have
to inspect the results line by line as I had done for the
@lst-marble-data. At first I will give a short introduction what each
line does. In the next steps I will explain each step more in detail and
show the result of the corresponding lines of code.

::: {.callout-warning style="color: orange;"}
###### Preliminary Explanation of the Next Graph

The grid approximation used for the Bayesian updating in the next graph
are explained later in this chapter.#

In the meanwhile I called the code from the following code chunk line by
line and inspected the result to understand what it does.
:::

::: {.callout-note style="color: blue;"}
###### Changed parameter name

In the following listing I had to change in the `lag()` function the
parameter `k` of the Kurz'sche version to `default` as it is described
in the corresponding [help
file](https://dplyr.tidyverse.org/reference/lead-lag.html). I don't
understand why `k`\`was used. Maybe `k` was the name of the parameter of
a previous {**dplyr**} version?
:::

```{r}
#| label: fig-bayesian-model-learning
#| echo: true
#| fig-cap: "How a Bayesian model learns"
#| code-summary: "Code: **How a Bayesian model learns**"
#| code-annotation: true

# starting data
d <- tibble(toss = c("w", "l", "w", "w", "w", "l", "w", "l", "w")) |>
    mutate(n_trials  = 1:9, n_success = cumsum(toss == "w"))

sequence_length <- 50  # <1>

d %>%                 # <1>
  expand_grid(p_water = seq(from = 0, to = 1,  # <1>
                            length.out = sequence_length)) %>%  # <1>
  group_by(p_water) %>% # <2>
  mutate(lagged_n_trials  = lag(n_trials, default = 1), # <3>
         lagged_n_success = lag(n_success, default = 1)) %>% # <3>
  ungroup() %>%  # <4>
  mutate(prior      = ifelse(n_trials == 1, .5, # <5>
                             dbinom(x    = lagged_n_success, # <5>
                                    size = lagged_n_trials,  # <5>
                                    prob = p_water)),        # <5>
         likelihood = dbinom(x    = n_success,  # <5>
                             size = n_trials,   # <5>
                             prob = p_water),   # <5>
         strip      = str_c("n = ", n_trials)) %>%  # <5>
  
  # normalize prior and likelihood 
  group_by(n_trials) %>% # <6>
  mutate(prior      = prior / sum(prior),               # <6>
         likelihood = likelihood / sum(likelihood)) %>% # <6>
  
  # plot the result
  ggplot(aes(x = p_water)) + # <7>
  geom_line(aes(y = prior), linetype = 2) +  # <7>
  geom_line(aes(y = likelihood)) +           # <7>
  scale_x_continuous("proportion water",     # <7>
                     breaks = c(0, .5, 1)) + # <7>
  scale_y_continuous("plausibility", breaks = NULL) +  # <7>
  theme(panel.grid = element_blank()) +                # <7>
  facet_wrap(~ strip, scales = "free_y")               # <7>

```

1.  Creates a tibble from all combinations of inputs.
    @sec-annotation-1-expand-grid.
2.  Group data by the parameter `p-water`. @sec-annotation-2-group_by.
3.  Create two columns filled with the value of the previous row using
    the `lag()` function. @sec-annotation-3-lag.
4.  Restore the original ungrouped data structure.
    @sec-annotation-4-ungroup.
5.  Calculate Prior and Likelihood and create a column for each
    parameter. @sec-annotation-5-dbinom.
6.  Normalize Prior and Likelihood to put both of them in a probability
    metric. @sec-annotation-6-normalize.
7.  Plot the result. @sec-annotation-7-ggplot.

##### Annotation (1): `tidyr::expand_grid()` {#sec-annotation-1-expand-grid}

`tidyr::expand_grid()` creates a tibble from all combinations of inputs.
Input are generalized vectors in contrast to `tidyr::expand()` that
generates all combination of variables as well but needs as input a
dataset. The range between 0 and 1 is divided into 50 part and then it
generates all combinations by varying all columns from left to right.
The first column is the slowest, the second is faster and so on.). It
generates 450 data points (50 \* 9 trials).

```{r}
#| label: bayesian-model-learning-anno1
#| echo: true

# starting data
tbl <- tibble(toss = c("w", "l", "w", "w", "w", "l", "w", "l", "w")) |> 
    mutate(n_trials  = 1:9, n_success = cumsum(toss == "w"))

# start of bayesian modeling

### add code lines of annotation here? #########################################
sequence_length <- 50

tbl %>% 
  expand_grid(p_water = seq(from = 0, to = 1, 
                            length.out = sequence_length))
```

##### Annotation (2): `dplyr::group_by()` {#sec-annotation-2-group_by}

At first I did not understand the line `group_by(p_water)`. Why has the
data to be grouped when every row has a different value --- as I have
thought from a cursory inspection of the result? But it turned out that
after 100 records the parameter `p_water` is repeating its value.

::: {.callout-tip style="color: green;"}
## Special table format

For a better comparison (and for a later cross reference I will append
to the next code chunk a new column ID with line number and a table
format where one can scroll the long table with it 450 rows.
:::

```{r}
#| label: tbl-bayesian-model-learning-anno2-1
#| tbl-cap: "Bayesian Updating: Lagged with groupping"

# starting data
tbl1 <- tibble(toss = c("w", "l", "w", "w", "w", "l", "w", "l", "w")) |>
    mutate(n_trials  = 1:9, n_success = cumsum(toss == "w"))

# start of bayesian modeling with grouping (as in the original)
sequence_length <- 50

tbl1 <- tbl %>% 
  expand_grid(p_water = seq(from = 0, to = 1, 
                            length.out = sequence_length)) %>% 
    
  #### add code lines to show effect of annoation <2> ######################
  group_by(p_water) %>% 
  mutate(lagged_n_trials  = lag(n_trials, default = 1),
         lagged_n_success = lag(n_success, default = 1)) %>% 
  ungroup() |> 
    

  # add new column ID with row numbers 
  # and relocate it to be the first column
  mutate(ID = row_number()) |> 
  relocate(ID, .before = toss) |> 

 # provide a different format and a scroll box for the table
  kableExtra::kbl() %>%
  kableExtra::kable_classic() %>%
  kableExtra::scroll_box(height = "600px")
tbl1
```

I want to see the differences in detail. So I will provide also the
ungrouped version.

```{r}
#| label: tbl-bayesian-model-learning-anno2-2
#| tbl-cap: "Bayesian Updating: Lagged without groupping (FALSE)"

# starting data
tbl <- tibble(toss = c("w", "l", "w", "w", "w", "l", "w", "l", "w")) |> 
    mutate(n_trials  = 1:9, n_success = cumsum(toss == "w"))

# start of bayesian modeling without grouping
sequence_length <- 50

tbl2 <- tbl %>% 
  expand_grid(p_water = seq(from = 0, to = 1, 
                            length.out = sequence_length)) |> 
    
  ### add code lines to show effect without annotation <2> ##################
  mutate(lagged_n_trials  = lag(n_trials, default = 1),
         lagged_n_success = lag(n_success, default = 1)) |> 
  
  # add new column ID with row numbers and relocate it as first column
  mutate(ID = row_number()) |> 
  relocate(ID, .before = toss) |> 

  # provide a different format and a scroll box for the table
  kableExtra::kbl() %>%
  kableExtra::kable_classic() %>%
  kableExtra::scroll_box(height = "600px")
tbl2
```

It turned out that the two version differ after 51 records. (Not after
50 as one would have assumed. Apparently this has to do with the `lag()`
command because the first 51 records are identical. Beginning with row
number 52 there are differences in the column `lagged_n_trials`:

-   In the original version `lagged_n_trials` remains `1` until 100
    (included), then it changes to `2`.
-   In the version without grouping however `lagged_n_trials` changes to
    `2` with record 52. (not 51)
-   This pattern is repeated: Original version always changes after 100
    records. The version without grouping changes after 50 rows but
    starting with row 51.
-   The same differences appear with `lagged_n_success` but 50 records
    later: The first difference appears after row 101.

##### Annotation (3): `dplyr::lag()` {#sec-annotation-3-lag}

The next line uses the `dplyr::lag()` command: The function (`lag()`)
finds the "previous" values in a vector (time series). This is useful
for comparing values behind of the current values. See [Compute lagged
or leading values](https://dplyr.tidyverse.org/reference/lead-lag.html).

We need to get the immediately previous values for drawing the prior
probabilities in the current graph (= dashed line or `linetype = 2` in
ggplot parlance). In the relation with the posterior probabilities the
difference form the prior possibility is always `1`\` (this is the
option `default = 1` in the `lag()` function. This is now the correct
explanation for the differences starting with rows 52 resp. 102 (and not
51 resp. 101).

The result is already shown under @sec-annotation-2-group_by in the
result of the code listing @tbl-bayesian-model-learning-anno2-1.

```{r}
#| label: bayesian-model-learning-anno3

# starting data
tbl <- tibble(toss = c("w", "l", "w", "w", "w", "l", "w", "l", "w")) |> 
    mutate(n_trials  = 1:9, n_success = cumsum(toss == "w"))

# start of bayesian modeling with grouping (as in the original)
sequence_length <- 50

tbl3 <- tbl %>% 
  expand_grid(p_water = seq(from = 0, to = 1, 
                            length.out = sequence_length)) %>% 
  group_by(p_water) %>% 
    
  ### add code lines of annotation <3> ####################################
  mutate(lagged_n_trials  = lag(n_trials, default = 1),
         lagged_n_success = lag(n_success, default = 1)) %>% 
  ungroup()
tbl3
```

##### Annotation (4): `dplyr::ungroup()` {#sec-annotation-4-ungroup}

This is just the reversion of the grouping command `group_by(p_water)`
mentioned in @sec-annotation-2-group_by.

##### Annotation (5): dbinom() {#sec-annotation-5-dbinom}

This is the core of the prior and likelihood calculation. It uses
`base::dbinom()`, to calculate two alternative events. `dbinom()` is the
R function for the binomial distribution, a distribution provided by the
probability theory for "coin tossing" problems.

The "`d`" in `dbinom()` stands for *density*. Functions named in this
way almost always have corresponding partners that begin with "`r`" for
random samples and that begin with "`p`" for cumulative probabilities.
See for example the [help
file](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Binomial.html).
See also the blog entries [An Introduction to the Binomial
Distribution](https://www.statology.org/binomial-distribution/) and [A
Guide to dbinom, pbinom, qbinom, and rbinom in
R](https://www.statology.org/dbinom-pbinom-qbinom-rbinom-in-r/) of the
Statalogy website.

The results of each of the different calculation (prior and pikelihood)
are collected in with `mutate()` into two new generated columns.

There is no prior for the first trial, so it is assumed that it is 0.5.
The formula for the binomial distribution uses for the prior the
lagged-version whereas the likelihood uses the current version. These
two lines provide the essential calculations: They match the 50 grid
points as assumed water probabilities of every trial to their trial
outcome (`W` or `L`) probabilities.

The third `mutate()` generates the `strip` variable consisting of the
prefix "N =" followed by the counts of the number of trials. This will
later provide the title for the the different facets of the plot.

```{r}
#| label: tbl-bayesian-model-learning-anno5
#| tbl-cap: "Bayesian Updating: Calculating prior and likelihood"

# starting data
tbl <- tibble(toss = c("w", "l", "w", "w", "w", "l", "w", "l", "w")) |> 
    mutate(n_trials  = 1:9, n_success = cumsum(toss == "w"))

# start of bayesian modeling with grouping (as in the original)
sequence_length <- 50

tbl5 <- tbl %>% 
  expand_grid(p_water = seq(from = 0, to = 1, 
                            length.out = sequence_length)) %>% 
  group_by(p_water) %>% 
    
  mutate(lagged_n_trials  = lag(n_trials, default = 1),
         lagged_n_success = lag(n_success, default = 1)) %>% 
  ungroup() %>% 
    
  ### add code lines of annotation <5> ########################
  mutate(prior      = ifelse(n_trials == 1, .5,
                             dbinom(x    = lagged_n_success, 
                                    size = lagged_n_trials, 
                                    prob = p_water)),
         likelihood = dbinom(x    = n_success, 
                             size = n_trials, 
                             prob = p_water),
         strip      = str_c("n = ", n_trials)) |> 
 
  # provide table scroll box
  mutate(ID = row_number()) |> 
  relocate(ID, .before = toss) |> 
  kableExtra::kbl() %>%
  kableExtra::kable_classic() %>%
  kableExtra::scroll_box(height = "600px")
tbl5
```

##### Annotation (6): Normalizing {#sec-annotation-6-normalize}

The code lines in annotation 6 normalize the prior and the likelihood by
grouping the data by `n_trials`. Dividing every prior and likelihood
values by their respective sum puts them both in a probability metric.
This metric is important for the comparisons of different probabilities.

> If you don't normalize (i.e., divide the density by the sum of the
> density), their respective heights don't match up with those in the
> text. Furthermore, it's the normalization that makes them directly
> comparable.

```{r}
#| label: tbl-bayesian-model-learning-anno6
#| tbl-cap: "Bayesian Updating: Normalizing prior and likelihood"

# starting data
tbl <- tibble(toss = c("w", "l", "w", "w", "w", "l", "w", "l", "w")) |> 
    mutate(n_trials  = 1:9, n_success = cumsum(toss == "w"))

# start of bayesian modeling with grouping (as in the original)
sequence_length <- 50

tbl6 <- tbl %>% 
  expand_grid(p_water = seq(from = 0, to = 1, 
                            length.out = sequence_length)) %>% 
  group_by(p_water) %>% 
  mutate(lagged_n_trials  = lag(n_trials, default = 1),
         lagged_n_success = lag(n_success, default = 1)) %>% 
  ungroup() %>% 
  mutate(prior      = ifelse(n_trials == 1, .5,
                             dbinom(x    = lagged_n_success, 
                                    size = lagged_n_trials, 
                                    prob = p_water)),
         likelihood = dbinom(x    = n_success, 
                             size = n_trials, 
                             prob = p_water),
         strip      = str_c("n = ", n_trials)) |> 
    
  ### add code lines of annotation <6> ################## 
  group_by(n_trials) %>% 
  mutate(prior      = prior / sum(prior),
         likelihood = likelihood / sum(likelihood)) %>% 
    
  # provide table scrool box for better comparison of the results
  mutate(ID = row_number()) |> 
  relocate(ID, .before = toss) |> 
  kableExtra::kbl() %>%
  kableExtra::kable_classic() %>%
  kableExtra::scroll_box(height = "600px")
tbl6
```

##### Annotation (7): {#sec-annotation-7-ggplot}

The remainder of the code prepares the plot by using the 50 grid points
in the range from 0 to 1 as the x-axis; prior and likelihood as y-axis.
To distinguish the prior from the likelihood is uses a dashed line for
the prior (`linetyp = 2`) and a full line (default) for the likelihood.
The x-axis has three breaks (`0, 0.5, 1`) whereas the y-axis has no
break and no scale (`scales = "free_y"`).

```{r}
#| label: fig-bayesian-model-learning-anno7
#| fig-cap: "Demonstrating Bayesian Updating"
#| echo: true

# starting data
d <- tibble(toss = c("w", "l", "w", "w", "w", "l", "w", "l", "w")) |> 
    mutate(n_trials  = 1:9, n_success = cumsum(toss == "w"))

sequence_length <- 50

d %>% 
  expand_grid(p_water = seq(from = 0, to = 1, 
                            length.out = sequence_length)) %>% # <1>
  group_by(p_water) %>% # <2>
  mutate(lagged_n_trials  = lag(n_trials, default = 1),
         lagged_n_success = lag(n_success, default = 1)) %>% # <3>
  ungroup() %>% # <4>
  mutate(prior      = ifelse(n_trials == 1, .5,
                             dbinom(x    = lagged_n_success, 
                                    size = lagged_n_trials, 
                                    prob = p_water)),
         likelihood = dbinom(x    = n_success, 
                             size = n_trials, 
                             prob = p_water),
         strip      = str_c("n = ", n_trials)) %>% # <5>
  
  group_by(n_trials) %>% # <6>
  mutate(prior      = prior / sum(prior),
         likelihood = likelihood / sum(likelihood)) %>% # <6>
  
  ### add code for annotation <7> for plotting the result ##############
  ggplot(aes(x = p_water)) + # <7>
  geom_line(aes(y = prior), 
            linetype = 2) + # <7>
  geom_line(aes(y = likelihood)) + # <7>
  scale_x_continuous("proportion water", breaks = c(0, .5, 1)) + # <7>
  scale_y_continuous("plausibility", breaks = NULL) + # <7>
  theme(panel.grid = element_blank()) + # <7>
  facet_wrap(~ strip, scales = "free_y") # <7>

```

## Components of the Model

### Original

We observed three components of the model:

1.  a **likelihood function**: "the number of ways each conjecture could
    produce an observation,"

2.  one or more **parameters**: "the accumulated number of ways each
    conjecture could produce the entire data," and

3.  **a prior**: "the initial plausibility of each conjectured cause of
    the data"

#### List Variables

> Variables are just symbols that can take on different values. In a
> scientific context, variables include things we wish to infer, such as
> proportions and rates, as well as things we might observe, the
> data....
>
> Unobserved variables are usually called **parameters**. (emphasis in
> the original)

Take as example the globe tossing models: There are three variables: `W`
and `L` (water or land) and the proportion of water and land `p`. We
observe the events of water or land but we calculate (do not observe
directly) the proportion of water and land. So `p` is a parameter as
defined above.

#### Define Variables

> In defining each \[variable\], we build a model that relates the
> variables to one another. Remember, the goal is to count all the ways
> the data could arise, given the assumptions.

##### Observed Variables

For each unobserved variable (parameter) we need to define the relative
number of ways---the probability---that the values of each observed
variable could arise. And then for each unobserved variable, we need to
define the prior plausibility of each value it could take.

For the count of water *W* and land *L* in the globe tossing model, we
**define how plausible any combination of *W* and *L* would be, for a
specific value of *p*.** (emphasis is mine)

**CHANGE OR MOVE THIS PARAGRAPH**: This idea is implemented in the
functions of `expand()`, `expand_grid()` and `crossing()` of the
{**tidyr**} package: They generate all combinations of variables found
as is demonstrated in the section [Bayesian Updating of the
brms-variant](02b-small-and-large-worlds.html#sec-expand_grid).

Instead of counting we can also use a mathematical function to calculate
the probability of all combinations. A distribution function assigned to
an observed variable is usually called a **LIKELIHOOD**.

In the case of the globe-tossing model the appropriate distributional
function is the **binomial distribution**. (Does this means that I have
to know more on probability theory to decide when to choose which
distribution?)

**Likelihood for prob = 0.5**

The likelihood in the globe-tossing example (9 trials, 6 with `W` and 3
with `L`) is easily computed:

```{r}
#| label: likelihood-prob-0.5-a

## R code 2.2
dbinom(6, size = 9, prob = 0.5)
```

In this example it is assumed that the probability of `W` and `L` are
equal distributed. We calculated how plausible the combination of *6W*
and *3L* would be, for the specific value of *p = 0.5*. The result is
with 16% a pretty low probability.

**Likelihood for many prob values**

To get a better idea what the best estimation of the probability is, we
could vary systematically the `p` value and look for the maximum. A
demonstration how this is done can be seen in @sec-calcu-10-probs. It
shows a maximum at *prob = 0.7*.

##### Unobserved Variables

Even variables that are not observed (= parameters) we need to define
them. In the globe-tossing model there is only one parameter (p), but
most models have more than one unobserved variables.

> In future chapters, there will be more parameters in your models. In
> statistical modeling, many of the most common questions we ask about
> data are answered directly by parameters:
>
> -   What is the average difference between treatment groups?
> -   How strong is the association between a treatment and an outcome?
> -   Does the effect of the treatment depend upon a covariate?
> -   How much variation is there among groups?
>
> \[We will\] see how these questions become extra parameters inside the
> distribution function we assign to the data.

::: callout-important
###### Parameter & Prior

For every parameter we must provide a distribution of prior
plausibility, its Prior. This is also true when the number of trials is
null (N = 0), e.g. even in the initial state of information we need a
prior.
:::

When you have a previous estimate, that can become the prior. As a
result, each estimate (posterior probability) becomes then the prior for
the next step. Where do priors come from? They are both engineering
assumptions, chosen to help the machine learn, and scientific
assumptions, chosen to reflect what we know about a phenomenon. Because
the prior is an assumption, it should be interrogated like other
assumptions: by altering it and checking how sensitive inference is to
the assumption.

::: callout-note
###### Data or Parameters

Data are measured and known; parameters are unknown and must be
estimated from data. But there is a deep identity between certainty
(data) and uncertainty (parameters): Sometimes we observe a variable
(data), sometimes not (parameter) but it could be that the same
distribution function applies. An exploitation of the identity between
data & parameters is it to incorporate measurement error and missing
data into your modeling.

::: callout-tip
For more in this topic, check out McElreath's lecture, [*Understanding
Bayesian statistics without frequentist
language*](https://youtu.be/yakg94HyWdE).
:::
:::

#### A Model is Born

> The observed variables *W* and *L* are given relative counts through
> the binomial distribution.

$$W∼Binomial(n,p)\space where\space N = W + L$$

The above is just a convention for communicating the assumption that the
relative counts of ways to realize *W* in *N* trials with probability
*p* on each trial comes from the binomial distribution.

Our binomial likelihood contains a parameter for an unobserved variable,
*p*. Parameters in Bayesian models are assigned priors:

$$p∼Uniform(0,1)$$

which expresses the model assumption that the entire range of possible
values for p are equally plausible.

### Tidyverse

Given a probability of .5, (e.g. equal probability to both events `W`
and `L`) we can use the `dbinom()` function to determine the likelihood
of 6 out of 9 tosses coming out water.

```{r}
#| label: likelihood-prob-0.5-b

dbinom(x = 6, size = 9, prob = 0.5)

```

McElreath suggests:

> Change the 0.5 to any other value, to see how the value changes.

#### Calculation likelihood with 10 different values of prob {#sec-calcu-10-probs}

```{r}
#| label: likelihood-10-probs

(d <- tibble(prob = seq(from = 0, to = 1, by = .1)) |> 
    mutate(likelihood = dbinom(x = 6, size = 9, prob = prob))
)

```

Filter the row with the `prob` maximum.

```{r}
#| label: filter-max

d  |>  
    filter(likelihood == max(likelihood))
```

In the series of values you will notice several point:

1.  The values start with zero until a maximum of 0.267 and decline to
    zero again. The maximum is with `prob = 0.7`, a proportion of `W`
    and `L` that is --- as we know from our large world knowledge
    (knowledge outside the small world of the model) --- already pretty
    near the real distribution of about 0.71. (see [How Much of the
    Earth Is Covered by
    Water?](https://www.thedailyeco.com/how-much-of-the-earth-is-covered-by-water-122.html))
2.  You see that the first prob (0) and last prob (1) values are both
    zero. From the result (6 `W` and 3 `L`) prob cannot be 0 or 1
    because there a both `W` and `L` in the observed sample.

The above code chunk is my interpretation from the quote "Change the 0.5
to any other value, to see how the value changes." Kurz has another
interpretation when he draws a graph of 100 prob values from 0 to 1:

#### Plot likelihood for 100 values of prob {#sec-likelihood-for-many-p-values-b}

```{r}
#| label: fig-likelihood-100-prob
#| fig-cap: "Likelihood for 100 values of prob, from 0 to 1, by steps of 0.01"

tibble(prob = seq(from = 0, to = 1, by = .01)) %>% 
  ggplot(aes(x = prob, y = dbinom(x = 6, size = 9, prob = prob))) +
  geom_line() +
  labs(x = "probability",
       y = "binomial likelihood") +
  theme(panel.grid = element_blank())

```

In contrast to *p = 0.5* with a probability of 0.16 the
@fig-likelihood-100-prob shows a maximum at about *p = 0.7* and a
probability estimated from the graph of about 0.26-0.28. We will get
more detailed data later in the book.

It is interesting to see that even the maximum probability is not very
high. The reason is that there are many other configurations
(distributions of `W`s and `L`s) to produce the result of *6W* and *3L*.
Even if all these other distributions have a small probability they
"eat" all with their share from the maximum.

(I wanted to write "from the maximum of 1.0" but I think this would not
be correct as the above graph displays the rate of change in cumulative
probability (the **probability density**) and not the probability itself
(the **probability mass**). See the following quote:

> For mathematical reasons, probability densities can be greater than 1.
> ... Probability *density* is the rate of change in cumulative
> probability. So where cumulative probability is increasing rapidly,
> density can easily exceed 1. But if we calculate the area under the
> density function, it will never exceed 1. Such areas are also called
> *probability mass*. (11.47 in calibre ebook-viewer reference mode)

In the literature the abbreviation **PDF ([probability density
function](https://en.wikipedia.org/wiki/Probability_density_function))**
is often used for the (probability) density. See also th Wikipedia entry
about [probability
distribution](https://en.wikipedia.org/wiki/Probability_distribution).

McElreath says:

> The prior is a probability distribution for the parameter. In general,
> for a uniform prior from *a* to *b*, the probability of any point in
> the interval is 1/(*b -- a*). If you're bothered by the fact that the
> probability of every value of *p* is 1, remember that every
> probability distribution must sum (integrate) to 1. The expression
> 1/(*b -- a*) ensures that the area under the flat line from *a* to *b*
> is equal to 1.

Kurz demonstrated the truth of this quote with several *b* values while
holding *a* constant:

```{r}
#| label: uniform-prior1

tibble(a = 0,
       b = c(1, 1.5, 2, 3, 9)) %>% 
  mutate(prob = 1 / (b - a))
```

Verified with a plot he divides the range of the *b* parameter (*0-9*)
into 500 segments (*parameter_space*) and uses the `dunif()`
distribution to calculate the probabilities for a uniform distribution:

```{r}
#| label: uniform-prior2

tibble(a = 0,
       b = c(1, 1.5, 2, 3, 9)) %>% 
  expand_grid(parameter_space = seq(from = 0, to = 9, length.out = 500)) %>% 
  mutate(prob = dunif(parameter_space, a, b),
         b    = str_c("b = ", b)) %>% 
  
  ggplot(aes(x = parameter_space, y = prob)) +
  geom_area() +
  scale_x_continuous(breaks = c(0, 1:3, 9)) +
  scale_y_continuous(breaks = c(0, 1/9, 1/3, 1/2, 2/3, 1),
                     labels = c("0", "1/9", "1/3", "1/2", "2/3", "1")) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank()) +
  facet_wrap(~ b, ncol = 5)

```

This figure demonstrates that the area in the whole paramater space is
*1.0*. It is a nice example how to calculate the probability *mass* (in
contrast to the curve of the probability *density*).

I will skip the excursus and plot about the equivalence of
`Uniform(0,1)` and the beta distribution calculated with `dbeta().`

## Making the Model Go

### Bayes' Theorem

#### Original

> Once you have named all the variables and chosen definitions for each,
> a Bayesian model can update all of the prior distributions to their
> purely logical consequences: the **POSTERIOR DISTRIBUTION**. For every
> unique combination of data, likelihood, parameters, and prior, there
> is a unique posterior distribution. This distribution contains the
> relative plausibility of different parameter values, conditional on
> the data and model. The posterior distribution takes the form of the
> probability of the parameters, conditional on the data.

In the case of the globe-tossing model we can write:

$$
Pr(p|W, L)
$$

This has to be interpreted as "the probability of each possible value of
*p*, conditional on the specific *W* and *L* that we observed."

$$
Pr(p|W,L) = \frac{Pr(W,L|p)Pr(p)}{Pr(W,L)}
$$

> And this is Bayes' theorem. It says that the probability of any
> particular value of *p*, considering the data, is equal to the product
> of the relative plausibility of the data, conditional on *p*, and the
> prior plausibility of *p*, divided by this thing Pr(*W, L*), which
> I'll call the *average probability of the data*.

Expressed in words:

$$
Posterior = \frac{Probability\space of\space the\space data\space ✕\space Prior}{Average\space probability\space of\space the\space data}
$$

Other names for the *average probability of the data*:

-   evidence
-   average likelihood
-   marginal likelihood

The job of the average probability of the data is just to standardize
the posterior, to ensure it sums (integrates) to one.

::: callout-important
###### Key lesson

The posterior is proportional to the product of the prior and the
probability of the data.
:::

> \[E\]ach specific value of *p*, the number of paths through the garden
> of forking data is the product of the prior number of paths and the
> new number of paths. **Multiplication is just compressed counting.**
> The average probability on the bottom just standardizes the counts so
> they sum to one. (emphasis is mine)

> \[The following graph\] illustrates the multiplicative interaction of
> a prior and a probability of data. On each row, a prior on the left is
> multiplied by the probability of data in the middle to produce a
> posterior on the right. The probability of data in each case is the
> same. The priors however vary. As a result, the posterior
> distributions vary.

![Figure 2.6 of the original book](img/SR2-fig2_6-min.jpg){#fig-2-6-book
fig-alt="The posterior distribution as a product of the prior distribution and likelihood. Top: A flat prior constructs a posterior that is simply proportional to the likelihood. Middle: A step prior, assigning zero probability to all values less than 0.5, results in a truncated posterior. Bottom: A peaked prior that shifts and skews the posterior, relative to the likelihood."}

For my understanding it is important to reproduce the above graph with R
code as it is shown in the next section.

#### Tidyverse

> \[The following graph\] illustrates the multiplicative interaction of
> a prior and a probability of data. On each row, a prior on the left is
> multiplied by the probability of data in the middle to produce a
> posterior on the right. The probability of data in each case is the
> same. The priors however vary. As a result, the posterior
> distributions vary.

```{r}
#| label: prepare-model
#| attr-source: '#lst-prepare-model lst-cap="The posterior distribution as a product of the prior distribution and likelihood."'

sequence_length <- 1e3

d <-
  tibble(probability = seq(from = 0, to = 1, length.out = sequence_length)) %>% 
  expand_grid(row = c("flat", "stepped", "Laplace"))  %>% 
  arrange(row, probability) %>% 
  mutate(prior = ifelse(row == "flat", 1,
                        ifelse(row == "stepped", rep(0:1, each = sequence_length / 2),
                               exp(-abs(probability - 0.5) / .25) / ( 2 * 0.25))),
         likelihood = dbinom(x = 6, size = 9, prob = probability)) %>% 
  group_by(row) %>% 
  mutate(posterior = prior * likelihood / sum(prior * likelihood)) %>% 
  pivot_longer(prior:posterior)  %>% 
  ungroup() %>% 
  mutate(name = factor(name, levels = c("prior", "likelihood", "posterior")),
         row  = factor(row, levels = c("flat", "stepped", "Laplace")))

```

In comparison to my very detailed code annotations of
@fig-bayesian-model-learning there are different lines of code, but
generally there is nothing conceptually new: We use again
`expand_grid()` to create a tibble of input combinations and create with
`mutate()` two columns for prior and likelihood. We do not use the
`lag()` functions as we calculate only for one prior and one likelihood.
Kurz advises us that is "easier to just make each column of the plot
separately. We can then use the elegant and powerful syntax from [Thomas
Lin Pedersen](https://www.data-imaginist.com/)'s (2022) [patchwork
package](https://patchwork.data-imaginist.com/) to combine them."

```{r}
#| label: fig-plot-model
#| fig-cap: "Figure 2.6 from book reproduced with tidyverse code shows the posterior distribution as a product of the prior distribution and likelihood. Top: A flat prior constructs a posterior that is simply proportional to the likelihood. Middle: A step prior, assigning zero probability to all values less than 0.5, results in a truncated posterior. Bottom: A peaked prior that shifts and skews the posterior, relative to the likelihood. Cormpare it with @fig-2-6-book."

library(patchwork)

p1 <-
  d %>%
  filter(row == "flat") %>% 
  ggplot(aes(x = probability, y = value)) +
  geom_line() +
  scale_x_continuous(NULL, breaks = NULL) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ name, scales = "free_y")

p2 <-
  d %>%
  filter(row == "stepped") %>% 
  ggplot(aes(x = probability, y = value)) +
  geom_line() +
  scale_x_continuous(NULL, breaks = NULL) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank(),
        strip.background = element_blank(),
        strip.text = element_blank()) +
  facet_wrap(~ name, scales = "free_y")

p3 <-
  d %>%
  filter(row == "Laplace") %>% 
  ggplot(aes(x = probability, y = value)) +
  geom_line() +
  scale_x_continuous(NULL, breaks = c(0, .5, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank(),
        strip.background = element_blank(),
        strip.text = element_blank()) +
  facet_wrap(~ name, scales = "free_y")

# combine
# library(patchwork) # defined in setup chunk
p1 / p2 / p3

```

@fig-plot-model shows that the same likelihood with a different prior
results in a different posterior.

### Motors

#### Original

> Recall that your Bayesian model is a machine, a figurative golem. It
> has built-in definitions for the likelihood, the parameters, and the
> prior. And then at its heart lies a motor that processes data,
> producing a posterior distribution. The action of this motor can be
> thought of as *conditioning* the prior on the data. As explained in
> the previous section, this conditioning is governed by the rules of
> probability theory, which defines a uniquely logical posterior for set
> of assumptions and observations.
>
> However, knowing the mathematical rule is often of little help,
> because many of the interesting models in contemporary science cannot
> be conditioned formally, no matter your skill in mathematics. And
> while some broadly useful models like linear regression can be
> conditioned formally, this is only possible if you constrain your
> choice of prior to special forms that are easy to do mathematics with.
> We'd like to avoid forced modeling choices of this kind, instead
> favoring conditioning engines that can accommodate whichever prior is
> most useful for inference.
>
> What this means is that various numerical techniques are needed to
> approximate the mathematics that follows from the definition of Bayes'
> theorem. In this book, you'll meet three different conditioning
> engines, numerical techniques for computing posterior distributions:

What are the numerical techniques for computing posterior distributions
explained in the book?

1.  Grid approximation
2.  Quadratic approximation
3.  Markov chain Monte Carlo (MCMC)

> There are many other engines, and new ones are being invented all the
> time. But the three you'll get to know here are common and widely
> useful. (p. 39)

::: callout-tip
**Rethinking: How you fit the model is part of the model**. Earlier in
this chapter, I implicitly defined the model as a composite of a prior
and a likelihood. That definition is typical. But in practical terms, we
should also consider how the model is fit to data as part of the model.
In very simple problems, like the globe tossing example that consumes
this chapter, calculation of the posterior density is trivial and
foolproof. In even moderately complex problems, however, the details of
fitting the model to data force us to recognize that our numerical
technique influences our inferences. This is because different mistakes
and compromises arise under different techniques. The same model fit to
the same data using different techniques may produce different answers.
When something goes wrong, every piece of the machine may be suspect.
And so our golems carry with them their updating engines, as much slaves
to their engineering as they are to the priors and likelihoods we
program into them.
:::

#### Tidyverse

In my own words: Processing the built-in definitions for the likelihood,
the parameters, and the prior produces the posterior distribution. This
process is governed by the rule of probability theory. But knowing the
mathematics does generally not help for two reasons:

-   Many of the interesting models in contemporary science cannot be
    conditioned formally
-   Though some broadly useful models like linear regression can be
    conditioned formally, this is only possible if you constrain your
    choice of prior to special forms that are easy to do mathematics
    with.

Therefore are various numerical techniques needed to approximate the
mathematics that follows from the definition of Bayes' theorem. From the
three widely useful methods (grid approximation, quadratic approximation
and MCMC) covered in the SR2-book the tidyverse version of this material
concentrates on using the [{**brms**}
package](https://paul-buerkner.github.io/brms/).

::: {.callout-caution style="color: orange;"}
###### Jumping quickly into MCMC

> The consequence is that this version will jump rather quickly into
> MCMC. This will be awkward at times because it will force us to
> contend with technical issues in earlier problems in the text than
> McElreath originally did.
:::

### Grid approximation

#### Original

> While most parameters are *continuous*, capable of taking on an
> infinite number of values, it turns out that we can achieve an
> excellent approximation of the continuous posterior distribution by
> considering only a finite grid of parameter values. At any particular
> value of a parameter, *p*', it's a simple matter to compute the
> posterior probability: just multiply the prior probability of *p*' by
> the likelihood at *p*'. Repeating this procedure for each value in the
> grid generates an approximate picture of the exact posterior
> distribution. This procedure is called **GRID APPROXIMATION**.

Grid approximation is very useful as a pedagogical tool. But in most of
your real modeling, grid approximation isn't practical because it scales
poorly, as the number of parameters increases.

1.  Define the grid. This means you decide how many points to use in
    estimating the posterior, and then you make a list of the parameter
    values on the grid.
2.  Compute the value of the prior at each parameter value on the grid.
3.  Compute the likelihood at each parameter value.
4.  Compute the unstandardized posterior at each parameter value, by
    multiplying the prior by the likelihood.
5.  Finally, standardize the posterior, by dividing each value by the
    sum of all values.

In the globe tossing model the five steps are as follows:

```{r}
#| label: fig-grid-approx-base1
#| fig-cap: "Grid Approximation with 20 points with `prior = 1`"

# 1. define grid
p_grid <- seq(from = 0, to = 1, length.out = 20)

# 2. define prior
prior <- rep(1, 20)

# 3. compute likelihood at each value in grid
likelihood <- dbinom(x = 6, size = 9, prob = p_grid)

# 4. compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# 5. standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

# 6 display the posterior distribution
## R code 2.4
plot(p_grid, posterior,
  type = "b",
  xlab = "probability of water", ylab = "posterior probability"
)
mtext('20 points with "prior = 1"')


```

``` r
#| label: lst-own-tidyverse
#| lst-cap: "My own tidyverse plot <-  just for learning purposes"

# # instead of books R code 2.4 I will use a tidyverse approach
# df <- dplyr::bind_cols("prob" = p_grid, "post" = posterior)
# ggplot2::ggplot(df, ggplot2::aes(x = prob, y = post)) +
#     ggplot2::geom_line() +
#     ggplot2::geom_point()
```

##### Change likelihood parameters

The parameters for the calculated likelihood is based on the binomial
distribution and is shaping the above plot:

-   x = number of water events `W`
-   size = number of sample trials = number of observations
-   prob = success probability on each trial = probability of `W` (water
    event)

It does not matter in the code for @fig-grid-approx-base1 what prior
probability is chosen in the range from 0 to 1, if the probability of
`W` is greater than zero and --- and by definition of the binomial
function --- equal for all 20 events. This does not only conform to
values but also for functions.

```{r}
#| label: fig-grid-approx-base1a
#| fig-cap: "Grid Approximation with 20 points with `prior = 0.1`"

# 1. define grid
p_grid <- seq(from = 0, to = 1, length.out = 20)

# 2. define prior
prior <- rep(0.1, 20)

# 3. compute likelihood at each value in grid
likelihood <- dbinom(x = 6, size = 9, prob = p_grid)

# 4. compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# 5. standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

# 6 display the posterior distribution
## R code 2.4
plot(p_grid, posterior,
  type = "b",
  xlab = "probability of water", ylab = "posterior probability"
)
mtext('20 points with "prior = 0.1"')
```

##### Changing prior parameters

To see the influence of the prior probability on the posterior
probability by using the same likelihood the book offers two code
snippets. Replace the definition of the prior from the `grid-approx-a`
chunk (number 2 in the code snippet) --- one at a time --- with the
following lines of code:

``` r
#| label: lst-different-priors
#| lst-cap: "Using two different priors"
prior <- ifelse(p_grid < 0.5, 0, 1)
prior <- exp(-5 * abs(p_grid - 0.5))
```

The rest of the code remains the same.

```{r}
#| label: fig-grid-approx-base2
#| fig-cap: "Grid Approximation with prior of `ifelse(p_grid < 0.5, 0, 1)`"

# 1. define grid
p_grid <- seq(from = 0, to = 1, length.out = 20)

# 2. define prior
prior <- ifelse(p_grid < 0.5, 0, 1)

# 3. compute likelihood at each value in grid
likelihood <- dbinom(x = 6, size = 9, prob = p_grid)

# 4. compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# 5. standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

# 6 display the posterior distribution 
## R code 2.4
plot(p_grid, posterior,
  type = "b",
  xlab = "probability of water", ylab = "posterior probability"
)
mtext('20 points with a prior of "ifelse(p_grid < 0.5, 0, 1)"')
```

@fig-grid-approx-base2 employs as prior `ifelse(p_grid < 0.5, 0, 1)`,
meaning that if `prob` is smaller than 0.5 use zero as prior otherwise
1.

```{r}
#| label: fig-grid-approx-base3
#| fig-cap: "Grid Approximation with prior of `exp(-5 * abs(p_grid - 0.5))`"

# 1. define grid
p_grid <- seq(from = 0, to = 1, length.out = 20)

# 2. define prior
prior <- exp(-5 * abs(p_grid - 0.5))

# 3. compute likelihood at each value in grid
likelihood <- dbinom(x = 6, size = 9, prob = p_grid)

# 4. compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# 5. standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

# 6 display the posterior distribution 
## R code 2.4
plot(p_grid, posterior,
  type = "b",
  xlab = "probability of water", ylab = "posterior probability"
)
mtext('20 points with prior of "exp(-5 * abs(p_grid - 0.5))"')
```

##### Disadvantage

Grid approximation is very expansive. The number of unique values to
consider in the grid grows rapidly as the number of parameters in the
model increases. For the single-parameter globe tossing model, it's no
problem to compute a grid of 100 or 1000 values. But for two parameters
approximated by 100 values each, that's already 100^2^ = 10.000 values
to compute. For 10 parameters, the grid becomes many billions of values.
These days, it's routine to have models with hundreds or thousands of
parameters. The grid approximation strategy scales very poorly with
model complexity, so it won't get us very far. But it is a very useful
didactically as it help to understand the general principle.

#### Reconsideration

::: callout-caution
###### Puzzlement

As demonstrated in @fig-plot-model a different prior with the same
likelihood has different posteriors. So the curves of all three examples
are different.

But what I do not understand is the fact that in the third example the
result is very different from the other two priors: About 0.5 and not
0.7. My explication: I have only 9 trials. With many more trials the
difference in the density would balance out bit by bit.

But it turns out, that all three show about the same maximum I thought
that the chosen prior did not have any effect the outcome. But it turns
out that different priors results in different PDFs.\
My explication: I have only 9 trials. With many more trials the
difference in the density would balance out bit by bit.
:::

I am going to test my hypothesis. To demonstrate that with more Bayesian
updates we will approach the correct result 0b about 0.7 I will work
with 1000 samples.

```{r}
#| label: fig-grid-approx-base4
#| fig-cap: "Grid Approximation with prior of `exp(-5 * abs(p_grid - 0.5))`"

# 1. define grid
p_grid <- seq(from = 0, to = 1, length.out = 20)

# 2. define prior
prior <- exp(-5 * abs(p_grid - 0.5))

# 3. compute likelihood at each value in grid
likelihood <- dbinom(x = 600, size = 1e3, prob = p_grid)

# 4. compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# 5. standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

# 6 display the posterior distribution 
## R code 2.4
plot(p_grid, posterior,
  type = "b",
  xlab = "probability of water", ylab = "posterior probability"
)
mtext('20 points for a 1000 samples with prior of "exp(-5 * abs(p_grid - 0.5))"')
```

In fact the posterior distribution is nearer to the correct result of
0.7. In @fig-grid-approx-base3 we have a maximum at about 0.52 and in
@fig-grid-approx-base4 we have already 0.58. We should also taking into
account that with more samples our proportion of W to L will approach
the real Water:Land proportion of about 0.71 too.

I will demonstrate this with 10000 samples and a W:L proportion of 7:3.

```{r}
#| label: fig-grid-approx-base5
#| fig-cap: "Grid Approximation with prior of `exp(-5 * abs(p_grid - 0.5))`"

# 1. define grid
p_grid <- seq(from = 0, to = 1, length.out = 20)

# 2. define prior
prior <- exp(-5 * abs(p_grid - 0.5))

# 3. compute likelihood at each value in grid
likelihood <- dbinom(x = 7e3, size = 1e4, prob = p_grid)

# 4. compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# 5. standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

# 6 display the posterior distribution 
## R code 2.4
plot(p_grid, posterior,
  type = "b",
  xlab = "probability of water", ylab = "posterior probability"
)
mtext('20 points for a 10000 samples with prior of "exp(-5 * abs(p_grid - 0.5))" and W:L = 7:3.' )
```

In this example the maximum of probability is already 0.68! We can say
that **with every chosen prior we will get finally the correct
result!**.

But the choice of the prior is still important as it determines how many
Bayesian updates we need to get the right result. If we have an awkward
prior and not the appropriate size of the sample we will get a posterior
distribution showing us a wrong maximum of probability. In that case the
process of approximation has not reached a state where the probability
maximum is near the correct result. The problem is: Most time we do not
know the correct solution and can't therefore decide if we have had
enough Bayesian updates.

#### Tidyverse

We just employed grid approximation in the @fig-plot-model chunk. To get
nice smooth lines, we computed in the @lst-prepare-model chunk the
posterior over 1,000 evenly-spaced points on the probability space. Here
we'll prepare for the left panel of Figure 2.7 with just 5 evenly-spaced
points.

As a reminder I will add comments for the five steps for the grid
approximation procedure. As it is explained in detail on several other
places I will not produce code annotations.

##### Produce grid data

```{r}
#| label: grid-approx1

(
  d <-
    # 1. define grid
    tibble(p_grid = seq(from = 0, to = 1, length.out = 5), 
           
    # 2. define (compute) prior  
           prior  = 1) %>%
        
    # 3. compute likelihood at each value in grid
    mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %>% 
        
    # 4. compute product of likelihood and prior
    mutate(unstd_posterior = likelihood * prior) %>%  
        
    # 5. standardize the posterior, so it sums to 1
    mutate(posterior = unstd_posterior / sum(unstd_posterior))   
)

```

##### Plot with only 5 points

The next step is to plot the above results to get the left panel of
Figure 2.5

```{r}
#| label: plot-grid-approx1

p1 <- 
    d |> 
    ggplot(aes(x = p_grid, y = posterior)) +
      geom_point() +
      geom_line() +
      labs(subtitle = "5 points",
           x = "probability of water",
           y = "posterior probability") +
      theme(panel.grid = element_blank())
p1
```

##### Produce grid with 20 points and plot result

Now the same with 20 evenly spaced points to get the right panel of
Figure 2.7.

```{r}
#| label: grid-approx2

p2 <-
  tibble(p_grid = seq(from = 0, to = 1, length.out = 20),
         prior  = 1) %>%
  mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %>%
  mutate(unstd_posterior = likelihood * prior) %>%
  mutate(posterior = unstd_posterior / sum(unstd_posterior)) %>% 
  
  ggplot(aes(x = p_grid, y = posterior)) +
  geom_point() +
  geom_line() +
  labs(subtitle = "20 points",
       x = "probability of water",
       y = "posterior probability") +
  theme(panel.grid = element_blank())
p2
```

##### Combine plots with {patchwork}

And finally we display the two graphics with the `+` operator of
{**patchwork**} side by side and annotate the plot with
`patchwork::plot_annotation()`.

```{r}
#| label: plot-grid-approx-b2

# needs library(patchwork) = defined in setup chunk
p1 + p2 + plot_annotation(title = "More grid points make for smoother approximations")
```

##### Using differnt prior functions

To see the influence of the prior probability on the posterior
probability by using the same likelihood the book offers two code
snippets. Replace the definition of the prior (number 2 in the code
snippet) --- one at a time --- with the following lines of code:

```{r}
#| label: lst-different-priors
#| lst-cap: "Two other prior functions to try out what happens with different priors."

prior <- ifelse(p_grid < 0.5, 0, 1)
prior <- exp(-5 * abs(p_grid - 0.5))
```

What follows is a condensed way to make the four plots all at once. It
is a pretty complex program snippet not only using
`tidyr::expand_grid()` --- as already explained ---, but also
`tidyr::unnest()` which expands a list-column containing data frames
into rows and columns.

```{r}
#| label: fig-different-priors-5-and-20-points
#| fig-cap: "The effect of different priors and of different amounts of grid points."


# prepare the plot by producing the data
tibble(n_points = c(5, 20)) %>% 
  mutate(p_grid = purrr::map(n_points, ~seq(from = 0, to = 1, length.out = .))) %>% 
  unnest(p_grid) %>% 
  expand_grid(priors = c("ifelse(p_grid < 0.5, 0, 1)", "exp(-5 * abs(p_grid - 0.5))")) %>% 
  mutate(prior = ifelse(priors == "ifelse(p_grid < 0.5, 0, 1)", 
                        ifelse(p_grid < 0.5, 0, 1),
                        exp(-5 * abs(p_grid - 0.5)))) %>% 
  mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %>% 
  mutate(posterior = likelihood * prior / sum(likelihood * prior)) %>% 
  mutate(n_points = str_c("# points = ", n_points),
         priors   = str_c("prior = ", priors)) %>% 
  
  # plot the data
  ggplot(aes(x = p_grid, y = posterior)) +
  geom_line() +
  geom_point() +
  labs(x = "probability of water",
       y = "posterior probability") +
  theme(panel.grid = element_blank()) +
  facet_grid(n_points ~ priors, scales = "free")
```

### Quadratic Approximation

#### Original

##### Concept

> Under quite general conditions, the region near the peak of the
> posterior distribution will be nearly Gaussian---or "normal"---in
> shape. This means the posterior distribution can be usefully
> approximated by a Gaussian distribution. A Gaussian distribution is
> convenient, because it can be completely described by only two
> numbers: the location of its center (mean) and its spread (variance).

> A Gaussian approximation is called "quadratic approximation" because
> the logarithm of a Gaussian distribution forms a parabola. And a
> parabola is a quadratic function.

Two steps: 1. Find the posterior mode with some algorithm. The procedure
does not know where the peak is but it knows the slope under it feet. 2.
Estimate the curvature near the peak to calculate a quadratic
approximation. This computation is done by some numerical technique.

##### Computing the quadratic approximation

To compute the quadratic approximation for the globe tossing data, we'll
use a tool in the {**rethinking**} package: `rethinking::quap()`.

```{r}
#| label: quadratic-approx-9

globe.qa <- rethinking::quap(
  alist(
    W ~ dbinom(W + L, p), # binomial likelihood
    p ~ dunif(0, 1) # uniform prior
  ),
  data = list(W = 6, L = 3)
)

# display summary of quadratic approximation
rethinking::precis(globe.qa)

```

::: callout-warning
###### `precis()` results not printed correctly from visual mode

The result of `rethinking::precis()` does not display correctly after
the chunk in RStudio visual mode. But it works in source mode and it
displayed correctly immediately after the chunk.

The columns of the table are too narrow so that you can't see the header
and inspect the values. Printing to the console or to the web is
correct.

A workaround is wrapping the result with `print()` or to render the
document in source mode. See my [bug
report](https://github.com/rstudio/rstudio/issues/13227).
:::

To use `quap()`, you provide a *formula*, a list of *data* with
`base::alist()`. `alist()` handles its arguments as if they described
function arguments. So the values are not evaluated, and tagged
arguments with no value are allowed. It is most often used in
conjunction with `base::formals()`.

The function `precis` presents a brief summary of the quadratic
approximation. In this case, it shows the posterior mean value of
$p = 0.67$, which it calls the "Mean." The curvature is labeled
"StdDev". This stands for *standard deviation*. This value is the
standard deviation of the posterior distribution, while the mean value
is its peak. Finally, the last two values in the `precis` output show
the 89% percentile interval, which you'll learn more about in the next
chapter. You can read this kind of approximation like: *Assuming the
posterior is Gaussian, it is maximized at 0.67, and its standard
deviation is 0.16*.

##### Computing analytical solution

We want to compare the quadratic approximation with the analytic
calculation.

```{r}
#| label: analytical-calc

# analytic calculation
W <- 6
L <- 3
curve(dbeta(x, W + 1, L + 1), from = 0, to = 1)

# quadratic approximation
curve(dnorm(x, 0.67, 0.16), lty = 2, add = TRUE)
```

The solid line curve is the analytical posterior and the dashed curve is
the quadratic approximation. The dashed curve does alright on its left
side, but looks pretty bad on its right side. It even assigns positive
probability to $p = 1$, which we know is impossible, since we saw at
least one land sample. As the amount of data increases, however, the
quadratic approximation gets better.

##### Disadvantage

The phenomenon, where the quadratic approximation improves with the
amount of data, is very common. It's one of the reasons that so many
classical statistical procedures are nervous about small samples.

Using the quadratic approximation in a Bayesian context brings with it
all the same concerns. But you can always lean on some algorithm other
than quadratic approximation, if you have doubts. Indeed, grid
approximation works very well with small samples, because in such cases
the model must be simple and the computations will be quite fast. You
can also use MCMC.

Sometimes the quadratic approximation fails and you will get an error
message about the "Hessian". A *Hessian* --- named after mathematician
Ludwig Otto Hesse (1811--1874) --- is a square matrix of second
derivatives. The standard deviation is typically computed from the
Hessian, so computing the Hessian is nearly always a necessary step. But
sometimes the computation goes wrong, and your golem will choke while
trying to compute the Hessian.

Some other drawbacks will be explicated in later chapter. Therefore MCMC
seems generally the best option for complex models.

#### Tidyverse

##### Quadratic approximation with different sample size

In the book the calculation is only done for $n = 9$ but McElreath also
display the graphs for $n = 18$ and $n = 36$ with the same proportion of
`W` and `L`. Kurz shows how this is done and results into Figure 2.8 in
the book.

```{r}
#| label: quap-different-sample-size
#| attr-source: '#lst-quap-different-sample-size lst-cap="Quadratic approximation with quap() showing the effect of different sample size"'

### quap() with 9 sample size #################################
globe_qa_9 <- rethinking::quap(
  alist(
    W ~ dbinom(W + L, p), # binomial likelihood
    p ~ dunif(0, 1) # uniform prior
  ),
  data = list(W = 6, L = 3)
)

# display summary of quadratic approximation
rethinking::precis(globe_qa_9)

### quap() with 18 sample size ###################################
globe_qa_18 <- rethinking::quap(
  alist(
    W ~ dbinom(W + L, p), # binomial likelihood
    p ~ dunif(0, 1) # uniform prior
  ),
  data = list(W = 12, L = 6)
)

# display summary of quadratic approximation
rethinking::precis(globe_qa_18)

### quap() with 36 sample size ###################################
globe_qa_36 <- rethinking::quap(
  alist(
    W ~ dbinom(W + L, p), # binomial likelihood
    p ~ dunif(0, 1) # uniform prior
  ),
  data = list(W = 24, L = 12)
)

# display summary of quadratic approximation
rethinking::precis(globe_qa_36)
```

::: callout-note
## Slightly different code

I used a slightly different code than Kurz. I have only changed the data
values.
:::

```{r}
#| label: fig-quadratic-approx
#| fig-cap: "Accuracy of the quadratic approximation. In each plot, the exact posterior distribution is plotted as solid curve, and the quadratic approximation is plotted as the dashed curve."
#| attr-source: '#lst-fig-quadratic-approx lst-cap="Accuracy of the quadratic approximation"'

n_grid <- 100

# wrangle
tibble(w = c(6, 12, 24),
       n = c(9, 18, 36),
       s = c(.16, .11, .08)) %>% 
  expand_grid(p_grid = seq(from = 0, to = 1, length.out = n_grid)) %>% 
  mutate(prior = 1,
         m     = .67)  %>%
  mutate(likelihood = dbinom(w, size = n, prob = p_grid)) %>%
  mutate(unstd_grid_posterior = likelihood * prior,
         unstd_quad_posterior = dnorm(p_grid, m, s)) %>%
  group_by(w) %>% 
  mutate(grid_posterior = unstd_grid_posterior / sum(unstd_grid_posterior),
         quad_posterior = unstd_quad_posterior / sum(unstd_quad_posterior),
         n              = str_c("n = ", n)) %>% 
  mutate(n = factor(n, levels = c("n = 9", "n = 18", "n = 36"))) %>% 
  
  # plot
  ggplot(aes(x = p_grid)) +
  geom_line(aes(y = grid_posterior)) +
  geom_line(aes(y = quad_posterior),
            color = "grey50") +
  labs(x = "proportion water",
       y = "density") +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ n, scales = "free")

```

##### Maximum Likelihood Estimation (MLE)

> The quadratic approximation, either with a uniform prior or with a lot
> of data, is often equivalent to a maximum likelihood estimate (MLE)
> and its standard error. The MLE is a very common non-Bayesian
> parameter estimate. This correspondence between a Bayesian
> approximation and a common non-Bayesian estimator is both a blessing
> and a curse. It is a blessing, because it allows us to re-interpret a
> wide range of published non-Bayesian model fits in Bayesian terms. It
> is a curse, because maximum likelihood estimates have some curious
> drawbacks, and the quadratic approximation can share them. (p. 44,
> emphasis, in the original)

Textbooks highlighting the maximum likelihood method for the generalized
linear model abound. If this is new to you and you'd like to learn more,
perhaps check out Roback and Legler's (2021) Beyond multiple linear
regression: Applied generalized linear models and multilevel models in
R, Agresti's (2015) Foundations of linear and generalized linear models
or Dunn and Smyth's (2018) Generalized linear models with examples in R.

### Markov Chain Monte Carlo (MCMC)

#### Original

> There are lots of important model types, like multilevel
> (mixed-effects) models, for which neither grid approximation nor
> quadratic approximation is always satisfactory. ... As a result,
> various counterintuitive model fitting techniques have arisen. The
> most popular of these is **MARKOV CHAIN MONTE CARLO** (MCMC), which is
> a family of conditioning engines capable of handling highly complex
> models.

> Instead of attempting to compute or approximate the posterior
> distribution directly, MCMC techniques merely draw samples from the
> posterior. You end up with a collection of parameter values, and the
> frequencies of these values correspond to the posterior
> plausibilities. You can then build a picture of the posterior from the
> histogram of these samples.

The understanding of this not intuitive technique is postponed to
chapter 9. What follows is just a demonstration of the technique.

```{r}
#| label: fig-demo-MCMC-rethinking
#| fig-cap: "Demo of the Markov Chain Monte Carlo (MCMC) method using the globe-tossing data and calculated and diplayed with the {**rethinking**} package."

## R code 2.8
n_samples <- 1000
p <- rep(NA, n_samples)
p[1] <- 0.5
W <- 6
L <- 3
for (i in 2:n_samples) {
  p_new <- rnorm(1, p[i - 1], 0.1)
  if (p_new < 0) p_new <- abs(p_new)
  if (p_new > 1) p_new <- 2 - p_new
  q0 <- dbinom(W, W + L, p[i - 1])
  q1 <- dbinom(W, W + L, p_new)
  p[i] <- ifelse(runif(1) < q1 / q0, p_new, p[i - 1])
}

## R code 2.9
rethinking::dens(p, xlim = c(0, 1))
curve(dbeta(x, W + 1, L + 1), lty = 2, add = TRUE)


```

It's weird. But it works. The above **METROPOLIS ALGORITHM** is
explained in Chapter 9.

#### Tidyverse

The {**brms**} package uses a version of MCMC to fit Bayesian models.
brms stands for Bayesian Regression Models using 'Stan'.

Since one of the main goals of \[the Kurz version of SR2\] is to
highlight {**brms**}, we may as well fit a model. This seems like an
appropriately named subsection to do so. First we'll have to load the
package. (If you haven't already installed {**brms**}, you can find
instructions on how to do on
[GitHub](https://github.com/paul-buerkner/brms#how-do-i-install-brms) or
on the [corresponding
website](https://paul-buerkner.github.io/brms/).)As an exercise we will
re-fit the model with $W = 24$ and $n = 36$ of
@lst-quap-different-sample-size and @lst-fig-quadratic-approx.

But before we use {**brms**} we need to detach the {**rethinking**}
package.

> **R** will not allow us to use a function from one package that shares
> the same name as a different function from another package if both
> packages are open at the same time. The **rethinking** and **brms**
> packages are designed for similar purposes and, unsurprisingly,
> overlap in some of their function names. To prevent problems, we will
> always make sure **rethinking** is detached before using **brms**. To
> learn more on the topic, see [this R-bloggers
> post](https://www.r-bloggers.com/2015/04/r-and-package-masking-a-real-life-example/).
> (This remark comes from [section 4.3.1 of the Kurz
> version](https://bookdown.org/content/4857/geocentric-models.html#the-data)).

This is the reason why I have not loading the rethinking packages in
code chunks of this file. Instead I referred to every function of the
{**rethinking**} packages directly adding `rethinking::` before the
function call. This has also the advantage to learn which functions come
from {**rethinking**}.

::: callout-warning
## First compiling

Be patient when you render the following chunk the first time. It need
some time. Furthermore it results in a very long processing message
under the compiled chunk. Again this happens only the first because the
result is stored in the "fits" folder which you have to create before
running the chunk.
:::

```{r}
#| label: demo-brms
#| attr-source: '#lst-demo-brms lst-cap="Demonstration of the `brm()` function of the {**brms**} package."'

b2.1 <-
  brms::brm(data = list(w = 24), 
      family = binomial(link = "identity"),
      w | trials(36) ~ 0 + Intercept,
      brms::prior(beta(1, 1), class = b, lb = 0, ub = 1),
      seed = 2,
      file = "fits/b02.01")
```

```{r}
#| label: print-result-b2.1
#| attr-source: '#lst-print-demo-brms lst-cap="Print the result of the demo of the {**brms**} package."' 

print(b2.1)
```

A detailed explanation is postponed to chapter 4. Here I will just copy
the notes by Kurz to get a first understand and a starting point for a
later further exploration.

> For now, focus on the 'Intercept' line. As we'll also learn in Chapter
> 4, the intercept of a typical regression model with no predictors is
> the same as its mean. In the special case of a model using the
> binomial likelihood, the mean is the probability of a 1 in a given
> trial, $\theta$.
>
> Also, with {**brms**}, there are many ways to summarize the results of
> a model. The `brms::posterior_summary()` function is an analogue to
> `rethinking::precis()`. We will, however, need to use `round()` to
> reduce the output to a reasonable number of decimal places.

```{r}
#| label: print-brms-summary

brms::posterior_summary(b2.1) %>% 
  round(digits = 2)
```

> The `b_Intercept` row is the probability. Don't worry about the second
> line, for now. We'll cover the details of {**brms}** model fitting in
> later chapters. To finish up, why not plot the results of our model
> and compare them with those from `rethinking::quap()`, above? (See
> @fig-demo-MCMC-rethinking)

```{r}
#| label: fig-demo-MCMC-brms
#| fig-cap: "Demo of the Markov Chain Monte Carlo (MCMC) method using the globe-tossing data with the {**brms**} package."

brms::as_draws_df(b2.1) %>% 
  mutate(n = "n = 36") %>%
  
  ggplot(aes(x = b_Intercept)) +
  geom_density(fill = "black") +
  scale_x_continuous("proportion water", limits = c(0, 1)) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ n)

```

> If you're still confused, cool. This is just a preview. We'll start
> walking through fitting models with **brms** in [Chapter
> 4](https://bookdown.org/content/4857/geocentric-models.html#geocentric-models)
> and we'll learn a lot about regression with the binomial likelihood in
> [Chapter
> 11](https://bookdown.org/content/4857/god-spiked-the-integers.html#god-spiked-the-integers).

#### Reconsideration

##### Bayesian Inference: Some Lessons to Draw

The following list summarizes differences between Bayesian and
Non-Bayesian inference ("Frequentism"):

1.  **No minimum sampling size**: The minimum sampling size in Bayesian
    inference is one. You are going to update each data point at its
    time. For instance you got an estimate every time when you toss the
    globe and the estimate is updated. --- Well, the sample size of one
    is not very informative but that is the power of Bayesian inference
    in not getting over confident. It is always accurately representing
    the relative confidence of plausability we should assign to each of
    the possible proportions.
2.  **Shape embodies sample size**: \*The shape of the posterior
    distribution embodies all the information that the sample has about
    the process of the proportions. Therefore you do not need to go back
    to the original dataset for new observations. Just take the
    posterior distribution and update it by multiplying the number of
    ways the new data could produce.
3.  **No point estimates**: The estimate is the whole distribution. It
    may be fine for communication purposes to talk about some summary
    points of the distribution like the mode and mean. But neither of
    these points is special as a point of estimate. When we do
    calculations we draw predictions from the whole distribution, never
    just from a point of it.
4.  **No one true interval**: Intervals are not important in Bayesian
    inference. They are merely summaries of the shape of the
    distribution. There is nothing special in any of these intervals
    because the endpoints of the intervals are not special. Nothing
    happens of the endpoints of the intervals because the interval is
    arbitrary. (The 95% in Non-Bayesian inference is essentially a
    dogma, a superstition. Even in Non-Bayesian statistics it is
    conceptualized as an arbitrary interval.)

## Synopsis

### Small worlds and the garden of forking data

The chapter starts to build Bayesian models and is focused on the small
world. It explains probability theory in its essential form: counting
the ways things can happen. This is shown with **the garden of forking
data**.

Bayesian inference is really just counting and comparing of
possibilities. ... In order to make good inference about what actually
happened, it helps to consider everything that could have happened. A
Bayesian analysis is a garden of forking data, in which alternative
sequences of events are cultivated. As we learn about what did happen,
some of these alternative sequences are pruned. In the end, what remains
is only what is logically consistent with our knowledge.

#### Bayesian updating: Counting

We may have additional information about the relative plausibility of
each conjecture. This information could arise from knowledge of how the
contents of the bag were generated. It could also arise from previous
data. Whatever the source, it would help to have a way to combine
different sources of information to update the plausibilities. Luckily
there is a natural solution: Just multiply the counts. ...
Multiplication is just a shortcut to enumerating and counting up all of
the paths through the garden that could produce all the observations.

A Bayesian model begins with one set of plausibilities assigned to each
of these possibilities. These are the prior plausibilities. Then it
updates them in light of the data, to produce the posterior
plausibilities. This updating process is a kind of learning, called
Bayesian updating.

How to start the updating process?

When there is no reason to say that one conjecture is more plausible
than another, weigh all of the conjectures equally.

#### From counts to probability

When we don't know what caused the data, potential causes that may
produce the data in more ways are more plausible.

### Building a model

#### Data story

Bayesian data analysis usually means producing a story for how the data
came to be. This story may be descriptive, specifying associations that
can be used to predict outcomes, given observations. Or it may be
causal, a theory of how some events produce other events.

#### Bayesian updating again

In contrast to non-Bayesian statistical inference with it widespread
superstition that 30 observations are needed before one can use a
Gaussian distribution (so-called *asymptotic* behavior), Bayesian
estimates are valid for any sample size.

This does not mean that more data isn't helpful---it certainly is.
Rather, the estimates have a clear and valid interpretation, no matter
the sample size. But the price for this power is dependency upon the
initial plausibilities, the prior. If the prior is a bad one, then the
resulting inference will be misleading.

#### Evaluation

1.  Model's certainty (curves narrow and tall) is no guarantee that the
    model is a good one, because the inferences are conditional on the
    model. Under a different model, things might look differently.
2.  It is important to supervise and critique your model's work and to
    check the model's inferences in light of aspects of the data it does
    not know about. This usually means asking and answering additional
    questions, beyond those that originally constructed the model.

### Components of the model

-   **List variables**, observed ones (data) and unobserved ones
    (parameter like proportions of observed variables).
-   **Define variables** by building a model that relates variables to
    one another.
    -   **Observed variables**: We don't have to literally count, we can
        use a mathematical function that tells us the right
        plausibility, the likelihood. For example in the case of the
        globe tossing model this is the *binomial distribution*.
    -   **Unobserved variables (parameters)**: In statistical modeling,
        many of the most common questions we ask about data are answered
        directly by parameters. For every parameter we intend our
        Bayesian machine to consider, we must provide a distribution of
        prior plausibility, its prior. Because the prior is an
        assumption, it should be interrogated like other assumptions: by
        altering it and checking how sensitive inference is to the
        assumption.

### Making the model go

As mathematical rules often cannot be conditioned formally or you have
to constrain your choice of prior to special forms that are easy to do
mathematics with, we need numerical techniques to approximate the
mathematics.

#### Grid approximation

We can achieve an excellent approximation of the continuous posterior
distribution by considering only a finite grid of parameter values. At
any particular value of a parameter, `p`, it's a simple matter to
compute the posterior probability: just multiply the prior probability
of `p` by the likelihood at `p`. Repeating this procedure for each value
in the grid generates an approximate picture of the exact posterior
distribution.

##### Base R

```{r}
#| label: grid-approx-base-demo
#| attr-source: '#lst-grid-approx-base-demo lst-cap="Five steps of the grid approximation technique using only functions of base R"'
#| echo: true
#| eval: false

## R code 2.3 ###########################################
p_grid <- seq(from = 0, to = 1, length.out = 20)    # <1>
prior <- rep(1, 20)                                 # <2>
likelihood <- dbinom(6, size = 9, prob = p_grid)    # <3>
unstd.posterior <- likelihood * prior               # <4>
posterior <- unstd.posterior / sum(unstd.posterior) # <5>
```

1.  Define the grid. This means you decide how many points to use in
    estimating the posterior, and then you make a list of the parameter
    values on the grid.
2.  Compute the value of the prior at each parameter value on the grid.
3.  Compute the likelihood at each parameter value.
4.  Compute the unstandardized posterior at each parameter value, by
    multiplying the prior by the likelihood.
5.  Finally, standardize the posterior, by dividing each value by the
    sum of all values.

##### Tidyverse

```{r}
#| label: grid-approx-tidyverse-demo
#| attr-source: '#lst-grid-approx-tidyverse-demo lst-cap="Five steps of the grid approximation technique using tidyverse functions"'
#| eval: false
#| echo: true

(
    df <-
        tibble(p_grid = seq(from = 0, to = 1, length.out = 20),      # <1>
               prior  = 1) %>%                                       # <2>
        mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %>%  # <3>
        mutate(unstd_posterior = likelihood * prior) %>%             # <4>
        mutate(posterior = unstd_posterior / sum(unstd_posterior))   # <5>
)
```

1.  Define grid.
2.  Define prior.
3.  Compute likelihood at each value in grid.
4.  Compute product of likelihood and prior.
5.  Standardize the posterior.

#### Quadratic approximation

Grid approximation has the disadvantage that the number of unique values
to consider in the grid grows rapidly as the number of parameters in the
model increases. In this case quadratic approximation may be an
alternative.

Under quite general conditions, the region near the peak of the
posterior distribution will be nearly Gaussian---or "normal"---in shape.
This means the posterior distribution can be usefully approximated by a
Gaussian distribution. A Gaussian distribution is convenient, because it
can be completely described by only two numbers: the location of its
center (mean) and its spread (variance).

A Gaussian approximation is called "quadratic approximation" because the
logarithm of a Gaussian distribution forms a parabola. And a parabola is
a quadratic function.

Two steps of the quadratic approximation technique:

1.  Find the posterior mode. This is usually accomplished by some
    optimization algorithm, a procedure that virtually "climbs" the
    posterior distribution, as if it were a mountain. The golem doesn't
    know where the peak is, but it does know the slope under its feet.
    There are many well-developed optimization procedures, most of them
    more clever than simple hill climbing. But all of them try to find
    peaks.
2.  Once you find the peak of the posterior, you must estimate the
    curvature near the peak. This curvature is sufficient to compute a
    quadratic approximation of the entire posterior distribution. In
    some cases, these calculations can be done analytically, but usually
    your computer uses some numerical technique instead.

##### Rethinking

```{r}
#| label: quadratic-approx-demo
#| attr-source: '#lst-quadratic-approx-demo lst-cap="Two steps of the quadratic approximation technique"'
#| eval: false
#| echo: true


globe_qa <- rethinking::quap( # <1>
  alist(                      # <2>
    W ~ dbinom(W + L, p),     # <3> 
    p ~ dunif(0, 1)           # <4> 
  ), 
  data = list(W = 6, L = 3)   # <5>
)

rethinking::precis(globe_qa)  # <6>
```

1.  To compute the quadratic approximation, the book will use as tool
    the `rethinking::quap()` function in the rethinking package. With
    `quap()` you will find the mode of posterior distribution and
    produce an approximation of the full posterior using the quadratic
    curvature at the mode.
2.  To use `quap()`, you provide a formula, a list of data. The formula
    defines the probability of the data and the prior. `base::alist()`
    handles its arguments as if they described function arguments. So
    the values are not evaluated, and tagged arguments with no value are
    allowed whereas list simply ignores them. `alist()` is most often
    used in conjunction with formulae.
3.  Provide formula for the calculation of the binomial likelihood.
4.  Provide the formula for the prior probability. In this example we
    choose the uniform distribution between 0 and 1.
5.  Provide the data for the calculation as a list of values. The chosen
    names (symbols) correspond to the formula.
6.  The function `rethinking::precis()` presents a brief summary of the
    quadratic approximation.

McElreath is using the quadratic approximation for much of the first
half of this book. For many of the most common procedures in applied
statistics---linear regression, for example---the approximation works
very well. Often, it is even exactly correct, not actually an
approximation at all. Computationally, quadratic approximation is very
inexpensive, at least compared to grid approximation and MCMC.

Additionally of using `rethinking::quap()`, I will also provide the
`brms::brm()` alternative as suggested by the {**tidyverse**} version of
Kurz.

#### Markov chain Monte Carlo (MCMC)

There are lots of important model types, like multilevel (mixed-effects)
models, for which neither grid approximation nor quadratic approximation
is always satisfactory. ... As a result, various counterintuitive model
fitting techniques have arisen. The most popular of these is MCMC, which
is a family of conditioning engines capable of handling highly complex
models.

The conceptual challenge with MCMC lies in its highly non-obvious
strategy. Instead of attempting to compute or approximate the posterior
distribution directly, MCMC techniques merely draw samples from the
posterior. You end up with a collection of parameter values, and the
frequencies of these values correspond to the posterior plausibilities.
You can then build a picture of the posterior from the histogram of
these samples.

##### Base R and Rethinking

I will add here without comments a demonstration of this important
technique. In later chapters we will learn more about it.

```{r}
#| label: demo-MCMC
#| attr-source: '#lst-demo-MCMC lst-cap="Demonstration how to estimate the posterior with MCMC"'
#| eval: false

## R code 2.8 ####################################
n_samples <- 1000
p <- rep(NA, n_samples)
p[1] <- 0.5
W <- 6
L <- 3
for (i in 2:n_samples) {
  p_new <- rnorm(1, p[i - 1], 0.1)
  if (p_new < 0) p_new <- abs(p_new)
  if (p_new > 1) p_new <- 2 - p_new
  q0 <- dbinom(W, W + L, p[i - 1])
  q1 <- dbinom(W, W + L, p_new)
  p[i] <- ifelse(runif(1) < q1 / q0, p_new, p[i - 1])
}

## R code 2.9 ####################################
rethinking::dens(p, xlim = c(0, 1))
curve(dbeta(x, W + 1, L + 1), lty = 2, add = TRUE)
```

##### brms

Again: The following demo comes without comments. It just will give a
feeling of the structure of the used function. Later I will dive with
Kurz into the details how to apply the functions of the {**brms**}
package.

```{r}
#| label: demo-using-brms
#| attr-source: '#lst-demo-using-brms lst-cap="Demonstration of using {brms}"'
#| eval: false

b2.1 <-
  brms::brm(data = list(w = 24), 
      family = binomial(link = "identity"),
      w | trials(36) ~ 0 + Intercept,
      brms::prior(beta(1, 1), class = b, lb = 0, ub = 1),
      seed = 2,
      file = "fits/b02.01")

print(b2.1)

brms::posterior_summary(b2.1) %>% 
  round(digits = 2)

```

## Practice

## I STOPPED HERE! (2023-08-01) TO BE CONTINUED {.unnumbered}

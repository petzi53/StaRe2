# 2b: Small and Large Worlds

The work by Solomon Kurz has many references to R specifics, so that
people new to R can follow the course. Most of these references are not
new to me, so I will not include them in my personal notes. There are
also very important references to other relevant articles I do not know.
But I will put these kind of references for now aside and will me mostly
concentrate on the replication and understanding the code examples.

Even if I have some plotting experiences with ggplot2 I will replicate
this code as well. It seems to me that the author is an graphics and
plotting expert and I want to get more experience in using ggplot2.

```{r}
#| label: setup

library(conflicted)
library(tidyverse)

conflicts_prefer(dplyr::lag)
```

## 2.1b Counting Possibilities

> If we're willing to code the marbles as 0 = "white" 1 = "blue", we can
> arrange the possibility data in a tibble as follows.

(I changed `rep()` to `rep.int()` and added L to the value of p1 resp.
p5 to get integer (instead of doubles).

```{r}
#| label: create-marble-data

d <-
  tibble(p1 = 0L,
         p2 = rep.int(1:0, times = c(1, 3)),
         p3 = rep.int(1:0, times = c(2, 2)),
         p4 = rep.int(1:0, times = c(3, 1)),
         p5 = 1L)

head(d)
```

> You might depict the possibility data in a plot.

```{r}
#| label: depict-marble-data

d %>% 
  set_names(1:5) %>% 
  mutate(x = 1:4) %>% 
  pivot_longer(-x, names_to = "possibility") %>% 
  mutate(value = value %>% as.character()) %>% 
  
  ggplot(aes(x = x, y = possibility, fill = value)) +
  geom_point(shape = 21, size = 5) +
  scale_fill_manual(values = c("white", "navy")) +
  scale_x_discrete(NULL, breaks = NULL) +
  theme(legend.position = "none")

```

At first I could not understand the code lines 4 to 7. In particular I
could not see what the code of lines 4 and 6 does. I had to execute line
by line to see what happens:

```{r}
#| label: using-set-names-b

set_names(d, 1:5)
```

`set_names()` comes from **rlang** and is exported to **purrr**. It is
equivalent to `stats::setNames()` but has more features and stricter
argument checking. I does nothing more as to change the column names
from p\<number\> to \<number\>. If one had used just numbers for the
probability columns this line wouldn't have been necessary. (I omitted
the `head()` argument of the last line as it is not necessary.)

```{r}
#| label: create-marble-data-2

df <-
  tibble(`1` = 0,
         `2` = rep(1:0, times = c(1, 3)),
         `3` = rep(1:0, times = c(2, 2)),
         `4` = rep(1:0, times = c(3, 1)),
         `5` = 1)

df
```

It is interesting to see that the first and last column are doubles and
not integers. I believe that the reason is that these two columns do not
have variations so that R assumes the more general type of numbers.

After understanding what set_names() does the next line with `mutate()`
is easy. It adds a new column with the values 1 to 4 for each row.

Another difficulty for me was line 6: I understood that the data frame
is converted from a wide to a long structure. But together with the pipe
and not naming the first parameter `-x` I did not catch the essence of
the command.

```{r}
#| label: try-different-pivots

(df <- mutate(df, x = 1:4))
(pivot_longer(data = df, cols = -x, names_to = "possibility"))
# this is equivalent with:
(pivot_longer(data = df, cols = c(`1`, `2`, `3`, `4`, `5`), names_to = "possibility"))
# to see the difference when all columns are transformed to longer
pivot_longer(data = df, cols = everything(), names_to = "possibility") |> print(n = 24)
```

This different code lines demonstrate: The `-x` parameter excludes the
`x` column from the wide to long transformation. It is a shorthand for
naming all 5 columns that should be transformed.

> Here's the basic structure of the possibilities per marble draw.

::: callout-note
## Adding tbl-cap

If you label the chunk using fl`extable()` then you have also to add
`tbl-cap` to the chunk options.
:::

```{r}
#| label: basic-prob-struct-b
#| tbl-cap: probility-strcuture

library(flextable)

tibble(draw    = 1:3,
       marbles = 4) %>% 
  mutate(possibilities = marbles ^ draw) %>% 
  flextable()

```

**Tables**:

I am completely new to the
[**flextable**](https://davidgohel.github.io/flextable/) package. Until
now I had used most of the time
[kableExtra](https://haozhu233.github.io/kableExtra/) and sometimes
[DT](https://rstudio.github.io/DT/). But there are also
[gt](https://github.com/rstudio/gt),
[formattable](https://renkun-ken.github.io/formattable/),
[reactable](https://glin.github.io/reactable/) and
[reactablefmtr](https://kcuilla.github.io/reactablefmtr/). It seems to
me that I should especially have a look into the {**gt**} package as it
is developed by the RStudio/Posit team.

See the short description quoted below from [Top 7 Packages for Making
Beautiful Tables in
R](https://towardsdatascience.com/top-7-packages-for-making-beautiful-tables-in-r-7683d054e541)
by Devashree Madhugiri:

-   **gt**: The gt package offers a different and easy-to-use set of
    functions that helps us build display tables from tabular data. The
    gt philosophy states that a comprehensive collection of table parts
    can be used to create a broad range of functional tables. These are
    the table body, the table footer, the spanner column labels, the
    column labels, and the table header.

    ![](https://gt.rstudio.com/reference/figures/gt_parts_of_a_table.svg)

-   **formattable**: Formattable data frames are data frames that will
    be displayed in HTML tables using formatter functions. This package
    includes techniques to produce data structures with predefined
    formatting rules, such that the objects maintain the original data
    but are formatted. The package consists of several standard
    formattable objects, including percent, comma, currency, accounting,
    and scientific.

-   **kableExtra**: The kableExtra package is used to extend the basic
    functionality of `knitr::kable tables()`. Although `knitr::kable()`
    is simple by design, it has many features missing which are usually
    available in other packages, and kableExtra has filled the gap
    nicely for `knitr::kable().` The best thing about kableExtra is that
    most of its table capabilities work for both HTML and PDF formats.

-   **DT**: dt is an abbreviation of 'DataTables.' Data objects in R can
    be rendered as HTML tables using the JavaScript library 'DataTables'
    (typically via R Markdown or Shiny).

-   **flextable**: flextable package helps you to create reporting table
    from a dataframe easily. You can merge cells, add headers, add
    footers, change formatting, and set how data in cells is displayed.
    Table content can also contain mixed types of text and image
    content. Tables can be embedded from R Markdown documents into HTML,
    PDF, Word, and PowerPoint documents and can be embedded using
    Package Officer for Microsoft Word or PowerPoint documents. Tables
    can also be exported as R plots or graphic files, e.g., png, pdf,
    and jpeg.

-   **reactable**: `reactable()` creates a data table from tabular data
    with sorting and pagination by default. The data table is an HTML
    widget that can be used in R Markdown documents and Shiny
    applications or viewed from an R console. It is based on the React
    Table library and made with reactR. Features are:

    -   It creates a data table with sorting, filtering, and pagination
    -   It has built-in column formatting
    -   It supports custom rendering via R or JavaScript
    -   It works seamlessly within R Markdown documents and the Shiny
        app

-   **ractablefmtr**: The
    [reactablefmtr](https://kcuilla.github.io/reactablefmtr/index.html)
    package improves the appearance and formatting of tables created
    using the reactable R library. The reactablefmtr package includes
    many conditional formatters that are highly customizable and easy to
    use.

I am not sure if there are other packages as well. But the above seven
packages are a first starting point to learn creating and displaying
sophisticated data tables in R.

## 2.2b Building a Model

**Globe tossing**

Data: `W L W W W L W L W` (W indicates water and L indicates land. )

We save our globe-tossing data in a tibble.

```{r}
#| label: globe-tossing-data

(d <- tibble(toss = c("w", "l", "w", "w", "w", "l", "w", "l", "w")))
```

### 2.2.1b A Data Story

### 2.2.2b Bayesian Updating {#sec-expand_grid}

For the updating process we need to add to the data the cumulative
number of trials, `n_trials`, and the cumulative number of successes,
`n_successes` (i.e., `toss == "w"`),

```{r}
#| label: bayesian-updating-1b

(
  d <-
  d %>% 
  mutate(n_trials  = 1:9,
         n_success = cumsum(toss == "w"))
  )

```

::: callout-important
## Change parameter name

I had to change the parameter `k` to `default`. Maybe `k` was the name
of the parameter of a previous {**dplyr**} version?
:::

```{r}
#| label: bayesian-updating-2b

sequence_length <- 50

d %>% 
  expand_grid(p_water = seq(from = 0, to = 1, length.out = sequence_length)) %>% 
  group_by(p_water) %>% 
  mutate(lagged_n_trials  = lag(n_trials, default = 1),
         lagged_n_success = lag(n_success, default = 1)) %>% 
  ungroup() %>% 
  mutate(prior      = ifelse(n_trials == 1, .5,
                             dbinom(x    = lagged_n_success, 
                                    size = lagged_n_trials, 
                                    prob = p_water)),
         likelihood = dbinom(x    = n_success, 
                             size = n_trials, 
                             prob = p_water),
         strip      = str_c("n = ", n_trials)) %>% 
  # the next three lines allow us to normalize the prior and the likelihood, 
  # putting them both in a probability metric 
  group_by(n_trials) %>% 
  mutate(prior      = prior / sum(prior),
         likelihood = likelihood / sum(likelihood)) %>%   
  
  # plot!
  ggplot(aes(x = p_water)) +
  geom_line(aes(y = prior), 
            linetype = 2) +
  geom_line(aes(y = likelihood)) +
  scale_x_continuous("proportion water", breaks = c(0, .5, 1)) +
  scale_y_continuous("plausibility", breaks = NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ strip, scales = "free_y")

```

::: callout-warning
## Warning

The skills for the next graph are explained later in this chapter. I
plan to come back here and fill in the details.
:::

In the meanwhile I called the code from the above code chunk line by
line and inspected the result to understand what it does. These are my
preliminary thoughts:

-   `tidyr::expand_grid()` creates a tibble from all combinations of
    inputs. Input are generalized vectors in contrast to `expand()` that
    generates all combination of variables as well but needs as input a
    dataset. The range between 0 and 1 is divided into 50 part and then
    it generates all combinations by varying all columns from left to
    right. The first column is the slowest, the second is faster and so
    on.). It generates 450 data points (50 \* 9 trials).
-   I do not understand why the data has to be grouped `group_by()` the
    variable `p_water` as every row has a different value. But as I
    tested: There is a difference between grouped and not grouped.
    Apparently it has to do with the `lag()` command because the first
    51 records are identical than beginning in row number 52
    `lagged_n_trials` is FALSE until 100. 101 is TRUE and beginning with
    row 102 `lagged_n_trials` and `lagged_n_success` are FALSE. From
    here every 50th records are TRUE/TRUE (e.g., 151, 201, 251 etc.) all
    the other rows are FALSE/FALSE.
-   The next line uses the `dplyr::lag()` command: To find the
    "previous" (`lag()`) values in a vector (time series) is useful for
    comparing values behind of the current values. See [Compute lagged
    or leading
    values](https://dplyr.tidyverse.org/reference/lead-lag.html). I
    think this is necessary to get the previous values (prior
    probabilities) in relation with the posterior probabilities.
-   The next two lines with `mutate()` calculate prior and likelihood.
    There is no prior for the first trial, so it is assumed that it is
    0.5. The formula for the binomial distribution uses for the prior
    the lagged-version whereas the likelihood uses the current version.
    These two lines provide the essential calculations: They match the
    50 grid points as assumed water probabilities of every trial to
    their trial outcome (`W` or `L`) probabilities.
-   The third `mutate()` generates the `strip` variable consisting of
    the prefix "N =" followed by the counts the number of trials. This
    will later provide the title for the the different facets of the
    plot.
-   The next three lines normalize the prior and the likelihood by
    grouping the data by n_trials. Dividing every prior and likelihood
    values by their respective sum puts them both in a probability
    metric.
-   The remainder of the code prepares the plot by using the 50 grid
    points in the range from 0 to 1 as the x-axis; prior and likelihood
    as y-axis. To distinguish the prior from the likelihood is uses a
    dashed line for the prior (`linetyp = 2`) and a full line (default)
    for the likelihood. The x-axis has three breaks (`0, 0.5, 1`)
    whereas the y-axis has no break and no scale (`scales = "free_y"`).

> If it wasn\'t clear in the code, the dashed curves are normalized
> prior densities. The solid ones are normalized likelihoods. If you
> don\'t normalize (i.e., divide the density by the sum of the density),
> their respective heights don\'t match up with those in the text.
> Furthermore, it\'s the normalization that makes them directly
> comparable.

#### **Likelihood for many p-values-b**  {#sec-likelihood-for-many-p-values-b}

```{r}
#| label: likelihood-many

tibble(prob = seq(from = 0, to = 1, by = .01)) %>% 
  ggplot(aes(x = prob, y = dbinom(x = 6, size = 9, prob = prob))) +
  geom_line() +
  labs(x = "probability",
       y = "binomial likelihood") +
  theme(panel.grid = element_blank())

```

In contrast to *p = 0.5* with a probability of 0.16% the graph above
shows a maximum at about *p = 0.7* and a probability of about 0.28%. We
will get more detailed data later in the book.

It is interesting to see that even the maximum probability is not very
high. The reason is that there are many other configurations
(distributions of Ws and Ls) to produce the result of *6W* and *3L*.
Even if all these other distributions have a small probability they
"eat" all with their share from the maximum.

(I wanted to write "from the maximum of 1.0" but I think this would not
be correct as the above graph displays the rate of change in cumulative
probability (the **probability density**) and not the probability itself
(the **probability mass**). See the following quote:

> For mathematical reasons, probability densities can be greater than 1.
> ... Probability *density* is the rate of change in cumulative
> probability. So where cumulative probability is increasing rapidly,
> density can easily exceed 1. But if we calculate the area under the
> density function, it will never exceed 1. Such areas are also called
> *probability mass*. (11.47 in calibre ebook-viewer reference mode)

In the literature often the abbreviation **PDF ([probability density
function](https://en.wikipedia.org/wiki/Probability_density_function))**
is often used for the (probability) density. See also th wikipedia entry
about [probability
distribution](https://en.wikipedia.org/wiki/Probability_distribution).

McElreath says:

> The prior is a probability distribution for the parameter. In general,
> for a uniform prior from *a* to *b*, the probability of any point in
> the interval is 1/(*b -- a*). If you\'re bothered by the fact that the
> probability of every value of *p* is 1, remember that every
> probability distribution must sum (integrate) to 1. The expression
> 1/(*b -- a*) ensures that the area under the flat line from *a* to *b*
> is equal to 1.

Kurz demonstrated the truth of this quote in the brms-version with
several *b* values while holding *a* constant:

```{r}
#| label: uniform-prior-1b

tibble(a = 0,
       b = c(1, 1.5, 2, 3, 9)) %>% 
  mutate(prob = 1 / (b - a))
```

Verified with a plot he divides the range of the *b* parameter (*0-9*)
into 500 segments (*parameter_space*) and uses the `dunif()`
distribution to calculate the probabilities:

```{r}
#| label: uniform-prior-2b

tibble(a = 0,
       b = c(1, 1.5, 2, 3, 9)) %>% 
  expand_grid(parameter_space = seq(from = 0, to = 9, length.out = 500)) %>% 
  mutate(prob = dunif(parameter_space, a, b),
         b    = str_c("b = ", b)) %>% 
  
  ggplot(aes(x = parameter_space, y = prob)) +
  geom_area() +
  scale_x_continuous(breaks = c(0, 1:3, 9)) +
  scale_y_continuous(breaks = c(0, 1/9, 1/3, 1/2, 2/3, 1),
                     labels = c("0", "1/9", "1/3", "1/2", "2/3", "1")) +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major.x = element_blank()) +
  facet_wrap(~ b, ncol = 5)

```

This figure demonstrates that the area in the whole paramatere space is
*1.0*. It is a nice example how to calculate the probability *mass* (in
contrast to the curve of the probability *density*).

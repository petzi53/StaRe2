# 2a: Small and Large Worlds

-   The **small world** is the self-contained logical world of the
    model.
-   The **large world** is the broader context in which one deploys a
    model.

**Meta Remark (Study Guide)**

> This chapter focuses on the small world. It explains probability
> theory in its essential form: counting the ways things can happen.
> Bayesian inference arises automatically from this perspective. Then
> the chapter presents the stylized components of a Bayesian statistical
> model, a model for learning from data. Then it shows you how to
> animate the model, to produce estimates.
>
> All this work provides a foundation for the next chapter, in which
> you'll learn to summarize Bayesian estimates, as well as begin to
> consider large world obligations.

## 2.1a The Garden of Forking Data

::: callout-important
Bayesian inference is really just counting and comparing of
possibilities. ... In order to make good inference about what actually
happened, it helps to consider everything that could have happened. A
Bayesian analysis is a garden of forking data, in which alternative
sequences of events are cultivated. As we learn about what did happen,
some of these alternative sequences are pruned. In the end, what remains
is only what is logically consistent with our knowledge.
:::

### 2.1.1a Counting possibilites

> Suppose there's a bag, and it contains four marbles. These marbles
> come in two colors: blue and white. We know there are four marbles in
> the bag, but we don't know how many are of each color. We do know that
> there are five possibilities: (1) \[⚪⚪⚪⚪\], (2) \[⚫⚪⚪⚪\], (3)
> \[⚫⚫⚪⚪\], (4) \[⚫⚫⚫⚪\], (5) \[⚫⚫⚫⚫\]. These are the only
> possibilities consistent with what we know about the contents of the
> bag. **Call these five possibilities the *conjectures*.**
>
> Our goal is to figure out which of these conjectures is most
> plausible, given some evidence about the contents of the bag. We do
> have some evidence: A sequence of three marbles is pulled from the
> bag, one at a time, replacing the marble each time and shaking the bag
> before drawing another marble. **The sequence that emerges is: ⚫⚪⚫,
> in that order. These are the data.** (bold emphasis pb)
>
> So now let's plant the garden and see how to use the data to infer
> what's in the bag. Let's begin by considering just the single
> conjecture, \[⚫⚪⚪⚪\], that the bag contains one blue and three
> white marbles. ...
>
> ...
>
> Notice that even though the three white marbles look the same from a
> data perspective---we just record the color of the marbles, after
> all---they are really different events. This is important, because it
> means that there are three more ways to see ⚪ than to see ⚫.

|              |                        |
|--------------|------------------------|
| Conjecture   | Ways to produce ⚫⚪⚫ |
| \[⚪⚪⚪⚪\] | 0 × 4 × 0 = 0          |
| \[⚫⚪⚪⚪\] | 1 × 3 × 1 = 3          |
| \[⚫⚫⚪⚪\] | 2 × 2 × 2 = 8          |
| \[⚫⚫⚫⚪\] | 3 × 1 × 3 = 9          |
| \[⚫⚫⚫⚫\] | 4 × 0 × 4 = 0          |

> Notice that the number of ways to produce the data, for each
> conjecture, can be computed by first counting the number of paths in
> each "ring" of the garden and then by multiplying these counts
> together. ... The fact that numbers are multiplied during calculation
> doesn't change the fact that this is still just counting of logically
> possible paths. This point will come up again, when you meet a formal
> representation of Bayesian inference.

### 2.1.2a Combining Other Information

> We may have additional information about the relative plausibility of
> each conjecture. This information could arise from knowledge of how
> the contents of the bag were generated. It could also arise from
> previous data. Whatever the source, it would help to have a way to
> combine different sources of information to update the plausibilities.
> Luckily there is a natural solution: Just multiply the counts.
>
> To grasp this solution, suppose we're willing to say each conjecture
> is equally plausible at the start. So we just compare the counts of
> ways in which each conjecture is compatible with the observed data.
> This comparison suggests that \[⚫⚫⚫⚪\] is slightly more plausible
> than \[⚫⚫⚪⚪\], and both are about three times more plausible than
> \[⚫⚪⚪⚪\]. **Since these are our initial counts, and we are going
> to update them next, let's label them *prior*.** (bold emphasis are
> mine)

Here's how to do it. First we count the numbers of ways each conjecture
could produce the new observation, ⚫. Then we multiply each of these
new counts by the prior numbers of ways for each conjecture. In table
form:

|     |              |     |                    |     |              |     |            |     |
|--------|--------|--------|--------|--------|--------|--------|--------|--------|
|     |              |     |                    |     |              |     |            |     |
|     | Conjecture   |     | Ways to produce ⚫ |     | Prior counts |     | New count  |     |
|     | \[⚪⚪⚪⚪\] |     | 0                  |     | 0            |     | 0 × 0 = 0  |     |
|     | \[⚫⚪⚪⚪\] |     | 1                  |     | 3            |     | 3 × 1 = 3  |     |
|     | \[⚫⚫⚪⚪\] |     | 2                  |     | 8            |     | 8 × 2 = 16 |     |
|     | \[⚫⚫⚫⚪\] |     | 3                  |     | 9            |     | 9 × 3 = 27 |     |
|     | \[⚫⚫⚫⚫\] |     | 4                  |     | 0            |     | 0 × 4 = 0  |     |

In the book the table header "Ways to produce" includes ⚪ instead of
--- as I think is correct --- ⚫.

**Principle of Indifference**

::: callout-tip
## Principle of Indifference

Before seeing any data the most common solution is to assign an equal
number of ways that each conjecture could be correct.
:::

> Which assumption should we use, when there is no previous information
> about the conjectures? The most common solution is to assign an equal
> number of ways that each conjecture could be correct, before seeing
> any data. This is sometimes known as the **PRINCIPLE OF
> INDIFFERENCE**: When there is no reason to say that one conjecture is
> more plausible than another, weigh all of the conjectures equally. ...
>
> For the sort of problems we examine in this book, the principle of
> indifference results in inferences very comparable to mainstream
> non-Bayesian approaches, most of which contain implicit equal
> weighting of possibilities. For example a typical non-Bayesian
> confidence interval weighs equally all of the possible values a
> parameter could take, regardless of how implausible some of them are.
> In addition, many non-Bayesian procedures have moved away from equal
> weighting, through the use of penalized likelihood and other methods.

### 2.1.3a From Counts to Probability

::: callout-tip
## Principle of honest ignorance

*When we don\'t know what caused the data, potential causes that may
produce the data in more ways are more plausible*.
:::

Two reasons for using probabilities instead of counts:

1.  Only relative value matters.
2.  Counts will very fast grow very large and difficult to manipulate.

![](img/plausability-formula-1-min.png){fig-align="center"}

**Standardizing the plausability**

![](img/plausability-formula-2-min.png){fig-align="center"}

```{r}
#| label: compute-plausabilities-a1

## R code 2.1
ways <- c(0, 3, 8, 9, 0)
ways / sum(ways)
```

**Names of the different parts of the formula**

Data = ⚫⚪⚫.

-   A conjectured proportion of blue marbles, *p*, is usually called a
    **PARAMETER** value. It\'s just a way of indexing possible
    explanations of the data. For instance one conjectured proportion of
    one blue marble could be: ⚫⚪⚪⚪ (`p = 1`). The others are:
    ⚪⚪⚪⚪ (`p = 0`), ⚫⚫⚪⚪ (`p = 2` , ⚫⚫⚫⚪ (`p = 3`), and
    ⚫⚫⚫⚫ (`p = 4` ways).
-   The relative number of ways that a value *p* can produce the data is
    usually called a **LIKELIHOOD**. It is derived by enumerating all
    the possible data sequences that could have happened and then
    eliminating those sequences inconsistent with the data. For
    instance: `0.00, 0.15, 0.40, 0.45, 0.00`
-   The prior plausibility of any specific *p* is usually called the
    **PRIOR PROBABILITY**. For instance: `0, 3, 8, 9, 0`
-   The new, updated plausibility of any specific *p* is usually called
    the **POSTERIOR PROBABILITY**. For instance: `0, 3, 16, 27, 0`

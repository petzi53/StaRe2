# 2a: Small and Large Worlds

-   The **small world** is the self-contained logical world of the
    model.
-   The **large world** is the broader context in which one deploys a
    model.

**Meta Remark (Study Guide)**

> This chapter focuses on the small world. It explains probability
> theory in its essential form: counting the ways things can happen.
> Bayesian inference arises automatically from this perspective. Then
> the chapter presents the stylized components of a Bayesian statistical
> model, a model for learning from data. Then it shows you how to
> animate the model, to produce estimates.
>
> All this work provides a foundation for the next chapter, in which
> you'll learn to summarize Bayesian estimates, as well as begin to
> consider large world obligations.

## 2.1a The Garden of Forking Data

::: callout-important
Bayesian inference is really just counting and comparing of
possibilities. ... In order to make good inference about what actually
happened, it helps to consider everything that could have happened. A
Bayesian analysis is a garden of forking data, in which alternative
sequences of events are cultivated. As we learn about what did happen,
some of these alternative sequences are pruned. In the end, what remains
is only what is logically consistent with our knowledge.
:::

### 2.1.1a Counting possibilites

> Suppose there's a bag, and it contains four marbles. These marbles
> come in two colors: blue and white. We know there are four marbles in
> the bag, but we don't know how many are of each color. We do know that
> there are five possibilities: (1) \[⚪⚪⚪⚪\], (2) \[⚫⚪⚪⚪\], (3)
> \[⚫⚫⚪⚪\], (4) \[⚫⚫⚫⚪\], (5) \[⚫⚫⚫⚫\]. These are the only
> possibilities consistent with what we know about the contents of the
> bag. **Call these five possibilities the *conjectures*.**
>
> Our goal is to figure out which of these conjectures is most
> plausible, given some evidence about the contents of the bag. We do
> have some evidence: A sequence of three marbles is pulled from the
> bag, one at a time, replacing the marble each time and shaking the bag
> before drawing another marble. **The sequence that emerges is: ⚫⚪⚫,
> in that order. These are the data.** (bold emphasis pb)
>
> So now let's plant the garden and see how to use the data to infer
> what's in the bag. Let's begin by considering just the single
> conjecture, \[⚫⚪⚪⚪\], that the bag contains one blue and three
> white marbles. ...
>
> ...
>
> Notice that even though the three white marbles look the same from a
> data perspective---we just record the color of the marbles, after
> all---they are really different events. This is important, because it
> means that there are three more ways to see ⚪ than to see ⚫.

|              |                        |
|--------------|------------------------|
| Conjecture   | Ways to produce ⚫⚪⚫ |
| \[⚪⚪⚪⚪\] | 0 × 4 × 0 = 0          |
| \[⚫⚪⚪⚪\] | 1 × 3 × 1 = 3          |
| \[⚫⚫⚪⚪\] | 2 × 2 × 2 = 8          |
| \[⚫⚫⚫⚪\] | 3 × 1 × 3 = 9          |
| \[⚫⚫⚫⚫\] | 4 × 0 × 4 = 0          |

> Notice that the number of ways to produce the data, for each
> conjecture, can be computed by first counting the number of paths in
> each "ring" of the garden and then by multiplying these counts
> together. ... The fact that numbers are multiplied during calculation
> doesn't change the fact that this is still just counting of logically
> possible paths. This point will come up again, when you meet a formal
> representation of Bayesian inference.

### 2.1.2a Combining Other Information

> We may have additional information about the relative plausibility of
> each conjecture. This information could arise from knowledge of how
> the contents of the bag were generated. It could also arise from
> previous data. Whatever the source, it would help to have a way to
> combine different sources of information to update the plausibilities.
> Luckily there is a natural solution: Just multiply the counts.
>
> To grasp this solution, suppose we're willing to say each conjecture
> is equally plausible at the start. So we just compare the counts of
> ways in which each conjecture is compatible with the observed data.
> This comparison suggests that \[⚫⚫⚫⚪\] is slightly more plausible
> than \[⚫⚫⚪⚪\], and both are about three times more plausible than
> \[⚫⚪⚪⚪\]. **Since these are our initial counts, and we are going
> to update them next, let's label them *prior*.** (bold emphasis are
> mine)

Here's how to do it. First we count the numbers of ways each conjecture
could produce the new observation, ⚫. Then we multiply each of these
new counts by the prior numbers of ways for each conjecture. In table
form:

|     |              |     |                    |     |              |     |            |     |
|-----|--------------|-----|--------------------|-----|--------------|-----|------------|-----|
|     |              |     |                    |     |              |     |            |     |
|     | Conjecture   |     | Ways to produce ⚫ |     | Prior counts |     | New count  |     |
|     | \[⚪⚪⚪⚪\] |     | 0                  |     | 0            |     | 0 × 0 = 0  |     |
|     | \[⚫⚪⚪⚪\] |     | 1                  |     | 3            |     | 3 × 1 = 3  |     |
|     | \[⚫⚫⚪⚪\] |     | 2                  |     | 8            |     | 8 × 2 = 16 |     |
|     | \[⚫⚫⚫⚪\] |     | 3                  |     | 9            |     | 9 × 3 = 27 |     |
|     | \[⚫⚫⚫⚫\] |     | 4                  |     | 0            |     | 0 × 4 = 0  |     |

In the book the table header "Ways to produce" includes ⚪ instead of
--- as I think is correct --- ⚫.

**Principle of Indifference**

::: callout-tip
## Principle of Indifference

Before seeing any data the most common solution is to assign an equal
number of ways that each conjecture could be correct.
:::

> Which assumption should we use, when there is no previous information
> about the conjectures? The most common solution is to assign an equal
> number of ways that each conjecture could be correct, before seeing
> any data. This is sometimes known as the **PRINCIPLE OF
> INDIFFERENCE**: When there is no reason to say that one conjecture is
> more plausible than another, weigh all of the conjectures equally. ...
>
> For the sort of problems we examine in this book, the principle of
> indifference results in inferences very comparable to mainstream
> non-Bayesian approaches, most of which contain implicit equal
> weighting of possibilities. For example a typical non-Bayesian
> confidence interval weighs equally all of the possible values a
> parameter could take, regardless of how implausible some of them are.
> In addition, many non-Bayesian procedures have moved away from equal
> weighting, through the use of penalized likelihood and other methods.

### 2.1.3a From Counts to Probability

::: callout-tip
## Principle of honest ignorance

*When we don't know what caused the data, potential causes that may
produce the data in more ways are more plausible*.
:::

Two reasons for using probabilities instead of counts:

1.  Only relative value matters.
2.  Counts will very fast grow very large and difficult to manipulate.

![](img/plausability-formula-1-min.png){fig-align="center"}

**Standardizing the plausability**

![](img/plausability-formula-2-min.png){fig-align="center"}

```{r}
#| label: compute-plausabilities-a1

## R code 2.1
ways <- c(0, 3, 8, 9, 0)
ways / sum(ways)
```

**Names of the different parts of the formula**

Data = ⚫⚪⚫.

-   A conjectured proportion of blue marbles, *p*, is usually called a
    **PARAMETER** value. It's just a way of indexing possible
    explanations of the data. For instance one conjectured proportion of
    one blue marble could be: ⚫⚪⚪⚪ (`p = 1`). The others are:
    ⚪⚪⚪⚪ (`p = 0`), ⚫⚫⚪⚪ (`p = 2` , ⚫⚫⚫⚪ (`p = 3`), and
    ⚫⚫⚫⚫ (`p = 4` ways).
-   The relative number of ways that a value *p* can produce the data is
    usually called a **LIKELIHOOD**. It is derived by enumerating all
    the possible data sequences that could have happened and then
    eliminating those sequences inconsistent with the data. For
    instance: `0.00, 0.15, 0.40, 0.45, 0.00`
-   The prior plausibility of any specific *p* is usually called the
    **PRIOR PROBABILITY**. For instance: `0, 3, 8, 9, 0`
-   The new, updated plausibility of any specific *p* is usually called
    the **POSTERIOR PROBABILITY**. For instance: `0, 3, 16, 27, 0`

## 2.2a Building a Model

**Globe tossing**

Data: `W L W W W L W L W` (W indicates water and L indicates land. )

1.  **Data story**: Motivate the model by narrating how the data might
    arise.
2.  **Update**: Educate your model by feeding it the data.
3.  **Evaluate**: All statistical models require supervision, leading to
    model revision.

### 2.2.1a A Data Story

> Bayesian data analysis usually means producing a story for how the
> data came to be. This story may be *descriptive*, specifying
> associations that can be used to predict outcomes, given observations.
> Or it may be *causal*, a theory of how some events produce other
> events.

All data stories have to be complete in the sense that they are
sufficient for specifying an algorithm for simulating new data.

> You can motivate your data story by trying to explain how each piece
> of data is born. This usually means describing aspects of the
> underlying reality as well as the sampling process. The data story in
> this case is simply a restatement of the sampling process:
>
> 1.  The true proportion of water covering the globe is *`p`*.
> 2.  A single toss of the globe has a probability *`p`* of producing a
>     water (W) observation. It has a probability `1 – p` of producing a
>     land (L) observation.
> 3.  Each toss of the globe is independent of the others.

### 2.2.2a Bayesian Updating

> A Bayesian model begins with one set of plausibilities assigned to
> each of these possibilities. These are the prior plausibilities. Then
> it updates them in light of the data, to produce the posterior
> plausibilities. This updating process is a kind of learning, called
> **BAYESIAN UPDATING**.
>
> ...
>
> For the sake of the example only, let's program our Bayesian machine
> to initially assign the same plausibility to every proportion of
> water, every value of *p*. We'll do better than this later.
>
> ...
>
> Every time a "W" is seen, the peak of the plausibility curve moves to
> the right, towards larger values of *p*. Every time an "L" is seen, it
> moves the other direction. The maximum height of the curve increases
> with each sample, meaning that fewer values of *p* amass more
> plausibility as the amount of evidence increases. As each new
> observation is added, the curve is updated consistent with all
> previous observations.

Figure 2.5 (How a Bayesian model learns) of the book is reproduced in my
notes of the brms-version.

## 2.2.3a Evaluate

Keep in mind two cautious principles:

1.  First, **the model's certainty is no guarantee that the model is a
    good one**. ... \[M\]odels of all sorts---Bayesian or not---can be
    very confident about an inference, even when the model is seriously
    misleading. This is because the inferences are conditional on the
    model. What your model is telling you is that, given a commitment to
    this particular model, it can be very sure that the plausible values
    are in a narrow range. Under a different model, things might look
    differently.
2.  Second, **it is important to supervise and critique your model's
    work**. ... When something is irrelevant to the machine, it won't
    affect the inference directly. But it may affect it indirectly ...
    So it is important to check the model's inferences in light of
    aspects of the data it does not know about. Such checks are an
    inherently creative enterprise, left to the analyst and the
    scientific community. Golems are very bad at it. (emphasis are mine)

Further keep in mind that

> the goal is not to test the truth value of the model's assumptions. We
> know the model's assumptions are never exactly right, in the sense of
> matching the true data generating process. ... Moreover, models do not
> need to be exactly true in order to produce highly precise and useful
> inferences.
>
> Instead, the objective is to check the model's adequacy for some
> purpose. This usually means asking and answering additional questions,
> beyond those that originally constructed the model. Both the questions
> and answers will depend upon the scientific context. So it's hard to
> provide general advice.

## 2.3a Components of the Model

We observed three components of the model:

1.  a **likelihood function**: "the number of ways each conjecture could
    produce an observation,"

2.  one or more **parameters**: "the accumulated number of ways each
    conjecture could produce the entire data," and

3.  **a prior**: "the initial plausibility of each conjectured cause of
    the data"

### 2.3.1a List the Variables

> Variables are just symbols that can take on different values. In a
> scientific context, variables include things we wish to infer, such as
> proportions and rates, as well as things we might observe, the
> data....
>
> Unobserved variables are usually called **parameters**. (emphasis in
> the original)

Take as example the globe tossing models: There are three variables: `W`
and `L` (water or land) and the proportion of water and land `p`. We
observe the events of water or land but we calculate (do not observe
directly) the proportion of water and land. So `p` is a parameter as
defined above.

### 2.3.2a Define the Variables

> In defining each \[variable\], we build a model that relates the
> variables to one another. Remember, the goal is to count all the ways
> the data could arise, given the assumptions.

#### 2.3.2.1a Observed Variables

For each unobserved variable (parameter) we need to define the relative
number of ways---the probability---that the values of each observed
variable could arise. And then for each unobserved variable, we need to
define the prior plausibility of each value it could take.

For the count of water *W* and land *L* in the globe tossing model, we
**define how plausible any combination of *W* and *L* would be, for a
specific value of *p*.** (emphasis is mine)

This idea is implemented in the functions of `expand()`, `expand_grid()`
and `crossing()` of the {**tidyr**} package: They generate all
combinations of variables found as is demonstrated in the section
[Bayesian Updating of the
brms-variant](02b-small-and-large-worlds.html#sec-expand_grid).

Instead of counting we can also use a mathemtical function to calculate
the probability of all combinations. A distribution function assigned to
an observed variable is usually called a **LIKELIHOOD**.

In the case of the globe-tossing model the appropriate distributional
function is the **binomial distribution**. (Does this means that I have
to know more on probability theory to decide when to choose which
distribution?)

##### 2.3.2.1.1a Likelihood for p= 0.5

The likelihood in the globe-tossing example (9 trials, 6 with `W` and 3
with `L`) is easily computed:

```{r}
#| label: likelihood-1

## R code 2.2
dbinom(6, size = 9, prob = 0.5)
```

In this example it is assumed that the probability of `W` and `L` are
equal distributed. We calculated how plausible the combination of *6W*
and *3L* would be, for the specific value of *p = 0.5*. The result is
with 16% a pretty low probability.

##### 2.3.2.1.2a Likelihood for many p-values

To get a better idea what the best estimation of the probability is, we
could vary systematically the `p` value and look for the maximum.

A demonstration how this is done can be seen in the brms-variant
reproduced in my notes
[here](02b-small-and-large-worlds.html#sec-likelihood-for-many-p-values-b).
It shows a maximum at about *p = 0.7*.

#### 2.3.2.2a Unobserved Variables

Even variables (= parameters) are not observed we need to define them.
In the globe-tossing model there is only one parameter (p), but most
models have more than one unobserved variables.

> In statistical modeling, many of the most common questions we ask
> about data are answered directly by parameters:
>
> -   What is the average difference between treatment groups?
> -   How strong is the association between a treatment and an outcome?
> -   Does the effect of the treatment depend upon a covariate?
> -   How much variation is there among groups?
>
> \[We will\] see how these questions become extra parameters inside the
> distribution function we assign to the data.

::: callout-important
## Parameter & Prior

For every parameter we must provide a distribution of prior
plausibility, its Prior. This is also true when the number of trials is
null (N = 0), e.g. even in the initial state of information we need a
prior.
:::

When you have a previous estimate, that can become the prior. As a
result, each estimate (posterior probility) becomes then the prior for
the next step. Where do priors come from? They are both engineering
assumptions, chosen to help the machine learn, and scientific
assumptions, chosen to reflect what we know about a phenomenon. Because
the prior is an assumption, it should be interrogated like other
assumptions: by altering it and checking how sensitive inference is to
the assumption.

::: callout-note
## Data or Parameters

Data are measured and known; parameters are unknown and must be
estimated from data. But there is a deep identity between certainty
(data) and uncertainty (parameters): Sometimes we observe a variable
(data), sometimes not (parameter) but it could be that the same
distribution function applies. An exploitation of the identity between
data & parameters is it to incorporate measurement error and missing
data into your modeling.

::: callout-tip
For more in this topic, check out McElreath's lecture, [*Understanding
Bayesian statistics without frequentist
language*](https://youtu.be/yakg94HyWdE).
:::
:::

### 2.3.3a A Model is Born

> The observed variables *W* and *L* are given relative counts through
> the binomial distribution.

$$W∼Binomial(n,p)\space where\space N = W + L$$

The above is just a convention for communicating the assumption that the
relative counts of ways to realize *W* in *N* trials with probability
*p* on each trial comes from the binomial distribution.

Our binomial likelihood contains a parameter for an unobserved variable,
*p*. Parameters in Bayesian models are assigned priors:

$$p∼Uniform(0,1)$$

which expresses the model assumption that the entire range of possible
values for p are equally plausible.

## 2.4a Making the Model Go

> Once you have named all the variables and chosen definitions for each,
> a Bayesian model can update all of the prior distributions to their
> purely logical consequences: the **POSTERIOR DISTRIBUTION**. For every
> unique combination of data, likelihood, parameters, and prior, there
> is a unique posterior distribution. This distribution contains the
> relative plausibility of different parameter values, conditional on
> the data and model. The posterior distribution takes the form of the
> probability of the parameters, conditional on the data.

In the case of the globe-tossing model we can write:

$$
Pr(p|W, L)
$$

This has to be interpreted as "the probability of each possible value of
*p*, conditional on the specific *W* and *L* that we observed."

### 2.4.1a Bayes' Theorem

$$
Pr(p|W,L) = \frac{Pr(W,L|p)Pr(p)}{Pr(W,L)}
$$

> And this is Bayes' theorem. It says that the probability of any
> particular value of *p*, considering the data, is equal to the product
> of the relative plausibility of the data, conditional on *p*, and the
> prior plausibility of *p*, divided by this thing Pr(*W, L*), which
> I'll call the *average probability of the data*.

Expressed in words:

$$
Posterior = \frac{Probability\space of\space the\space data\space ✕\space Prior}{Average\space probability\space of\space the\space data}
$$

Names of the average probability of the data:

-   evidence
-   average likelihood
-   marginal likelihood

The job of the average probability of the data is just to standardize
the posterior, to ensure it sums (integrates) to one.

::: callout-important
## Key lesson

The posterior is proportional to the product of the prior and the
probability of the data.
:::

> \[E\]ach specific value of *p*, the number of paths through the garden
> of forking data is the product of the prior number of paths and the
> new number of paths. **Multiplication is just compressed counting.**
> The average probability on the bottom just standardizes the counts so
> they sum to one. (emphasis is mine)

### 2.4.2a Motos

> Recall that your Bayesian model is a machine, a figurative golem. It
> has built-in definitions for the likelihood, the parameters, and the
> prior. And then at its heart lies a motor that processes data,
> producing a posterior distribution. The action of this motor can be
> thought of as *conditioning* the prior on the data. As explained in
> the previous section, this conditioning is governed by the rules of
> probability theory, which defines a uniquely logical posterior for set
> of assumptions and observations.
>
> However, knowing the mathematical rule is often of little help,
> because many of the interesting models in contemporary science cannot
> be conditioned formally, no matter your skill in mathematics. And
> while some broadly useful models like linear regression can be
> conditioned formally, this is only possible if you constrain your
> choice of prior to special forms that are easy to do mathematics with.
> We'd like to avoid forced modeling choices of this kind, instead
> favoring conditioning engines that can accommodate whichever prior is
> most useful for inference.
>
> What this means is that various numerical techniques are needed to
> approximate the mathematics that follows from the definition of Bayes'
> theorem. In this book, you'll meet three different conditioning
> engines, numerical techniques for computing posterior distributions:

What are the numerical techniques for computing posterior distributions
explained in the book?

1.  Grid approximation
2.  Quadratic approximation
3.  Markov chain Monte Carlo (MCMC)

### 2.4.3a Grid approximation

> While most parameters are *continuous*, capable of taking on an
> infinite number of values, it turns out that we can achieve an
> excellent approximation of the continuous posterior distribution by
> considering only a finite grid of parameter values. At any particular
> value of a parameter, *p*', it's a simple matter to compute the
> posterior probability: just multiply the prior probability of *p*' by
> the likelihood at *p*'. Repeating this procedure for each value in the
> grid generates an approximate picture of the exact posterior
> distribution. This procedure is called **GRID APPROXIMATION**.

Grid approximation is very useful as a pedagogical tool. But in most of
your real modeling, grid approximation isn't practical because it scales
poorly, as the number of parameters increases.

1.  Define the grid. This means you decide how many points to use in
    estimating the posterior, and then you make a list of the parameter
    values on the grid.
2.  Compute the value of the prior at each parameter value on the grid.
3.  Compute the likelihood at each parameter value.
4.  Compute the unstandardized posterior at each parameter value, by
    multiplying the prior by the likelihood.
5.  Finally, standardize the posterior, by dividing each value by the
    sum of all values.

In the globe tossing model the five steps are as follows:

```{r}
#| label: grid-approx-a

# 1. define grid
p_grid <- seq(from = 0, to = 1, length.out = 20)

# 2. define prior
prior <- rep(1, 20)

# 3. compute likelihood at each value in grid
likelihood <- dbinom(x = 6, size = 9, prob = p_grid)

# 4. compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# 5. standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

# 6 display the posterior distribution 
# as an exception I am using here in the original version ggplot2::ggplot 
# instead of books base::plot (to see & learn how the code has to be changed) 
df <- dplyr::bind_cols("prob" = p_grid, "post" = posterior)
ggplot2::ggplot(df, ggplot2::aes(x = prob, y = post)) +
    ggplot2::geom_line() +
    ggplot2::geom_point()
```

::: callout-important
## Effectively agreeing
:::

#### 2.4.3.1a Change likelihood parameters

The parameters for the calculated likelihood is based on the binomial
distribution and is shaping the above plot:

-   x = number of water events `W`
-   size = number of sample trials = number of observations
-   prob = success probability on each trial = probability of `W` (water
    event)

In the code example above it does not matter what prior probability is
chosen in the range from 0 to 1, if the probability of `W` is greater
than zero and --- and by definition of the binomial function --- equal
for all 20 events.

The book example features 6 `W` of 9 obs. You can produce the sequence
of diagrams from Figure 2.5 by changing the likelyhood parameters `x`
and `size` according to the sample: `WLWWWLWLW`. The `prior probability`
in this step-by-step version is always the previous PDF ([Probability
density
function](https://en.wikipedia.org/wiki/Probability_density_function)).

```{r}
#| label: different-likelihoods
likelihood1 <- dbinom(x = 1, size = 1, prob = p_grid) # event Water
likelihood2 <- dbinom(x = 1, size = 2, prob = p_grid) # event Land
likelihood3 <- dbinom(x = 2, size = 3, prob = p_grid) # event Water
likelihood4 <- dbinom(x = 3, size = 4, prob = p_grid) # event Water
likelihood5 <- dbinom(x = 4, size = 5, prob = p_grid) # event Water
likelihood6 <- dbinom(x = 4, size = 6, prob = p_grid) # event Land
likelihood7 <- dbinom(x = 5, size = 7, prob = p_grid) # event Water
likelihood8 <- dbinom(x = 5, size = 8, prob = p_grid) # event Land
likelihood9 <- dbinom(x = 6, size = 9, prob = p_grid) # event Water
```

![Replicated Figure 2.5 from the 2nd book
edition](img/bayesian_model_learns_step_by_step-min.png)

See @bayesian-updating-2b

See @lst-mytest

Then we query the customers database (\@lst-customers).

#### 2.4.3.2a Changing prior parameters

To see the influence of the prior probability on the posterior
probability by using the same likelihood the book offers two code
snippets. Replace the definition of the prior from the `grid-approx-a`
chunk (number 2 in the code snippet) --- one at a time --- with the
following lines of code:

``` r
prior <- ifelse(p_grid < 0.5, 0, 1)
prior <- exp(-5 * abs(p_grid - 0.5))
```

The rest of the code remains the same.

```{r}
#| label: grid-approx-a2

# 1. define grid
p_grid <- seq(from = 0, to = 1, length.out = 20)

# 2. define prior
prior <- ifelse(p_grid < 0.5, 0, 1)

# 3. compute likelihood at each value in grid
likelihood <- dbinom(x = 6, size = 9, prob = p_grid)

# 4. compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# 5. standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

# 6 display the posterior distribution 
# using ggplot2::ggplot instead of books base::plot 
## R code 2.4
plot(p_grid, posterior,
  type = "b",
  xlab = "probability of water", ylab = "posterior probability"
)
mtext("20 points")
```

```{r}
#| label: grid-approx-a3

# 1. define grid
p_grid <- seq(from = 0, to = 1, length.out = 20)

# 2. define prior
prior <- exp(-5 * abs(p_grid - 0.5))

# 3. compute likelihood at each value in grid
likelihood <- dbinom(x = 6, size = 9, prob = p_grid)

# 4. compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# 5. standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

# 6 display the posterior distribution 
## R code 2.4
plot(p_grid, posterior,
  type = "b",
  xlab = "probability of water", ylab = "posterior probability"
)
mtext("20 points")
```

::: callout-caution
## Puzzlement

I thought that the chosen prior did not have any effect the outcome. But
it turns out that different priors results in different PDFs.\
My explication: I have only 9 trials. With many more trials the
difference in the density would balance out bit by bit.
:::

The next graph uses the same proprotion 6:9 of success (`W`) to number
of trials but with an increase of 10 times (= 60:90). I use the second
of the different prior calculations
(`prior <- exp(-5 * abs(p_grid - 0.5))`) because it has definitely a
different result as the calculation with `prior = 1` (about 0.5 versus
aproximately 0.7).

To get a better comparison I need to increase the number of points also
10 times in each variant.

```{r}
#| label: grid-approx-a4

# 1. define grid
p_grid <- seq(from = 0, to = 1, length.out = 200)

# 2. define prior
prior <- rep(1, 20)

# 3. compute likelihood at each value in grid
likelihood <- dbinom(x = 6, size = 9, prob = p_grid)

# 4. compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# 5. standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

# 6 display the posterior distribution 
## R code 2.4
plot(p_grid, posterior,
  type = "b",
  xlab = "probability of water", ylab = "posterior probability"
)
mtext("200 points; prior = 1")
```

```{r}
#| label: grid-approx-a5

# 1. define grid
p_grid <- seq(from = 0, to = 1, length.out = 200)

# 2. define prior
prior <- exp(-5 * abs(p_grid - 0.5))

# 3. compute likelihood at each value in grid
likelihood <- dbinom(x = 60, size = 90, prob = p_grid)

# 4. compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# 5. standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

# 6 display the posterior distribution 
## R code 2.4
plot(p_grid, posterior,
  type = "b",
  xlab = "probability of water", ylab = "posterior probability"
)
mtext("200 points: prior = exp(-5 * abs(p_grid - 0.5))")
```

```{r}
#| label: grid-approx-a6

# 1. define grid
p_grid <- seq(from = 0, to = 1, length.out = 200)

# 2. define prior
prior <- ifelse(p_grid < 0.5, 0, 1)

# 3. compute likelihood at each value in grid
likelihood <- dbinom(x = 60, size = 90, prob = p_grid)

# 4. compute product of likelihood and prior
unstd.posterior <- likelihood * prior

# 5. standardize the posterior, so it sums to 1
posterior <- unstd.posterior / sum(unstd.posterior)

# 6 display the posterior distribution 
## R code 2.4
plot(p_grid, posterior,
  type = "b",
  xlab = "probability of water", ylab = "posterior probability"
)
mtext("200 points: prior = prior <- ifelse(p_grid < 0.5, 0, 1)")
```

::: callout-note
## Effectively agreeing

Indeed! The modus of both curve now very similar (about 6.5). The shape
is still different but that is explicable: The second prior is
:::

#### 2.4.3.1.3a Disadvantage

Grid approximation is very expansive. The number of unique values to
consider in the grid grows rapidly as the number of parameters in the
model increases. For the single-parameter globe tossing model, it's no
problem to compute a grid of 100 or 1000 values. But for two parameters
approximated by 100 values each, that's already 100^2^ = 10.000 values
to compute. For 10 parameters, the grid becomes many billions of values.
These days, it's routine to have models with hundreds or thousands of
parameters. The grid approximation strategy scales very poorly with
model complexity, so it won't get us very far. But it is a very useful
didactically as it help to understand the general principle.

### 2.3.2a Quadratic Approximation

#### 2.3.2.1a Concept

Under quite general conditions, the region near the peak of the
posterior distribution will be nearly Gaussian---or "normal"---in shape.
This means the posterior distribution can be usefully approximated by a
Gaussian distribution. A Gaussian distribution is convenient, because it
can be completely described by only two numbers: the location of its
center (mean) and its spread (variance).

A Gaussian approximation is called "quadratic approximation" because the
logarithm of a Gaussian distribution forms a parabola. And a parabola is
a quadratic function.

1.  Find the posterior mode with some algorithm. The procedure does not
    know where the peak is but it knows the slope under it feet.
2.  Estimate the curvature near the peak to calculate a quadratic
    approximation. This computation is done by some numerical technique.

```{r}
#| label: quadratic-approx

library(rethinking)

globe.qa <- quap(
  alist(
    W ~ dbinom(W + L, p), # binomial likelihood
    p ~ dunif(0, 1) # uniform prior
  ),
  data = list(W = 6, L = 3)
)

# display summary of quadratic approximation
# precis(globe.qa)
print(precis(globe.qa))
```

::: {.callout-warning style="color: red;"}
## precis results not displayed correctly

The result of `precis()` does not display visually correctly in
Quarto/RStudio. . The columns of the table are too narrow so that you
can't see the header and inspect the values. Printing to the console or
the display in the RStudio environment variable is correct.

A workaround is wrapping the result with `print()`.

See my [bug report](https://github.com/rstudio/rstudio/issues/13227).
:::

Read the result as: *Assuming the posterior is Gaussian, it is maximized
at 0.67, and its standard deviation is 0.16*.

#### 2.3.2.2a Compare quadratic approximation with analytic calculation

```{r}
#| label: analytical-calc

# analytical calculation
W <- 6
L <- 3
curve(dbeta(x, W + 1, L + 1), from = 0, to = 1)

# quadratic approximation
curve(dnorm(x, 0.67, 0.16), lty = 2, add = TRUE)
```

#### 2.3.2.3a Disadvantage

As the amount of data increases, however, the quadratic approximation
gets better. This phenomenon, where the quadratic approximation improves
with the amount of data, is very common. It's one of the reasons that so
many classical statistical procedures are nervous about small samples.

Using the quadratic approximation in a Bayesian context brings with it
all the same concerns. But you can always lean on some algorithm other
than quadratic approximation, if you have doubts. Indeed, grid
approximation works very well with small samples, because in such cases
the model must be simple and the computations will be quite fast. You
can also use MCMC.

Sometimes the quadratic approximation fails and you will get an error
message about the "Hessian". A *Hessian* --- named after mathematician
Ludwig Otto Hesse (1811--1874) --- is a square matrix of second
derivatives. The standard deviation is typically computed from the
Hessian, so computing the Hessian is nearly always a necessary step. But
sometimes the computation goes wrong, and your golem will choke while
trying to compute the Hessian.

Some other drawbacks will be explicated in later chapter. Therefore MCMC
seems generally the best option for complex models.

### 2.3.3a Markov Chain Monte Carlo (MCMC)

There are lots of important model types, like multilevel (mixed-effects)
models, for which neither grid approximation nor quadratic approximation
is always satisfactory. As a result, various counterintuitive model
fitting techniques have arisen. The most popular of these is **MARKOV
CHAIN MONTE CARLO** (MCMC), which is a family of conditioning engines
capable of handling highly complex models.

The understanding of this not intuitive technique is postponed to
chapter 9. Instead of attempting to compute or approximate the posterior
distribution directly, MCMC techniques merely draw samples from the
posterior. You end up with a collection of parameter values, and the
frequencies of these values correspond to the posterior plausibilities.
You can then build a picture of the posterior from the histogram of
these samples.

```{r}
#| label: MCMC-globe-tossing

n_samples <- 1000
p <- rep(NA, n_samples)
p[1] <- 0.5
W <- 6
L <- 3

for (i in 2:n_samples) {
  p_new <- rnorm(1, p[i - 1], 0.1)
  if (p_new < 0) p_new <- abs(p_new)
  if (p_new > 1) p_new <- 2 - p_new
  q0 <- dbinom(W, W + L, p[i - 1])
  q1 <- dbinom(W, W + L, p_new)
  p[i] <- ifelse(runif(1) < q1 / q0, p_new, p[i - 1])
}

rethinking::dens(p, xlim = c(0, 1))
curve(dbeta(x, W + 1, L + 1), lty = 2, add = TRUE)

```

It's weird. But it works. The above **METROPOLIS ALGORITHM** is
explained in [Chapter 9](javascript:void(0)).

## 2.4a Bayesian Inference: Some Lessons to Draw

The following list summarizes differences between Bayesian and
Non-Bayesian inference ("Frequentism"):

1.  **No minimum sampling size**: The minimum sampling size in Bayesian
    inference is one. You are going to update each data point at its
    time. For instance you got an estimate every time when you toss the
    globe and the estimate is updated. --- Well, the sample size of one
    is not very informative but that is the power of Bayesian inference
    in not getting over confident. It is always accurately representing
    the relative confidence of plausability we should assign to each of
    the possible proportions.
2.  **Shape embodies sample size**: The shape of the posterior
    distribution embodies all the information that the sample has about
    the process of the proportions. Therefore you do not need to go back
    to the original dataset for new observations. Just take the
    posterior distribution and update it by multiplying the number fo
    ways the new data could produce.
3.  **No point estimates**: The estimate is the whole distribution. It
    may be fine for communication purposes to talk about some summary
    points of the distribution like the mode and mean. But neither of
    these points is special as a point of estimate. When we do
    calculations we draw predictions from the whole distribution, never
    just from a point of it.
4.  **No one true interval**: Intervals are not important in Bayesian
    inference. They are merely summaries of the shape of the
    distribution. There is nothing special in any of these intervals
    because the endpoints of the intervals are not special. Nothing
    happens of the endpoints of the intervals because the interval is
    arbitrary. (The 95% in Non-Bayesian inference is essentially a
    dogma, a superstition. Even in Non-Bayesian statistics it is
    conceptualized as an arbitrary interval.)

## 2.5a Practice

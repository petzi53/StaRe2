{
  "hash": "51f7356cfdd86b48bab35683e8176906",
  "result": {
    "markdown": "---\nformat: html\nexecute: \n  cache: true\nfilters: \n  - quarto\n  - nameref\n---\n\n\n# Geocentric Models {#sec-chap04}\n\n::: my-objectives\n::: my-objectives-header\nLearning Objectives:\n:::\n\n::: my-objectives-container\n> \"This chapter introduces <a class='glossary' title='Linear regression is used to predict the value of an outcome variable Y based on one or more input predictor variables X. The aim is to establish a linear relationship (a mathematical formula) between the predictor variable(s) and the response variable, so that, we can use this formula to estimate the value of the response Y, when only the predictors (Xs) values are known. (r-statistics.co) (Chap.4)'>linear regression</a> as a\n> Bayesian procedure. Under a probability interpretation, which is\n> necessary for Bayesian work, linear regression uses a Gaussian\n> (normal) distribution to describe our <a class='glossary' title='A golem (goh-lem) is a clay robot from Jewish folklore. It is used in SR2 as a methapher for a statistical model. (SR, Chap.1)'>golem</a>'s\n> uncertainty about some measurement of interest.\" ([McElreath, 2020, p.\n> 71](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=90&annotation=PHU5R9MI))\n:::\n:::\n\n\n## Why normal distributions? {#sec-why-normal-dist-a}\n\nWhy are there so many distribution approximately normal, resulting in a\nGaussian curve? Because there will be more combinations of outcomes that\nsum up to a \"central\" value, rather than to some extreme value.\n\n::: my-important\n::: my-important-header\nWhy are normal distributions normal?\n:::\n\n::: my-important-container\n> \"Any process that adds together random values from the same\n> distribution converges to a normal.\" ([McElreath, 2020, p.\n> 73](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=92&annotation=F8VYZH4I))\n:::\n:::\n\n### Normal by addition\n\n> \"Whatever the average value of the source distribution, each sample\n> from it can be thought of as a fluctuation from that average value.\n> When we begin to add these fluctuations together, they also begin to\n> cancel one another out. A large positive fluctuation will cancel a\n> large negative one. The more terms in the sum, the more chances for\n> each fluctuation to be canceled by another, or by a series of smaller\n> ones in the opposite direction. So eventually the most likely sum, in\n> the sense that there are the most ways to realize it, will be a sum in\n> which every fluctuation is canceled by another, a sum of zero\n> (relative to the mean).\" ([McElreath, 2020, p.\n> 73](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=92&annotation=PIK9MN9I))\n\n> \"It doesn't matter what shape the underlying distribution possesses.\n> It could be uniform, like in our example above, or it could be\n> (nearly) anything else. Depending upon the underlying distribution,\n> the convergence might be slow, but it will be inevitable. Often, as in\n> this example, convergence is rapid.\" ([McElreath, 2020, p.\n> 74](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=93&annotation=GLCXRF8V))\n\n::: my-resource\n::: my-resource-header\nResources: Why normal distributions?\n:::\n\n::: my-resource-container\nSee the excellent article [Why is normal distribution so\nubiquitous?](https://ekamperi.github.io/mathematics/2021/01/29/why-is-normal-distribution-so-ubiquitous.html)\nwhich also explains the example of random walks from SR2. See also the\nscientific paper [Why are normal distribution\nnormal?](https://www.journals.uchicago.edu/doi/pdf/10.1093/bjps/axs046)\nof the The British Journal for the Philosophy of Science.\n:::\n:::\n\n### Normal by multiplication\n\nThis is not only valid for addition but also for multiplication of small\nvalues: Multiplying small numbers is approximately the same as addition.\n\n### Normal by log-multipliation\n\nBut even the multiplication of large values tend to produce Gaussian\ndistributions on the log scale.\n\n### Using Gaussian distributions\n\nThe justifications for using the Gaussian distribution fall into two\nbroad categories:\n\n1.  **Ontological justification**: The Gaussian distributions is a\n    widespread pattern, appearing again and again at different scales\n    and in different domains.\n2.  **Epistemological justification**: When all we know is the mean and\n    variance of a distribution then the Gaussian distribution arises as\n    the most consistent with these assumptions.\n\n::: my-watch-out\n::: my-watch-out-header\nWATCH OUT! Many processes have heavier tails than the Gaussian\ndistribution\n:::\n\n::: my-watch-out-container\n> \"... the Gaussian distribution has some very thin tails---there is\n> very little probability in them. Instead most of the mass in the\n> Gaussian lies within one standard deviation of the mean. Many natural\n> (and unnatural) processes have much heavier tails.\" ([McElreath, 2020,\n> p. 76](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=95&annotation=LFRDF45M))\n:::\n:::\n\n::: my-definition\n::: my-definition-header\n::: {#def-mass-density}\n: Probability mass and probability density\n:::\n:::\n\n::: my-definition-container\n-   Probability distributions with only discrete outcomes, like the\n    binomial, are called <a class='glossary' title='A probability mass function (PMF) is a function that gives the probability that a discrete random variable is exactly equal to some value. Sometimes it is also known as probability function, frequency function or discrete probability density function. (Wikipedia)'>probability mass function</a>s and\n    denoted `Pr`.\n\n-   Continuous ones like the Gaussian are called\n    <a class='glossary' title='A probability density function (pdf) tells us the probability that a random variable takes on a certain value. (Statology) The probability density function (PDF) for a given value of random variable X represents the density of probability (probability per unit random variable) within a particular range of that random variable X. Probability densities can take values larger than 1. (StackExchange Mathematics) We can use a continuous probability distribution to calculate the probability that a random variable lies within an interval of possible values. To do this, we use the continuous analogue of a sum, an integral. However, we recognise that calculating an integral is equivalent to calculating the area under a probability density curve. We use p(value) for probability densities and Pr for probabilities. (Bayesian Statistics, Chap.3)'>probability density function</a>s, denoted with $p$ or\n    just plain old $f$, depending upon author and tradition.\n\n> \"Probability *density* is the rate of change in cumulative\n> probability. So where cumulative probability is increasing rapidly,\n> density can easily exceed 1. But if we calculate the area under the\n> density function, it will never exceed 1. Such areas are also called\n> *probability mass*.\" ([McElreath, 2020, p.\n> 76](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=95&annotation=577IRETG))\n\nFor example `dnorm(0,0,0.1)` which is the way to make R calculate\n$p(0 \\mid 0, 0.1)$ results to 3.9894228.\n:::\n:::\n\n> \"The Gaussian distribution is routinely seen without σ but with\n> another parameter, $\\tau$ . The parameter $\\tau$ in this context is\n> usually called *precision* and defined as $\\tau = 1/σ^2$. When\n> $\\sigma$ is large, $\\tau$ is small.\" ([McElreath, 2020, p.\n> 76](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=95&annotation=4UW673GU))\n\n> \"This form is common in Bayesian data analysis, and Bayesian model\n> fitting software, such as <a class='glossary' title='The BUGS (Bayesian inference Using Gibbs Sampling) project is concerned with flexible software for the Bayesian analysis of complex statistical models using Markov chain Monte Carlo (MCMC) methods. (The BUGS Project) It is – together with some newer derivates like OpenBUGS and MultiBUGS – currently not under active development. Use JAGS, Stan platform or NIMBLE.'>BUGS</a> or\n> <a class='glossary' title='JAGS is Just Another Gibbs Sampler.  It is a program for analysis of Bayesian hierarchical models using Markov Chain Monte Carlo (MCMC) simulation. (JAGS)'>JAGS</a>, sometimes requires using $\\tau$ rather than\n> $\\sigma$.\" ([McElreath, 2020, p.\n> 76](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=95&annotation=9VQCJW2G))\n\n## Model describing language\n\n::: my-procedure\n::: my-procedure-header\n::: {#prp-model-language}\n: General recept for describing models\n:::\n:::\n\n::: my-procedure-container\n1.  First, we recognize a set of variables to work with. Some of these\n    variables are observable. We call these data. Others are\n    unobservable things like rates and averages. We call these\n    parameters.\n2.  We define each variable either in terms of the other variables or in\n    terms of a probability distribution.\n3.  The combination of variables and their probability distributions\n    defines a joint generative model that can be\" ([McElreath, 2020, p.\n    77](zotero://select/groups/5243560/items/NFUEVASQ))\n    ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=96&annotation=IC4KGA5D))\n:::\n:::\n\n::: my-definition\n::: my-definition-header\n::: {#def-model}\n: What are statistical models?\n:::\n:::\n\n::: my-definition-container\n<a class='glossary' title='A statistical model is an expression that attempts to explain patterns in the observed values of a response variable by relating the response variable to a set of predictor variables and parameters. (Monash University)Statistical models are mappings of one set of variables through a probability distribution onto another set of variables. Fundamentally, these models define the ways values of some variables can arise, given values of other variables, because it can be quite hard to anticipate how priors influence the observable variables. (Chap.4)'>Models</a> are \"mappings of one set of\nvariables through a probability distribution onto another set of\nvariables.\" ([McElreath, 2020, p.\n77](zotero://select/groups/5243560/items/NFUEVASQ))\n([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=96&annotation=NSVA3R2F))\n:::\n:::\n\n### Re-describing the globe tossing model\n\n::: my-example\n::: my-example-header\n::: {#exm-formula-glob-tossing-model}\n: Describe the globe tossing model from @sec-chap03\n:::\n:::\n\n::: my-example-container\n$$\n\\begin{align*}\nW \\sim \\operatorname{Binomial}(N, p) \\space \\space (1)\\\\\np \\sim \\operatorname{Uniform}(0, 1)  \\space \\space (2)\n\\end{align*}\n$$ {#eq-globe-tossing-model}\n\n-   `W`: observed count of water\n-   `N`: total number of tosses\n-   `p`: proportion of water on the globe\n\nThe first line in these kind of models always defines the likelihood\nfunction used in <a class='glossary' title='This is the theorem that gives Bayesian data analysis its name. But the theorem itself is a trivial implication of probability theory. The mathematical definition of the posterior distribution arises from Bayes’ Theorem. The key lesson is that the posterior is proportional to the product of the prior and the probability of the data. (Chap.2)'>Bayes’ theorem</a>. The other lines define\npriors.\n\nRead the above statement as:\n\n1.  **First line**: The count W is distributed binomially with sample\n    size `N` and probability `p`.\n2.  **Second line**: The prior for `p` is assumed to be uniform between\n    zero and one.\n\n------------------------------------------------------------------------\n\nBoth of the lines in the model of @eq-globe-tossing-model are\n<a class='glossary' title='A stochastic relationship is a mapping of a variable or parameter onto a distribution. It is said to be “stochastic” because no single instance of the variable on the left of the equation is known with certainty. Instead, the mapping is probabilistic: Some values are more plausible than others, but very many different values are plausible under any model. (Chap.4)'>stochastic</a>, as indicated by the `~` symbol. A stochastic\nrelationship is just a mapping of a variable or parameter onto a\ndistribution. It is stochastic because no single instance of the\nvariable on the left is known with certainty. Instead, the mapping is\nprobabilistic: Some values are more plausible than others, but very many\ndifferent values are plausible under any model. Later, we'll have models\nwith deterministic definitions in them.\n:::\n:::\n\n## Gaussian model of height {#sec-gaussian-model-of-height}\n\nIn this section we want a single measurement variable to model as a\nGaussian distribution. It is a preparation for the linear regression\nmodel in @sec-linear-prediction-a where we will construct and add a\npredictor variable to the model.\n\n> \"There will be two parameters describing the distribution's shape, the\n> <a class='glossary' title='The arithmetic mean, also known as “arithmetic average”, is the sum of the values divided by the number of values. If the data set were based on a series of observations obtained by sampling from a statistical population, the arithmetic mean is the sample mean \\(\\overline{x}\\) (pronounced x-bar) to distinguish it from the mean, or expected value, of the underlying distribution, the population mean \\(\\mu\\) (pronounced /’mjuː/). (Wikipedia)'>mean</a> `μ` and the\n> <a class='glossary' title='The standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of its variance. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter \\(\\sigma\\) (sigma), for the population standard deviation, or the Latin letter \\(s\\) for the sample standard deviation. (Wikipedia)'>standard deviation</a> `σ`.\n> <a class='glossary' title='A Bayesian model begins with one set of plausibilities assigned to each of these possibilities. These are the prior plausibilities. Then it updates them in light of the data, to produce the posterior plausibilities. This updating process is a kind of learning. (Chap.2)'>Bayesian updating</a> will allow us to consider every\n> possible combination of values for μ and σ and to score each\n> combination by its relativ // plausibility, in light of the data.\n> These relative plausibilities are the\n> <a class='glossary' title='It is the revised or updated probability of an event occurring after taking into consideration new information. (Investopedia). Posterior probability = prior probability + new evidence (called likelihood). (Statistics How To) The posterior distribution will be a distribution of Gaussian distributions. (SR, Chap.4). It quantifies exactly how much our observed data changes our beliefs: P(belief | data) (BF, Chap.8)'>posterior probabilities</a> of\n> each combination of values μ, σ.\" ([McElreath, 2020, p.\n> 78/79](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=98&annotation=RAYXZIGU))\n\n### The data\n\n::: my-resource\n::: my-resource-header\nResource: Nancy Howell data\n:::\n\n::: my-resource-container\n-   Howell, N. (2001). Demography of the Dobe! Kung (2nd ed.).\n    Routledge.\n-   Howell, N. (2010). Life Histories of the Dobe !kung: Food, Fatness,\n    and Well-being over the Life Span: Food, Fatness, and Well-Being\n    Over the Life-Span Volume 4. University of California Press.\n\nThe data contained in `data(Howell1)` are partial census data for the\nDobe area !Kung San, compiled from interviews conducted by Nancy Howell\nin the late 1960s.\n\nMuch more raw data is available for download from the [University of\nToronto\nLibrary](https://tspace.library.utoronto.ca/simple-search?query=nancy+howell&filter_field_1=author&filter_type_1=equals&filter_value_1=Howell%2C+Nancy&sort_by=score&order=desc&rpp=10&etal=0&start=0)\n\n> \"For the non-anthropologists reading along, the !Kung San are the most\n> famous foraging population of the twentieth century, largely because\n> of detailed quantitative studies by people like Howell.\" ([McElreath,\n> 2020, p. 79](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=98&annotation=XUZLTB86))\n:::\n:::\n\n::: my-watch-out\n::: my-watch-out-header\nWATCH OUT! Loading data without attaching the package with `library()`\n:::\n\n::: my-watch-out-container\n::: panel-tabset\n###### Standard\n\nLoading data from a package with the `data()` function is only possible\nif you have already loaded the package.\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-standard-data-loading}\n: Data loading from a package -- Standard procedure\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/loading-data-from-package1_a_4e23a375cb4f3beb52ae626dd877450e'}\n\n```{.r .cell-code}\n## R code 4.7 not executed! #######################\n# library(rethinking)\n# data(Howell1)\n# d_a <- Howell1\n```\n:::\n\n\nThe standard loading of data from packages with\n\n```         \n`library(rethinking)`\n`data(Howell1)`\n```\n\nis in this book not executed: I want to prevent clashes with loading\n{**rethinking**} and {**brms**} at the same time, because of their\nsimilar functions.\n:::\n:::\n\n###### Unusual (Original)\n\nBecause of many function name conflicts with {**brms**} I do not want to\nload {**rethinking**} and will call the function of these conflicted\npackages with `<package name>::<function name>()` Therefore I have to\nuse another, not so usual loading strategy of the data set.\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-unusual-data-loading-a}\na: Data loading from a package -- Unusual procedure (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/loading-data-from-package2_a_459e43b3da004e5fbf335a2bdd5804b7'}\n\n```{.r .cell-code}\ndata(package = \"rethinking\", list = \"Howell1\")\nd_a <- Howell1\nhead(d_a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    height   weight age male\n#> 1 151.765 47.82561  63    1\n#> 2 139.700 36.48581  63    0\n#> 3 136.525 31.86484  65    0\n#> 4 156.845 53.04191  41    1\n#> 5 145.415 41.27687  51    0\n#> 6 163.830 62.99259  35    1\n```\n\n\n:::\n:::\n\n:::\n:::\n\nThe advantage of this unusual strategy is that I have not always to\ndetach the {**rethinking**} package and to make sure {**rethinking**} is\ndetached before using {**brms**} as it is necessary in the Kurz's\n{**tidyverse**} / {**brms**} version.\n\n###### Unusual (Tidyverse)\n\nBecause of many function name conflicts with {**brms**} I do not want to\nload {**rethinking**} and will call the function of these conflicted\npackages with `<package name>::<function name>()` Therefore I have to\nuse another, not so usual loading strategy of the data set.\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-unusual-data-loading-b}\nb: Data loading from a package -- Unusual procedure (Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/loading-data-from-package_2b_10210672f0d607b4e5181a5d6e6a7037'}\n\n```{.r .cell-code}\ndata(package = \"rethinking\", list = \"Howell1\")\nd_b <- tibble::as_tibble(Howell1)\nhead(d_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 6 × 4\n#>   height weight   age  male\n#>    <dbl>  <dbl> <dbl> <int>\n#> 1   152.   47.8    63     1\n#> 2   140.   36.5    63     0\n#> 3   137.   31.9    65     0\n#> 4   157.   53.0    41     1\n#> 5   145.   41.3    51     0\n#> 6   164.   63.0    35     1\n```\n\n\n:::\n:::\n\n:::\n:::\n\nThe advantage of this unusual strategy is that I have not always to\ndetach the {**rethinking**} package and to make sure {**rethinking**} is\ndetached before using {**brms**} as it is necessary in the Kurz's\n{**tidyverse**} / {**brms**} version.\n:::\n:::\n:::\n\n#### Show the data\n\n::: my-example\n::: my-example-header\n::: {#exm-show-data}\n: Show and inspect the data\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### str()\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-show-data-str-a}\na: Compactly Display the Structure of an Arbitrary R Object (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/show-howell-data-str-a_99b7751fc8fef9f0103b69ed9e02a589'}\n\n```{.r .cell-code}\n## R code 4.8 ####################\nstr(d_a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> 'data.frame':\t544 obs. of  4 variables:\n#>  $ height: num  152 140 137 157 145 ...\n#>  $ weight: num  47.8 36.5 31.9 53 41.3 ...\n#>  $ age   : num  63 63 65 41 51 35 32 27 19 54 ...\n#>  $ male  : int  1 0 0 1 0 1 0 1 0 1 ...\n```\n\n\n:::\n:::\n\n\n`utils::str()` displays compactly the internal **str**ucture of any\nreasonable R object.\n\nOur Howell1 data contains four columns. Each column has 544 entries, so\nthere are 544 individuals in these data. Each individual has a recorded\nheight (centimeters), weight (kilograms), age (years), and \"maleness\" (0\nindicating female and 1 indicating male).\n:::\n:::\n\n###### precis()\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-show-data-precis-a}\na: Displays concise parameter estimate information for an existing model\nfit (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/show-howell-data-precis-a_18045f3cfb55fe6627729915bfe0429c'}\n\n```{.r .cell-code}\n## R code 4.9 ###################\nrethinking::precis(d_a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>               mean         sd      5.5%     94.5%     histogram\n#> height 138.2635963 27.6024476 81.108550 165.73500 ▁▁▁▁▁▁▁▂▁▇▇▅▁\n#> weight  35.6106176 14.7191782  9.360721  54.50289 ▁▂▃▂▂▂▂▅▇▇▃▂▁\n#> age     29.3443934 20.7468882  1.000000  66.13500     ▇▅▅▃▅▂▂▁▁\n#> male     0.4724265  0.4996986  0.000000   1.00000    ▇▁▁▁▁▁▁▁▁▇\n```\n\n\n:::\n:::\n\n\n`rethinking::precis()` creates a table of estimates and standard errors,\nwith optional confidence intervals and parameter correlations.\n\nIn this case we see the mean, the standard deviation, the width of a 89%\nposterior interval and a small histogram of four variables: height\n(centimeters), weight (kilograms), age (years), and \"maleness\" (0\nindicating female and 1 indicating male).\n\nAdditionally there is also a console output. In our case:\n`'data.frame': 544 obs. of 4 variables:` .\n:::\n:::\n\n###### glimpse()\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-show-data-glimpse-b}\nb: Get a glimpse of your data (Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/show-howell-data-glimpse-b_18dc36526734f5153d13c47a1a534d9a'}\n\n```{.r .cell-code}\nd_b |>\n    dplyr::glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Rows: 544\n#> Columns: 4\n#> $ height <dbl> 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149…\n#> $ weight <dbl> 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99259, 38.…\n#> $ age    <dbl> 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 54.0, 47.…\n#> $ male   <int> 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, …\n```\n\n\n:::\n:::\n\n\n`pillar::glimpse()` is re-exported by {**dplyr**} and is the tidyverse\nanalogue for `str()`. It works like a transposed version of `print()`:\ncolumns run down the page, and data runs across.\n\n`dplyr::glimpse()` shows that the Howell1 data contains four columns.\nEach column has 544 entries, so there are 544 individuals in these data.\nEach individual has a recorded height (centimeters), weight (kilograms),\nage (years), and \"maleness\" (0 indicating female and 1 indicating male).\n:::\n:::\n\n###### summary()\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-show-data-summary-b}\n: Object summaries (Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/show-howell-data-summary-b_cf3772bef3fafc14b821e47e413bf712'}\n\n```{.r .cell-code}\nd_b |> \n    base::summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>      height           weight            age             male       \n#>  Min.   : 53.98   Min.   : 4.252   Min.   : 0.00   Min.   :0.0000  \n#>  1st Qu.:125.09   1st Qu.:22.008   1st Qu.:12.00   1st Qu.:0.0000  \n#>  Median :148.59   Median :40.058   Median :27.00   Median :0.0000  \n#>  Mean   :138.26   Mean   :35.611   Mean   :29.34   Mean   :0.4724  \n#>  3rd Qu.:157.48   3rd Qu.:47.209   3rd Qu.:43.00   3rd Qu.:1.0000  \n#>  Max.   :179.07   Max.   :62.993   Max.   :88.00   Max.   :1.0000\n```\n\n\n:::\n:::\n\n\nKurz tells us that the {**brms**} package does not have a function that\nworks like `rethinking::precis()` for providing numeric and graphical\nsummaries of variables, as seen in @cnj-show-data-precis-a Kurz suggests\ntherefore to use `base::summary()` to get some of the information from\n`rethinking::precis()`.\n:::\n:::\n\n###### skim()\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-show-data-skim-b}\nb: Skim a data frame, getting useful summary statistics\n:::\n:::\n\n::: my-r-code-container\nI think `skimr::skim()` is a better option as an alternative to\n`rethinking::precis()` as `base::summary()` because it also has a\ngraphical summary of the variables. {**skimr**} has many other useful\nfunctions and is very adaptable. I propose to install and to try it out.\n\n\n::: {.cell hash='04-geocentric-models_cache/html/show-howell-data-skim-b_de96ac0fd51ee21d35ad5720ba3a0174'}\n\n```{.r .cell-code}\nd_b |>            \n    skimr::skim() \n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |     |\n|:------------------------|:----|\n|Name                     |d_b  |\n|Number of rows           |544  |\n|Number of columns        |4    |\n|_______________________  |     |\n|Column type frequency:   |     |\n|numeric                  |4    |\n|________________________ |     |\n|Group variables          |None |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|    sd|    p0|    p25|    p50|    p75|   p100|hist  |\n|:-------------|---------:|-------------:|------:|-----:|-----:|------:|------:|------:|------:|:-----|\n|height        |         0|             1| 138.26| 27.60| 53.98| 125.10| 148.59| 157.48| 179.07|▁▂▂▇▇ |\n|weight        |         0|             1|  35.61| 14.72|  4.25|  22.01|  40.06|  47.21|  62.99|▃▂▃▇▂ |\n|age           |         0|             1|  29.34| 20.75|  0.00|  12.00|  27.00|  43.00|  88.00|▇▆▅▂▁ |\n|male          |         0|             1|   0.47|  0.50|  0.00|   0.00|   0.00|   1.00|   1.00|▇▁▁▁▇ |\n\n\n:::\n:::\n\n:::\n:::\n\n###### slice_sample()\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-show-data-slice-sample}\n: Show a random number of data records\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-show-data-slice-sample_d60daa23f362d4600c5cd8df80f74f81'}\n\n```{.r .cell-code}\nset.seed(4)\nd_b |> \n  dplyr::slice_sample(n = 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 6 × 4\n#>   height weight   age  male\n#>    <dbl>  <dbl> <dbl> <int>\n#> 1  124.   21.5  11        1\n#> 2  130.   24.6  13        1\n#> 3   83.8   9.21  1        0\n#> 4  131.   25.3  15        0\n#> 5  112.   17.8   8.90     1\n#> 6  162.   51.6  36        1\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n###### as_tbl_obs()\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-show-data-as-tbl-obs-b}\nb: Randomly select a small number of observations and put it into\n`knitr::kable()`\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/show-howell-data-as-tbl-obs-b_e61b57d5d4746540e674fee51cccd4b3'}\n\n```{.r .cell-code}\nset.seed(4)\nd_b |>            \n    bayr::as_tbl_obs() \n```\n\n::: {.cell-output-display}\n\n\nTable: Data set with 5 variables, showing 8 of 544 observations.\n\n| Obs|   height|    weight|  age| male|\n|---:|--------:|---------:|----:|----:|\n|  62| 164.4650| 45.897841| 50.0|    1|\n|  71| 129.5400| 24.550667| 13.0|    1|\n| 130| 149.2250| 42.155707| 27.0|    0|\n| 307| 130.6068| 25.259404| 15.0|    0|\n| 312| 111.7600| 17.831836|  8.9|    1|\n| 371|  83.8200|  9.213587|  1.0|    0|\n| 414| 161.9250| 51.596090| 36.0|    1|\n| 504| 123.8250| 21.545620| 11.0|    1|\n\n\n\n:::\n:::\n\n\nI just learned another method to print variables from a data frame. In\nbase R there is `utils::head()` and `utils::tail()` with the\ndisadvantage that the start resp. the end of data file could be atypical\nfor the variable values. The standard tibble printing method has the\nsame problem. In contrast `bayr::as_tbl_obs()` prints a random selection\nof maximal 8 rows as a compact and nice output, that works on both,\nconsole and {**knitr**} output.\n\nAlthough `bayr::as_tbl_obs()` does not give a data *summary* as\ndiscussed here in @exm-show-data but I wanted mention this printing\nmethod as I have always looked for an easy way to display a\nrepresentative sample of some values of data frame.\n:::\n:::\n\n###### print()\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-show-data-print-as-tibble-b}\nb: Show data with the internal printing method of tibbles\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/show-howell-data-print-as-tibble-b_518ff916aaa084accd3e95e7f120fee5'}\n\n```{.r .cell-code}\nprint(d_b, n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 544 × 4\n#>    height weight   age  male\n#>     <dbl>  <dbl> <dbl> <int>\n#>  1   152.   47.8    63     1\n#>  2   140.   36.5    63     0\n#>  3   137.   31.9    65     0\n#>  4   157.   53.0    41     1\n#>  5   145.   41.3    51     0\n#>  6   164.   63.0    35     1\n#>  7   149.   38.2    32     0\n#>  8   169.   55.5    27     1\n#>  9   148.   34.9    19     0\n#> 10   165.   54.5    54     1\n#> # ℹ 534 more rows\n```\n\n\n:::\n:::\n\n\nAnother possibility is to use the `tbl_df` internal printing method, one\nof the main features of tibbles. Printing can be tweaked for a one-off\ncall by calling `print()` explicitly and setting arguments like $n$ and\n$width$. More persistent control is available by setting the options\ndescribed in `pillar::pillar_options`.\n\nAgain this printing method does not give a data summary as is featured\nin @exm-show-data. But it is an easy method -- especially as you are\nalready working with tibbles -- and sometimes this method is enough to\nget a sense of the data.\n:::\n:::\n:::\n:::\n:::\n\n#### Select the height data of adults\n\n> \"All we want for now are heights of adults in the sample. The reason\n> to filter out nonadults for now is that height is strongly correlated\n> with age, before adulthood.\" ([McElreath, 2020, p.\n> 80](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=99&annotation=VCYRP6W4))\n\n::: my-example\n::: my-example-header\n::: {#exm-adult-data}\n: Select the height data of adults (individuals older or equal than 18\nyears)\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### Original\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-adult-data-a}\na: Select individuals older or equal than 18 years (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/select-height-adults-a_7ec31caa336aa17a363e980e6a4d8de3'}\n\n```{.r .cell-code}\n## R code 4.11a ###################\nd2_a <- d_a[d_a$age >= 18, ]\nstr(d2_a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> 'data.frame':\t352 obs. of  4 variables:\n#>  $ height: num  152 140 137 157 145 ...\n#>  $ weight: num  47.8 36.5 31.9 53 41.3 ...\n#>  $ age   : num  63 63 65 41 51 35 32 27 19 54 ...\n#>  $ male  : int  1 0 0 1 0 1 0 1 0 1 ...\n```\n\n\n:::\n:::\n\n:::\n:::\n\n###### Tidyverse\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-adult-data-b}\nb: Select individuals older or equal than 18 years (Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/select-height-adults-b_f482b8fc7b12ba3b13abbbe85fa3d455'}\n\n```{.r .cell-code}\n## R code 4.11b ###################\nd2_b <- \n  d_b |> \n  dplyr::filter(age >= 18) \n\ndplyr::glimpse(d2_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Rows: 352\n#> Columns: 4\n#> $ height <dbl> 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149…\n#> $ weight <dbl> 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99259, 38.…\n#> $ age    <dbl> 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 54.0, 47.…\n#> $ male   <int> 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, …\n```\n\n\n:::\n:::\n\n:::\n:::\n:::\n:::\n:::\n\n### The model\n\nOur goal is to model the data using a Gaussian distribution.\n\n#### Plot the distribution of heights\n\n::: my-example\n::: my-example-header\n::: {#exm-plot-height-dist}\n: Plotting the distribution of height\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### Original\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-plot-height-dist-a}\na: Plot the distribution of the heights of adults, overlaid by an ideal\nGaussian distribution (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-dist-heights-a_931a4b6c8faecf2e68a6c9c93a5cb592'}\n\n```{.r .cell-code}\nrethinking::dens(d2_a$height, adj= 1, norm.comp = TRUE)\n```\n\n::: {.cell-output-display}\n![The distribution of the heights data, overlaid by an ideal Gaussian distribution (Original)](04-geocentric-models_files/figure-html/fig-dist-heights-a-1.png){#fig-dist-heights-a width=672}\n:::\n:::\n\n:::\n:::\n\nWith the option `norm.comp = TRUE` I have overlaid a Gaussian\ndistribution to see the differences to the actual data. There are some\ndifferences locally, especially on the peak of the distribution. But the\ntails looks nice and we can say that the overall impression of the curve\nis Gaussian.\n\n###### Tidyverse\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-plot-height-dist-b}\nb: Plot the distribution of the heights of adults, overlaid by an ideal\nGaussian distribution (Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-dist-heights-b_1258075fb6ea93bad06edb1190c82a8c'}\n\n```{.r .cell-code}\nd2_b |> \n    ggplot2::ggplot(ggplot2::aes(height)) +\n    ggplot2::geom_density() +\n\n    ggplot2::stat_function(\n        fun = dnorm,\n        args = with(d2_b, c(mean = mean(height), sd = sd(height)))\n        ) +\n    ggplot2::labs(\n      x = \"Height in cm\",\n      y = \"Density\"\n    ) +\n    ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![The distribution of the heights data, overlaid by an ideal Gaussian distribution: tidyverse version](04-geocentric-models_files/figure-html/fig-dist-heights-b-1.png){#fig-dist-heights-b width=672}\n:::\n:::\n\n:::\n:::\n\nThe plot of the heights distribution compared with the standard Gaussian\ndistribution is missing in Kurz's version. I added this plot after an\ninternet research by using the last example of [How to Plot a Normal\nDistribution in\nR](https://www.statology.org/plot-normal-distribution-r/). It uses the\n`ggplot2::stat_function()` to compute and draw a function as a\ncontinuous curve. This makes it easy to superimpose a function on top of\nan existing plot.\n:::\n:::\n:::\n\n::: my-watch-out\n::: my-watch-out-header\nWATCH OUT! Looking at the raw data is not enough for a model decision\n:::\n\n::: my-watch-out-container\n> \"Gawking at the raw data, to try to decide how to model them, is\n> usually not a good idea. The data could be a mixture of different\n> Gaussian distributions, for example, and in that case you won't be\n> able to detect the underlying normality just by eyeballing the outcome\n> distribution.\" ([McElreath, 2020, p.\n> 81](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=100&annotation=DMU8TC6L))\n:::\n:::\n\n::: my-theorem\n::: my-theorem-header\n::: {#thm-def-heights}\n: Define the heights as normally distributed with a mean $\\mu$ and\nstandard deviation $\\sigma$\n:::\n:::\n\n::: my-theorem-container\n$$\nh_{i} \\sim \\operatorname{Normal}(σ, μ) \n$$ {#eq-height-normal-dist}\n\n-   **Symbol h**: refers to the list of heights\n-   **Subscript i**: refers to each individual element of the list. It\n    is conventional to use $i$ because it stands for index. The index\n    $i$ takes on row numbers, and so in this example can take any value\n    from 1 to 352 (the number of heights in `d2_a$height`). As such, the\n    model above is saying that all the golem knows about each height\n    measurement is defined by the same normal distribution, with mean\n    $\\mu$ and standard deviation $\\sigma$.\n\n@eq-height-normal-dist assumes that the values $h_{i}$ are\n<a class='glossary' title='independent and identically distributed. Random variables are independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent. This property is usually abbreviated as i.i.d., iid, or IID. (Wikipedia) Example: Succession of fair coins throws are independent as the coin has no memory, so all thorws are independent. And every throw is 50:50 (heads:tails), so the coin is and stays fair - the distribution from which every throw is drawn, so to speak, is and stays the same: “identically distributed”. (Cross Validated)'>i.i.d.</a> (independent and identically distributed)\n:::\n:::\n\n> \"The i.i.d. assumption doesn't have to seem awkward, as long as you\n> remember that probability is inside the golem, not outside in the\n> world. The i.i.d. assumption is about how the golem represents its\n> uncertainty. It is an *epistemological* assumption. It is not a\n> physical assumption about the world, an *ontological* one. E. T.\n> Jaynes (1922--1998) called this the *mind projection fallacy*, the\n> mistake of confusing epistemological claims with ontological claims.\"\n> ([McElreath, 2020, p.\n> 81](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=100&annotation=VFIF5ITN))\n\n> \"To complete the model, we're going to need some priors. The\n> parameters to be estimated are both $\\mu$ and $\\sigma$, so we need a\n> prior $Pr(\\mu, \\sigma)$, the joint prior probability for all\n> parameters. In most cases, priors are specified independently for each\n> parameter, which amounts to assuming\n> $Pr(\\mu, \\sigma) = Pr(\\mu)Pr(\\sigma)$.\" ([McElreath, 2020, p.\n> 82](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=101&annotation=5HVI6LB4))\n\n::: my-theorem\n::: my-theorem-header\n::: {#thm-linear-heights-model}\n: Define the linear heights model\n:::\n:::\n\n::: my-theorem-container\n$$\n\\begin{align*}\nh_{i} \\sim \\operatorname{Normal}(\\mu, \\sigma) \\space \\space (1) \\\\ \nμ \\sim \\operatorname{Normal}(178, 20)  \\space \\space (2) \\\\ \nμ \\sim \\operatorname{Uniform}(0, 50)   \\space \\space (3)      \n\\end{align*}\n$$ {#eq-height-linear-model-m4-1}\n\n------------------------------------------------------------------------\n\n1.  First line represents the likelihood.\n2.  Second line is the chosen $\\mu$ (mu, mean) prior. It is a broad\n    Gaussian prior, centered on 178 cm, with 95% of probability between\n    178 ± 40 cm.\n3.  Third line is the chosen $\\sigma$ (sigma, standard deviation) prior.\n:::\n:::\n\nLet's think about the chosen value for the priors more in detail:\n\n**1. Choosing the mean prior**\n\n-   **Why normal distribution?**: As we have stated before in\n    @thm-def-heights the heights distribution of adults is a Gaussian\n    distribution.\n-   **Why 178cm?**: \"Your author is 178 cm tall. And the range from 138\n    cm to 218 cm encompasses a huge range of plausible mean heights for\n    human populations. So domain-specific information has gone into this\n    prior.\" ([McElreath, 2020, p.\n    82](zotero://select/groups/5243560/items/NFUEVASQ))\n    ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=101&annotation=DUN4YP7H))\n-   **Why 95% and 40cm?**: 40cm (T= twice sigma = 2\\* 20) and 95% is a\n    reference to the [68--95--99.7\n    rule](https://en.wikipedia.org/w/index.php?title=68%E2%80%9395%E2%80%9399.7_rule&oldid=1187581793)\n    that helps to remember how many percentages of values lie within an\n    interval estimate in a normal distribution:\n\n$$\n\\begin{align}\n  \\Pr(\\mu-1\\sigma \\le X \\le \\mu+1\\sigma) & \\approx 68.27\\% \\\\\n  \\Pr(\\mu-2\\sigma \\le X \\le \\mu+2\\sigma) & \\approx 95.45\\% \\\\\n  \\Pr(\\mu-3\\sigma \\le X \\le \\mu+3\\sigma) & \\approx 99.73\\%\n\\end{align}\n$$\n\n**2. Choosing the sigma prior**\n\n-   **Why uniform distribution?**: We assume <a class='glossary' title='independent and identically distributed. Random variables are independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent. This property is usually abbreviated as i.i.d., iid, or IID. (Wikipedia) Example: Succession of fair coins throws are independent as the coin has no memory, so all thorws are independent. And every throw is 50:50 (heads:tails), so the coin is and stays fair - the distribution from which every throw is drawn, so to speak, is and stays the same: “identically distributed”. (Cross Validated)'>i.i.d.</a>,\n    e.g., the standard deviation is over the whole distribution\n    identical.\n-   **Why 0 as lower limit?**: \"A standard deviation like σ must be\n    positive, so bounding it at zero makes sense.\" ([McElreath, 2020, p.\n    82](zotero://select/groups/5243560/items/NFUEVASQ))\n    ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=101&annotation=9KXSZF9A))\n-   **Why 50 as upper limit?** \"... a standard deviation of 50 cm would\n    imply that 95% of individual heights lie within 100 cm of the\n    average height.\" ([McElreath, 2020, p.\n    82](zotero://select/groups/5243560/items/NFUEVASQ))\n    ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=101&annotation=5653WGIL))\n    Thas is a range large enough to include variation of heights.\n\n::: my-important\n::: my-important-header\nPlot the chosen priors!\n:::\n\n::: my-important-container\nIt is important to plot the priors to get an idea about the assumptions\nthey build into your model.\n:::\n:::\n\n::: my-example\n::: my-example-header\n::: {#exm-ID-text}\n: Numbered Example Title\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### $\\mu$ (Original)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-code-name-a}\na: Plot the chosen mean prior (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-mean-prior-a_1d89eb6f6103457d12b0aada36d8290f'}\n\n```{.r .cell-code}\n## R code 4.12 ###############################\ngraphics::curve(stats::dnorm(x, 178, 20), from = 100, to = 250)\n```\n\n::: {.cell-output-display}\n![Plot of the chosen mean prior (Original)](04-geocentric-models_files/figure-html/fig-mean-prior-a-1.png){#fig-mean-prior-a width=672}\n:::\n:::\n\n:::\n:::\n\nYou can see that the golem is assuming that the average height (not each\nindividual height) is almost certainly between 140 cm and 220 cm. So\nthis prior carries a little information, but not a lot.\n\n###### $\\sigma$ (Original)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-code-name-b}\na: Plot chosen prior for the standard deviation (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-sd-prior-a_4ac860cce9bcbcb2933f8c9b3e2f87c9'}\n\n```{.r .cell-code}\n## R code 4.13 ###########################\ngraphics::curve(stats::dunif(x, 0, 50), from = -10, to = 60)\n```\n\n::: {.cell-output-display}\n![Plot the chosen prior for the standard deviation (Original)](04-geocentric-models_files/figure-html/fig-sd-prior-a-1.png){#fig-sd-prior-a width=672}\n:::\n:::\n\n:::\n:::\n\n###### $\\mu$ (Tidyverse)\n\n::: my-r-code\n::: my-r-code-header\n<div>\n\nb: Plot the chosen mean prior (Tidyverse)\n\n</div>\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-mean-prior-b_d910b19d414747b6a8ed94cdfa0cd09f'}\n\n```{.r .cell-code}\ntibble::tibble(x = base::seq(from = 100, to = 250, by = .1)) |> \n    \n  ggplot2::ggplot(ggplot2::aes(x = x, y = stats::dnorm(x, mean = 178, sd = 20))) +\n  ggplot2::geom_line() +\n  ggplot2::scale_x_continuous(breaks = base::seq(from = 100, to = 250, by = 25)) +\n  ggplot2::labs(title = \"mu ~ dnorm(178, 20)\",\n       y = \"density\") +\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Plot of the chosen mean prior (Tidyverse)](04-geocentric-models_files/figure-html/fig-mean-prior-b-1.png){#fig-mean-prior-b width=576}\n:::\n:::\n\n:::\n:::\n\nAs there is only one variable $y$ (= `dnorm(x, mean = 178, sd = 20)`) we\nneed to specify $x$ as a sequence of 1501 points to provide a $x$ and\n$y$ aesthetic for the plot.\n\n###### $\\sigma$ (Tidyverse)\n\n::: my-r-code\n::: my-r-code-header\n<div>\n\nb: Plot the chosen prior for the standard deviation (Tidyverse)\n\n</div>\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-sd-prior-b_31fa30174bd895abca46f88d6eb03e1d'}\n\n```{.r .cell-code}\ntibble::tibble(x = base::seq(from = -10, to = 60, by = .1)) |>\n\n  ggplot2::ggplot(ggplot2::aes(x = x, y = stats::dunif(x, min = 0, max = 50))) +\n  ggplot2::geom_line() +\n  ggplot2::scale_x_continuous(breaks = c(0, 50)) +\n  ggplot2::scale_y_continuous(NULL, breaks = NULL) +\n  ggplot2::ggtitle(\"sigma ~ dunif(0, 50)\") +\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Plot the chosen prior for the standard deviation (Tidyverse)](04-geocentric-models_files/figure-html/fig-sd-prior-b-1.png){#fig-sd-prior-b width=576}\n:::\n:::\n\n:::\n:::\n\nWe don't really need the $y$-axis when looking at the shapes of a\ndensity, so we'll just remove it with\n`scale_y_continuous(NULL, breaks = NULL)`.\n:::\n:::\n:::\n\n#### Prior predictive simulation\n\n::: my-important\n::: my-important-header\nSimulate the prior predictive distribution!\n:::\n\n::: my-important-container\n> \"<a class='glossary' title='It is an essential part of modeling. Once you’ve chosen priors for all variables these priors imply a joint prior distribution. By simulating from this distribution, you can see what your choices imply about the observable variable. This helps to diagnose bad choices. Prior predictive simulation is therefore very useful for assigning sensible priors. (Chap.4)'>Prior predictive simulation</a> is very useful for\n> assigning sensible priors, because it can be quite hard to anticipate\n> how priors influence the observable variables.\" ([McElreath, 2020, p.\n> 83](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=102&annotation=GLSLZXTF))\n:::\n:::\n\nTo see the difference we will look at two prior predictive\ndistributions:\n\n1.  The first one with our reflections and data from\n    @eq-height-linear-model-m4-1.\n2.  For the second predictive distribution we will choose a much flatter\n    and less informative prior for $\\mu$, like\n    $\\mu \\sim Normal(178, 100)$. Priors with such large standard\n    deviations are quite common in Bayesian models, but they are hardly\n    ever sensible.\n\n> \"Okay, so how to do this? You can quickly simulate heights by sampling\n> from the prior, like you sampled from the posterior back in\n> @sec-chap03. Remember, every posterior is also potentially a prior for\n> a subsequent analysis, so you can process priors just like\n> posteriors.\" ([McElreath, 2020, p.\n> 82](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=101&annotation=TTZZ63ZW))\n\n::: my-example\n::: my-example-header\n::: {#exm-prior-predictive-sim}\n: Prior Predictive Simulation\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### Original 1\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-prior-predictive-sim1-a}\na: Simulate heights by sampling from the priors with\n$\\mu \\sim Normal(178, 20)$ (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-prior-predictive-sim1-a_b5f83d87f15886d79701a9e7acc98d07'}\n\n```{.r .cell-code}\nN_sim_height_a <- 1e4\nset.seed(4) # to make example reproducible\n\n## R code 4.14a adapted #######################################\nsample_mu_a <- rnorm(N_sim_height_a, 178, 20)\nsample_sigma_a <- runif(N_sim_height_a, 0, 50)\npriors_height_a <- rnorm(N_sim_height_a, sample_mu_a, sample_sigma_a)\nrethinking::dens(priors_height_a, \n                 adj = 1, \n                 norm.comp = TRUE,\n                 show.zero = TRUE,\n                 col = \"red\")\ngraphics::abline(v = 272, lty = 2)\n```\n\n::: {.cell-output-display}\n![Simulate heights by sampling from the priors with µ ~ Normal(178,20) (Original)](04-geocentric-models_files/figure-html/fig-prior-predictive-sim1-a-1.png){#fig-prior-predictive-sim1-a width=672}\n:::\n:::\n\n:::\n:::\n\nThe prior predictive simulation generates a plausible distribution;\nThere are no values negative (left dashed vertical line at 0) and one of\nthe tallest people in recorded history, [Robert Pershing\nWadlow](https://en.wikipedia.org/wiki/Robert_Wadlow) (1918--1940) with\n272 cm (right dashed vertical line) has only a small probability.\n\nThe prior probability distribution of height is not itself Gaussian\nbecause it is approaching the mean too thin and to high, respectively\nits tails are too thick. But this is ok.\n\n> \"The distribution you see is not an empirical expectation, but rather\n> the distribution of relative plausibilities of different heights,\n> before seeing the data.\" ([McElreath, 2020, p.\n> 83](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=102&annotation=FJYK52J2))\n\n###### Original 2\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-prior-predictive-sim2-a}\na: Simulate heights by sampling from the priors with\n$\\mu \\sim Normal(178, 100)$ (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-prior-predictive-sim2-a_4de2014d037d94c95fd321f539dde156'}\n\n```{.r .cell-code}\nN_sim_height_a <- 1e4\nset.seed(4) # to make example reproducible\n\n## R code 4.14a adapted #######################################\nsample_mu2_a <- rnorm(N_sim_height_a, 178, 100)\nsample_sigma2_a <- runif(N_sim_height_a, 0, 50)\npriors_height2_a <- rnorm(N_sim_height_a, sample_mu2_a, sample_sigma2_a)\nrethinking::dens(priors_height2_a, \n                 adj = 1, \n                 norm.comp = TRUE,\n                 show.zero = TRUE,\n                 col = \"red\")\ngraphics::abline(v = 272, lty = 2)\n```\n\n::: {.cell-output-display}\n![Simulate heights by sampling from the priors with µ ~ Normal(178,100) (Original)](04-geocentric-models_files/figure-html/fig-prior-predictive-sim2-a-1.png){#fig-prior-predictive-sim2-a width=672}\n:::\n:::\n\n:::\n:::\n\nThe results of @fig-prior-predictive-sim2-a contradicts our scientific\nknowledge --- but also our common sense --- about possible height values\nof humans. Now the model, before seeing the data, expects people to have\nnegative height. It also expects some giants. One of the tallest people\nin recorded history, [Robert Pershing\nWadlow](https://en.wikipedia.org/wiki/Robert_Wadlow) (1918--1940) stood\n272 cm tall. In our prior predictive simulation many people are taller\nthan this.\n\n> \"Does this matter? In this case, we have so much data that the silly\n> prior is harmless. But that won't always be the case. There are plenty\n> of inference problems for which the data alone are not sufficient, no\n> matter how numerous. Bayes lets us proceed in these cases. But only if\n> we use our scientific knowledge to construct sensible priors. Using\n> scientific knowledge to build priors is not cheating. The important\n> thing is that your prior not be based on the values in the data, but\n> only on what you know about the data before you see it.\" ([McElreath,\n> 2020, p. 84](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=103&annotation=RDI64UXM))\n\n###### Tidyverse 1\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-prior-predictive-sim1-b}\nb: Simulate heights by sampling from the priors with\n$\\mu \\sim Normal(178, 20)$ (Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-prior-predictive-sim1-b_6243f945621a89eb54a784e0c68f1540'}\n\n```{.r .cell-code}\nN_sim_height_b <- 1e4\nset.seed(4) # to make example reproducible\n\n## R code 4.14b #######################################\n\nsim_height_b <-\n  tibble::tibble(sample_mu_b = stats::rnorm(N_sim_height_b, mean = 178, sd  = 20),\n                 sample_sigma_b = stats::runif(N_sim_height_b, min = 0, max = 50)) |> \n  dplyr::mutate(priors_height_b = stats::rnorm(N_sim_height_b, \n                                               mean = sample_mu_b, \n                                               sd = sample_sigma_b))\n  \nsim_height_b |> \n  ggplot2::ggplot(ggplot2::aes(x = priors_height_b)) +\n  ggplot2::geom_density(color = \"red\") +\n  ggplot2::stat_function(\n        fun = dnorm,\n        args = with(sim_height_b, c(mean = mean(priors_height_b), sd = sd(priors_height_b)))\n        ) +\n  ggplot2::geom_vline(xintercept = c(0, 272), linetype = \"dashed\") +\n  ggplot2::ggtitle(\"height ~ dnorm(178, 20)\") +\n  ggplot2::labs(x = \"Height in cm\", y = \"Density\") +\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Simulate heights by sampling from the priors with µ ~ Normal(178,20) (Tidyverse)](04-geocentric-models_files/figure-html/fig-prior-predictive-sim1-b-1.png){#fig-prior-predictive-sim1-b width=672}\n:::\n:::\n\n\n`ggplot2::geom_density()` computes and draws kernel density estimates,\nwhich is a smoothed version of the histogram. Note that there is no data\nmentioned explicitly in the call of `ggplot2::geom_density()`. When this\nis the case (data = `NULL`) then the data will be inherited from the\nplot data as specified in the call to `ggplot2::ggplot()`. Otherwise the\nfunction needs a data frame or a function with a single argument to\noverride the plot data. ([geom_density() help\nfile](https://ggplot2.tidyverse.org/reference/geom_density.html)).\n:::\n:::\n\nThe prior predictive simulation generates a plausible distribution;\nThere are no values negative (left dashed vertical line at 0) and one of\nthe tallest people in recorded history, [Robert Pershing\nWadlow](https://en.wikipedia.org/wiki/Robert_Wadlow) (1918--1940) with\n272 cm (right dashed vertical line) has only a small probability.\n\nThe prior probability distribution of height is not itself Gaussian\nbecause it is approaching the mean too thin and to high, respectively\nits tails are too thick. But this is ok.\n\n> \"The distribution you see is not an empirical expectation, but rather\n> the distribution of relative plausibilities of different heights,\n> before seeing the data.\" ([McElreath, 2020, p.\n> 83](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=102&annotation=FJYK52J2))\n\n###### Tidyverse 2\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-prior-predictive-sim2-b}\nb: Simulate heights by sampling from the priors with\n$\\mu \\sim Normal(178, 100)$ (Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-prior-predictive-sim2-b_c9025fd01c7a168762b74aebd51b1b50'}\n\n```{.r .cell-code}\nN_sim_height_b <- 1e4\nset.seed(4) # to make example reproducible\n\n## R code 4.14b #######################################\n\nsim_height2_b <-\n  tibble::tibble(sample_mu2_b = stats::rnorm(N_sim_height_b, mean = 178, sd  = 100),\n                 sample_sigma2_b = stats::runif(N_sim_height_b, min = 0, max = 50)) |> \n  dplyr::mutate(priors_height2_b = stats::rnorm(N_sim_height_b, \n                                               mean = sample_mu2_b, \n                                               sd = sample_sigma2_b))\n  \nsim_height2_b |> \n  ggplot2::ggplot(ggplot2::aes(x = priors_height2_b)) +\n  ggplot2::geom_density(color = \"red\") +\n  ggplot2::stat_function(\n        fun = dnorm,\n        args = with(sim_height2_b, c(mean = mean(priors_height2_b), sd = sd(priors_height2_b)))\n        ) +\n  ggplot2::geom_vline(xintercept = c(0, 272), linetype = \"dashed\") +\n  ggplot2::ggtitle(\"height ~ dnorm(178, 100)\") +\n  ggplot2::labs(x = \"Height in cm\", y = \"Density\") +\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Simulate heights by sampling from the priors with µ ~ Normal(178,100)  (Tidyverse)](04-geocentric-models_files/figure-html/fig-prior-predictive-sim2-b-1.png){#fig-prior-predictive-sim2-b width=672}\n:::\n:::\n\n:::\n:::\n\nThe results of @fig-prior-predictive-sim2-b contradicts our scientific\nknowledge --- but also our common sense --- about possible height values\nof humans. Now the model, before seeing the data, expects people to have\nnegative height. It also expects some giants. One of the tallest people\nin recorded history, [Robert Pershing\nWadlow](https://en.wikipedia.org/wiki/Robert_Wadlow) (1918--1940) stood\n272 cm tall. In our prior predictive simulation many people are taller\nthan this.\n\n> \"Does this matter? In this case, we have so much data that the silly\n> prior is harmless. But that won't always be the case. There are plenty\n> of inference problems for which the data alone are not sufficient, no\n> matter how numerous. Bayes lets us proceed in these cases. But only if\n> we use our scientific knowledge to construct sensible priors. Using\n> scientific knowledge to build priors is not cheating. The important\n> thing is that your prior not be based on the values in the data, but\n> only on what you know about the data before you see it.\" ([McElreath,\n> 2020, p. 84](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=103&annotation=RDI64UXM))\n:::\n:::\n:::\n\n### Grid approximation of the posterior distribution\n\nWe are going to map out the posterior distribution through brute force\ncalculations.\n\nThis is not recommended because it is\n\n-   laborious and computationally expensive\n-   usually so impractical as to be essentially impossible.\n\nTherefore the grid approximation technique has limited relevance. Later\non we will use the quadratic approximation with `rethinking::quap()`.\n\nThe strategy is the same grid approximation strategy as before in\n@sec-chap02-grid-approx. But now there are two dimensions, and so there\nis a geometric (literally) increase in bother.\n\n::: my-procedure\n::: my-procedure-header\n::: {#prp-grid-approx-r-code}\n: Grid approximation as R code (compare with @prp-grid-approx)\n:::\n:::\n\n::: my-procedure-container\n1.  **Define the grid - the first two code lines**: Establish the range\n    of $\\mu$ and $\\sigma$ values that you want to calculate over and\n    decide how many points to calculate in-between.\n2.  **Compute the values of all combinations of the priors on the grid -\n    third line**: Expands the $\\mu$ and $\\sigma$ values into a matrix of\n    all combinations of $\\mu$ and $\\sigma$. The matrix is stored in the\n    data frame `post_a`.\n3.  **Compute the log-likelihood at each parameter value - fourth\n    line**: Because of rounding error to zero, we need to do all\n    calculations at the log scale. `base::sapply()` passes the unique\n    combination of $\\mu$ and $\\sigma$ on each row of `post_a` to a\n    function that computes the log-likelihood of each observed height,\n    and adds all of these log-likelihoods together with `base::sum()`.\n4.  **Compute the unstandardized posterior at each parameter value -\n    fifth line**: Multiply prior by likelihood by adding them, because\n    we are using the log scale.\n5.  **Return to probability scale - sixth line**: To prevent rounding to\n    zero we can't use the standard approach with\n    `base::exp(post_a$prod)` but have to scale all log-products by the\n    maximum log-product.\n:::\n:::\n\n::: my-important\n::: my-important-header\nUse log-probability to prevent rounding to zero!\n:::\n\n::: my-important-container\n> \"Remember, in large samples, all unique samples are unlikely. This is\n> why you have to work with log-probability.\" ([McElreath, 2020, p.\n> 562](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=581&annotation=7LPX8HMD))\n:::\n:::\n\n::: my-example\n::: my-example-header\n<div>\n\n: Grid approximation of the posterior distribution\n\n</div>\n:::\n\n::: my-example-container\n::: panel-tabset\n###### Original\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-grid-approx-post-dist-a}\na: Grid approximation of the posterior distribution (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/grid-approx-post-dist-a_525f201da0771128f89e36dbfca65722'}\n\n```{.r .cell-code}\n## R code 4.16a Grid approx. ##################################\n\n## 1. Define the grid ##########\nmu.list_a <- seq(from = 150, to = 160, length.out = 100)  \nsigma.list_a <- seq(from = 7, to = 9, length.out = 100)   \n\n## 2. All Combinations of μ & σ ##########\npost_a <- expand.grid(mu_a = mu.list_a, sigma_a = sigma.list_a) \n\n## 3. Compute log-likelihood #######\npost_a$LL <- sapply(1:nrow(post_a), function(i) {                 \n  sum(                                                            \n    dnorm(d2_a$height, post_a$mu[i], post_a$sigma[i], log = TRUE) \n  )                                                               \n})                                                                \n\n## 4. Multiply prior by likelihood ##########\n## as the priors are on the log scale adding = multiplying\npost_a$prod <- post_a$LL + dnorm(post_a$mu_a, 178, 20, TRUE) +    \n  dunif(post_a$sigma_a, 0, 50, TRUE)                              \n\n## 5. Back to probability scale #########\n## without rounding error \npost_a$prob <- exp(post_a$prod - max(post_a$prod))       \n\n## define plotting area as one row and two columns\npar(mfrow = c(1, 2))\n\n## R code 4.17a Contour plot ##################################\nrethinking::contour_xyz(post_a$mu_a, post_a$sigma_a, post_a$prob)\n\n## R code 4.18a Heat map ##################################\nrethinking::image_xyz(post_a$mu_a, post_a$sigma_a, post_a$prob)\n```\n\n::: {.cell-output-display}\n![Contour plot (left) and heat map (right)](04-geocentric-models_files/figure-html/grid-approx-post-dist-a-1.png){width=672}\n:::\n:::\n\n:::\n:::\n\n###### Tidyverse\n\n::: my-r-code\n::: my-r-code-header\n<div>\n\nb: Numbered R Code Title (Tidyverse)\n\n</div>\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/grid-approx-post-dist-b_77040830d22a6d112cfb3f570b4aa72f'}\n\n```{.r .cell-code}\n## R code 4.16b Grid approx ##################################\n\n## 1./2. Define grid & combinations ##########\nd_grid_b <-\n  tidyr::expand_grid(mu_b = base::seq(from = 150, to = 160, length.out = 100),\n                  sigma_b = base::seq(from = 7, to = 9, length.out = 100))\n\n## 3a. Compute log-likelihood #######\ngrid_function <- function(mu, sigma) {\n  stats::dnorm(d2_b$height, mean = mu, sd = sigma, log = TRUE) |> \n    sum()\n}\n\nd_grid2_b <-\n  d_grid_b |> \n  \n  ## 3b. Compute log-likelihood #######\n  dplyr::mutate(log_likelihood_b = purrr::map2(mu_b, sigma_b, grid_function)) |>\n  tidyr::unnest(log_likelihood_b) |> \n  dplyr::mutate(prior_mu_b  = stats::dnorm(mu_b, mean = 178, sd = 20, log = T),\n         prior_sigma_b      = stats::dunif(sigma_b, min = 0, max = 50, log = T)) |> \n  \n  ## 4. Multiply prior by likelihood ##########\n  ## as the priors are on the log scale adding = multiplying\n  dplyr::mutate(product_b = log_likelihood_b + prior_mu_b + prior_sigma_b) |> \n  \n  ## 5. Back to probability scale #########\n  dplyr::mutate(probability_b = exp(product_b - max(product_b)))\n\n\n## R code 4.17b Contour plot ##################################\np1_b <- \n  d_grid2_b |> \n    ggplot2::ggplot(ggplot2::aes(x = mu_b, y = sigma_b, z = probability_b)) + \n    ggplot2::geom_contour() +\n    ggplot2::labs(x = base::expression(mu),\n                  y = base::expression(sigma)) +\n    ggplot2::coord_cartesian(xlim = c(153.5, 155.7),\n                         ylim = c(7, 8.5)) +\n    ggplot2::theme_bw()\n\n## R code 4.18b Heat map ##################################\np2_b <- \n  d_grid2_b |> \n    ggplot2::ggplot(ggplot2::aes(x = mu_b, y = sigma_b, fill = probability_b)) + \n    ggplot2::geom_raster(interpolate = TRUE) +\n    ggplot2::scale_fill_viridis_c(option = \"B\") +\n    ggplot2::labs(x = base::expression(mu),\n         y = base::expression(sigma)) +\n    ggplot2::coord_cartesian(xlim = c(153.5, 155.7),\n                         ylim = c(7, 8.5)) +\n    ggplot2::theme_bw()\n\nlibrary(patchwork)\np1_b + p2_b\n```\n\n::: {.cell-output-display}\n![Contour plot (left) and heat map (right)](04-geocentric-models_files/figure-html/grid-approx-post-dist-b-1.png){width=672}\n:::\n:::\n\n\n-   The produced tibble contains data frames in its cells, so that we\n    have to use the `tidyr::unnest()` function to expand the list-column\n    containing data frames into rows and columns.\n-   With `ggplot2::coord_cartesian()` I zoomed into the graph to\n    concentrate on the most important x/y ranges.\n-   The axes uses the symbols of $\\mu$ and $\\sigma$ provided by\n    unevaluated expressions through `base::expression()`.\n:::\n:::\n:::\n:::\n:::\n\n| PURPOSE                        | ORIGINAL                  | TIDYVERSE               |\n|---------------------------|-----------------------|----------------------|\n| All combinations               | base::expand.grid()       | tidyr::expand_grid()    |\n| Apply function to each element | base::sapply()            | purrr::map2()           |\n| Contour plot                   | rethinking::contour_xyz() | ggplot2::geom_contour() |\n| Heat map                       | rethinking::image_xyz()   | ggplot2::geom_raster()  |\n\n: Function equivalence between Original and Tidyverse\n{#tbl-functions-equivalence-rethinking-tidyverse}\n\n::: my-note\n::: my-note-header\nCreating data frame resp. tibble of all combinations\n:::\n\n::: my-note-container\nThere are several related function for `base::expand.grid()` in\n{**tidyverse**}:\n\n-   `tidyr::expand_grid()`: Create a tibble from all combination of\n    inputs. This is the most similar function to `base::expand.grid()`\n    as its input are vectors rather than a data frame. But it different\n    in five aspects:\n    1.  Produces sorted output (by varying the first column the slowest,\n        rather than the fastest).\n    2.  Returns a tibble, not a data frame.\n    3.  Never converts strings to factors.\n    4.  Does not add any additional attributes.\n    5.  Can expand any generalised vector, including data frames.\n-   `tidyr::expand()`: Generates all combination of variables found in a\n    dataset. It is paired with\n    1.  `tidyr::crossing()`: A wrapper around `tidyr::expand_grid()` the\n        de-duplicates and sorts the inputs.\n    2.  `tidyr::nesting()`: Finds only combinations already present in\n        the data.\n:::\n:::\n\n### Sampling from the posterior {#sec-chap04-sampling-heights-from-posterior}\n\n> \"To study this posterior distribution in more detail, again I'll push\n> the flexible approach of sampling parameter values from it. This works\n> just like it did in @cnj-chap03-sample-posterior-globe-tossing, when\n> you sampled values of $p$ from the posterior distribution for the\n> globe tossing example.\" ([McElreath, 2020, p.\n> 85](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=104&annotation=L8LYZ2VZ))\n\n::: my-procedure\n::: my-procedure-header\n::: {#prp-chap04-sampling-heights-from-posterior}\n: Sampling from the posterior\n:::\n:::\n\n::: my-procedure-container\nSince there are two parameters, and we want to sample combinations of\nthem:\n\n1.  Randomly sample row numbers in `post_a` in proportion to the values\n    in `post_a$prob`.\n2.  Pull out the parameter values on those randomly sampled rows.\n:::\n:::\n\n::: my-example\n::: my-example-header\n::: {#exm-chap04-sampling-heights-from-posterior}\n: Sampling from the posterior\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### Original\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-sampling-heights-from-posterior-a}\na: Samples from the posterior distribution for the heights data\n(Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-posterior-sample-heights-a_f5fabf321e35abe7005a43b7518baac6'}\n\n```{.r .cell-code}\n## R code 4.19a ###########################\n\n# 1. Sample row numbers #########\n# randomly sample row numbers in post_a \n# in proportion to the values in post_a$prob. \nsample.rows <- sample(1:nrow(post_a),\n  size = 1e4, replace = TRUE,\n  prob = post_a$prob\n)\n\n# 2. pull out parameter values ########\nsample.mu_a <- post_a$mu[sample.rows]\nsample.sigma_a <- post_a$sigma[sample.rows]\n\n## R code 4.20a ###########################\nplot(sample.mu_a, sample.sigma_a, \n     cex = 0.8, pch = 21, \n     col = rethinking::col.alpha(rethinking:::rangi2, 0.1)\n     )\n```\n\n::: {.cell-output-display}\n![Samples from the posterior distribution for the heights data. (Original)](04-geocentric-models_files/figure-html/fig-chap04-posterior-sample-heights-a-1.png){#fig-chap04-posterior-sample-heights-a width=672}\n:::\n:::\n\n\n-   `rethinking::col.alpha()` is part of the {**rethinking**} R package.\n    It makes colors transparent for a better inspections of values where\n    data overlap.\n-   `rethinking:::rangi2` itself is just the [definition of a hex color\n    code](https://github.com/rmcelreath/rethinking/blob/2f01a9c5dac4bc6e9a6f95eec7cae268200a8181/R/colors.r#L22)\n    (\"#8080FF\") specifying the shade of blue.\n\nAdjust the plot to your tastes by playing around with `cex` (character\nexpansion, the size of the points), `pch` (plot character), and the\n$0.1$ transparency value.\n:::\n:::\n\nThe density of points is highest in the center, reflecting the most\nplausible combinations of $\\mu$ and $\\sigma$. There are many more ways\nfor these parameter values to produce the data, conditional on the\nmodel.\n\n###### Tidyverse\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-sampling-heights-from-posterior-b}\nb: Samples from the posterior distribution for the heights data\n(Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-posterior-sample-b_b742f709c5922862a8022fb48545e38e'}\n\n```{.r .cell-code}\nset.seed(4)\n\nd_grid_samples_b <- \n  d_grid2_b |> \n  dplyr::slice_sample(n = 1e4, \n               replace = TRUE, \n               weight_by = probability_b\n               )\n\nd_grid_samples_b |> \n  ggplot2::ggplot(ggplot2::aes(x = mu_b, y = sigma_b)) + \n  ggplot2::geom_point(size = 1.8, alpha = 1/15, color = \"#8080FF\") +\n  ggplot2::scale_fill_viridis_c() +\n  ggplot2::labs(x = expression(mu[samples]),\n       y = expression(sigma[samples])) +\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Samples from the posterior distribution for the heights data. (Tidyverse)](04-geocentric-models_files/figure-html/fig-posterior-sample-b-1.png){#fig-posterior-sample-b width=672}\n:::\n:::\n\n\nKurz used the superseded `dplyr::sample_n()` to sample rows with\nreplacement from `d_grid2_b`. I used instead the newer\n`dplyr::slice_sample()` that should used to sample rows.\n:::\n:::\n\nThe density of points is highest in the center, reflecting the most\nplausible combinations of μ and σ. There are many more ways for these\nparameter values to produce the data, conditional on the model.\n:::\n:::\n:::\n\n#### Marginal posterior densities of μ and σ\n\nThe jargon <a class='glossary' title='It is the probability distribution of each of the individual variables. A marginal distribution gets it’s name because it appears in the margins of a probability distribution table. (Statology, Statistics How To) (Chap.4)'>marginal</a> here means\n\"averaging over the other parameters.\"\n\nWe described the distribution of confidence in each combination of $\\mu$\nand $\\sigma$ by summarizing the samples. Think of them like data and\ndescribe them, just like in @sec-chap03-sampling-to-summarize. For\nexample, to characterize the shapes of the marginal posterior densities\nof $\\mu$ and $\\sigma$, all we need to do is to call `rethinking::dens()`\nwith the appropriate vector `sample.mu_a` resp. `sample.sigma_a`.\n\n::: my-example\n::: my-example-header\n::: {#exm-chap04-heights-posterior-densities}\n: Marginal posterior densities of $\\mu$ and $\\sigma$ for the heights\ndata\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### Original\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-heights-posterior-densities-a}\na: Marginal posterior densities of $\\mu$ and $\\sigma$ for the heights\ndata (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-heights-posterior-densities-a_5b6ee65e76ed6695dbe8eaad7ddbcaab'}\n\n```{.r .cell-code}\n## define plotting area as one row and two columns\npar(mfrow = c(1, 2))\n\n## R code 4.21a adapted #########################\nrethinking::dens(sample.mu_a, adj = 1, show.HPDI = 0.89, \n                 norm.comp = TRUE, col = \"red\")\nrethinking::dens(sample.sigma_a, adj = 1, show.HPDI = 0.89,\n                 norm.comp = TRUE, col = \"red\")\n```\n\n::: {.cell-output-display}\n![Marginal posterior densities of μ and σ for the heights data (Original)](04-geocentric-models_files/figure-html/fig-chap04-heights-posterior-densities-a-1.png){#fig-chap04-heights-posterior-densities-a width=672}\n:::\n:::\n\n\nFor a comparison I have overlaid the normal distribution and shown the\n.89% HPDI. Compare the grayed area with the calculation of the values in\n@exm-chap04-summarize-pi.\n:::\n:::\n\n###### Tidyverse\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-heights-posterior-densities-b}\nb: Marginal posterior densities of $\\mu$ and $\\sigma$ for the heights\ndata (Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-heights-posterior-densities-b_6077dda2e5de7a2d1366355da1e8454e'}\n\n```{.r .cell-code}\nd_grid_samples_b |> \n  tidyr::pivot_longer(mu_b:sigma_b) |> \n\n  ggplot2::ggplot(ggplot2::aes(x = value)) + \n  ggplot2::geom_density(color = \"red\") +\n  # ggplot2::scale_y_continuous(NULL, breaks = NULL) +\n  # ggplot2::xlab(NULL) +\n  ggplot2::stat_function(\n      fun = dnorm,\n      args = with(d_grid_samples_b, c(\n        mean = mean(mu_b),\n        sd = sd(mu_b)))\n      ) +\n  ggplot2::stat_function(\n    fun = dnorm,\n    args = with(d_grid_samples_b, c(\n      mean = mean(sigma_b),\n      sd = sd(sigma_b)))\n    ) +\n  ggplot2::labs(x = \"mu (left), sigma (right)\",\n                y = \"Density\") +\n  ggplot2::theme_bw() +\n  ggplot2::facet_wrap(~ name, scales = \"free\",\n                      labeller = ggplot2::label_parsed)\n```\n\n::: {.cell-output-display}\n![Marginal posterior densities of μ and σ for the heights data (Tidyverse)](04-geocentric-models_files/figure-html/fig-chap04-heights-posterior-densities-b-1.png){#fig-chap04-heights-posterior-densities-b width=672}\n:::\n:::\n\n\nKurz used `tidyr::pivot_longer()` and then `ggplot2::facet_wrap()` to\nplot the densities for both `mu` and `sigma` at once. For a comparison I\nhave overlaid the normal distribution. But I do not know how to prevent\nthe base line at density = 0. See Tidyverse 2\n(@fig-chap04-heights-posterior-densities2-b) where I have constructed\nthe plots of both distribution separately.\n:::\n:::\n\n###### Tidyverse 2\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-heights-posterior-densities2-b}\nb: Marginal posterior densities of $\\mu$ and $\\sigma$ for the heights\ndata (Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-heights-posterior-densities2-b_362b4a4b16a8446bad1d7d4bc4402d68'}\n\n```{.r .cell-code}\nplot_mu_b <-\n  d_grid_samples_b |> \n    ggplot2::ggplot(ggplot2::aes(x = mu_b)) + \n    ggplot2::geom_density(color = \"red\") +\n    ggplot2::stat_function(\n        fun = dnorm,\n        args = with(d_grid_samples_b, c(\n          mean = mean(mu_b), \n          sd = sd(mu_b)))\n        ) +\n    ggplot2::labs(x = expression(mu),\n                  y = \"Density\") +\n    ggplot2::theme_bw()\n\nplot_sigma_b <- \n  d_grid_samples_b |> \n    ggplot2::ggplot(ggplot2::aes(x = sigma_b)) + \n    ggplot2::geom_density(color = \"red\") +\n    ggplot2::stat_function(\n        fun = dnorm,\n        args = with(d_grid_samples_b, c(\n          mean = mean(sigma_b), \n          sd = sd(sigma_b)))\n        ) +\n    ggplot2::labs(x = expression(sigma),\n                  y = \"Density\") +\n    ggplot2::theme_bw()\n\nlibrary(patchwork)\nplot_mu_b + plot_sigma_b\n```\n\n::: {.cell-output-display}\n![Marginal posterior densities of μ and σ for the heights data (Tidyverse)](04-geocentric-models_files/figure-html/fig-chap04-heights-posterior-densities2-b-1.png){#fig-chap04-heights-posterior-densities2-b width=672}\n:::\n:::\n\n:::\n:::\n:::\n\n> \"These densities are very close to being normal distributions. And\n> this is quite typical. As sample size increases, posterior densities\n> approach the normal distribution. If you look closely, though, you'll\n> notice that the density for $\\sigma$ has a longer right-hand tail.\n> I'll exaggerate this tendency a bit later, to show you that this\n> condition is very common for standard deviation parameters.\"\n> ([McElreath, 2020, p.\n> 86](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=105&annotation=U26FFH8V))\n:::\n:::\n\n#### Posterior compatibility intervals (PIs)\n\nSince the drawn samples of @exm-chap04-sampling-heights-from-posterior\nare just vectors of numbers, you can compute any statistic from them\nthat you could from ordinary data: mean, median, or quantile, for\nexample.\n\nAs examples we will compute <a class='glossary' title='PERCENTILE intervals (PI) assign equal mass to each tail. I uses for the computation QUANTILEs. (Help for HPDI {rethinking})'>PI</a> and\n<a class='glossary' title='Highest Posterior Density Interval (HPDI) is the Highest Density Interval (HDI) or Highest Density Region (HDR) of all possible regions of probability coverage, the HDR has the smallest region possible in the sample space. For a unimodal distribution it will include the mode (the maximum a posteriori, or MAP). (Cross Validated).'>HPDI</a>/<a class='glossary' title='Highest-density continuous interval, also known as the shortest probability interval. The same as HPDI. (tidybayes/ggdist)'>HDCI</a>.\n\nWe'll use the {**tidybayes**} resp. {**ggdist**} package to compute\ntheir posterior modes of the 89% HDIs (and not the standard 95%\nintervals, as recommended by McElreath).\n\n::: my-resource\n::: my-resource-header\n{**tidybayes**} has a companion package {**ggdist**}\n:::\n\n::: my-resource-container\nThere is a companion package {**ggdist**} which is imported completely\nby {**tidybayes**}. Whenever you cannot find the function in\n{**tidybayes**} then look at the documentation of {**ggdist**}. This is\nalso the case for the `tidybayes::mode_hdi()` function. In the help\nfiles of {**tidybayes**} you will just find notes about a deprecated\n`tidybayes::mode_hdih()` function but not the arguments of its new\nversion without the last `h` (for horizontal) `tidybayes::mode_hdi()`.\nBut you can look up these details in the {**ggdist**} documentation.\nThis observation is valid for many families of deprecated functions.\n\nThere is a division of functionality between {**tidybayes**} and\n{**ggdist**}:\n\n-   {**tidybayes**}: Tidy Data and 'Geoms' for Bayesian Models: Compose\n    data for and extract, manipulate, and visualize posterior draws from\n    Bayesian models in a tidy data format. Functions are provided to\n    help extract tidy data frames of draws from Bayesian models and that\n    generate point summaries and intervals in a tidy format.\n-   {**ggdist**}: Visualizations of Distributions and Uncertainty:\n    Provides primitives for visualizing distributions using\n    {**ggplot2**} that are particularly tuned for visualizing\n    uncertainty in either a frequentist or Bayesian mode. Both\n    analytical distributions (such as frequentist confidence\n    distributions or Bayesian priors) and distributions represented as\n    samples (such as bootstrap distributions or Bayesian posterior\n    samples) are easily visualized.\n:::\n:::\n\n::: my-example\n::: my-example-header\n::: {#exm-chap04-summarize-pi}\n: Summarize the widths with posterior compatibility intervals\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### PI (Original)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-summarize-pi-a}\na: Posterior compatibility interval (PI) (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-summarize-pi-a_744b250488ce3e467be779393e04837f'}\n\n```{.r .cell-code}\n## R code 4.22a ####################\nrethinking::PI(sample.mu_a)\nrethinking::PI(sample.sigma_a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>       5%      94% \n#> 153.9394 155.2525 \n#>       5%      94% \n#> 7.323232 8.252525\n```\n\n\n:::\n:::\n\n\nThe first two lines refers to the heights $\\mu$ samples, the other lines\nto the heights $\\sigma$ samples.\n:::\n:::\n\n###### HPDI (Original)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-summarize-hpdi-a}\na: Highest Posterior Density Interval (HPDI) (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-summarize-hpdi-a_a121fec2c8b96b01d0989b90e69d7a95'}\n\n```{.r .cell-code}\n## R code 4.22a ####################\nrethinking::HPDI(sample.mu_a)\nrethinking::HPDI(sample.sigma_a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>    |0.89    0.89| \n#> 153.8384 155.1515 \n#>    |0.89    0.89| \n#> 7.303030 8.212121\n```\n\n\n:::\n:::\n\n\nThe first two lines refers to the heights $\\mu$ samples, the other lines\nto the heights $\\sigma$ samples.\n:::\n:::\n\n###### PI (Tidyverse)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-summarize-pi-b}\nb: Posterior compatibility interval (PI) (Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-summarize-pi-b_b22ddcb5926c99ea96f31086e5105547'}\n\n```{.r .cell-code}\nd_grid_samples_b |> \n  tidyr::pivot_longer(mu_b:sigma_b) |> \n  dplyr::group_by(name) |> \n  ggdist::mode_hdi(value, .width = 0.89) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 2 × 7\n#>   name     value .lower .upper .width .point .interval\n#>   <chr>    <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n#> 1 mu_b    155.   154.   155.     0.89 mode   hdi      \n#> 2 sigma_b   7.75   7.30   8.23   0.89 mode   hdi\n```\n\n\n:::\n:::\n\n:::\n:::\n\n###### HCDI (Tidyverse)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-summarize-hpdi-b}\nb: Highest Density Continuous Interval (HDCI) (Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-summarize-hdci-b_7092f9bfc924ae5dcf31d8168170f242'}\n\n```{.r .cell-code}\nd_grid_samples_b |> \n  tidyr::pivot_longer(mu_b:sigma_b) |> \n  dplyr::group_by(name) |> \n  ggdist::mode_hdci(value, .width = 0.89) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 2 × 7\n#>   name     value .lower .upper .width .point .interval\n#>   <chr>    <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n#> 1 mu_b    155.   154.   155.     0.89 mode   hdci     \n#> 2 sigma_b   7.75   7.30   8.23   0.89 mode   hdci\n```\n\n\n:::\n:::\n\n\nIn {**tidybayes**} resp. {**ggdist**} the shortest probability interval\n(= Highest Posterior Density Interval (HPDI)) is called Highest Density\nContinuous Interval (HDCI).\n:::\n:::\n:::\n:::\n:::\n\n::: my-note\n::: my-note-header\n{**rethinking**} versus {**tidybayes/ggdist**}\n:::\n\n::: my-note-container\nThere are small differences in the results of both packages\n(**rethinking**) and {**ggdist/tidybayes**} that are not important.\n:::\n:::\n\n> \"Before moving on to using quadratic approximation (quap) as shortcut\n> to all of this inference, it is worth repeating the analysis of the\n> height data above, but now with only a fraction of the original data.\n> The reason to do this is to demonstrate that, in principle, the\n> posterior is not always so Gaussian in shape. There's no trouble with\n> the mean, μ. For a Gaussian likelihood and a Gaussian prior on μ, the\n> posterior distribution is always Gaussian as well, regardless of\n> sample size. It is the standard deviation σ that causes problems. So\n> if you care about σ---often people do not---you do need to be careful\n> of abusing the quadratic approximation.\" ([McElreath, 2020, p.\n> 86](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=105&annotation=HP4LEB6D))\n\n::: my-example\n::: my-example-header\n::: {#exm-chap04-sample-size-sigma}\n: Sample size and the normality of $\\sigma$'s posterior\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### Original\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-sample-size-sigma-a}\na: Sample size and the normality of $\\sigma$'s posterior (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-sample-size-sigma-a_ae44320b681d4a0ca4e251e5ef9ddff5'}\n\n```{.r .cell-code}\n## R code 4.23a ######################################\nd3_a <- sample(d2_a$height, size = 20)\n\n## R code 4.24a ######################################\nmu2_a.list <- seq(from = 150, to = 170, length.out = 200)\nsigma2_a.list <- seq(from = 4, to = 20, length.out = 200)\npost2_a <- expand.grid(mu = mu2_a.list, sigma = sigma2_a.list)\npost2_a$LL <- sapply(1:nrow(post2_a), function(i) {\n  sum(dnorm(d3_a,\n    mean = post2_a$mu[i], sd = post2_a$sigma[i],\n    log = TRUE\n  ))\n})\npost2_a$prod <- post2_a$LL + dnorm(post2_a$mu, 178, 20, TRUE) +\n  dunif(post2_a$sigma, 0, 50, TRUE)\npost2_a$prob <- exp(post2_a$prod - max(post2_a$prod))\nsample2_a.rows <- sample(1:nrow(post2_a),\n  size = 1e4, replace = TRUE,\n  prob = post2_a$prob\n)\nsample2_a.mu <- post2_a$mu[sample2_a.rows]\nsample2_a.sigma <- post2_a$sigma[sample2_a.rows]\n\n## define plotting area as one row and two columns\npar(mfrow = c(1, 2))\nplot(sample2_a.mu, sample2_a.sigma,\n  cex = 0.5,\n  col = rethinking::col.alpha(rethinking:::rangi2, 0.1),\n  xlab = \"mu\", ylab = \"sigma\", pch = 16\n)\n\n## R code 4.25a ############\nrethinking::dens(sample2_a.sigma, \n                 norm.comp = TRUE,\n                 col = \"red\")\n```\n\n::: {.cell-output-display}\n![Sample 20 heights to see that sigma is not Gaussian anymore (Original)](04-geocentric-models_files/figure-html/fig-chap04-sample-size-sigma-a-1.png){#fig-chap04-sample-size-sigma-a width=672}\n:::\n:::\n\n:::\n:::\n\n###### Tidyverse\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-sample-size-sigma-b}\nb: Sample size and the normality of $\\sigma$'s posterior (Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-sample-size-sigma-b_b320479a7bc029d2f3722c90aef1836e'}\n\n```{.r .cell-code}\nset.seed(4)\nd3_b <- sample(d2_b$height, size = 20)\n\nn <- 200\n\n# note we've redefined the ranges of `mu` and `sigma`\nd3_grid_b <-\n  tidyr::crossing(mu3_b    = seq(from = 150, to = 170, length.out = n),\n           sigma3_b = seq(from = 4, to = 20, length.out = n))\n\ngrid_function3_b <- function(mu, sigma) {\n  dnorm(d3_b, mean = mu, sd = sigma, log = T) |> \n    sum()\n}\n\nd3_grid_b <-\n  d3_grid_b |>  \n  dplyr::mutate(log_likelihood3_b = \n         purrr::map2_dbl(mu3_b, sigma3_b, grid_function3_b))  |>  \n  dplyr::mutate(prior3_mu_b    = stats::dnorm(mu3_b, mean = 178, sd = 20, log = T),\n         prior3_sigma_b = stats::dunif(sigma3_b, min = 0, max = 50, log = T)) |> \n  dplyr::mutate(product3_b = log_likelihood3_b + prior3_mu_b + prior3_sigma_b) |> \n  dplyr::mutate(probability3_b = base::exp(product3_b - base::max(product3_b)))\n\nset.seed(4)\n\nd3_grid_samples_b <- \n  d3_grid_b |> \n  dplyr::slice_sample(n = 1e4, \n           replace = T, \n           weight_by = probability3_b)\n\nplot3_d3_scatterplot <- \n  d3_grid_samples_b |> \n    ggplot2::ggplot(ggplot2::aes(x = mu3_b, y = sigma3_b)) + \n    ggplot2::geom_point(size = 1.8, alpha = 1/15, color = \"#8080FF\") +\n    ggplot2::labs(x = base::expression(mu[samples]),\n         y = base::expression(sigma[samples])) +\n    ggplot2::theme_bw()\n\nplot3_d3_sigma3_b <- \n  d3_grid_samples_b |> \n    ggplot2::ggplot(ggplot2::aes(x = sigma3_b)) + \n    ggplot2::geom_density(color = \"red\") +\n    ggplot2::stat_function(\n        fun = dnorm,\n        args = with(d3_grid_samples_b, c(\n          mean = mean(sigma3_b), \n          sd = sd(sigma3_b)))\n        ) +\n    ggplot2::labs(x = expression(sigma),\n                  y = \"Density\") +\n    ggplot2::theme_bw()\n\nlibrary(patchwork)\nplot3_d3_scatterplot + plot3_d3_sigma3_b\n```\n\n::: {.cell-output-display}\n![](04-geocentric-models_files/figure-html/fig-chap04-sample-size-sigma-b-1.png){#fig-chap04-sample-size-sigma-b width=672}\n:::\n:::\n\n:::\n:::\n:::\n\nCompare the left panel with @fig-posterior-sample-b and the right panel\nwith the right panel of @fig-chap04-heights-posterior-densities2-b to\nsee that now $\\sigma$ has a long right tail and does not follow a\nGaussian distribution.\n\n> \"The deep reasons for the posterior of σ tending to have a long\n> right-hand tail are complex. But a useful way to conceive of the\n> problem is that variances must be positive. As a result, there must be\n> more uncertainty about how big the variance (or standard deviation) is\n> than about how small it is.\" ([McElreath, 2020, p.\n> 86](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=105&annotation=RRXCJMTH))\n\n> \"For example, if the variance is estimated to be near zero, then you\n> know for sure that it can't be much smaller. But it could be a lot\n> bigger.\" ([McElreath, 2020, p.\n> 87](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=106&annotation=7XZ87ZJT))\n:::\n:::\n\n### Finding the posterior distribution with quadratic approximation\n\n> \"To build the quadratic approximation, we'll use `quap()`, a command\n> in the {**rethinking**} package. The `quap()` function works by using\n> the model definition you were introduced to earlier in this chapter.\n> Each line in the definition has a corresponding definition in the form\n> of R code. The engine inside quap then uses these definitions to\n> define the posterior probability at each combination of parameter\n> values. Then it can climb the posterior distribution and find the\n> peak, its <a class='glossary' title='In Bayesian statistics a Maximum A Posteriori probability or MAP is essentially the mode of posterior distribution. (CDS, p.272)'>MAP</a> (**Maximum A Posteriori** estimate).\n> Finally, it estimates the quadratic curvature at the MAP to produce an\n> approximation of the posterior distribution. Remember: This procedure\n> is very similar to what many non-Bayesian procedures do, just without\n> any priors.\" ([McElreath, 2020, p.\n> 87](zotero://select/groups/5243560/items/NFUEVASQ), parenthesis and\n> emphasis are mine)\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=106&annotation=FFDN2FE2))\n\n::: my-procedure\n::: my-procedure-header\n::: {#prp-chap04-m4-1}\n: Finding the posterior distribution\n:::\n:::\n\n::: my-procedure-container\n1.  We start with the `Howell1` data frame for adults (age \\>= 18)\n    (**Code 4.26**).\n2.  We place the R code equivalents into `base::alist()` We are going to\n    use the @eq-height-linear-model-m4-1 (**Code 4.27**).\n3.  We can add some additional options like start values (**Code 4.30**)\n4.  We fit the model to the data of our data frame and store the fitted\n    model (**Code 4.28**).\n5.  Now we can have a look at the posterior distribution (**Code\n    4.29**).\n:::\n:::\n\n::: my-example\n::: my-example-header\n::: {#exm-chap04-m4-1}\n: Finding the posterior distribution\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### Original\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-m4-1a}\na: Finding the posterior distribution with `rethinking::quap()`\n(Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-m4-1a_71800dacbba7febe325675f3edaf4ec1'}\n\n```{.r .cell-code}\n## R code 4.26a ######################\ndata(package = \"rethinking\", list = \"Howell1\")\nd_a <- Howell1\nd2_a <- d_a[d_a$age >= 18, ]\n\n## R code 4.27a ######################\nflist <- alist(\n  height ~ dnorm(mu, sigma),\n  mu ~ dnorm(178, 20),\n  sigma ~ dunif(0, 50)\n)\n\n## R code 4.30a #####################\nstart <- list(\n  mu = mean(d2_a$height),\n  sigma = sd(d2_a$height)\n)\n\n## R code 4.28a ######################\nm4.1a <- rethinking::quap(flist, data = d2_a, start = start)\nm4.1a\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> \n#> Quadratic approximate posterior distribution\n#> \n#> Formula:\n#> height ~ dnorm(mu, sigma)\n#> mu ~ dnorm(178, 20)\n#> sigma ~ dunif(0, 50)\n#> \n#> Posterior means:\n#>         mu      sigma \n#> 154.607024   7.731333 \n#> \n#> Log-likelihood: -1219.41\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nThe `rethinking::quap()` function returns a \"map\" object.\n\nSometimes I got an error message when computing this code chunk. The\nreason was that `quap()` has chosen an inconvenient value to start for\nits estimation of the posterior. I believe that one could visualize the\nproblem with a metaphor: Instead of climbing up the hill `quap()`\nstarted with a value where it was captured in a narrow valley.\n:::\n:::\n\n::: my-note\n::: my-note-header\nThe three parts of `rethinking::quap()`\n:::\n\n::: my-note-container\n1.  A formula or `base::alist()` of formulas that define the likelihood\n    and priors.\n2.  A data frame or list containing the data.\n3.  Some options like start values of method for search optimization.\n    (Not used here in this example). Note that the list of start values\n    is a regular `list()`, not an `alist()` like the formula list is.\n:::\n:::\n\n::: my-resource\n::: my-resource-header\nHow to use formulae in statistical models in R\n:::\n\n::: my-resource-container\nTo learn more about using formulae read \"Statistical Models in R\"\n[online](https://cran.r-project.org/doc/manuals/R-intro.html#Statistical-models-in-R)\nor chapter 11 in the\n[PDF](https://cran.r-project.org/doc/manuals/R-intro.pdf).\n:::\n:::\n\n###### precis\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-precis-m4-1a}\na: Printing with `rethinking::precis` (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-precis-m4-1a_3775381312bb656b9630960d83079dfb'}\n\n```{.r .cell-code}\n## R code 4.29a ######################\n(precis_m4.1a <- rethinking::precis(m4.1a))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>             mean        sd       5.5%      94.5%\n#> mu    154.607024 0.4119947 153.948576 155.265471\n#> sigma   7.731333 0.2913860   7.265642   8.197024\n```\n\n\n:::\n:::\n\n\n> \"These numbers provide Gaussian approximations for each parameter's\n> marginal distribution. This means the plausibility of each value of μ,\n> after averaging over the plausibilities of each value of σ, is given\n> by a Gaussian distribution with mean 154.6 and standard deviation 0.4.\n> The 5.5% and 94.5% quantiles are percentile interval boundaries,\n> corresponding to an 89% compatibility interval. Why 89%? It's just the\n> default. It displays a quite wide interval, so it shows a\n> high-probability range of parameter values. If you want another\n> interval, such as the conventional and mindless 95%, you can use\n> precis(m4.1,prob=0.95). But I don't recommend 95% intervals, because\n> readers will have a hard time not viewing them as significance tests.\n> 89 is also a prime number, so if someone asks you to justify it, you\n> can stare at them meaningfully and incant, \"Because it is prime.\"\n> That's no worse justification than the conventional justification for\n> 95%.\" ([McElreath, 2020, p.\n> 88](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=107&annotation=5ARWQP2N))\n:::\n:::\n\nWhen you compare the 89% boundaries with the result of the grid\napproximation in @exm-chap04-summarize-pi you will see that they are\nalmost identical as the posterior is approximately Gaussian.\n\n###### Tidyverse\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-m4-1b}\nb: Finding the posterior distribution with `brms::brm()`\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-m4-1b_c36a571be8954a4a21fa0162b7d4a9fb'}\n\n```{.r .cell-code}\n## R code 4.26b ######################\ndata(package = \"rethinking\", list = \"Howell1\")\nd_b <- Howell1\nd2_b <- \n  d_b |> \n  dplyr::filter(age >= 18) \n\n\n## R code 4.27b ######################\n## R code 4.28b ######################\nm4.1b <- \n  brms::brm(\n      formula = height ~ 1,                                           # 1\n      data = d2_b,                                                    # 2\n      family = gaussian(),                                            # 3\n      prior = c(brms::prior(normal(178, 20), class = Intercept),      # 4\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)), # 4\n      iter = 2000,               # 5\n      warmup = 1000,             # 6\n      chains = 4,                # 7\n      cores = 4,                 # 8 \n      seed = 4,                  # 9\n      file = \"brm_fits/m04.01b\") # 10\nm4.1b\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ 1 \n#>    Data: d2_b (Number of observations: 352) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   154.60      0.41   153.81   155.42 1.00     2763     2635\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     7.77      0.29     7.21     8.39 1.00     3400     2561\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\nThe `brms::brm()` function returns a \"brmsfit\" object.\n\nIf you don't want to specify the result more in detail (for instance to\nchange the PI) than you get the same result with\n`brms:::print.brmsfit(m4.1b)` and `brms:::summary.brmsfit(m4.1b)`.\n\n::: my-resource\n::: my-resource-header\nDescription of the convergence diagnostics `Rhat`, `Bulk_ESS`, and\n`Tail_ESS`\n:::\n\n::: my-resource-container\n-   The convergence diagnostics `Rhat`, `Bulk_ESS`, and `Tail_ESS` are\n    described in detail in [@vehtari2021].\n-   The code for the paper is available on Github\n    (https://github.com/avehtari/rhat_ess).\n-   Examples and even a larger variety of numerical experiments are\n    available in the online appendix at\n    https://avehtari.github.io/rhat_ess/rhat_ess.html.\n:::\n:::\n:::\n:::\n\n`brms::brm()` has more than 40 arguments but only three (formula, data\nand prior) are for our case mandatory. All the other have sensible\ndefault values. The correspondence of these three arguments to the\n{**rethinking**} version is obvious.\n\nIn the simple demonstration of `brms::brm()` in the toy globe example\n(@cnj-chap02-brms-globe-tossing), I have just used Kurz' code lines\nwithout any explanation. This time I will explain all 10 arguments from\nKurz' example.\n\n------------------------------------------------------------------------\n\n1.  **formula** describes the relation between dependent and independent\n    variables in the form of a linear model. The left hand side are the\n    dependent variables, the right hand side the independent. The\n    independent variables are used to calculate the trend component of\n    the linear model, the residuals are then assumed to have some kind\n    of distribution. When the independent are equal to one `~ 1`, the\n    trend component is a single value, e.g. the mean value of the data,\n    i.e. the linear model only has an intercept.\n    ([StackOverflow](https://stackoverflow.com/a/13366973/7322615)) In\n    other words, it is the value the dependent variable is expected to\n    have when the independent variables are zero or have no influence.\n    ([StackOverflow](https://stackoverflow.com/a/13367260/7322615)). The\n    formula `y ~ 1` is just a model with a constant (intercept) and no\n    regressor\n    ([StackOverflow](https://stackoverflow.com/questions/53812741/tilde-operator-in-r)).\n    Or more understandable for our case are Kurz' explication: \"... the\n    intercept of a typical regression model with no predictors is the\n    same as its mean. In the special case of a model using the binomial\n    likelihood, the mean is the probability of a 1 in a given trial,\n    $\\theta$.\"\n    ([Kurz](https://bookdown.org/content/4857/small-worlds-and-large-worlds.html))\n    .\n2.  **data**: A data frame that contains all the variables used in the\n    model.\n3.  **family**: A description of the response distribution and link\n    function to be used in the model. This can be a family function, a\n    call to a family function or a character string naming the family.\n    By default a linear `gaussian` model is applied. So this line would\n    not have been necessary. There are [standard family functions\n    `stats::family()`](https://www.stat.ethz.ch/R-manual/R-devel/library/stats/html/family.html)that\n    will work with {**brms**}, but there are also [special family\n    functions\n    `brms::brmsfamily()`](https://paul-buerkner.github.io/brms/reference/brmsfamily.html)\n    that work only for {**brms**} models. Additionally you can [specify\n    custom\n    families](https://paul-buerkner.github.io/brms/reference/custom_family.html)\n    for use in brms with the `brms::custom_family()` function.\n4.  **prior**: The next two lines specify priors for the normal and the\n    uniform distribution. As you can see this is another place where\n    parts for the formula are provided for the `brms::brm()` function\n    work. --- `class` specifies the parameter class. It defaults to \"b\"\n    ((i.e. population-level -- 'fixed' -- effects)). (There is also the\n    argument `group` for grouping of factors for group-level effects.\n    Not used in this code example.) --- Besides the \"b\" class there are\n    other classes for the \"Intercept\" and the standard deviation \"Sigma\"\n    on the population level: There is also a \"sd\" class for the standard\n    deviation of group-level effects. Finally there is the special case\n    of `class = \"cor\"` to set the same prior on every correlation\n    matrix. --- `ub = 50` sets the upper bound to 50. There is also a\n    `lb` (lower bound). Both bounds are for parameter restriction, so\n    that population-level effects must fall within a certain interval\n    using the `lb` and `ub` arguments. `lb` and `ub` default to `NULL`,\n    i.e. there is no restriction.\n5.  **iter**: Number of total iterations per chain (including `warmup`;\n    defaults to 2000).\n6.  **warmup**: A positive integer specifying number of warmup\n    iterations. This also specifies the number of iterations used for\n    stepsize adaptation, so warmup draws should not be used for\n    inference. The number of warmup should not be larger than `iter` and\n    the default is $iter/2$.\n7.  **chains**: Number of <a class='glossary' title='A Markov chain or Markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. Informally, this may be thought of as, “What happens next depends only on the state of affairs now.” (Wikipedia) For example, if you made a Markov chain model of a baby’s behavior, you might include “playing,” “eating”, “sleeping,” and “crying” as states, which together with other behaviors could form a ‘state space’: a list of all possible states. In addition, on top of the state space, a Markov chain tells you the probabilitiy of hopping, or “transitioning,” from one state to any other state—e.g., the chance that a baby currently playing will fall asleep in the next five minutes without crying first. (Explained visually)'>Markov chains</a>\n    (defaults to 4).\n8.  **cores**: Number of cores to use when executing the chains in\n    parallel, which defaults to 1 but we recommend setting the\n    `mc.cores` option to be as many processors as the hardware and RAM\n    allow (up to the number of chains).\n9.  **seed**: The seed for random number generation to make results\n    reproducible. Kurz has always used for `set.seed()` in other code\n    chunks the chapter number. If `NA` (the default), Stan will set the\n    seed randomly.\n10. **file**: Either `NULL` or a character string. In the latter case,\n    the fitted model object is saved via `base::saveRDS()` in a file\n    named after the string supplied in file. The `.rds` extension is\n    added automatically. If the file already exists, `brms::brm()` will\n    load and return the saved model object instead of refitting the\n    model. Unless you specify the `file_refit` argument as well, the\n    existing files won't be overwritten, you have to manually remove the\n    file in order to refit and save the model under an existing file\n    name. The file name is stored in the brmsfit object for later usage.\n\n-   **init**: Not used here: Within the `brm()` function, you use the\n    `init` argument for the start values for the sampler. \"If NULL (the\n    default) or\"random\", Stan will randomly generate initial values for\n    parameters in a reasonable range. If 0, all parameters are\n    initialized to zero on the unconstrained space. This option is\n    sometimes useful for certain families, as it happens that default\n    random initial values cause draws to be essentially constant.\n    Generally, setting init = 0 is worth a try, if chains do not\n    initialize or behave well. Alternatively, init can be a list of\n    lists containing the initial values ...\" (Help file)\n\n###### print\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-print-m4-1b}\nb: Print the specified results of the `brmsfit` object\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-print-m4-1b_d9056e5839515e0cfcd43a691886ee51'}\n\n```{.r .cell-code}\n## R code 4.29b print ######################\nbrms:::print.brmsfit(m4.1b, prob = .89)\n\n## brms:::summary.brmsfit(m4.1b, prob = .89) # (same result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ 1 \n#>    Data: d2_b (Number of observations: 352) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   154.60      0.41   153.94   155.25 1.00     2763     2635\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     7.77      0.29     7.33     8.26 1.00     3400     2561\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n::: my-resource\n::: my-resource-header\nDescription of the convergence diagnostics `Rhat`, `Bulk_ESS`, and\n`Tail_ESS`\n:::\n\n::: my-resource-container\n-   The convergence diagnostics `Rhat`, `Bulk_ESS`, and `Tail_ESS` are\n    described in detail in [@vehtari2021].\n-   The code for the paper is available on Github\n    (https://github.com/avehtari/rhat_ess).\n-   Examples and even a larger variety of numerical experiments are\n    available in the online appendix at\n    https://avehtari.github.io/rhat_ess/rhat_ess.html.\n:::\n:::\n\n`brms:::summary.brmsfit()` results in the same output\n`as brms:::print.brmsfit()`. Both return a `brmssummary` object. But\nthere are some internal differences:\n\n> There is also a\n> ⁠$print()⁠ method that prints the same summary stats but removes the extra formatting used for printing tibbles and returns the fitted model object itself. The ⁠$print()⁠\n> method may also be faster than\n> ⁠$summary()⁠ because it is designed to only compute the summary statistics for the variables that will actually fit in the printed output whereas ⁠$summary()⁠\n> will compute them for all of the specified variables in order to be\n> able to return them to the user.\n\nUsing `print.brmsfit()` or `summary.brmsfit()` defaults to 95%\nintervals. As {**rethinking**} defaults to 89% intervals, I have changed\nthe `prob` parameter of the print method also to 89%.\n\n::: my-watch-out\n::: my-watch-out-header\nWATCH OUT! Using three colons to address generic functions of S3 classes\n:::\n\n::: my-watch-out-container\nAs I have learned shortly: `print()` or `summary()` are generic\nfunctions where one can add new printing methods with new classes. In\nthis case `class(m4.1b)` = brmsfit. This means I do not need to\nadd `brms::` to secure that I will get the {**brms**} printing or\nsummary method as I didn't load the {**brms**} package. Quite the\ncontrary: Adding `brms::` would result into the message: \"Error:\n'summary' is not an exported object from 'namespace:brms'\".\n\nAs I really want to specify explicitly the method these generic\nfunctions should use, I need to use the syntax with *three* colons, like\n`brms:::print.brmsfit()` or `brms:::summary.brmsfit()` respectively.\n\n::: my-resource\n::: my-resource-header\nLearning more about S3 classes in R\n:::\n\n::: my-resource-container\nIn this respect I have to learn more about S3 classes. There are many\nimportant web resources about this subject that I have found with the\nsearch string \"r what is s3 class\". Maybe I should start with the [S3\nchapter in Advanced R](https://adv-r.hadley.nz/s3.html).\n:::\n:::\n:::\n:::\n:::\n:::\n\nFor the interpretation of this output I am going to use the explication\nin the [How to use\nbrms](https://github.com/paul-buerkner/brms#how-to-use-brms) section of\nthe {**brms**} GitHup page.\n\n1.  **Top**: On the top of the output, some general information on the\n    model is given, such as family, formula, number of iterations and\n    chains.\n2.  **Upper Middle**: If the data were grouped the next part would\n    display group-level effects separately for each grouping factor in\n    terms of standard deviations and (in case of more than one\n    group-level effect per grouping factor) correlations between\n    group-level effects. (This part is absent above as there are no\n    grouping factors.)\n3.  **Lower Middle: here Middle**: Next follow the display of the\n    population-level effects (i.e. regression coefficients). If\n    incorporated, autocorrelation effects and family specific parameters\n    (e.g., the residual standard deviation 'sigma' in normal models) are\n    also given. In general, every parameter is summarized using the mean\n    (`Estimate`) and the standard deviation (`Est.Error`) of the\n    posterior distribution as well as two-sided 95% credible intervals\n    (`l-95% CI` and `u-95% CI`) based on quantiles. The last three\n    values (`ESS_bulk`, `ESS_tail`, and `Rhat`) provide information on\n    how well the algorithm could estimate the posterior distribution of\n    this parameter. If `Rhat` is considerably greater than 1, the\n    algorithm has not yet converged and it is necessary to run more\n    iterations and / or set stronger priors.\n4.  **Bottom**: The last part is some short explanation of the sampling\n    procedure. <a class='glossary' title='The abbreviation “NUTS” stands for No U-Turn Sampler and is a Hamiltonian Monte Carlo (HMC) Method. This means that it is not a Markov Chain method and thus, this algorithm avoids the random walk part, which is often deemed as inefficient and slow to converge. Instead of doing the random walk, NUTS does jumps of length x. Each jump doubles as the algorithm continues to run. This happens until the trajectory reaches a point where it wants to return to the starting point. (CrossValidated) (Chap.4 in my notes)'>NUTS</a> stands for **No U-Turn Sampler** and\n    is a Hamiltonian Monte Carlo (<a class='glossary' title='Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) method that uses the derivatives of the density function being sampled to generate efficient transitions spanning the posterior. It uses an approximate Hamiltonian dynamics simulation based on numerical integration which is then corrected by performing a Metropolis acceptance step. (Stan Reference Manual)'>HMC</a> Method. This means\n    that it is not a <a class='glossary' title='A Markov chain or Markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. Informally, this may be thought of as, “What happens next depends only on the state of affairs now.” (Wikipedia) For example, if you made a Markov chain model of a baby’s behavior, you might include “playing,” “eating”, “sleeping,” and “crying” as states, which together with other behaviors could form a ‘state space’: a list of all possible states. In addition, on top of the state space, a Markov chain tells you the probabilitiy of hopping, or “transitioning,” from one state to any other state—e.g., the chance that a baby currently playing will fall asleep in the next five minutes without crying first. (Explained visually)'>Markov Chain</a> method and thus, this\n    algorithm avoids the random walk part, which is often deemed as\n    inefficient and slow to converge. Instead of doing the random walk,\n    NUTS does jumps of length x. Each jump doubles as the algorithm\n    continues to run. This happens until the trajectory reaches a point\n    where it wants to return to the starting point.\n\n###### fit\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-fit-m4-1b}\nb: Print a Stan like summary\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-fit-m4-1b_1dce2535f6d521e498bcedb59d3eec73'}\n\n```{.r .cell-code}\n## R code 4.29b stan like summary ######################\nm4.1b$fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Inference for Stan model: anon_model.\n#> 4 chains, each with iter=2000; warmup=1000; thin=1; \n#> post-warmup draws per chain=1000, total post-warmup draws=4000.\n#> \n#>                 mean se_mean   sd     2.5%      25%      50%      75%    97.5%\n#> b_Intercept   154.60    0.01 0.41   153.81   154.31   154.59   154.88   155.42\n#> sigma           7.77    0.01 0.29     7.21     7.57     7.76     7.97     8.39\n#> lprior         -8.51    0.00 0.02    -8.56    -8.53    -8.51    -8.50    -8.46\n#> lp__        -1227.04    0.02 0.97 -1229.63 -1227.44 -1226.75 -1226.33 -1226.06\n#>             n_eff Rhat\n#> b_Intercept  2778    1\n#> sigma        3404    1\n#> lprior       2773    1\n#> lp__         1798    1\n#> \n#> Samples were drawn using NUTS(diag_e) at Mon Dec  4 22:27:38 2023.\n#> For each parameter, n_eff is a crude measure of effective sample size,\n#> and Rhat is the potential scale reduction factor on split chains (at \n#> convergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\nKurz refers to [RStan: the R interface to\nStan](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html)\nfor a detailed description. I didn't study the extensive documentation\nbut I found out two items:\n\n-   `lp__` is the logarithm of the (unnormalized) posterior density as\n    calculated by Stan. This log density can be used in various ways for\n    model evaluation and comparison.\n-   `Rhat` estimates the degree of convergence of a random Markov Chain\n    based on the stability of outcomes between and within chains of the\n    same length. Values close to one indicate convergence to the\n    underlying distribution. Values greater than 1.1 indicate inadequate\n    convergence.\n:::\n:::\n\n###### plot\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-plot-m4-1b}\nb: Plot the visual chain diagnostics of the `brmsfit` object\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-plot-m4-1b_2bff02d300d9ac9a98b09b91313dbc3f'}\n\n```{.r .cell-code}\n## R code 4.29b plot ######################\nbrms:::plot.brmsfit(m4.1b)\n```\n\n::: {.cell-output-display}\n![Plot model m4.1b](04-geocentric-models_files/figure-html/fig-chap04-plot-m4-1b-1.png){#fig-chap04-plot-m4-1b width=672}\n:::\n:::\n\n\n> After running a model fit with <a class='glossary' title='Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) method that uses the derivatives of the density function being sampled to generate efficient transitions spanning the posterior. It uses an approximate Hamiltonian dynamics simulation based on numerical integration which is then corrected by performing a Metropolis acceptance step. (Stan Reference Manual)'>HMC</a>, it's a good idea\n> to inspect the chains. As we'll see, McElreath covered visual chain\n> diagnostics in @sec-chap09. ... If you want detailed diagnostics for\n> the HMC chains, call `launch_shinystan(m4.1b)`.\n> ([Kurz](https://bookdown.org/content/4857/geocentric-models.html#finding-the-posterior-distribution-with-quap-brm.))\n\nJust a quick preview: It is important over the whole length of the\nsamples (not counting the warmups, e.g., in our case: 1000 iteration)\nthe width is constant and that the graph show all (four) chains at the\ntop resp. bottom. Unfortunately is looking at the (small) graph not\nalways conclusive. (See also tab \"trace plot\" in\n@exm-find-post-dist-m4-3.)\n\n::: my-resource\n::: my-resource-header\nLearn more about {**shinystan**}\n:::\n\n::: my-resource-container\nI haven't applied `launch_shinystan(m4.1b)` as it takes much time and I\ndo not (yet) understand the detailed report anyway. To learn to work\nwith {**shinystan**} see the [ShinyStan\nwebsite](https://mc-stan.org/users/interfaces/shinystan) and the\nvignettes of the R package vignettes ([Deploying to\nshinyapps.io](https://rdrr.io/cran/shinystan/f/vignettes/deploy_shinystan.Rmd),\n[Getting\nStarted](https://rdrr.io/cran/shinystan/f/vignettes/shinystan-package.Rmd))\nand documentation.\n:::\n:::\n:::\n:::\n:::\n:::\n:::\n\n::: my-resource\n::: my-resource-header\nPackage documentation of Stan and friends\n:::\n\n::: my-resource-container\n-   [Interface to\n    shinystan](https://paul-buerkner.github.io/brms/reference/launch_shinystan.brmsfit.html)\n    (`brms::launch_shinystan`)\n-   I believe that it is also very important to understand [RStan: the R\n    interface to\n    Stan](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html).\n-   Possibly I should read [Using the ShinyStan GUI with rstanarm\n    models](https://mc-stan.org/rstanarm/reference/launch_shinystan.stanreg.html)\n-   And maybe it could be also helpful to read selected chapters from\n    the [rstanarm\n    documentation](https://mc-stan.org/rstanarm/index.html), from the\n    [{**rstan**} documentation](https://mc-stan.org/rstan/) or generally\n    from [Stan User's\n    Guide](https://mc-stan.org/docs/stan-users-guide/index.html) resp.\n    \\[Stan Language Reference\n    Manual\\](https://mc-stan.org/docs/reference-manual/index.html.\n\nOoops, this opens up Pandora's box!\n\nI do not even understand completely what the different packages do. What\nfollows is a first try where I copied from the documentation pages:\n\n> {**rstanarm**} is an R package that emulates other R model-fitting\n> functions but uses Stan (via the {**rstan**} package) for the back-end\n> estimation. The primary target audience is people who would be open to\n> Bayesian inference if using Bayesian software were easier but would\n> use frequentist software otherwise.\n\n> RStan is the R interface to Stan. It is distributed on CRAN as the\n> {**rstan**} package and its source code is hosted on GitHub.\n\n> Stan is a state-of-the-art platform for statistical modeling and\n> high-performance statistical computation.\n:::\n:::\n\nThe next example shows the effect a very narrow $\\mu$ prior has. Instead\nof a sigma of 20 we provide only a standard deviation of 0.1.\n\n::: my-example\n::: my-example-header\n::: {#exm-chap04-narrow-mu-prior-m4-2}\n: The same model but with a more informative, e.g., very narrow $\\mu$\nprior\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### Original\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-narrow-mu-prior-m4.2a}\na: Model with a very narrow $\\mu$ prior (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-narrow-mu-prior-m4.2a_a9296c7999cc6d0731f51b226999a576'}\n\n```{.r .cell-code}\n## R code 4.27a ######################\nflist <- alist(\n  height ~ dnorm(mu, sigma),\n  mu ~ dnorm(178, .1),\n  sigma ~ dunif(0, 50)\n)\n\n## R code 4.30a #####################\nstart <- list(\n  mu = mean(d2_a$height),\n  sigma = sd(d2_a$height)\n)\n\n## R code 4.28a ######################\nm4.2a <- rethinking::quap(flist, data = d2_a, start = start)\nrethinking::precis(m4.2a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>            mean        sd      5.5%    94.5%\n#> mu    177.86382 0.1002354 177.70363 178.0240\n#> sigma  24.51934 0.9290875  23.03448  26.0042\n```\n\n\n:::\n:::\n\n\n> \"Notice that the estimate for μ has hardly moved off the prior. The\n> prior was very concentrated around 178. So this is not surprising. But\n> also notice that the estimate for σ has changed quite a lot, even\n> though we didn't change its prior at all. Once the golem is certain\n> that the mean is near 178---as the prior insists---then the golem has\n> to estimate σ conditional on that fact. This results in a different\n> posterior for σ, even though all we changed is prior information about\n> the other parameter.\" ([McElreath, 2020, p.\n> 89](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=108&annotation=KW4QU3CM))\n:::\n:::\n\n###### brms\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-narrow-mu-prior-m4.2b}\nb: Model with a very narrow $\\mu$ prior (brms)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-narrow-mu-prior-m4.2b_35b5dd93596f7b4e79a16b9d01c8567c'}\n\n```{.r .cell-code}\nm4.2b <- \n  brms::brm(\n      formula = height ~ 1,                               \n      data = d2_b,                                                   \n      family = gaussian(),      \n      prior = c(prior(normal(178, 0.1), class = Intercept),\n                prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"brm_fits/m04.02b\")\n\nbase::rbind(\n      brms:::summary.brmsfit(m4.1b, prob = .89)$fixed,\n      brms:::summary.brmsfit(m4.2b, prob = .89)$fixed\n      )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>            Estimate Est.Error l-89% CI u-89% CI     Rhat Bulk_ESS Tail_ESS\n#> Intercept  154.5959 0.4144632 153.9365 155.2540 1.000708 2762.982 2635.159\n#> Intercept1 177.8647 0.1052528 177.6973 178.0278 1.001224 2720.796 2586.491\n```\n\n\n:::\n:::\n\n\nSubsetting the `summary.brmsfit()` output of the `brmssummary` object\nwith `$fixed` provides a convenient way to compare the Intercept\nsummaries between `m4.1b` and `m4.2b`.\n:::\n:::\n:::\n:::\n:::\n\n### Sampling\n\n#### Sampling from `rethinking::quap()`\n\nHow do we get samples from the quadratic approximate posterior\ndistribution?\n\n> \"... a quadratic approximation to a posterior distribution with more\n> than one parameter dimension---μ and σ each contribute one\n> dimension---is just a multi-dimensional Gaussian distribution.\"\n> ([McElreath, 2020, p.\n> 90](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=109&annotation=B666RAFR))\n\n> \"As a consequence, when R constructs a quadratic approximation, it\n> calculates not only standard deviations for all parameters, but also\n> the covariances among all pairs of parameters. Just like a mean and\n> standard deviation (or its square, a variance) are sufficient to\n> describe a one-dimensional Gaussian distribution, a list of means and\n> a matrix of variances and covariances are sufficient to describe a\n> multi-dimensional Gaussian distribution.\" ([McElreath, 2020, p.\n> 90](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=109&annotation=XN54B26K))\n\n::: my-watch-out\n::: my-watch-out-header\nWATCH OUT! Rethinking and Tidyverse (brms) Codes in separate examples\n:::\n\n::: my-watch-out-container\nAs there are quite big differences in the calculation of the\n<a class='glossary' title='Variance is the squared deviation from the mean of a random variable. The variance is also often defined as the square of the standard deviation. Variance is a measure of dispersion, meaning it is a measure of how far a set of numbers is spread out from their average value. It is the second central moment of a distribution, and the covariance of the random variable with itself, and it is often represented by σ, σ^2, VAR(x), var(x) or V(x). (Wikipedia)'>variance</a>-<a class='glossary' title='Covariance is a measure of how much two random variables vary together. It’s similar to variance, but where variance tells you how a single variable varies, co variance tells you how two variables vary together. (Statistics How To(https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/covariance/))'>covariance</a>\nmatrix, I will explain the appropriate steps in different examples.\n:::\n:::\n\n::: my-example\n::: my-example-header\n::: {#exm-chap04-sampling-quap-m4-1a}\na: Sampling from a `quap()` (Original)\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### vcov\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-vcov-a}\na: Variance-covariance matrix (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-vcov-m4-1a_2b25224de48f3c3011389ddb9f114f00'}\n\n```{.r .cell-code}\n## R code 4.32a vcov #########\nrethinking::vcov(m4.1a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                 mu        sigma\n#> mu    0.1697396109 0.0002180307\n#> sigma 0.0002180307 0.0849058224\n```\n\n\n:::\n:::\n\n\n> \"The above is a variance-covariance matrix. It is the\n> multi-dimensional glue of a quadratic approximation, because it tells\n> us how each parameter relates to every other parameter in the\n> posterior distribution. A variance-covariance matrix can be factored\n> into two elements: (1) a vector of variances for the parameters and\n> (2) a correlation matrix that tells us how changes in any parameter\n> lead to correlated changes in the others. This decomposition is\n> usually easier to understand.\" ([McElreath, 2020, p.\n> 90](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=109&annotation=69ZF8LKP))\n\n`vcov()` returns the variance-covariance matrix of the main parameters\nof a fitted model object. In the above {**rethinking**} version is uses\nthe class `map2stan` for a fitted Stan model as `m4.1a` is of class\n`map`.\n:::\n:::\n\n::: my-watch-out\n::: my-watch-out-header\nWATCH OUT! Two different `vcov()` functions\n:::\n\n::: my-watch-out-container\nI am explicitly using the package {**rethinking**} for the `vcov()`\nfunction. The same function is also available as a base R function with\n`stats::vcov()`. But this generates an error because there is no method\nknown for an object of class `map` from the {**rethinking**} package.\nThe help file for `stats::vcov()` only says that the `vcov` object is an\nS3 method for classes `lm`, `glm`, `mlm` and `aov` but not for `map`.\n\n> Error in UseMethod(\"vcov\") : no applicable method for 'vcov' applied\n> to an object of class \"map\"\n\nI could have used only `vcov()`. But this only works when the\n{**rethinking**} package is already loaded. In that case R knows because\nof the class of the object which `vcov()` version to use. In this case:\nclass of object = `class(m4.1a)` map.\n:::\n:::\n\n###### var\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-var-a}\na: List of variances (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-var-m4-1a_44d781697c05635da4ae007af10a9878'}\n\n```{.r .cell-code}\n## R code 4.33a var adapted ##########\n(var_list <- base::diag(rethinking::vcov(m4.1a)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>         mu      sigma \n#> 0.16973961 0.08490582\n```\n\n\n:::\n:::\n\n\n> \"The two-element vector in the output is the list of variances. If you\n> take the square root of this vector, you get the standard deviations\n> that are shown in `rethinking::precis()` (@exm-chap04-m4-1) output.\"\n> ([McElreath, 2020, p.\n> 90](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=109&annotation=TAVMWSMM))\n\nLet's check this out:\n\n| Result / Parameter             | mu sd, sigma sd                  |\n|--------------------------------|----------------------------------|\n| `sqrt(base::unname(var_list))` | 0.4119947, 0.291386 |\n| `precis_m4.1a[[\"sd\"]]`         | 0.4119947, 0.291386         |\n\n: Convert list of variances to standard deviations and compare with the\nprecis result\n:::\n:::\n\n###### cor to cov\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-cor-a}\na: Correlation matrix (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-cor-m4-1a_29a0d20bc041a804f42801fda4d84238'}\n\n```{.r .cell-code}\n## R code 4.33a cor #############\nstats::cov2cor(rethinking::vcov(m4.1a))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                mu       sigma\n#> mu    1.000000000 0.001816174\n#> sigma 0.001816174 1.000000000\n```\n\n\n:::\n:::\n\n\n> \"The two-by-two matrix in the output is the correlation matrix. Each\n> entry shows the correlation, bounded between −1 and +1, for each pair\n> of parameters. The 1's indicate a parameter's correlation with itself.\n> If these values were anything except 1, we would be worried. The other\n> entries are typically closer to zero, and they are very close to zero\n> in this example. This indicates that learning μ tells us nothing about\n> σ and likewise that learning σ tells us nothing about μ. This is\n> typical of simple Gaussian models of this kind. But it is quite rare\n> more generally, as you'll see in later chapters.\" ([McElreath, 2020,\n> p. 90](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=109&annotation=Y9WJAQYQ))\n:::\n:::\n\n###### cor to cov\n\n::: my-r-code\n::: my-r-code-header\n<div>\n\na: Compute covariance matrix from the correlation using `base::sweep()`\n(Original)\n\n</div>\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-cov-to-cor-m4-1a_29f1cab7270595f8226496c1a6524772'}\n\n```{.r .cell-code}\nR <- stats::cov2cor(rethinking::vcov(m4.1a))\nS <- sqrt(base::diag(rethinking::vcov(m4.1a)))\n\nsweep(sweep(R, 1, S, \"*\"), 2, S, \"*\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                 mu        sigma\n#> mu    0.1697396109 0.0002180307\n#> sigma 0.0002180307 0.0849058224\n```\n\n\n:::\n:::\n\n\nI wonder how to compute the correlation matrix by hand form the\ncovariance-variance matrix. I thought that I have to use `sqrt()`, but\nit didn't work. After I inspected the code of the `cov2cor()` function I\nnoticed that it uses the expression `sqrt(1/diag(V))`.\n\nFrom the `stats::cor()` help file:\n\n> Scaling a covariance matrix into a correlation one can be achieved in\n> many ways, mathematically most appealing by multiplication with a\n> diagonal matrix from left and right, or more efficiently by using\n> `base::sweep(.., FUN = \"/\")` twice. The `stats::cov2cor()` function is\n> even a bit more efficient, and provided mostly for didactical reasons.\n\nFor computing the covariance matrix with `base::sweep()` see the answer\nin [StackOverflow](https://stats.stackexchange.com/a/407954/207389).\n:::\n:::\n\n###### Samples1\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-samples1-m4-1a}\na: Samples from the multi-dimensional posterior (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-samples-m4.1a_a37355dfd22341edb0bdcac2b113148b'}\n\n```{.r .cell-code}\n## R code 4.34a ########################\nset.seed(4)\npost3_a <- rethinking::extract.samples(m4.1a, n = 1e4)\n\nbayr::as_tbl_obs(post3_a)\n```\n\n::: {.cell-output-display}\n\n\nTable: Data set with 3 variables, showing 8 of 10000 observations.\n\n|  Obs|       mu|    sigma|\n|----:|--------:|--------:|\n| 1349| 153.8802| 7.432261|\n| 2281| 154.8268| 7.526618|\n| 3240| 154.1099| 7.540981|\n| 3418| 154.1668| 7.634100|\n| 4467| 155.0349| 7.321992|\n| 6128| 154.0472| 7.421992|\n| 7317| 154.7729| 8.097497|\n| 7892| 154.1290| 7.631920|\n\n\n\n:::\n:::\n\n\n> \"You end up with a data frame, post, with 10,000 (1e4) rows and two\n> columns, one column for μ and one for σ. Each value is a sample from\n> the posterior, so the mean and standard deviation of each column will\n> be very close to the <a class='glossary' title='In Bayesian statistics a Maximum A Posteriori probability or MAP is essentially the mode of posterior distribution. (CDS, p.272)'>MAP</a> values from before.\"\n> ([McElreath, 2020, p.\n> 91](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=110&annotation=4ZE9H4NT))\n:::\n:::\n\n###### precis\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-precis2-m4.1a}\na: Summary from the samples of the multi-dimensional posterior\n(Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-precis2-m4-1a_bc9a256b32366b3eb3a06954f05f7976'}\n\n```{.r .cell-code}\n## R code 4.35a precis ##################\nrethinking::precis(post3_a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>            mean        sd       5.5%      94.5%     histogram\n#> mu    154.61196 0.4106276 153.960323 155.272265       ▁▁▅▇▂▁▁\n#> sigma   7.73298 0.2936393   7.267091   8.199897 ▁▁▁▁▂▅▇▇▃▁▁▁▁\n```\n\n\n:::\n:::\n\n\nCompare these values to the output from the summaries with\n`rethinking::precis()` in @exm-chap04-m4-1.\n:::\n:::\n\n###### plot\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-precis-m4.1a}\na: Plot the samples distribution from the multi-dimensional posterior\n(Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-plot-m4-1a_121edd1142aeb568376ea705de99a750'}\n\n```{.r .cell-code}\n## plot sample posterior a  ##################\nplot(post3_a, col = rethinking::col.alpha(rethinking:::rangi2, 0.1))\n```\n\n::: {.cell-output-display}\n![Samples distribution from the multi-dimensional posterior](04-geocentric-models_files/figure-html/fig-chap04-plot-m4-1a-1.png){#fig-chap04-plot-m4-1a width=672}\n:::\n:::\n\n\n> \"see how much they resemble the samples from the grid approximation in\n> @fig-chap04-posterior-sample-heights-a. These samples also preserve\n> the covariance between $\\mu$ and $\\sigma$. This hardly matters right\n> now, because $\\mu$ and $\\sigma$ don't covary at all in this model. But\n> once you add a predictor variable to your model, covariance will\n> matter a lot.\" ([McElreath, 2020, p.\n> 91](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=110&annotation=2B983ITL))\n:::\n:::\n\n###### Samples2\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-sample2-m4-1a}\na: Extract samples from the vectors of values from a multi-dimensional\nGaussian distribution with `MASS::mvrnorm()` and plot the result\n(Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-plot2-m4.1a_4c8a0e0cbdae4a36b3d91c3abba2292f'}\n\n```{.r .cell-code}\n## R code 4.36a ######################\npost4_a <- MASS::mvrnorm(n = 1e4, mu = rethinking::coef(m4.1a), \n                      Sigma = rethinking::vcov(m4.1a))\n\nplot(post4_a, col = rethinking::col.alpha(rethinking:::rangi2, 0.1))\n```\n\n::: {.cell-output-display}\n![Samples distribution from a multi-dimensional Gaussian distribution with `MASS::mvrnorm()`](04-geocentric-models_files/figure-html/fig-chap04-plot2-m4.1a-1.png){#fig-chap04-plot2-m4.1a width=672}\n:::\n:::\n\n\nThe function `rethinking::extract.samples()` in the \"Sample1\" tab is for\nconvenience. It is just running a simple simulation of the sort you\nconducted near the end of @sec-chap03 with @cnj-fig-post-pred-sim-a.\n\nUnder the hood the work of `rethinking::extract.samples()` is done by a\nmulti-dimensional version of `stats::rnorm()`, `MASS::mvrnorm()`. The\nfunction `stats::rnorm()` simulates random Gaussian values, while\n`MASS::mvrnorm()` simulates random vectors of multivariate Gaussian\nvalues.\n:::\n:::\n:::\n:::\n:::\n\n::: my-note\n::: my-note-header\nHow to interpret covariances?\n:::\n\n::: my-note-container\n> A large covariance can mean a strong relationship between variables.\n> However, you can't compare variances over data sets with different\n> scales (like pounds and inches). A weak covariance in one data set may\n> be a strong one in a different data set with different scales.\n\n> The main problem with interpretation is that the wide range of results\n> that it takes on makes it hard to interpret. For example, your data\n> set could return a value of 3, or 3,000. This wide range of values is\n> caused by a simple fact: *The larger the X and Y values, the larger\n> the covariance*. A value of 300 tells us that the variables are\n> correlated, but unlike the correlation coefficient, that number\n> doesn't tell us exactly how strong that relationship is. The problem\n> can be fixed by dividing the covariance by the standard deviation to\n> get the correlation coefficient. ([Statistics How\n> To](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/covariance/))\n:::\n:::\n\n::: my-watch-out\n::: my-watch-out-header\nWATCH OUT! Confusing `m4.1a` with `m4.3a`\n:::\n\n::: my-watch-out-container\nFrankly speaking I had troubles to understand why the correlation in\n`m4.1a` is almost 0. It turned out that I had unconsciously in mind a\ncorrelation between height and weight, an issue that is raised later in\nthe chapter with `m4.3a`.\n\nAlthough with `m4.1b`is a multi-dimensional Gaussian distribution in\ndiscussion but only with the height correlation of $\\mu$ and $\\sigma$.\n$\\mu$ of the height distribution does not help you in the estimation of\n$\\sigma$ in this distribution -- and vice-versa.\n:::\n:::\n\n#### Sampling from a `brms::brm()` fit\n\nIn contrast to the {**rethinking**} approach the {**brms**} doesn't seem\nto have the same convenience functions and therefore we have to use\ndifferent workarounds to get the same results. To get equivalent output\nit is the best strategy to put the Hamilton Monte Carlo\n(<a class='glossary' title='Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) method that uses the derivatives of the density function being sampled to generate efficient transitions spanning the posterior. It uses an approximate Hamiltonian dynamics simulation based on numerical integration which is then corrected by performing a Metropolis acceptance step. (Stan Reference Manual)'>HMC</a>) chains in a data frame and then apply the\nappropriate functions.\n\n::: my-example\n::: my-example-header\n::: {#exm-chap04-sampling-brm-m4-1b}\nb: Sampling with `as_draw()` functions from a `brms::brm()` fit\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### vcov1\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-vcov-m4-1b}\nb: Variance-covariance matrix (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-vcov1-m4-1b_90490e344fc8f233eb1474d4572869d9'}\n\n```{.r .cell-code}\nbrms:::vcov.brmsfit(m4.1b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>           Intercept\n#> Intercept 0.1717797\n```\n\n\n:::\n:::\n\n:::\n:::\n\nThe `vcov()` function working with `brmsfit` objects only returns the\nfirst element in the matrix it did for {**rethinking**}. That is, it\nappears `brms::vcov.brmsfit()` only returns the variance/covariance\nmatrix for the single-level `_β_` parameters.\n\nIf we want the same information, we put the HMC chains in a data frame\nwith the `brms::as_draws_df()` function as shown in the next tab\n\"draws\".\n\n###### draws1\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-draws-m4-1b}\nb: Extract the iteration of the Hamilton Monte Carlo (HMC) chains into a\ndata frame (Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-draws-m4-1b_28408f2b07393090341798e0a2defc63'}\n\n```{.r .cell-code}\nset.seed(4)\npost_b <- brms::as_draws_df(m4.1b)\n\nbayr::as_tbl_obs(post_b)\n```\n\n::: {.cell-output-display}\n\n\nTable: Data set with 8 variables, showing 8 of 4000 observations.\n\n|  Obs| b_Intercept|    sigma|    lprior|      lp__| .chain| .iteration| .draw|\n|----:|-----------:|--------:|---------:|---------:|------:|----------:|-----:|\n|   71|    154.9925| 7.868132| -8.488375| -1226.553|      1|         71|    71|\n|  587|    154.3334| 7.626869| -8.526828| -1226.342|      1|        587|   587|\n|  684|    154.8032| 8.290861| -8.499311| -1227.720|      1|        684|   684|\n| 1528|    154.4008| 7.591280| -8.522846| -1226.302|      2|        528|  1528|\n| 1795|    155.7369| 7.777169| -8.446249| -1229.762|      2|        795|  1795|\n| 2038|    154.7231| 7.355423| -8.503959| -1227.029|      3|         38|  2038|\n| 2419|    154.5437| 7.982696| -8.514439| -1226.377|      3|        419|  2419|\n| 2867|    154.8770| 7.750134| -8.495038| -1226.252|      3|        867|  2867|\n\n\n\n:::\n:::\n\n:::\n:::\n\n::: my-note\n::: my-note-header\nFamily of `as_draws()` functions and the {**posterior**} package\n:::\n\n::: my-note-container\nThe functions of the family `as_draws()` transform `brmsfit` objects to\n`draws` objects, a format supported by the {**posterior**} package.\n{**brms**} currently imports the family of `as_draws()`functions from\nthe {**posterior**} package, a tool for working with posterior\ndistributions, i.e. for fitting Bayesian models or working with output\nfrom Bayesian models. (See as an introduction [The posterior R\npackage](https://mc-stan.org/posterior/articles/posterior.html))\n\n------------------------------------------------------------------------\n\nIt's also noteworthy that the `as_draws_df()` is part of a larger class\nof `as_draws()` functions {**brms**} currently imports from the\n{**posterior**} package.\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-class-as-draws}\nb: Class of `as_draws()` functions {**brms**}\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-class-as-draws-b_f4e73bb95682bf6fb2dfd156dd2e73d1'}\n\n```{.r .cell-code}\nclass(post_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] \"draws_df\"   \"draws\"      \"tbl_df\"     \"tbl\"        \"data.frame\"\n```\n\n\n:::\n:::\n\n:::\n:::\n\nBesides of class `tbl_df` and `tbl`, [subclasses of data.frame with\ndifferent\nbehavior](https://tibble.tidyverse.org/reference/tbl_df-class.html) the\n`as_draws_df()` function has created the `draws` class, the parent class\nof all supported [draws\nformats](https://mc-stan.org/posterior/articles/posterior.html#draws-formats).\n:::\n:::\n\n###### vcov2\n\n::: my-r-code\n::: my-r-code-header\n<div>\n\nb: Vector of variances and correlation matrix for `b_Intercept` and\n$\\sigma$ (Original)\n\n</div>\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-vcov2-m4-1b_b6577b1bfa3383f1f495fe2263a25ebb'}\n\n```{.r .cell-code}\ndplyr::select(post_b, b_Intercept:sigma) |>\n  stats::cov() |>\n  base::diag()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> b_Intercept       sigma \n#>  0.17177971  0.08681616\n```\n\n\n:::\n:::\n\n\nThis result is now the equivalent of the `rethinking::vcov()` in panel\n\"vcov\" of @exm-chap04-sampling-quap-m4-1a.\n:::\n:::\n\n###### cov\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-cov-m4-1b}\nb: Covariance matrix (Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-cov-m4.1b_12bf73081ec8c2967a8750246e850efa'}\n\n```{.r .cell-code}\nbrms:::vcov.brmsfit(m4.1b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>           Intercept\n#> Intercept 0.1717797\n```\n\n\n:::\n:::\n\n:::\n:::\n\n###### cor\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-cor-m4-1b}\nb: Corrrelation (Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-cor-m4-1b_10e0c57854c42c14e314c85b96ff73b3'}\n\n```{.r .cell-code}\npost_b |>\n  dplyr::select(b_Intercept, sigma) |>\n  stats::cor()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>              b_Intercept        sigma\n#> b_Intercept  1.000000000 -0.004462902\n#> sigma       -0.004462902  1.000000000\n```\n\n\n:::\n:::\n\n:::\n:::\n\n###### str\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-str-m4-1b}\nb: Variance-covariance matrix (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-str-m4-1b_068096fd4b5c6d2e5a541acc8df181df'}\n\n```{.r .cell-code}\nutils::str(post_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> draws_df [4,000 × 7] (S3: draws_df/draws/tbl_df/tbl/data.frame)\n#>  $ b_Intercept: num [1:4000] 155 155 154 154 155 ...\n#>  $ sigma      : num [1:4000] 7.55 7.02 7.58 7.95 7.63 ...\n#>  $ lprior     : num [1:4000] -8.49 -8.49 -8.52 -8.52 -8.52 ...\n#>  $ lp__       : num [1:4000] -1227 -1230 -1226 -1226 -1226 ...\n#>  $ .chain     : int [1:4000] 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ .iteration : int [1:4000] 1 2 3 4 5 6 7 8 9 10 ...\n#>  $ .draw      : int [1:4000] 1 2 3 4 5 6 7 8 9 10 ...\n```\n\n\n:::\n:::\n\n\n> The `post_b` object is not just a data frame, but also of class\n> `draws_df`, which means it contains three metadata variables ----\n> `.chain`, `.iteration`, and `.draw` --- which are often hidden from\n> view, but are there in the background when needed. As you'll see,\n> we'll make good use of the `.draw` variable in the future. Notice how\n> our post data frame also includes a vector named `lp__`. That's the\n> log posterior. (Kurz,\n> [Sec.4.3.6](https://bookdown.org/content/4857/geocentric-models.html#sampling-from-a-quap-brm-fit.))\n\nFor details, see: - The [Log-Posterior (function and\ngradient)](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html#the-log-posterior-function-and-gradient)\nsection of the Stan Development Team's (2023) vignette [RStan: the R\ninterface to\nStan](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html)\nand - Stephen Martin's [explanation of the log\nposterior](https://discourse.mc-stan.org/t/basic-question-what-is-lp-in-posterior-samples-of-a-brms-regression/17567/2)\non the Stan Forums.\n:::\n:::\n\n###### summary1\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-summary1-m4-1b}\nb: Summarize the extracted iterations of the HMC chains: `base()`\nversion\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-base-summary1-m4.1b_27ad92bd79ae7886385eaec67e2aa542'}\n\n```{.r .cell-code}\nbase::summary(post_b[, 1:2])\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>   b_Intercept        sigma      \n#>  Min.   :153.3   Min.   :6.841  \n#>  1st Qu.:154.3   1st Qu.:7.567  \n#>  Median :154.6   Median :7.757  \n#>  Mean   :154.6   Mean   :7.771  \n#>  3rd Qu.:154.9   3rd Qu.:7.971  \n#>  Max.   :156.2   Max.   :8.864\n```\n\n\n:::\n:::\n\n:::\n:::\n\n###### summary2\n\n::: my-r-code\n::: my-r-code-header\n<div>\n\nb: Summarize the extracted iterations of the HMC chains: `posterior()`\nversion\n\n</div>\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-posterior-summary2-m4.1b_f0f222be6877c1ae748952b21a774f64'}\n\n```{.r .cell-code}\nposterior:::summary.draws(post_b[, 1:2])\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 2 × 10\n#>   variable      mean median    sd   mad     q5    q95  rhat ess_bulk ess_tail\n#>   <chr>        <dbl>  <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n#> 1 b_Intercept 155.   155.   0.414 0.421 154.   155.    1.00    2722.    2624.\n#> 2 sigma         7.77   7.76 0.295 0.301   7.31   8.27  1.00    3369.    2537.\n```\n\n\n:::\n:::\n\n:::\n:::\n\n###### skim\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-skim1-m4-1b}\nb: Summarize with `skimr::skim()`\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-skim-m4.1b_2c5eda8b3fe6e1e4ab51b1b30114a716'}\n\n```{.r .cell-code}\nskimr::skim(post_b[, 1:2])\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n\n:::\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |              |\n|:------------------------|:-------------|\n|Name                     |post_b[, 1:2] |\n|Number of rows           |4000          |\n|Number of columns        |2             |\n|_______________________  |              |\n|Column type frequency:   |              |\n|numeric                  |2             |\n|________________________ |              |\n|Group variables          |None          |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|   sd|     p0|    p25|    p50|    p75|   p100|hist  |\n|:-------------|---------:|-------------:|------:|----:|------:|------:|------:|------:|------:|:-----|\n|b_Intercept   |         0|             1| 154.60| 0.41| 153.29| 154.31| 154.59| 154.88| 156.22|▁▆▇▂▁ |\n|sigma         |         0|             1|   7.77| 0.29|   6.84|   7.57|   7.76|   7.97|   8.86|▁▆▇▂▁ |\n\n\n:::\n:::\n\n:::\n:::\n\n::: my-note\n::: my-note-header\nCreate summaries of samples that include tiny histograms\n:::\n\n::: my-note-container\nKurz didn't mention my suggestion to use skimr::skim() to get tiny\nhistograms as part of the summary but proposes several other methods:\n\n-   A base R approach by using the transpose of a `stats::quantile()`\n    call nested within `base::apply()`\n-   A {**tidyverse**} approach\n-   A {**brms**} approach by just putting the `brm()` fit object into\n    `posterior_summary()`\n-   A {**tidybayes**} approach using `tidybayes::mean_hdi()` if you're\n    willing to drop the posterior `sd` and\n-   Using additionally the [function\n    `histospark()`](https://github.com/hadley/precis/blob/master/R/histospark.R)\n    (from the unfinished {**precis**} package by Hadley Wickham supposed\n    to replace `base::summary()`) to get the tiny histograms and to add\n    them into the tidyverse approach.\n:::\n:::\n:::\n:::\n:::\n\n## Linear prediction {#sec-linear-prediction-a}\n\n> \"What we've done above is a Gaussian model of height in a population\n> of adults. But it doesn't really have the usual feel of \"regression\"\n> to it. Typically, we are interested in modeling how an outcome is\n> related to some other variable, a <a class='glossary' title='Predictor variable – also known sometimes as the independent or explanatory variable – is the counterpart to the response or dependent variable. Predictor variables are used to make predictions for dependent variables. (DeepAI, MiniTab)'>predictor variable</a>.\n> If the predictor variable has any statistical association with the\n> outcome variable, then we can use it to predict // the outcome. When\n> the predictor variable is built inside the model in a particular way,\n> we'll have <a class='glossary' title='Linear regression is used to predict the value of an outcome variable Y based on one or more input predictor variables X. The aim is to establish a linear relationship (a mathematical formula) between the predictor variable(s) and the response variable, so that, we can use this formula to estimate the value of the response Y, when only the predictors (Xs) values are known. (r-statistics.co) (Chap.4)'>linear regression</a>.\" ([McElreath, 2020, p.\n> 91/92](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=111&annotation=UYJ937ER))\n\n::: my-example\n::: my-example-header\n::: {#exm-scatterplot-adult-height-weight}\n: Scatterplot of adult height versus weight\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### Original\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-scatterplot-adult-height-weight-a}\na: Scatterplot of adult height versus weight (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-height-against-weight-a_b3c0221b21c4f1449f53b044874c1d5b'}\n\n```{.r .cell-code}\n## R code 4.37a #####################\nplot(d2_a$height ~ d2_a$weight)\n```\n\n::: {.cell-output-display}\n![Adult height and weight against one another](04-geocentric-models_files/figure-html/fig-height-against-weight-a-1.png){#fig-height-against-weight-a width=672}\n:::\n:::\n\n:::\n:::\n\n###### Tidyverse\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-scatterplot-adult-height-weight-b}\nb: Scatterplot of adult height versus weight (Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-height-against-weight-b_11a7bc20b070e5c1cfd460b1de869397'}\n\n```{.r .cell-code}\n## R code 4.37b #####################\nd2_b |> \n    ggplot2::ggplot(ggplot2::aes(height, weight)) + \n    ggplot2::geom_point() +\n    ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Adult height and weight against one another](04-geocentric-models_files/figure-html/fig-height-against-weight-b-1.png){#fig-height-against-weight-b width=672}\n:::\n:::\n\n:::\n:::\n:::\n\nThere's obviously a relationship: Knowing a person's weight helps to\npredict height.\n:::\n:::\n\n> \"To make this vague observation into a more precise quantitative model\n> that relates values of `weight` to plausible values of `height`, we\n> need some more technology. How do we take our Gaussian model from\n> @sec-gaussian-model-of-height and incorporate predictor variables?\"\n> ([McElreath, 2020, p.\n> 92](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=111&annotation=BEMZLGBA))\n\n### The linear model strategy\n\n#### Model definition\n\n> \"The <a class='glossary' title='A linear model specifies a linear relationship between a dependent variable and n independent variables. It conforms to a mathematical model represented by a linear equation of the form Y = b_{1}X_{1} + b_{2}X_{2} + … + b_{n}X_{n}. (Oxford Reference)'>linear model</a> strategy instructs the golem to\n> assume that the predictor variable has a constant and additive\n> relationship to the mean of the outcome. The golem then computes the\n> posterior distribution of this constant relationship.\" ([McElreath,\n> 2020, p. 92](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=111&annotation=FU3R3CJZ))\n\n> \"For each combination of values, the machine computes the posterior\n> probability, which is a measure of relative plausibility, given the\n> model and data. So the posterior distribution ranks the infinite\n> possible combinations of parameter values by their logical\n> plausibility.\" ([McElreath, 2020, p.\n> 92](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=111&annotation=28Z2C2SY))\n\n::: my-example\n::: my-example-header\n::: {#exm-lm-height-weight}\n: Linear model definition: Height against weight\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### only height\n\n::: my-theorem\n::: my-theorem-header\n<div>\n\n: Define the linear heights model\n\n</div>\n:::\n\n::: my-theorem-container\n$$\n\\begin{align*}\nh_{i} \\sim \\operatorname{Normal}(\\mu, \\sigma) \\space \\space (1) \\\\ \nμ \\sim \\operatorname{Normal}(178, 20)  \\space \\space (2) \\\\ \nμ \\sim \\operatorname{Uniform}(0, 50)   \\space \\space (3)      \n\\end{align*}\n$$ {#eq-height-linear-model2-m4-1}\n\nRemember @eq-height-linear-model-m4-1:\n:::\n:::\n\n1.  **Likelihood**: Represented by the first line.\n2.  **Mean prior**: Second line is the chosen $\\mu$ (mu, mean) prior. It\n    is a broad Gaussian prior, centered on 178 cm, with 95% of\n    probability between 178 ± 40 cm.\n3.  **Standard deviation prior**: Third line is the chosen $\\sigma$\n    (sigma, standard deviation) prior.\n\n###### height against weight\n\n::: my-theorem\n::: my-theorem-header\n<div>\n\n: Define the linear model heights against weights (V1)\n\n</div>\n:::\n\n::: my-theorem-container\n$$\n\\begin{align*}\nh_{i} \\sim \\operatorname{Normal}(μ_{i}, σ) \\space \\space (1) \\\\ \nμ_{i} = \\alpha + \\beta(x_{i}-\\overline{x}) \\space \\space (2) \\\\\n\\alpha \\sim \\operatorname{Normal}(178, 20) \\space \\space (3)  \\\\ \n\\beta \\sim \\operatorname{Normal}(0,10) \\space \\space (4) \\\\\n\\sigma \\sim \\operatorname{Uniform}(0, 50) \\space \\space (5)      \n\\end{align*}\n$$ {#eq-height-weight-linear-model-v1-m4-3}\n\nCompare the differences with definition inside the Tab \"only height\".\n:::\n:::\n\n(1) **Likelihood (Probability of the data)**: The first line is nearly\n    identical to before, except now there is a little index $i$ on the\n    $μ$ as well as on the $h$. You can read $h_{i}$ as \"each height\" and\n    $\\mu_{i}$ as \"each $μ$\" The mean $μ$ now depends upon unique values\n    on each row $i$. So the little $i$ on $\\mu_{i}$ indicates that *the\n    mean depends upon the row*.\n\n(2) **Linear model**: The mean $μ$ is no longer a parameter to be\n    estimated. Rather, as seen in the second line of the model,\n    $\\mu_{i}$ is constructed from other parameters, $\\alpha$ and\n    $\\beta$, and the observed variable $x$. This line is not a\n    stochastic relationship ----- there is no `~` in it, but rather an\n    `=` in it ----- because the definition of $\\mu_{i}$ is\n    deterministic. That is to say that, once we know $\\alpha$ and\n    $\\beta$ and $x_{i}$, we know $\\mu_{i}$ with certainty. (More details\n    follow in @sec-chap04-linear-model.)\n\n(3) **includes (3),(4) and(5) with** $\\alpha, \\beta, \\sigma$ priors: The\n    remaining lines in the model define distributions for the unobserved\n    variables. These variables are commonly known as parameters, and\n    their distributions as priors. There are three parameters:\n    $\\alpha, \\beta, \\sigma$. You've seen priors for $\\alpha$ and\n    $\\sigma$ before, although $\\alpha$ was called $\\mu$ back then. (More\n    details in @sec-chap04-priors)\n:::\n:::\n:::\n\n#### Linear model {#sec-chap04-linear-model}\n\n> \"The value $x_{i}$ [in the second line of\n> @eq-height-weight-linear-model-v1-m4-3] is just the weight value on\n> row $i$. It refers to the same individual as the height value,\n> $h_{i}$, on the same row. The parameters $\\alpha$ and $\\beta$ are more\n> mysterious. Where did they come from? We made them up. The parameters\n> $\\mu$ and $\\sigma$ are necessary and sufficient to describe a Gaussian\n> distribution. But $\\alpha$ and $\\beta$ are instead devices we invent\n> for manipulating $\\mu$, allowing it to vary systematically across\n> cases in the data.\" ([McElreath, 2020, p.\n> 93](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=112&annotation=V3R75T23))\n\nThe second line $μ_{i} = \\alpha + \\beta(x_{i}-\\overline{x})$ of the\nmodel definition in @eq-height-weight-linear-model2-m4-3 tells us\n\n> \"that you are asking two questions about the mean of the outcome:\n\n> 1.  What is the expected height when $x_{i} = \\overline{x}$? The\n>     parameter $\\alpha$ answers this question, because when\n>     $x_{i} = \\overline{x}$, $\\mu_{i} = \\alpha$. For this reason,\n>     $\\alpha$ is often called the <a class='glossary' title='The intercept is the value of the dependent variables if all independent variables have the value zero. (Intercept, Slope in Regression)'>intercept</a>. But we\n>     should think not in terms of some abstract line, but rather in\n>     terms of the meaning with respect to the observable variables.\n> 2.  What is the change in expected height, when $x_{i}$ changes by 1\n>     unit? The parameter $\\beta$ answers this question. It is often\n>     called a <a class='glossary' title='The slope is the increase in the dependent variable when the independent variable increases with one unit and all other independent variables remain the same. (Intercept, Slope in Regression)'>slope</a>, again because of the abstract\n>     line. Better to think of it as a rate of change in expectation.\n>\n> Jointly these two parameters ask the golem to find a line that relates\n> x to h, a line that passes through α when x\\_{i} = \\overline{x} and\n> has slope $\\beta$.\" ([McElreath, 2020, p.\n> 94](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=113&annotation=Y36FAY8E))\n\n#### Priors {#sec-chap04-priors}\n\n> \"The prior for $\\beta$ in @eq-height-weight-linear-model1-m4-3\n> deserves explanation. Why have a Gaussian prior with mean zero? This\n> prior places just as much probability below zero as it does above\n> zero, and when $\\beta = 0$, // weight has no relationship to height.\n> To figure out what this prior implies, we have to simulate the prior\n> predictive distribution.\" ([McElreath, 2020, p.\n> 94/95](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=114&annotation=DD56M4KY))\n\nThe goal in @exm-sim-height-m4-3 is to simulate heights from the model,\nusing only the priors.\n\n::: my-example\n::: my-example-header\n::: {#exm-sim-height-m4-3}\n: Simulating heights from the model, using only the priors\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### Original\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-sim-height-m4-3a}\na: Simulating heights from the model, using only the priors (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-sim-heights-m4-3a_1a1c4ad0c4a02a6d24958e2551802f2c'}\n\n```{.r .cell-code}\n## range of weight values to simulate \n## R code 4.38a #####################\nN_100_a <- 100 # 100 lines\n\n## set seed for exact reproduction\nset.seed(2971)\n\n## simulate lines implied by the priors for alpha and beta\na <- rnorm(N_100_a, 178, 20)\nb <- rnorm(N_100_a, 0, 10)\n\n\n## R code 4.39a #####################\nplot(NULL,\n  xlim = range(d2_a$weight), ylim = c(-100, 400),\n  xlab = \"weight\", ylab = \"height\"\n)\n\n## added reference line for 0 and biggest man ever\nabline(h = 0, lty = 2)\nabline(h = 272, lty = 1, lwd = 0.5)\nmtext(\"b ~ dnorm(0,10)\")\nxbar <- mean(d2_a$weight)\nfor (i in 1:N_100_a) {\n  curve(a[i] + b[i] * (x - xbar),\n    from = min(d2_a$weight), to = max(d2_a$weight), add = TRUE,\n    col = rethinking::col.alpha(\"black\", 0.2)\n  )\n}\n```\n\n::: {.cell-output-display}\n![Simulating heights from the model, using only the priors (Original)](04-geocentric-models_files/figure-html/fig-sim-heights-m4-3a-1.png){#fig-sim-heights-m4-3a width=672}\n:::\n:::\n\n:::\n:::\n\n###### Tidyverse\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-sim-height-m4-3b}\nb: Simulating heights from the model, using only the priors (Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-sim-heights-m4-3b_5aff02eabef5178ccad970c3ab8cfa1d'}\n\n```{.r .cell-code}\nset.seed(2971)\n# how many lines would you like?\nn_lines <- 100\n\nlines <-\n  tibble::tibble(n = 1:n_lines,\n         a = stats::rnorm(n_lines, mean = 178, sd = 20),\n         b = stats::rnorm(n_lines, mean = 0, sd = 10)) |> \n  tidyr::expand_grid(weight = base::range(d2_b$weight)) |> \n  dplyr::mutate(height = a + b * (weight - base::mean(d2_b$weight)))\n\n\nlines |> \n  ggplot2::ggplot(ggplot2::aes(x = weight, y = height, group = n)) +\n  ggplot2::geom_hline(yintercept = c(0, 272), linetype = 2:1, linewidth = 1/3) +\n  ggplot2::geom_line(alpha = 1/10) +\n  ggplot2::coord_cartesian(ylim = c(-100, 400)) +\n  ggplot2::ggtitle(\"b ~ dnorm(0, 10)\") +\n  ggplot2::theme_classic()\n```\n\n::: {.cell-output-display}\n![Simulating heights from the model, using only the priors (Tidyverse)](04-geocentric-models_files/figure-html/fig-sim-heights-m4-3b-1.png){#fig-sim-heights-m4-3b width=672}\n:::\n:::\n\n:::\n:::\n:::\n\nThe dashed line are reference lines. One at zero---no one is shorter\nthan zero---and one at 272 cm for [Robert\nWadlow](https://en.wikipedia.org/wiki/Robert_Wadlow) the world's tallest\nperson.\n\n\"The pattern doesn't look like any human population at all. It\nessentially says that the relationship // between weight and height\ncould be absurdly positive or negative. Before we've even seen the data,\nthis is a bad model.\" ([McElreath, 2020, p.\n94/95](zotero://select/groups/5243560/items/NFUEVASQ))\n([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=114&annotation=VIJ8XZIB))\n:::\n:::\n\n::: my-theorem\n::: my-theorem-header\n::: {#thm-log-normal-m4-3}\n: Defining the $\\beta$ prior as a Log-Normal distribution\n:::\n:::\n\n::: my-theorem-container\n$$\n\\beta \\sim \\operatorname{Log-Normal}(0,1)\n$$ {#eq-prior-log-normal-m4-3}\n\n------------------------------------------------------------------------\n\nDefining $\\beta$ as $Log-Normal(0,1)$ means to claim that the logarithm\nof $\\beta$ has a $Normal(0,1)$ distribution.\" ([McElreath, 2020, p.\n96](zotero://select/groups/5243560/items/NFUEVASQ))\n([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=115&annotation=YQFLM6PR))\n:::\n:::\n\n::: my-example\n::: my-example-header\n<div>\n\n: Log-Normal distribution\n\n</div>\n:::\n\n::: my-example-container\n::: panel-tabset\n###### Original\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-log-normal-m4-3a}\na: Log-Normal distribution (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-log-normal-m4-3a_c6481ce6fcbe4d4f3c447cbff2e336f3'}\n\n```{.r .cell-code}\nset.seed(4) # to reproduce with tidyverse version\n## R code 4.40a ####################\nb <- stats::rlnorm(1e4, 0, 1)\nrethinking::dens(b, xlim = c(0, 5), adj = 0.1)\n```\n\n::: {.cell-output-display}\n![Log-Normal distributiom (Original)](04-geocentric-models_files/figure-html/fig-log-normal-m4-3a-1.png){#fig-log-normal-m4-3a width=672}\n:::\n:::\n\n:::\n:::\n\n###### Tidyverse\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-log-normal-m4-3b}\nb: Log-Normal distribution (Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-log-normal-m4-3b_2d1dd30e793c8a589933b2f44ce210cd'}\n\n```{.r .cell-code}\nset.seed(4)\n\ntibble::tibble(b = stats::rlnorm(1e4, meanlog = 0, sdlog = 1)) |> \n  ggplot2::ggplot(ggplot2::aes(x = b)) +\n  ggplot2::geom_density() +\n  ggplot2::coord_cartesian(xlim = c(0, 5)) +\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Log-Normal distribution (Tidyverse)](04-geocentric-models_files/figure-html/fig-log-normal-m4-3b-1.png){#fig-log-normal-m4-3b width=672}\n:::\n:::\n\n:::\n:::\n\n::: my-watch-out\n::: my-watch-out-header\nWATCH OUT! Argument matching\n:::\n\n::: my-watch-out-container\nKurz wrote just `mean` and `sd` instead of `meanlog` and `sdlog.` These\nshorter argument names work because of the [partial matching feature in\nargument\nevaluation](https://cran.r-project.org/doc/manuals/R-lang.html#Argument-matching)\nof R functions. But for educational reason (misunderstanding, clashing\nwith other matching arguments and less readable code) I apply this\ntechnique only sometimes in interactive use.\n:::\n:::\n\nBase R provides the `dlnorm()` and `rlnorm()` densities for working with\nlog-normal distributions.\n\nUsing the Log-Normal distribution prohibits negative values. This is an\nimportant constraint for height and weight as these variables cannot be\nunder $0$.\n\n> \"The reason is that `exp(x)` is greater than zero for any real number\n> $x$. This is the reason that Log-Normal priors are commonplace. They\n> are an easy way to enforce positive relationships.\" ([McElreath, 2020,\n> p. 96](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=115&annotation=8DAGFAH2))\n\n###### Compare\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-comparenormal-log-normal-m4-3b}\nb: Compare Normal(0,1) with log(Log-Normal(0,1))\"\n:::\n:::\n\n::: my-r-code-container\n> If you're unfamiliar with the log-normal distribution, it is the\n> distribution whose logarithm is normally distributed. For example,\n> here's what happens when we compare Normal(0,1) with\n> log(Log-Normal(0,1)).\n> ([Kurz](https://bookdown.org/content/4857/geocentric-models.html#priors.))\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-compare-normal-log-normal_dd4152e20cbb6c4881fcb50175821f3a'}\n\n```{.r .cell-code}\nset.seed(4)\n\ntibble::tibble(rnorm             = stats::rnorm(1e5, mean = 0, sd = 1),\n       `log(rlognorm)` = base::log(stats::rlnorm(1e5, meanlog = 0, sdlog = 1))) |> \n  tidyr::pivot_longer(tidyr::everything()) |> \n\n  ggplot2::ggplot(ggplot2::aes(x = value)) +\n  ggplot2::geom_density(fill = \"grey92\") +\n  ggplot2::coord_cartesian(xlim = c(-3, 3)) +\n  ggplot2::theme_bw() +\n  ggplot2::facet_wrap(~ name, nrow = 2)\n```\n\n::: {.cell-output-display}\n![Compare Normal(0,1) with log(Log-Normal(0,1))](04-geocentric-models_files/figure-html/fig-compare-normal-log-normal-1.png){#fig-compare-normal-log-normal width=672}\n:::\n:::\n\n\n> Those values are ~~what~~ the mean and standard deviation of the\n> output from the `rlnorm()` function **after** they are log\n> transformed. The formulas for the actual mean and standard deviation\n> for the log-normal distribution itself are complicated (see\n> [Wikipedia](https://en.wikipedia.org/wiki/Log-normal_distribution)).\n> ([Kurz](https://bookdown.org/content/4857/geocentric-models.html#priors.))\n:::\n:::\n\n###### Log-Normal (Original)\n\n::: my-r-code\n::: my-r-code-header\n<div>\n\n: Prior predictive simulation again, now with the Log-Normal prior:\n(Original)\n\n</div>\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-prior-pred-sim-a_5b7f92aec6addda7979b10a8cabf1664'}\n\n```{.r .cell-code}\n## R code 4.41a ###################\nset.seed(2971)\nN_100_a <- 100 # 100 lines\na <- rnorm(N_100_a, 178, 20)\nb <- rlnorm(N_100_a, 0, 1)\n\n## R code 4.39a ###################\nplot(NULL,\n  xlim = range(d2_a$weight), ylim = c(-100, 400),\n  xlab = \"weight\", ylab = \"height\"\n)\nabline(h = 0, lty = 2)\nabline(h = 272, lty = 1, lwd = 0.5)\nmtext(\"b ~ dnorm(0,10)\")\nxbar <- mean(d2_a$weight)\nfor (i in 1:N_100_a) {\n  curve(a[i] + b[i] * (x - xbar),\n    from = min(d2_a$weight), to = max(d2_a$weight), add = TRUE,\n    col = rethinking::col.alpha(\"black\", 0.2)\n  )\n}\n```\n\n::: {.cell-output-display}\n![Prior predictive simulation again, now with the Log-Normal prior: rethinking version](04-geocentric-models_files/figure-html/fig-prior-pred-sim-a-1.png){#fig-prior-pred-sim-a width=672}\n:::\n:::\n\n:::\n:::\n\nThis is much more sensible. There is still a rare impossible\nrelationship. But nearly all lines in the joint prior for $\\alpha$ and\n$\\beta$ are now within human reason.\n\n###### Log-Normal (Tidyverse)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-ID-text}\nb: Prior predictive simulation again, now with the Log-Normal prior\n(Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-prior-pred-sim-b_ec47087b63ad4a50e45c658c38f53fe8'}\n\n```{.r .cell-code}\n# make a tibble to annotate the plot\ntext <-\n  tibble::tibble(weight = c(34, 43),\n         height = c(0 - 25, 272 + 25),\n         label  = c(\"Embryo\", \"World's tallest person (272 cm)\"))\n\n# simulate\nbase::set.seed(2971)\n\ntibble::tibble(n = 1:n_lines,\n       a = stats::rnorm(n_lines, mean = 178, sd = 20),\n       b = stats::rlnorm(n_lines, mean = 0, sd = 1)) |> \n  tidyr::expand_grid(weight = base::range(d2_b$weight)) |> \n  dplyr::mutate(height = a + b * (weight - base::mean(d2_b$weight))) |>\n  \n  # plot\n  ggplot2::ggplot(ggplot2::aes(x = weight, y = height, group = n)) +\n  ggplot2::geom_hline(yintercept = c(0, 272), linetype = 2:1, linewidth = 1/3) +\n  ggplot2::geom_line(alpha = 1/10) +\n  ggplot2::geom_text(data = text,\n            ggplot2::aes(label = label),\n            size = 3) +\n  ggplot2::coord_cartesian(ylim = c(-100, 400)) +\n  ggplot2::ggtitle(\"log(b) ~ dnorm(0, 1)\") +\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Prior predictive simulation again, now with the Log-Normal prior (Tidyverse)](04-geocentric-models_files/figure-html/fig-prior-pred-sim-b-1.png){#fig-prior-pred-sim-b width=672}\n:::\n:::\n\n:::\n:::\n:::\n:::\n:::\n\n::: my-note\n::: my-note-header\nWhat is the correct prior?\n:::\n\n::: my-note-container\n> \"There is no more a uniquely correct prior than there is a uniquely\n> correct likelihood. ...\n>\n> In choosing priors, there are simple guidelines to get you started.\n> Priors encode states of information before seeing data. So priors\n> allow us to explore the consequences of beginning with different\n> information. In cases in which we have good prior information that\n> discounts the plausibility of some parameter values, like negative\n> associations between height and weight, we can encode that information\n> directly into priors. When we don't have such information, we still\n> usually know enough about the plausible range of values. And you can\n> vary the priors and repeat the analysis in order to study // how\n> different states of initial information influence inference.\n> Frequently, there are many reasonable choices for a prior, and all of\n> them produce the same inference.\" ([McElreath, 2020, p.\n> 95/96](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=115&annotation=UJG4Y8MH))\n:::\n:::\n\n::: my-watch-out\n::: my-watch-out-header\nWATCH OUT! Prior predictive simulation and p-hacking\n:::\n\n::: my-watch-out-container\n> \"A serious problem in contemporary applied statistics is\n> <a class='glossary' title='P-hacking is a set of statistical decisions and methodology choices during research that artificially produces statistically significant results. These decisions increase the probability of false positives—where the study indicates an effect exists when it actually does not. P-hacking is also known as data dredging, data fishing, and data snooping. (Statistics by Jim)'>p-hacking</a>, the practice of adjusting the model and the\n> data to achieve a desired result. The desired result is usually a\n> p-value less then 5%. The problem is that when the model is adjusted\n> in light of the observed data, then p-values no longer retain their\n> original meaning. False results are to be expected.\" ([McElreath,\n> 2020, p. 97](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=116&annotation=MKCQIVRV))\n\n::: my-resource\n::: my-resource-header\nPrior predictive simulation and <a class='glossary' title='P-hacking is a set of statistical decisions and methodology choices during research that artificially produces statistically significant results. These decisions increase the probability of false positives—where the study indicates an effect exists when it actually does not. P-hacking is also known as data dredging, data fishing, and data snooping. (Statistics by Jim)'>p-hacking</a>\n:::\n\n::: my-resource-container\nThe paper by Simmons, Nelson and Simonsohn (2011), [False-positive\npsychology: Undisclosed flexibility in data collection and analysis\nallows presenting anything as\nsignificant](https://journals.sagepub.com/doi/10.1177/0956797611417632),\nis often cited as an introduction to the problem.\n\nAnother more recent publication is: MacCoun, R. J. (2022). MacCoun, R.\nJ. (2022). P-hacking: A Strategic Analysis. In L. Jussim, J. A.\nKrosnick, & S. T. Stevens (Eds.), Research Integrity: Best Practices for\nthe Social and Behavioral Sciences (pp. 295--315). Oxford University\nPress. https://doi.org/10.1093/oso/9780190938550.003.0011.\n\nA book wide treatment is: Chambers, C. (2017). Seven Deadly Sins of\nPsychology: A Manifesto for Reforming the Culture of Scientific Practice\n(Illustrated Edition). Princeton University Press.\n:::\n:::\n:::\n:::\n\n### Finding the posterior distribution\n\nIn @exm-lm-height-weight we have compared the linear heights model\n(@eq-height-linear-model2-m4-1) with the linear model heights against\nweights (@eq-height-weight-linear-model-v1-m4-3). Now we repeat the\nlinear model heights against weights and compare it with the\ncorresponding R Code.\n\n::: my-example\n::: my-example-header\n::: {#exm-lm-height-weight-code-m4-3}\n: Compare formula and R code for linear model heights against weights\n(V2)\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### Formula\n\n::: my-theorem\n::: my-theorem-header\n::: {#thm-linear-heights-model-v2-m4-3}\n: Define the linear model heights against weights (V2): Formula\n:::\n:::\n\n::: my-theorem-container\n$$\n\\begin{align*}\nh_{i} \\sim \\operatorname{Normal}(μ_{i}, σ) \\space \\space (1) \\\\ \nμ_{i} = \\alpha + \\beta(x_{i}-\\overline{x}) \\space \\space (2) \\\\\n\\alpha \\sim \\operatorname{Normal}(178, 20) \\space \\space (3)  \\\\ \n\\beta \\sim \\operatorname{Normal}(0,10) \\space \\space (4) \\\\\n\\sigma \\sim \\operatorname{Uniform}(0, 50) \\space \\space (5)      \n\\end{align*}\n$$ {#eq-height-weight-linear-model2-m4-3}\n:::\n:::\n\n###### R Code\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-lm-code-v2-m4-3}\n: Define the linear model heights against weights (V2): R Code\n:::\n:::\n\n::: my-r-code-container\n```         \nheight ~ dnorm(mu, sigma)     # (1)\nmu <- a + b * (weight - xbar) # (2)\na ~ dnorm(178, 20)            # (3)        \nb ~ dlnorm(0, 10)             # (4)        \nsigma ~ dunif(0, 50)          # (5)       \n```\n:::\n:::\n\nNotice that the linear model, in the R code on the right-hand side, uses\nthe R assignment operator, `<-` instead of the symbol `=`.\n:::\n:::\n:::\n\n::: my-example\n::: my-example-header\n::: {#exm-find-post-dist-m4-3}\n: Find the posterior distribution of the linear height-weight model\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### Original1\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-find-post-dist-m4-3-a}\na: Find the posterior distribution of the linear height-weight model\n(Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/find-post-dist-m4-3a_de9548d2d9e25c4dd74a91d0018ea9f8'}\n\n```{.r .cell-code}\n## R code 4.42a #############################\n\n# define the average weight, x-bar\nxbar_a <- mean(d2_a$weight)\n\n# fit model\nm4.3a <- rethinking::quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + b * (weight - xbar_a),\n    a ~ dnorm(178, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ),\n  data = d2_a\n)\n\n# summary result\n## R code 4.44a ############################\nrethinking::precis(m4.3a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>              mean         sd        5.5%       94.5%\n#> a     154.6012591 0.27030995 154.1692516 155.0332666\n#> b       0.9033071 0.04192398   0.8363045   0.9703097\n#> sigma   5.0719240 0.19115884   4.7664153   5.3774327\n```\n\n\n:::\n:::\n\n:::\n:::\n\n###### Original2 (log)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-find-post-dist-log-m4-3a_2}\na: Find the posterior distribution of the linear height-weight model:\nLog version (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/find-post-dist-log-m4-3a_2_3aba147d0d493add0b2bb153570611bb'}\n\n```{.r .cell-code}\n## R code 4.43a ############################\nm4.3a_2 <- rethinking::quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + exp(log_b) * (weight - xbar_a),\n    a ~ dnorm(178, 20),\n    log_b ~ dnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ),\n  data = d2_a\n)\n\nrethinking::precis(m4.3a_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>               mean         sd        5.5%        94.5%\n#> a     154.60136683 0.27030697 154.1693641 155.03336957\n#> log_b  -0.09957465 0.04626322  -0.1735122  -0.02563709\n#> sigma   5.07186793 0.19115333   4.7663680   5.37736786\n```\n\n\n:::\n:::\n\n:::\n:::\n\nNote the `exp(log_b)` in the definition of `mu`. This is the same model\nas `m4.3`. It will make the same predictions. But instead of `β` in the\nposterior distribution, you get `log((β)`. It is easy to translate\nbetween the two, because $\\beta = exp(log(\\beta))$. In code form:\n`b <- exp(log_b)`.\n\n###### Tidyverse\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-find-post-dist-m4-3b_2}\nb: Find the posterior distribution of the linear height-weight model\n(Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n\n::: {.cell hash='04-geocentric-models_cache/html/find-post-dist-m4-3b_e2f7cec731925c035616926a7c4973c7'}\n\n```{.r .cell-code}\nd2_b <-\n  d2_b |> \n  dplyr::mutate(weight_c = weight - base::mean(weight))\n\nm4.3b <- \n  brms::brm(data = d2_b, \n      family = gaussian,\n      height ~ 1 + weight_c,\n      prior = c(brms::prior(normal(178, 20), class = Intercept),\n                brms::prior(lognormal(0, 1), class = b, lb = 0),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"brm_fits/m04.03b\")\n\nbrms:::summary.brmsfit(m4.3b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ 1 + weight_c \n#>    Data: d2_b (Number of observations: 352) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   154.61      0.27   154.09   155.14 1.00     4546     2895\n#> weight_c      0.90      0.04     0.82     0.99 1.00     4278     2978\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     5.11      0.20     4.72     5.51 1.00     4439     2773\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n:::\n:::\n\nRemember: The detailed explication of the syntax for the following\n`brms::brm()` function is in the \"Tidyverse\" tab of @exm-chap04-m4-1.\n\nUnlike with McElreath's `rethinking::quap()` formula syntax, Kurz is not\naware if we can just specify something like `weight – xbar` in the\n`formula` argument in `brms::brm()`.\n\nHowever, the alternative is easy: Just make a new variable in the data\nthat is equivalent to `weight – mean(weight)`. We'll call it `weight_c`.\n\n###### Tidyverse2 (log)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-find-post-dist-log-m4-3b_2}\nFind the posterior distribution of the linear height-weight model (log\nversion) (Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/find-post-dist-log-m4-3b_2_d6070e576f5a31c1e319822daa4eaee0'}\n\n```{.r .cell-code}\nm4.3b_log <- \n  brms::brm(data = d2_b, \n      family = gaussian,\n      brms::bf(height ~ a + exp(lb) * weight_c,\n         a ~ 1,\n         lb ~ 1,\n         nl = TRUE),\n      prior = c(brms::prior(normal(178, 20), class = b, nlpar = a),\n                brms::prior(normal(0, 1), class = b, nlpar = lb),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"brm_fits/m04.03b_log\")\n\nbrms:::summary.brmsfit(m4.3b_log)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ a + exp(lb) * weight_c \n#>          a ~ 1\n#>          lb ~ 1\n#>    Data: d2_b (Number of observations: 352) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> a_Intercept    154.61      0.27   154.08   155.12 1.00     3524     2646\n#> lb_Intercept    -0.10      0.05    -0.20    -0.01 1.00     4412     2848\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     5.11      0.19     4.75     5.51 1.00     3697     2629\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\nRemember: The detailed explication of the syntax for the following\n`brms::brm()` function is in the \"Tidyverse\" tab of @exm-chap04-m4-1.\n\nThe difference is for the $\\beta$ parameter, which we called `lb` in the\n`m4.3b_log` model. If we term that parameter from `m4.3b` as\n$\\beta^{m4.3b}$ and the one from our new log model $\\beta^{m4.3b_log}$, it\nturns out that $\\beta^{m4.3b} = exp(\\beta^{m4.3b_log})$.\n:::\n:::\n\nCompare the result with the previous tab \"Tidyverse\" in\n@cnj-find-post-dist-m4-3b_2.\n\n###### fixef\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-fixef-m4-3b}\nb: Extract and compare the population-level (fixed) effects from object\n`m4.3b` and the form log version `m4.3b_log`\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-fixed-effects-m4-3b_ab22a7089ad8091046c120eb5839d2ba'}\n\n```{.r .cell-code}\nbrms::fixef(m4.3b)[\"weight_c\", \"Estimate\"]\nbrms::fixef(m4.3b_log)[\"lb_Intercept\", \"Estimate\"] |> exp()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.9031995\n#> [1] 0.9037418\n```\n\n\n:::\n:::\n\n:::\n:::\n\nThey're the same within simulation variance.\n\n###### Trace plot\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-trace-plot-m4-3b}\nDisplay trace plots (Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/trace-plot-m4-3b_2_5aeb3fe12309c3e0d06ff544224ecbeb'}\n\n```{.r .cell-code}\nbrms:::plot.brmsfit(m4.3b, widths = c(1, 2))\n```\n\n::: {.cell-output-display}\n![](04-geocentric-models_files/figure-html/trace-plot-m4-3b_2-1.png){width=672}\n:::\n:::\n\n:::\n:::\n\n::: my-note\n::: my-note-header\nChecking MCMC chains with trace plots and trank plots\n:::\n\n::: my-note-container\nAt the moment I didn't learn how to interpret these types of graphic\noutput. McElreath's explains in later video lectures that it is\nimportant that <a class='glossary' title='A trace plot is a chain visualization that plots the samples in sequential order, joined by a line. A trace plot isn’t the last thing analysts do to inspect MCMC output. But it’s often the first. A healty trace plot is not a guarantee that you have a functioning Markov chain. (Chap.9)'>trace plot</a> cover the same location in the\nvertical axis (e.g. they do not jump around) and show that all different\nchains alternate in their (top) positions. I mentioned here the top\nposition because this is the place where irregularities can be detected\nmore easily.\n\nIn the above example it is difficult to decide if this is the case\nbecause the color differences of the different chains are weak. But it\nis general difficult to inspect trace plot, therefore McElreath proposes\ntrace rank plots or <a class='glossary' title='Trace Rank Plot or as McElreath’s suggest a Trank Plot visualizes the chains as a distribution of the ranked samples. What this means is to take all the samples for each individual parameter and rank them. The lowest sample gets rank 1. The largest gets the maximum rank (the number of samples across all chains). Then we draw a histogram of these ranks for each individual chain. Why do this? Because if the chains are exploring the same space efficiently, the histograms should be similar to one another and largely overlapping. (Chap.9)'>trank plot</a> (his terminus) in\n@sec-checking-the-chain.\n:::\n:::\n:::\n:::\n:::\n\nTo understand the differences in syntax between {**rethinking**} and\n{**brms**} it is also quite revealing to look at\n@tbl-mirror-rethinking-tidyverse. I have inserted the following table\nfrom Kurz' explanation of model `b4.3`.\n\n| {**rethinking**} package                                    | {**brms**} package:                        |\n|------------------------------------|------------------------------------|\n| $\\text{height}_i \\sim \\operatorname{Normal}(\\mu_i, \\sigma)$ | `family = gaussian`                        |\n| $\\mu_i = \\alpha + \\beta \\text{weight}_i$                    | `height ~ 1 + weight_c`                    |\n| $\\alpha \\sim \\operatorname{Normal}(178, 20)$                | `prior(normal(178, 20), class = Intercept` |\n| $\\beta \\sim \\operatorname{Log-Normal}(0, 1)$                | `prior(lognormal(0, 1), class = b)`        |\n| $\\sigma \\sim \\operatorname{Uniform}(0, 50)$                 | `prior(uniform(0, 50), class = sigma)`     |\n\n: Compare statistical notation of rethinking with brms package\n{#tbl-mirror-rethinking-tidyverse}\n\n### Interpreting the posterior distribution\n\n::: my-important\n::: my-important-header\nWhat do parameters mean?\n:::\n\n::: my-important-container\n\"Posterior probabilities of parameter values describe the relative\ncompatibility of different states of the world with the data, according\nto the model. These are small world (@sec-chap02) numbers.\" ([McElreath,\n2020, p. 99](zotero://select/groups/5243560/items/NFUEVASQ))\n([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=118&annotation=N3YFPKMW))\n:::\n:::\n\nStatistical models are hard to interpret. Plotting posterior\ndistributions and posterior predictions is better than attempting to\nunderstand a table.\n\n#### Table of marginal distributions\n\nThere are many different options in the tidyverse approach, repsectively\nwith {**brms**}. The most\n\n::: my-example\n::: my-example-header\n::: {#exm-chap04-table-interpretation}\nInspect the marginal posterior distributions of the parameters\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### precis (O)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-inspect-lm-table-a}\na: Inspect the marginal posterior distributions of the parameters\n(Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-inspect-lm-table-4-3a_f10a0256869e8b77762b5f463505587a'}\n\n```{.r .cell-code}\n## R code 4.44 ##########\nrethinking::precis(m4.3a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>              mean         sd        5.5%       94.5%\n#> a     154.6012591 0.27030995 154.1692516 155.0332666\n#> b       0.9033071 0.04192398   0.8363045   0.9703097\n#> sigma   5.0719240 0.19115884   4.7664153   5.3774327\n```\n\n\n:::\n:::\n\n\n------------------------------------------------------------------------\n\n1.  First row: quadratic approximation for $\\alpha$\n2.  Second row: quadratic approximation for $\\beta$\n3.  Third row: quadratic approximation for $\\sigma$\n:::\n:::\n\nLet's focus on b ($\\beta$), because it's the new parameter. Since\n($\\beta$) is a slope, the value 0.90 can be read as *a person 1 kg\nheavier is expected to be 0.90 cm taller*. 89% of the posterior\nprobability ($94.5-5.5$) lies between 0.84 and 0.97. That suggests that\n($\\beta$) values close to zero or greatly above one are highly\nincompatible with these data and this model. It is most certainly not\nevidence that the relationship between weight and height is linear,\nbecause the model only considered lines. It just says that, if you are\ncommitted to a line, then lines with a slope around 0.9 are plausible\nones.\n\n###### vcov (O)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-lm-vcov-a}\nb: Display variance-covariance matrix (Original)\n:::\n:::\n\n::: my-r-code-container\nThe numbers in the default `rethinking::precis()` output aren't\nsufficient to describe the quadratic posterior completely. Therefore we\nalso need to inspect the variance-covariance matrix.\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-lm-vcov-m4-3a_1accb89831a2cf28afb590deaa1231e0'}\n\n```{.r .cell-code}\n## R code 4.45 ################\nround(rethinking::vcov(m4.3a), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>           a     b sigma\n#> a     0.073 0.000 0.000\n#> b     0.000 0.002 0.000\n#> sigma 0.000 0.000 0.037\n```\n\n\n:::\n:::\n\n:::\n:::\n\nThere is very little covariation among the parameters in this case.\n\n###### pairs (O)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-lm-pairs-a}\nb: Display variance-covariance matrix (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-lm-pairs-m4-3a_d6f641fcb0e1f602d838c6fb8c2df05f'}\n\n```{.r .cell-code}\nrethinking::pairs(m4.3a)\n```\n\n::: {.cell-output-display}\n![](04-geocentric-models_files/figure-html/chap04-lm-pairs-m4-3a-1.png){width=672}\n:::\n:::\n\n:::\n:::\n\nThe graphic shows both the marginal posteriors and the covariance.\n\n###### brms (T)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-post-summary-b}\nb: Display the marginal posterior distributions of the parameters (brms)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-post-summary-m4-3b_cd27c9776375181495d8d457dbf13b35'}\n\n```{.r .cell-code}\nbrms::posterior_summary(m4.3b, probs = c(0.055, 0.945))[1:3, ] |> \n  round(digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>             Estimate Est.Error   Q5.5  Q94.5\n#> b_Intercept   154.61      0.27 154.18 155.05\n#> b_weight_c      0.90      0.04   0.84   0.97\n#> sigma           5.11      0.20   4.80   5.43\n```\n\n\n:::\n:::\n\n\nLooking up `brms::posterior_summary()` I learned that the \"function\nmainly exists to retain backwards compatibility\". It will eventually be\nreplaced by functions of the {**posterior**} package\".\n([brms](https://paul-buerkner.github.io/brms/reference/posterior_summary.html))\n:::\n:::\n\n::: my-watch-out\n::: my-watch-out-header\nWATCH OUT! How to insert coefficients into functions with {**brms**}?\n:::\n\n::: my-watch-out-container\n> {**brms**} does not allow users to insert coefficients into functions\n> like `exp()` within the conventional `formula` syntax. We can fit a\n> {**brms**} model like McElreath's `m4.3a` if we adopt what's called\n> the [non-linear\n> syntax](https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html).\n> The non-linear syntax is a lot like the syntax McElreath uses in\n> {**rethinking**} in that it typically includes both predictor and\n> variable names in the `formula`.\n> ([Kurz](https://bookdown.org/content/4857/geocentric-models.html#overthinking-logs-and-exps-oh-my.))\n\nKurz promises to explain later in his ebook what non-linear syntax\nexactly means.\n:::\n:::\n\n###### draws1\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-post-summarize-draws-m4-3b}\nb: Display the marginal posterior distributions of the parameters with\n`summarize_draws()` from {**posterior**}\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-post-summarize-draws-m4-3b_5e40174976ecd6d6d722a8566acf8c25'}\n\n```{.r .cell-code}\nposterior::summarize_draws(m4.3b, \"mean\", \"median\", \"sd\", \n                           ~quantile(., probs = c(0.055, 0.945)),\n                           .num_args = list(sigfig = 2))[1:3, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 3 × 6\n#>   variable      mean median    sd `5.5%` `94.5%`\n#>   <chr>        <dbl>  <dbl> <dbl>  <dbl>   <dbl>\n#> 1 b_Intercept 155.   155.   0.27  154.    155.  \n#> 2 b_weight_c    0.90   0.90 0.042   0.84    0.97\n#> 3 sigma         5.1    5.1  0.20    4.8     5.4\n```\n\n\n:::\n:::\n\n:::\n:::\n\n###### draws2\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-summary-draws-m4-3b}\nb: Display the marginal posterior distributions of the parameters of\n`brms::as_draws_array()` objects\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-summary-draws-m4-3b_f09ebd3f303677ff373b81e957e6ef50'}\n\n```{.r .cell-code}\n## summary for draws object\nsummary(brms::as_draws_array(m4.3b))[1:3, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 3 × 10\n#>   variable    mean  median     sd    mad      q5     q95  rhat ess_bulk ess_tail\n#>   <chr>      <dbl>   <dbl>  <dbl>  <dbl>   <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n#> 1 b_Inter… 155.    155.    0.272  0.275  154.    155.     1.00    4546.    2895.\n#> 2 b_weigh…   0.903   0.903 0.0424 0.0414   0.834   0.973  1.00    4278.    2978.\n#> 3 sigma      5.11    5.10  0.198  0.190    4.79    5.44   1.00    4439.    2773.\n```\n\n\n:::\n:::\n\n:::\n:::\n\n###### vcov1 (T)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-vcov1-b}\nb: Display variance-covariance matrix (brms)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-vcov-m4-3b_fdc311814b270bdca753d6c3ce01d62c'}\n\n```{.r .cell-code}\nbrms:::vcov.brmsfit(m4.3b) |> \n  round(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>           Intercept weight_c\n#> Intercept     0.074    0.000\n#> weight_c      0.000    0.002\n```\n\n\n:::\n:::\n\n\nWe got the variance/covariance matrix of the `intercept` and `weight_c`\ncoefficient but not $\\sigma$ however. To get that, we'll have to extract\nthe posterior draws and use the `cov()` function, instead. (See next tab\n\"vcov2 (T)\")\n:::\n:::\n\n###### vcov2 (T)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-vcov2-b}\nb: Variance-covariance matrix with {**posterior**} draws objects (brms)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-cov-m4-3b_42f1b18fcfd1a7980f68aca6e736fe17'}\n\n```{.r .cell-code}\nbrms::as_draws_df(m4.3b) |>\n  dplyr::select(b_Intercept:sigma) |>\n  stats::cov() |>\n  base::round(digits = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>             b_Intercept b_weight_c sigma\n#> b_Intercept       0.074      0.000 0.000\n#> b_weight_c        0.000      0.002 0.000\n#> sigma             0.000      0.000 0.039\n```\n\n\n:::\n:::\n\n:::\n:::\n\n###### pairs (T)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-lm-pairs-b}\nb: Display variance-covariance matrix (brms)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-lm-pairs-m4-3b_7ce50e9ae619ed9f74651a855d16b02a'}\n\n```{.r .cell-code}\nbrms:::pairs.brmsfit(m4.3b)\n```\n\n::: {.cell-output-display}\n![](04-geocentric-models_files/figure-html/chap04-lm-pairs-m4-3b-1.png){width=672}\n:::\n:::\n\n:::\n:::\n\nThe graphic shows both the marginal posteriors and the covariance.\n:::\n:::\n:::\n\n#### Plotting posterior inference against the data\n\n::: my-example\n::: my-example-header\n::: {#exm-chap04-plot-post-inf-against-data}\n: Height plotted against weight with linear regression (line at the\nposterior mean)\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### Original\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-plot-raw-data-line-m4-3a}\na: Height in centimeters (vertical) plotted against weight in kilograms\n(horizontal), with the line at the posterior mean plotted in black\n(Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-raw-data-line-m4-3a_ca16d1d5d99198b0a782a0f81a2dc2fd'}\n\n```{.r .cell-code}\n## R code 4.46a ############################################\nplot(height ~ weight, data = d2_a, col = rethinking::rangi2)\npost_m4.3a <- rethinking::extract.samples(m4.3a)\na_map <- mean(post_m4.3a$a)\nb_map <- mean(post_m4.3a$b)\ncurve(a_map + b_map * (x - xbar_a), add = TRUE)\n```\n\n::: {.cell-output-display}\n![Height in centimeters (vertical) plotted against weight in kilograms (horizontal), with the line at the posterior mean plotted in black: rethinking version](04-geocentric-models_files/figure-html/fig-raw-data-line-m4-3a-1.png){#fig-raw-data-line-m4-3a width=672}\n:::\n:::\n\n:::\n:::\n\nEach point in this plot is a single individual. The black line is\ndefined by the mean slope $\\beta$ and mean intercept $\\alpha$. This is\nnot a bad line. It certainly looks highly plausible. But there are an\ninfinite number of other highly plausible lines near it. See next tab\n\n###### Tidyverse1\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-plot-raw-data-line-m4-3b}\nb: Height in centimeters (vertical) plotted against weight_c\n(horizontal), with the line at the posterior mean plotted in black\n(Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-raw-data-line-m4-3b_c6bd6ad88dfb19c997deea3fd40d8d2c'}\n\n```{.r .cell-code}\nd2_b |>\n  ggplot2::ggplot(ggplot2::aes(x = weight_c, y = height)) +\n  ggplot2::geom_abline(intercept = brms:::fixef.brmsfit(m4.3b)[1], \n              slope     = brms:::fixef.brmsfit(m4.3b)[2]) +\n  ggplot2::geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![](04-geocentric-models_files/figure-html/fig-raw-data-line-m4-3b-1.png){#fig-raw-data-line-m4-3b width=672}\n:::\n:::\n\n\nNote how the breaks on our `x`-axis look off. That's because we fit the\nmodel with `weight_c` and we plotted the points in that metric, too.\nSince we computed `weight_c` by subtracting the mean of weight from the\ndata, we can adjust the `x`-axis break point labels by simply adding\nthat value back. (See next tab \"Tidyverse2\")\n:::\n:::\n\nFurther note the use of the `brms:::fixef.brmsfit()` function within\n`ggplot2::geom_abline()`. The function extracts the population-level\n('fixed') effects from a `brmsfit` object.\n\n###### Tidyverse2\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-plot-raw-data-line2-m4-3b}\nb: Height in centimeters (vertical) plotted against weight_c\n(horizontal), with the line at the posterior mean plotted in black\n(Tidyverse)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-raw-data-line2-b4.3_9731ef968d38faad534bc1710afc2ad8'}\n\n```{.r .cell-code}\nlabels <-\n  c(-10, 0, 10) + base::mean(d2_b$weight) |> \n  base::round(digits = 0)\n\nd2_b |>\n  ggplot2::ggplot(ggplot2::aes(x = weight_c, y = height)) +\n  ggplot2::geom_abline(intercept = brms::fixef(m4.3b, probs = c(0.055, 0.945))[[1]], \n              slope     = brms::fixef(m4.3b, probs = c(0.055, 0.945))[[2]]) +\n  ggplot2::geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  ggplot2::scale_x_continuous(\"weight\",\n                     breaks = c(-10, 0, 10),\n                     labels = labels) +\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Height in centimeters (vertical) plotted against weight in kilograms (horizontal), with the line at the posterior mean plotted in black: tidyverse version](04-geocentric-models_files/figure-html/fig-raw-data-line2-b4.3-1.png){#fig-raw-data-line2-b4.3 width=672}\n:::\n:::\n\n:::\n:::\n\nNote the use of the `brms:::fixef.brmsfit()` function within\n`ggplot2::geom_abline()`. The function extracts the population-level\n('fixed') effects from a `brmsfit` object.\n:::\n:::\n:::\n\n::: my-note\n::: my-note-header\nWhat are population-level (\"fixed\") effects?\n:::\n\n::: my-note-container\nI didn't know what it meant exactly that `brms::fixef()` extracts the\npopulation-level ('fixed') effect from a `brmsfit` object. After reading\nother books I now understand that this means effects of the whole\npopulation in contrast to effects on the idividual level (so-called\n\"random effect\"). (The terms \"fixed\" and \"random\" effects are\nmisleading, as both effects are random.)\n\n> \"Generally, these are the three types of parameters in multi-level\n> models: the population-level estimate (commonly called fixed effects),\n> the participant-level estimates (random effects) and the\n> participant-level variation.\" ([Schmettow, 2022, p.\n> 268](zotero://select/groups/5254846/items/HCW7A9WK))\n> ([pdf](zotero://open-pdf/groups/5254846/items/WF7URHBJ?page=276&annotation=E5FND2P9))\n\n> \"There is a lot of confusion about the type of models that we deal\n> with in this chapter. They have also been called hierarchical models\n> or mixed effects models. The \"mixed\" stands for a mixture of so called\n> fixed effects and random effects. The problem is: if you start by\n> understanding what fixed effects and random effects are, confusion is\n> programmed, not only because there exist several very different\n> definitions. In fact, it does not matter so much whether an estimate\n> is a fixed effect or random effect. As we will see, you can construct\n> a multi-level model by using just plain descriptive summaries. What\n> matters is that a model contains estimates on population level and on\n> participant level. The benefit is, that a multi-level model can answer\n> the same question for the population as a whole and for every single\n> participant.\" ([Schmettow, 2022, p.\n> 278](zotero://select/groups/5254846/items/HCW7A9WK))\n> ([pdf](zotero://open-pdf/groups/5254846/items/WF7URHBJ?page=286&annotation=N4M4YSCS))\n\n::: my-resource\n::: my-resource-header\nFixed and random effects\n:::\n\n::: my-resource-container\nAfter I googled it turned out that there is a great discussion about\n[Fixed Effects in Linear\nRegression](https://statisticsglobe.com/fixed-effects-linear-regression)\nand generally about fixed, random and mixed models (See Cross Validated\n[here](https://stats.stackexchange.com/questions/4700/what-is-the-difference-between-fixed-effect-random-effect-and-mixed-effect-mode)\nand\n[here](https://stats.stackexchange.com/questions/21760/what-is-a-difference-between-random-effects-fixed-effects-and-marginal-model)).\n\nThere is also a glossary entry in\n[statistics.com](https://www.statistics.com/glossary/fixed-effects/).\n\nAnd last but not least there are academic papers on this topic:\n\n-   [Let's Talk About Fixed\n    Effects](https://link.springer.com/article/10.1007/s11577-020-00699-8)),\n-   tutorials ([Fixed Effects\n    Regression](https://www.econometrics-with-r.org/10-3-fixed-effects-regression.html),\n-   [Fixed or Random\n    Effects](https://bookdown.org/Yuleng/polimethod/fixed.html)) and\n-   R packages (\\[fixest\\]https://lrberge.github.io/fixest/index.html,\n    see it's\n-   [introduction](https://lrberge.github.io/fixest/articles/fixest_walkthrough.html))\n    dedicated especially to this subject.\n:::\n:::\n:::\n:::\n\n#### Adding uncertainty around the mean\n\n> \"Plots of the average line, like @fig-raw-data-line-m4-3a, are useful\n> for getting an impression of the magnitude of the estimated influence\n> of a variable. But they do a poor job of communicating uncertainty.\n> Remember, the posterior distribution considers every possible\n> regression line connecting height to weight. It assigns a relative\n> plausibility to each. This means that each combination of $\\alpha$ and\n> $\\beta$ has a posterior probability. It could be that there are many\n> lines with nearly the same posterior probability as the average line.\n> Or it could be instead that the posterior distribution is rather\n> narrow near the average line.\" ([McElreath, 2020, p.\n> 101](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=120&annotation=YN73Y4A4))\n\n::: my-example\n::: my-example-header\n::: {#exm-chap04-uncertainty-around-mean}\n: Inspect the uncertainty around the mean\n:::\n:::\n\n::: my-example-container\n::: panel-tabset\n###### rows (O)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-show-some-post-samples-m4-3a}\na: Show some posterior data rows (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-show-some-post-samples-m4-3a_abb3c071970381c28c843306bf358e63'}\n\n```{.r .cell-code}\n## R code 4.47a ################\n# post_m4.3a <- rethinking::extract.samples(m4.3a) # already in previous listing\nset.seed(4)\nbayr::as_tbl_obs(post_m4.3a)\n```\n\n::: {.cell-output-display}\n\n\nTable: Data set with 4 variables, showing 8 of 10000 observations.\n\n|  Obs|        a|         b|    sigma|\n|----:|--------:|---------:|--------:|\n|  307| 154.2976| 0.9186361| 4.841837|\n|  587| 154.9650| 0.8467550| 4.994881|\n|  684| 154.2021| 0.9094732| 5.415900|\n| 1795| 155.1948| 0.8750700| 5.593173|\n| 2867| 154.8796| 0.8997763| 5.146971|\n| 4167| 153.9071| 0.9294830| 5.028728|\n| 4794| 154.6078| 0.8424997| 4.918431|\n| 5624| 154.7940| 0.9329589| 5.006664|\n\n\n\n:::\n:::\n\n:::\n:::\n\n> \"Each row is a correlated random sample from the joint posterior of\n> all three parameters, using the covariances provided by `vcov(m4.3a)`\n> in Tab\"vcov (O)\" in @exm-chap04-table-interpretation. The paired\n> values of `a` and `b` on each row define a line. The average of very\n> many of these lines is the posterior mean line. But the scatter around\n> that average is meaningful, because it alters our confidence in the\n> relationship between the predictor and the outcome\" ([McElreath, 2020,\n> p. 101](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=120&annotation=KS7NLQ9U))\n\n###### plot1 (O)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-fig-chap04-plot-10-points-m4-3a}\na: Plot 20 sampled lines of 10 data points to the uncertainty around the\nmean (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-plot-10-points-m4-3a_336e39d005d1113b0e3b546172059638'}\n\n```{.r .cell-code}\n## R code 4.48a ##########################\nN10_a <- 10\ndN10_a <- d2_a[1:N10_a, ]\nmN10_a <- rethinking::quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + b * (weight - mean(weight)),\n    a ~ dnorm(178, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ),\n  data = dN10_a\n)\n\n## R code 4.49a ##############################\n# extract 20 samples from the posterior\nset.seed(4)\npost_20_m4.3a <- rethinking::extract.samples(mN10_a, n = 20)\n\n# display raw data and sample size\nplot(dN10_a$weight, dN10_a$height,\n  xlim = range(d2_a$weight), ylim = range(d2_a$height),\n  col = rethinking::rangi2, xlab = \"weight\", ylab = \"height\"\n)\nmtext(rethinking:::concat(\"N = \", N10_a))\n\n# plot the lines, with transparency\nfor (i in 1:20) {\n  curve(post_20_m4.3a$a[i] + post_20_m4.3a$b[i] * (x - mean(dN10_a$weight)),\n    col = rethinking::col.alpha(\"black\", 0.3), add = TRUE\n  )\n}\n```\n\n::: {.cell-output-display}\n![Samples from the quadratic approximate posterior distribution for the height/weight model, m4.3a. 20 lines sampled from 10 data points of the posterior distribution, showing the uncertainty in the regression relationship (Original)](04-geocentric-models_files/figure-html/fig-chap04-plot-10-points-m4-3a-1.png){#fig-chap04-plot-10-points-m4-3a width=672}\n:::\n:::\n\n:::\n:::\n\n> \"By plotting multiple regression lines, sampled from the posterior, it\n> is easy to see both the highly confident aspects of the relationship\n> and the less confident aspects. The cloud of regression lines displays\n> greater uncertainty at extreme values for weight.\" ([McElreath, 2020,\n> p. 102](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=121&annotation=CX5K4AHV))\n\n###### plot2 (O)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-fig-chap04-plot-50-points-m4-3a}\na: Plot 20 sampled lines of 50 data points to the uncertainty around the\nmean (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-plot-50-points-m4-3a_9e26fd63d61374c3cb45869b16b52150'}\n\n```{.r .cell-code}\n## R code 4.48a ######################\nN50_a <- 50\ndN50_a <- d2_a[1:N50_a, ]\nmN50_a <- rethinking::quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + b * (weight - mean(weight)),\n    a ~ dnorm(178, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ),\n  data = dN50_a\n)\n\n## R code 4.49a ######################\n# extract 20 samples from the posterior\nset.seed(4)\npost_50_m4.3a <- rethinking::extract.samples(mN50_a, n = 20)\n\n# display raw data and sample size\nplot(dN50_a$weight, dN50_a$height,\n  xlim = range(d2_a$weight), ylim = range(d2_a$height),\n  col = rethinking::rangi2, xlab = \"weight\", ylab = \"height\"\n)\nmtext(rethinking:::concat(\"N = \", N50_a))\n\n# plot the lines, with transparency\nfor (i in 1:20) {\n  curve(post_50_m4.3a$a[i] + post_50_m4.3a$b[i] * (x - mean(dN50_a$weight)),\n    col = rethinking::col.alpha(\"black\", 0.3), add = TRUE\n  )\n}\n```\n\n::: {.cell-output-display}\n![Samples from the quadratic approximate posterior distribution for the height/weight model, m4.3a. 20 lines sampled from 50 data points of the posterior distribution, showing the uncertainty in the regression relationship (Original)](04-geocentric-models_files/figure-html/fig-chap04-plot-50-points-m4-3a-1.png){#fig-chap04-plot-50-points-m4-3a width=672}\n:::\n:::\n\n:::\n:::\n\n> \"Notice that the cloud of regression lines grows more compact as the\n> sample size increases. This is a result of the model growing more\n> confident about the location of the mean.\" ([McElreath, 2020, p.\n> 102](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=121&annotation=92MJX4AG))\n\n###### plot3 (O)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-fig-chap04-plot-352-points-m4-3a}\na: Plot 20 sampled lines of 352 data points to the uncertainty around\nthe mean (Original)\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-plot-352-points-m4-3a_9555e3090a41c8b865652ab0fee0b830'}\n\n```{.r .cell-code}\n## R code 4.48, 4.49 ###########################\nN352_a <- 352\ndN352_a <- d2_a[1:N352_a, ]\nmN352_a <- rethinking::quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + b * (weight - mean(weight)),\n    a ~ dnorm(178, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ),\n  data = dN352_a\n)\n\n# extract 20 samples from the posterior\npost_352_m4.3a <- rethinking::extract.samples(mN352_a, n = 20)\n\n# display raw data and sample size\nplot(dN352_a$weight, dN352_a$height,\n  xlim = range(d2_a$weight), ylim = range(d2_a$height),\n  col = rethinking::rangi2, xlab = \"weight\", ylab = \"height\"\n)\nmtext(rethinking:::concat(\"N = \", N352_a))\n\n# plot the lines, with transparency\nfor (i in 1:20) {\n  curve(post_352_m4.3a$a[i] + post_352_m4.3a$b[i] * (x - mean(dN352_a$weight)),\n    col = rethinking::col.alpha(\"black\", 0.3), add = TRUE\n  )\n}\n```\n\n::: {.cell-output-display}\n![Samples from the quadratic approximate posterior distribution for the height/weight model, m4.3a. 20 lines sampled from all 352 data points of the posterior distribution, showing the uncertainty in the regression relationship.](04-geocentric-models_files/figure-html/fig-chap04-plot-352-points-m4-3a-1.png){#fig-chap04-plot-352-points-m4-3a width=672}\n:::\n:::\n\n:::\n:::\n\n> \"Notice that the cloud of regression lines grows more compact as the\n> sample size increases. This is a result of the model growing more\n> confident about the location of the mean.\" ([McElreath, 2020, p.\n> 102](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=121&annotation=92MJX4AG))\n\n###### rows (T)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-chap04-show-some-post-samples-m4-3b}\nb: Extract the iteration of the Hamiliton Monte Carlo\n(<a class='glossary' title='Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) method that uses the derivatives of the density function being sampled to generate efficient transitions spanning the posterior. It uses an approximate Hamiltonian dynamics simulation based on numerical integration which is then corrected by performing a Metropolis acceptance step. (Stan Reference Manual)'>HMC</a>) chains into a data frame and show 8 rows\n(Tidyverse)\"\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-show-some-post-samples-m4-3b_b0826f1300cf5a28c95054407dc9ba37'}\n\n```{.r .cell-code}\npost_m4.3b <- brms::as_draws_df(m4.3b)\nset.seed(4)\nbayr::as_tbl_obs(post_m4.3b)\n```\n\n::: {.cell-output-display}\n\n\nTable: Data set with 9 variables, showing 8 of 4000 observations.\n\n|  Obs| b_Intercept| b_weight_c|    sigma|    lprior|      lp__| .chain| .iteration| .draw|\n|----:|-----------:|----------:|--------:|---------:|---------:|------:|----------:|-----:|\n|   71|    154.4093|  0.9211252| 4.990186| -9.362502| -1079.373|      1|         71|    71|\n|  587|    154.5356|  0.8923309| 5.097617| -9.326425| -1079.011|      1|        587|   587|\n|  684|    154.9519|  0.8758742| 5.187928| -9.285901| -1080.125|      1|        684|   684|\n| 1528|    154.0199|  0.8707605| 4.943669| -9.335626| -1081.977|      2|        528|  1528|\n| 1795|    155.0464|  0.8917125| 5.051194| -9.296172| -1080.359|      2|        795|  1795|\n| 2038|    154.6438|  0.8601186| 5.262894| -9.288191| -1079.917|      3|         38|  2038|\n| 2419|    154.6285|  1.0054228| 5.048592| -9.433840| -1081.835|      3|        419|  2419|\n| 2867|    154.4709|  0.8487887| 5.262154| -9.287149| -1080.317|      3|        867|  2867|\n\n\n\n:::\n:::\n\n\n:::::{.my-note}\n:::{.my-note-header}\nPrintout interpreted\n:::\n::::{.my-note-container}\n-   `b_Intercept` represents `a` in the rethinking version.\n-   `b_weight_c` represents `b` in the rethinking version. But why did\n    we have to calculate it different?\n-   `sigma` is the quadratic approximation of the standard deviation.\n-   `lprior` is -- I assume -- the log prior.\n-   `l__` is what??\n::::\n:::::\n\n\n:::\n:::\n\nInstead of `rethinking::extract.samples()` the {**brms**} packages\nextract all the posterior draws with `brms::as_draws_df()`. We have\nalready done this with model `m4.1b` (@exm-chap04-sampling-brm-m4-1b in\ntab \"draws1\").\n\n###### plots (T)\n\n::: my-r-code\n::: my-r-code-header\n<div>\n\nb: Plot 20 sampled lines of 10, 50, 150 and 352 data points to the\nuncertainty around the mean (Original)\n\n</div>\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-plot-sampled-lines-m4-3b_a614f3e56c5ec2f634713a4fdd01f6aa'}\n\n```{.r .cell-code}\n## 1. Calculate all four models ################\ndN10_b <- 10\n\nm4.3b_010 <- \n  brms::brm(data = d2_b |>\n      dplyr::slice(1:dN10_b),  # note our tricky use of `N` and `slice()`\n      family = gaussian,\n      height ~ 1 + weight_c,\n      prior = c(brms::prior(normal(178, 20), class = Intercept),\n                brms::prior(lognormal(0, 1), class = b),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"brm_fits/m04.03b_010\")\n\ndN50_b <- 50\n\nm4.3b_050 <- \n  brms::brm(data = d2_b |>\n      dplyr::slice(1:dN50_b), \n      family = gaussian,\n      height ~ 1 + weight_c,\n      prior = c(brms::prior(normal(178, 20), class = Intercept),\n                brms::prior(lognormal(0, 1), class = b),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"brm_fits/m04.03b_050\")\n\ndN150_b <- 150\n\nm4.3b_150 <- \n  brms::brm(data = d2_b |>\n      dplyr::slice(1:dN150_b), \n      family = gaussian,\n      height ~ 1 + weight_c,\n      prior = c(brms::prior(normal(178, 20), class = Intercept),\n                brms::prior(lognormal(0, 1), class = b),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"brm_fits/m04.03b_150\")\n\ndN352_b <- 352\n\nm4.3b_352 <- \n  brms::brm(data = d2_b |>\n      dplyr::slice(1:dN352_b), \n      family = gaussian,\n      height ~ 1 + weight_c,\n      prior = c(brms::prior(normal(178, 20), class = Intercept),\n                brms::prior(lognormal(0, 1), class = b),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"brm_fits/m04.03b_352\")\n\n\n## 2. put model chains into dfs ##########\npost010_m4.3b <- brms::as_draws_df(m4.3b_010)\npost050_m4.3b <- brms::as_draws_df(m4.3b_050)\npost150_m4.3b <- brms::as_draws_df(m4.3b_150)\npost352_m4.3b <- brms::as_draws_df(m4.3b_352)\n\n\n## 3. prepare plots ##########\np10 <- \n  ggplot2::ggplot(data =  d2_b[1:10, ], \n         ggplot2::aes(x = weight_c, y = height)) +\n  ggplot2::geom_abline(data = post010_m4.3b |> dplyr::slice(1:20),\n              ggplot2::aes(intercept = b_Intercept, slope = b_weight_c),\n              linewidth = 1/3, alpha = .3) +\n  ggplot2::geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  ggplot2::coord_cartesian(xlim = base::range(d2_b$weight_c),\n                  ylim = base::range(d2_b$height)) +\n  ggplot2::labs(subtitle = \"N = 10\")\n\np50 <-\n  ggplot2::ggplot(data =  d2_b[1:50, ], \n         ggplot2::aes(x = weight_c, y = height)) +\n  ggplot2::geom_abline(data = post050_m4.3b |> dplyr::slice(1:20),\n              ggplot2::aes(intercept = b_Intercept, slope = b_weight_c),\n              linewidth = 1/3, alpha = .3) +\n  ggplot2::geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  ggplot2::coord_cartesian(xlim = base::range(d2_b$weight_c),\n                  ylim = base::range(d2_b$height)) +\n  ggplot2::labs(subtitle = \"N = 50\")\n\np150 <-\n  ggplot2::ggplot(data =  d2_b[1:150, ], \n         ggplot2::aes(x = weight_c, y = height)) +\n  ggplot2::geom_abline(data = post150_m4.3b |> dplyr::slice(1:20),\n              ggplot2::aes(intercept = b_Intercept, slope = b_weight_c),\n              linewidth = 1/3, alpha = .3) +\n  ggplot2::geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  ggplot2::coord_cartesian(xlim = base::range(d2_b$weight_c),\n                  ylim = base::range(d2_b$height)) +\n  ggplot2::labs(subtitle = \"N = 150\")\n\np352 <- \n  ggplot2::ggplot(data =  d2_b[1:352, ], \n         ggplot2::aes(x = weight_c, y = height)) +\n  ggplot2::geom_abline(data = post352_m4.3b |> dplyr::slice(1:20),\n              ggplot2::aes(intercept = b_Intercept, slope = b_weight_c),\n              linewidth = 1/3, alpha = .3) +\n  ggplot2::geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  ggplot2::coord_cartesian(xlim = base::range(d2_b$weight_c),\n                  ylim = base::range(d2_b$height)) +\n  ggplot2::labs(subtitle = \"N = 352\")\n\n## 4. display plots {patchwork} ########\nlibrary(patchwork)\n(p10 + p50 + p150 + p352) &\n  ggplot2::scale_x_continuous(\"weight\",\n                     breaks = c(-10, 0, 10),\n                     labels = labels) &\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![](04-geocentric-models_files/figure-html/chap04-plot-sampled-lines-m4-3b-1.png){width=672}\n:::\n:::\n\n:::\n:::\n\n> \"Notice that the cloud of regression lines grows more compact as the\n> sample size increases. This is a result of the model growing more\n> confident about the location of the mean.\" ([McElreath, 2020, p.\n> 102](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=121&annotation=92MJX4AG))\n:::\n:::\n:::\n\n\n\n#### Plotting regression intervals and contours\n\n> “The cloud of regression lines in @exm-chap04-uncertainty-around-mean is an appealing display, because it communicates uncertainty about the relationship in a way that many people find intuitive. But it’s more common, and often much clearer, to see the uncertainty displayed by plotting an interval or contour around the average regression line.” ([McElreath, 2020, p. 102](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=121&annotation=EUY9378D))\n\n:::::{.my-procedure}\n:::{.my-procedure-header}\n:::::: {#prp-chap04-generate-predictions}\n: Generating predictions and intervals from the posterior of a fit model\n::::::\n:::\n::::{.my-procedure-container}\n\n> 1. Use link to generate distributions of posterior values for $\\mu$. The default behavior of link is to use the original data, so you have to pass it a list of new horizontal axis values you want to plot posterior predictions across. \n> 2. Use summary functions like `base::mean()` or `rethinking::PI()` to find averages and lower and upper bounds of $\\mu$ for each value of the predictor variable. \n> 3. Finally, use plotting functions like `graphics::lines()` and `rethinking::shade()` to draw the lines and intervals. Or you might plot the distributions of the predictions, or do further numerical calculations with them. ([McElreath, 2020, p. 107](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=126&annotation=GHV2HBLI))\n::::\n:::::\n\nTo understand the first task of @prp-chap04-generate-predictions McElreath explains several preparation steps. As there are for the whole procedure nine different steps I will separate original and tidyverse approach into @exm-chap04-generating-predictions-and-intervals-m4-3a (Original) and @exm-chap04-generating-predictions-and-intervals-m4-3b (Tidyverse).\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap04-generating-predictions-and-intervals-m4-3a}\na: Plotting regression intervals and contours (Original)\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### mean\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-predict-mean-50-m4-3a}\na: Calculate uncertainty around the average regression line at mean of 50kg (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-predict-mean-50-m4-3a_f34f18281b5400acc21612e679382d2e'}\n\n```{.r .cell-code}\n## R code 4.50a ##########################\n\nset.seed(4)                                             # 1\npost_m4.3a <- rethinking::extract.samples(m4.3a)        # 2\nmu_at_50_a <- post_m4.3a$a + post_m4.3a$b * (50 - xbar) # 3\nstr(mu_at_50_a)                                         # 4\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  num [1:10000] 159 159 160 159 160 ...\n```\n\n\n:::\n:::\n\n\nComments for the different R code lines:\n\n1.  Set seed for exact reproducibility\n2.  Repeating the code for drawing (extracting and collecting) from the\n    fitted model `m4.3a` (already done in @fig-raw-data-line-m4-3a). `extract.samples()` returns from a `map` object a data.frame containing samples for each parameter in the posterior distribution. These samples are cleaned of dimension attributes and the `lp__`, `dev`, and `log_lik` traces that are used internally. For `map` and other types, it uses the variance-covariance matrix and coefficients to define a multivariate Gaussian posterior to draw $n$ samples from.\n3.  The code to the right of the `<-` takes it's form from the equation\n    for $\\mu_{i} = \\alpha + \\beta(x_{i} - \\overline{x})$. The value of\n    $x_{i}$ in this case is $50$.\n4.  The result is a vector of predicted means, one for each random\n    sample from the posterior. Since joint `a` and `b` went into\n    computing each, the variation across those means incorporates the\n    uncertainty in and correlation between both parameters.\n    [@mcelreath2023a, p.103 and help file from `extract.samples()`]\n\n\n::::\n:::::\n\n\n###### dens\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-density-m4-3a}\na: The quadratic approximate posterior distribution of the mean height, $\\mu$, when weight is $50$ kg. This distribution represents the relative plausibility of different values of the mean (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-density-mean-50-m4-3a_73672785c657e17d7dc07dd097dd00f1'}\n\n```{.r .cell-code}\n## R code 4.51a ##################\nrethinking::dens(mu_at_50_a, col = rethinking::rangi2, lwd = 2, xlab = \"mu|weight=50\")\n```\n\n::: {.cell-output-display}\n![The quadratic approximate posterior distribution of the mean height, μ, when weight is 50 kg. This distribution represents the relative plausibility of different values of the mean (Original)](04-geocentric-models_files/figure-html/fig-density-mean-50-m4-3a-1.png){#fig-density-mean-50-m4-3a width=672}\n:::\n:::\n\n\n::::\n:::::\n\n###### PI\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-PI-mu-at-50-m4-3a}\na: 89% compatibility interval of μ at 50 kg (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/PI-mu-at-50-m4-3a_46f5dc3e2b620b57ed502dc9823b9665'}\n\n```{.r .cell-code}\n## R code 4.52a ##############\nrethinking::PI(mu_at_50_a, prob = 0.89)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>       5%      94% \n#> 158.5758 159.6682\n```\n\n\n:::\n:::\n\n::::\n:::::\n\n> “What these numbers mean is that the central 89% of the ways for the model to produce the data place the average height between about 159 cm and 160 cm (conditional on the model and data), assuming the weight is 50 kg. \n> That’s good so far, but we need to repeat the above calculation for every weight value on the horizontal axis, not just when it is 50 kg.” ([McElreath, 2020, p. 104](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=123&annotation=9XI839NS))\n\n###### link1\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-link1-m4-3a}\na: Calculate $\\mu$ for each case in the data and sample from the posterior distribution (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-link1-m4-3a_42d1f5415819df1e18b61d03046f5268'}\n\n```{.r .cell-code}\n## R code 4.53a ##############\nmu_352_m4.3a <- rethinking::link(m4.3a)\nstr(mu_352_m4.3a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  num [1:1000, 1:352] 157 157 157 158 157 ...\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n> “You end up with a big matrix of values of $\\mu$. Each row is a sample from the posterior distribution. The default is 1000 samples, but you can use as many or as few as you like. Each column is a case (row) in the data. There are 352 rows in `d2_a`, corresponding to 352 individuals. So there are 352 columns in the matrix `mu_m4.3a` above.” ([McElreath, 2020, p. 105](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=124&annotation=DC889A57))\n\n> “The function `link()` provides a posterior distribution of $\\mu$ for each case we feed it. So above we have a distribution of $\\mu$ for each individual in the original data. We actually want something slightly different: a distribution of $\\mu$ for each unique weight value on the horizontal axis. It’s only slightly harder to compute that, by just passing `link()` some new data.” ([McElreath, 2020, p. 105](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=124&annotation=MSEJ4D4N)) (See next tab \"link2\".)\n\n###### link2\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-link2-m4-3a}\na: Calculate a distribution of $\\mu$ for each unique weight value on the horizontal axis (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-link2-m4-3a_85fb7eb243914c648d5c12c82e4a5214'}\n\n```{.r .cell-code}\n## R code 4.54a ###############################\n# define sequence of weights to compute predictions for\n# these values will be on the horizontal axis\nweight.seq <- seq(from = 25, to = 70, by = 1)\n\n# use link to compute mu\n# for each sample from posterior\n# and for each weight in weight.seq\nmu_46_m4.3a <- rethinking::link(m4.3a, data = data.frame(weight = weight.seq))\nstr(mu_46_m4.3a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  num [1:1000, 1:46] 136 136 137 136 136 ...\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\nAnd now there are only 46 columns in $\\mu$, because we fed it 46 different\nvalues for weight.\n\n###### plot1\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-dist-100-m4-3a}\na: The first 100 values in the distribution of $\\mu$ at each weight value (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-dist-mu-height-100-m4-3a_c7833269612c9873e7cc39097f44aaa5'}\n\n```{.r .cell-code}\n## R code 4.55a ##################\n# use type=\"n\" to hide raw data\nbase::plot(height ~ weight, d2_a, type = \"n\")\n\n# loop over samples and plot each mu value\nfor (i in 1:100) {\n  graphics::points(weight.seq, \n                   mu_46_m4.3a[i, ], \n                   pch = 16, \n                   col = rethinking::col.alpha(rethinking::rangi2, 0.1))\n}\n```\n\n::: {.cell-output-display}\n![The first 100 values in the distribution of μ at each weight value. (Original)](04-geocentric-models_files/figure-html/fig-dist-mu-height-100-m4-3a-1.png){#fig-dist-mu-height-100-m4-3a width=672}\n:::\n:::\n\n\n\n\n::::\n:::::\nAt each weight value in `weight.seq`, a pile of computed $\\mu$ values are\nshown. Each of these piles is a Gaussian distribution, like that in tab \"dens\" of\n@exm-chap04-gnerating-predictions-and-intervals. You can see now that the amount of\nuncertainty in $\\mu$ depends upon the value of `weight`. And this is the\nsame fact you saw in @exm-chap04-uncertainty-around-mean.\n\n###### sum\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-sum-dist-weight-m4-3a}\na: Summary of the distribution for each weight value (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-sum-dist-weight-m4-3a_c001bcd1891d3e9ff45f0b0121710a60'}\n\n```{.r .cell-code}\n## R code 4.56a #####################\n# summarize the distribution of mu\nmu_46.mean_m4.3a <- apply(mu_46_m4.3a, 2, mean)         # 1\nmu_46.PI_m4.3a <- apply(mu_46_m4.3a, 2, \n                      rethinking::PI, prob = 0.89)      # 2\nstr(mu_46_m4.3a)                                        # 3\nhead(mu_46.mean_m4.3a)                                  # 4\nhead(mu_46.PI_m4.3a)[ , 1:6]                            # 5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  num [1:1000, 1:46] 136 136 137 136 136 ...\n#> [1] 136.5276 137.4315 138.3354 139.2393 140.1433 141.0472\n#>         [,1]     [,2]     [,3]     [,4]     [,5]     [,6]\n#> 5%  135.1206 136.0954 137.0654 138.0321 139.0101 139.9782\n#> 94% 137.9998 138.8414 139.6767 140.5174 141.3442 142.1896\n```\n\n\n:::\n:::\n\n\n1.  Read `apply(mu_46_m4.3a,2,mean)` as \"compute the mean of each column\n    (dimension '2') of the matrix `mu46_m4.3a`\". Now `mu_46.mean_m4.3a` contains the\n    average $\\mu$ at each weight value.\n2.  `mu_46.PI_m4.3a` contains 89% lower and upper bounds for each weight value.\n3.  Displaying the structure shows that there are only 46 columns in `mu_46_m4-3a`, because we fed it 46 different values for `weight`.\n4.  Display the means of first six columns of the matrix `mu46_m4.3a` (= `mu_46.mean_m4.3a)`\n5.  Display the first six 89% PI lower and upper bounds for each `weight` value. \n\n***\n`mu_46.mean_m4.3a` and `mu_46.PI_m4.3a` are just different kinds of summaries of the distributions in `mu_46_m4.3a`, with each column being for a different weight value. These summaries are only summaries. The \"estimate\" is the entire distribution.\n\n\n::::\n:::::\n\n###### plot2\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-sum-shaded-m4-3a}\na: The !Kung height data with 89% compatibility interval of the mean indicated by the shaded region (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-sum-shaded-m4-3a_a97c3d2464f562d81d3619d882637945'}\n\n```{.r .cell-code}\n## R code 4.57a ###########################\n# plot raw data\n# fading out points to make line and interval more visible\nplot(height ~ weight, data = d2_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))\n\n# plot the MAP line, aka the mean mu for each weight\ngraphics::lines(weight.seq, mu_46.mean_m4.3a)\n\n# plot a shaded region for 89% PI\nrethinking::shade(mu_46.PI_m4.3a, weight.seq)\n```\n\n::: {.cell-output-display}\n![The !Kung height data with 89% compatibility interval of the mean indicated by the shaded region. Compare with tab 'dist1'](04-geocentric-models_files/figure-html/fig-sum-shaded-m4-3a-1.png){#fig-sum-shaded-m4-3a width=672}\n:::\n:::\n\n\n::::\n:::::\n\n###### link3\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-own-link-m4-3a}\n: Writing your own `link()` function (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-own-link-m4-3a_66199a12535638f4a77f9fa92e4d9142'}\n\n```{.r .cell-code}\n## R code 4.58a ################################\npost_m4.3a <- rethinking::extract.samples(m4.3a)\nmu.link_m4.3a <- function(weight) post_m4.3a$a + post_m4.3a$b * (weight - xbar)\nweight.seq <- seq(from = 25, to = 70, by = 1)\nmu3_m4.3a <- sapply(weight.seq, mu.link_m4.3a)\nmu3.mean_m4.3a <- apply(mu3_m4.3a, 2, mean)\nmu3.CI_m4.3a <- apply(mu3_m4.3a, 2, rethinking::PI, prob = 0.89)\nhead(mu3.mean_m4.3a)\nhead(mu3.CI_m4.3a)[ , 1:6]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 136.5589 137.4615 138.3641 139.2666 140.1692 141.0718\n#>         [,1]     [,2]     [,3]     [,4]     [,5]     [,6]\n#> 5%  135.1421 136.1088 137.0737 138.0387 139.0046 139.9712\n#> 94% 137.9686 138.8058 139.6454 140.4852 141.3257 142.1673\n```\n\n\n:::\n:::\n\n\nAnd the values in `mu3.mean_m4.3a` and `mu3.CI_m4.3a` should be very similar\n(allowing for simulation variance) to what you got the automated way,\nusing `rethinking::link()` in tab 'sum'.\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n\nWhat follows now is the tidyverse procedure: Since we used `weight_c` to fit our model, we might first want to understand what exactly the mean value is for weight: `mean(d2_b$weight)` = 44.9904855. \"Just a hair under 45. If we're interested in $\\mu$ at `weight` = 50, that implies we're also interested in $\\mu$ at `weight_c` + 5.01. Within the context of our model, we compute this with\n$\\alpha + \\beta \\cdot 5.01$.\" ([Kurz](https://bookdown.org/content/4857/geocentric-models.html#plotting-regression-intervals-and-contours.))\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap04-generating-predictions-and-intervals-m4-3b}\nb: Plotting regression intervals and contours (Tidyverse)\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### mean\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-predict-mean-50-m4-3b}\na: Calculate uncertainty around the average regression line at mean of 50kg (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-predict-mean-50-m4-3b_e46ab5b4ecbc38125b680cdf4e5b8af1'}\n\n```{.r .cell-code}\nmu_at_50_b <- \n  post_m4.3b |> \n  dplyr::mutate(mu_at_50_b = b_Intercept + b_weight_c * 5.01, .keep = \"none\")\n \nhead(mu_at_50_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 6 × 1\n#>   mu_at_50_b\n#>        <dbl>\n#> 1       159.\n#> 2       159.\n#> 3       159.\n#> 4       159.\n#> 5       159.\n#> 6       159.\n```\n\n\n:::\n:::\n\n\n\n::::\n:::::\n\n\n###### dens\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-density-m4-3b}\na: The quadratic approximate posterior distribution of the mean height, $\\mu$, when weight is $50$ kg. This distribution represents the relative plausibility of different values of the mean (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-density-m4-3b_6888667d276d4ec878aa6ab3647631ab'}\n\n```{.r .cell-code}\nmu_at_50_b |>\n  ggplot2::ggplot(ggplot2::aes(x = mu_at_50_b)) +\n  tidybayes::stat_halfeye(point_interval = tidybayes::mode_hdi, .width = .95,\n               fill = \"deepskyblue\") +\n  ggplot2::scale_y_continuous(NULL, breaks = NULL) +\n  ggplot2::xlab(expression(mu[\"height | weight = 50\"])) +\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![The quadratic approximate posterior distribution of the mean height, μ, when weight is 50 kg. This distribution represents the relative plausibility of different values of the mean. Tidyverse version](04-geocentric-models_files/figure-html/fig-chap04-density-m4-3b-1.png){#fig-chap04-density-m4-3b width=672}\n:::\n:::\n\nHere we expressed the 95% HPDIs on the density plot with `tidybayes::stat_halfeye()`. Since `tidybayes::stat_halfeye()` also returns a point estimate, we'll throw in the mode.\n\n::::\n:::::\n\n###### HPDI\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-PI-mu-at-50-m4-3a}\nb: 89% and 95% Highest Priority Intensity Intervals (HPDIs) of $\\mu$ at 50 kg (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-PI-mu-at-50-m4-3a_ae51754d2f64fa0d4aed4c3172217bc7'}\n\n```{.r .cell-code}\ntidybayes::mean_hdi(mu_at_50_b[, 1], .width = c(.89, .95))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 2 × 6\n#>   mu_at_50_b .lower .upper .width .point .interval\n#>        <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n#> 1       159.   159.   160.   0.89 mean   hdi      \n#> 2       159.   158.   160.   0.95 mean   hdi\n```\n\n\n:::\n:::\n\n\n\n::::\n:::::\n\n###### fitted1\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-fitted1-m4-3b}\nb: Calculate $\\mu$ for each case in the data and sample from the posterior distribution (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-fitted1-m4-3b_6e4259f8f26ec9feffa6f5ca505a5971'}\n\n```{.r .cell-code}\nmu2_m4.3b <- brms:::fitted.brmsfit(m4.3b, summary = F)\nstr(mu2_m4.3b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  num [1:4000, 1:352] 157 157 157 157 157 ...\n```\n\n\n:::\n:::\n\n\n***\n\nWith {**brms**} the equivalence for `rethinking::link()` (tab \"link1\" in @exm-chap04-generating-predictions-and-intervals-m4-3a) is the `fitted()` function.\n\nWith `brms:::fitted.brmsfit()`, it's quite easy to plot a regression\nline and its intervals. Just omit the `summary = T` argument.\n\n\n> \"When you specify `summary = F`, `brms:::fitted.brmsfit()` returns a\nmatrix of values with as many rows as there were post-warmup draws\nacross your Hamilton Monte Carlo (<a class='glossary' title='Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) method that uses the derivatives of the density function being sampled to generate efficient transitions spanning the posterior. It uses an approximate Hamiltonian dynamics simulation based on numerical integration which is then corrected by performing a Metropolis acceptance step. (Stan Reference Manual)'>HMC</a>) chains and as many columns as\nthere were cases in your analysis. Because we had 4,000 post-warmup\ndraws and $n=352$, `brms:::fitted.brmsfit()` returned a matrix of 4,000\nrows and 352 vectors. If you omitted the `summary = F` argument, the\ndefault is TRUE and `brms:::fitted.brmsfit()` will return summary\ninformation instead.\" ([Kurz](https://bookdown.org/content/4857/geocentric-models.html#plotting-regression-intervals-and-contours.))\n\n::::\n:::::\n\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! How to apply `fitted()`?\n:::\n::::{.my-watch-out-container}\nKurz applies the function `fitted()` in the code, but in the text he\nuses twice `brms::fitted()` which doesn't exist. I used\n`brms:::fitted.brmsfit()`. \n\nBut with `stats::fitted()` you will get the same result!\n\nThe object `m4.3b` is of class `brmsfit` but in the help file of\n`stats::fitted()` you can read: \"`fitted` is a generic function which\nextracts fitted values from objects returned by modeling functions.\n**All object classes which are returned by model fitting functions\nshould provide a `fitted` method.** ([Extract Model Fitted Values](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/fitted.values.html), emphasis is mine)\n\nMy interpretation therefore is that `stats::fitted()` is using\n`brms:::fitted.brmsfit()`. That's why the results are identical.\n::::\n:::::\n\n###### fitted2\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-fitted2-m4-3b}\nb: Calculate a distribution of $\\mu$ for each unique weight value on the horizontal axis (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-fitted2-m4-3b_056de8cce5cee9a8c7b5bf0f7681c574'}\n\n```{.r .cell-code}\nweight_seq <- \n  tibble::tibble(weight = 25:70) |> \n  dplyr::mutate(weight_c = weight - base::mean(d2_b$weight))\n\nmu3_m4.3b <-\n  brms:::fitted.brmsfit(m4.3b,\n         summary = F,\n         newdata = weight_seq) |>\n  tibble::as_tibble() |>\n  # here we name the columns after the `weight` values from which they were computed\n  rlang::set_names(25:70) |> \n  dplyr::mutate(iter = 1:dplyr::n())\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n#> `.name_repair` is omitted as of tibble 2.0.0.\n#> ℹ Using compatibility `.name_repair`.\n```\n\n\n:::\n\n```{.r .cell-code}\nmu3_m4.3b[1:6, 1:6]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 6 × 6\n#>    `25`  `26`  `27`  `28`  `29`  `30`\n#>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1  137.  138.  139.  140.  141.  142.\n#> 2  137.  138.  139.  140.  141.  142.\n#> 3  136.  137.  138.  139.  140.  141.\n#> 4  137.  138.  139.  139.  140.  141.\n#> 5  137.  138.  139.  140.  141.  141.\n#> 6  136.  137.  138.  139.  140.  141.\n```\n\n\n:::\n:::\n\n\n***\n\nMuch like `rethinking::link()`, `brms:::fitted.brmsfit()` can\naccommodate custom predictor values with its `newdata` argument.\n\nTo differentiate: The {rethinking} version used the variable `weight.seq` whereas here I am using `weight_seq`.\n\n::::\n:::::\n\n###### plot1\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-dist-100-m4-3b}\nb: The first 100 values in the distribution of $\\mu$ at each weight value (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-dist-100-m4-3b_3f8cedfaf39e7eefa2ec8da9528a1481'}\n\n```{.r .cell-code}\nmu4_m4.3b <- \n  mu3_m4.3b |>\n  tidyr::pivot_longer(-iter,\n               names_to = \"weight\",\n               values_to = \"height\") |> \n  # we might reformat `weight` to numerals\n  dplyr::mutate(weight = base::as.numeric(weight))\n\nd2_b |>\n  ggplot2::ggplot(ggplot2::aes(x = weight, y = height)) +\n  ggplot2::geom_point(data = mu4_m4.3b |> dplyr::filter(iter < 101), \n             color = \"navyblue\", alpha = .05) +\n  ggplot2::coord_cartesian(xlim = c(30, 65)) +\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![The first 100 values in the distribution of μ at each weight value (Tidyverse)](04-geocentric-models_files/figure-html/fig-chap04-dist-100-m4-3b-1.png){#fig-chap04-dist-100-m4-3b width=672}\n:::\n:::\n\n::::\n:::::\n\nBut did a little more data processing with the aid\nof `tidyr::pivot_longer()`, which will convert the data from the wide\nformat to the long format.\n\n:::::{.my-resource}\n:::{.my-resource-header}\nWide and long data\n:::\n::::{.my-resource-container}\nIf you are new to the distinction between wide and long data, you can\nlearn more from the \n\n- [Pivot data from wide to\nlong](https://tidyr.tidyverse.org/reference/pivot_longer.html) vignette\nfrom the tidyverse team (2020); \n- Simon Ejdemyr's blog post, [Wide & long\ndata](https://sejdemyr.github.io/r-tutorials/basics/wide-and-long/) or\n- Karen Grace-Martin's blog post, [The wide and long data format for\nrepeated measures\ndata](https://www.theanalysisfactor.com/wide-and-long-data/).\n::::\n:::::\n\n###### fitted3\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-fitted3-mean-m4-3b}\nb: Draws predicted *mean* response values from the posterior predictive distribution (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-fitted3-mean-m4-3b_c9b3ee6b99233174ea1cec27b43fc08f'}\n\n```{.r .cell-code}\nmu_summary_m4.3b <-\n  brms:::fitted.brmsfit(m4.3b, \n         newdata = weight_seq,\n         probs = c(0.055, 0.945)) |>\n  tibble::as_tibble() |>\n  dplyr::bind_cols(weight_seq)\n\nset.seed(4)\nmu_summary_m4.3b |> \n  dplyr::slice_sample(n = 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 6 × 6\n#>   Estimate Est.Error  Q5.5 Q94.5 weight weight_c\n#>      <dbl>     <dbl> <dbl> <dbl>  <int>    <dbl>\n#> 1     146.     0.506  145.  146.     35    -9.99\n#> 2     138.     0.813  137.  140.     27   -18.0 \n#> 3     142.     0.655  141.  143.     31   -14.0 \n#> 4     163.     0.467  162.  163.     54     9.01\n#> 5     170.     0.769  169.  171.     62    17.0 \n#> 6     137.     0.853  136.  139.     26   -19.0\n```\n\n\n:::\n:::\n\n\n\n:::::{.my-note}\n:::{.my-note-header}\nHow to change the {**brms**) default CI value form 95% to 89%?\n:::\n::::{.my-note-container}\n\nIn {**brms**} I had to change the `probs` argument to `c(0.055, 0.945)` and the resulting third and fourth vectors from the `fitted()` object to `Q5.5` and `Q94.5`. This was necessary to get the same 89% interval as in the book version. The default `probs` value for `brms:::fitted.brmsfit()` would have been `c(0.025, 0.975)` resulting in quantiles of `Q2.5` and `Q97.5`. The Q prefix stands for quantile. See [Rename summary columns of predict() and related methods](https://github.com/paul-buerkner/brms/issues/425).\n\n::::\n:::::\n\n\n\n\n\n\n::::\n:::::\n\n###### plot2\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-sum-shaded-m4-3b}\na: The !Kung height data with 89% compatibility interval of the mean indicated by the shaded region (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-sum-shaded-m4-3b_ab6a228e861c98c21d0626724b27b36c'}\n\n```{.r .cell-code}\nd2_b |>\n  ggplot2::ggplot(ggplot2::aes(x = weight, y = height)) +\n  ggplot2::geom_smooth(data = mu_summary_m4.3b,\n              ggplot2::aes(y = Estimate, ymin = Q5.5, ymax = Q94.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/2) +\n  ggplot2::geom_point(color = \"navyblue\", shape = 1, size = 1.5, alpha = 2/3) +\n  ggplot2::coord_cartesian(xlim = range(d2_b$weight)) +\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Plot of the summaries on top of the !Kung height data again, now with 89% compatibility interval of the mean indicated by the shaded region. Compare this region to the distributions of blue points in tab 'plot1'](04-geocentric-models_files/figure-html/fig-sum-shaded-m4-3b-1.png){#fig-sum-shaded-m4-3b width=672}\n:::\n:::\n\n\n::::\n:::::\n\nIf you wanted to use intervals other than the default 89% ones, you'd\ninclude the `probs` argument like this:\n`brms:::fitted.brmsfit(b4.3, newdata = weight.seq, probs = c(.25, .75))`.\nThe resulting third and fourth vectors from the `fitted()` object would\nbe named `Q25` and `Q75` instead of the default `Q5.5` and `Q94.5`. The\n[Q prefix](https://github.com/paul-buerkner/brms/issues/425) stands for\nquantile.\n\nSimilar to `rethinking::link()`, `brms:::fitted.brmsfit()` uses the\nformula from your model to compute the model expectations for a given\nset of predictor values. I used it a lot in this project. If you follow\nalong, you'll get a good handle on it. \n\n:::::{.my-resource}\n:::{.my-resource-header}\nHow to use `brms:::fitted.brmsfit()`\n:::\n::::{.my-resource-container}\n\nTo dive deeper about the `fitted()` function, you can [go for the\ndocumentation](https://rdrr.io/cran/brms/man/fitted.brmsfit.html).\nThough Kurz won't be using it in this project, {**brms**} he informs users that `fitted()` is also an alias for the `brms::posterior_epred()` function, about which you might [learn more here](https://rdrr.io/cran/brms/man/posterior_epred.brmsfit.html). Users\ncan always learn more about them and other functions in the [{**brms**}\nreference manual](https://cran.r-project.org/web/packages/brms/brms.pdf).\n\n::::\n:::::\n\n\n\n:::\n\n::::\n:::::\n\n\n\n#### Prediction intervals\n\n> “What you’ve done so far is just use samples from the posterior to visualize the uncertainty in $μ_{i}$, the linear model of the mean.” ([McElreath, 2020, p. 107](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=126&annotation=DK5AQD7T))\n\n> “Now let’s walk through generating an 89% prediction interval for actual heights, not just the average height, $\\mu$. This means we’ll incorporate the standard deviation $\\sigma$ and its uncertainty as well.” ([McElreath, 2020, p. 107](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=126&annotation=YPK9QEW5))\n\n\n::: my-procedure\n::: my-procedure-header\n::: {#prp-chap04-sim-heights-m3-4}\n: Simulating heights\n:::\n:::\n\n::: my-procedure-container\n1.  For any unique weight value, you sample from a Gaussian distribution\n    -   with the correct mean $\\mu$ for that weight,\n    -   using the correct value of $\\sigma$ sampled from the posterior\n        distribution.\n2.  Do this\n    -   for every sample from the posterior,\n    -   for every weight value.\n3.  You will end up with a collection of simulated heights\n    -   that embody the uncertainty in the posterior\n    -   as well as the uncertainty in the Gaussian distribution of\n        heights.\n:::\n:::\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap04-predict-intervals}\n: Generate an 89% prediction interval for actual heights\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### sim1 (O)\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-sim-post-m4-3a}\na: Simulation of the posterior observations (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-sim-post-m4-3a_270a2f4407181d6164417b5fbea53bf0'}\n\n```{.r .cell-code}\n## R code 4.59a #######################\nsim.height_m4.3a <- rethinking::sim(m4.3a, data = list(weight = weight.seq))\nstr(sim.height_m4.3a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  num [1:1000, 1:46] 138 133 137 136 134 ...\n```\n\n\n:::\n:::\n\n\nThis matrix is much like the earlier one in tab 'sum' of @exm-chap04-generating-predictions-and-intervals-m4-3a, but it contains\nsimulated heights, not distributions of plausible average height, $\\mu$.\n\n\n::::\n:::::\n\n\n###### PI (O)\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-sum-pi-heights-m4-3a}\na: Summarize simulated heights of the 89% posterior prediction interval of\nobservable heights (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-sum-pi-heights-m4-3a_0502195f7ca04b34538ae53151393672'}\n\n```{.r .cell-code}\n## R code 4.60a ###################\nheight.PI_m4.3a <- apply(sim.height_m4.3a, 2, rethinking::PI, prob = 0.89)\n```\n:::\n\n\n\n`height.PI_m4-3a` contains the 89% posterior prediction interval of\nobservable (according to the model) heights, across the values of weight\nin `weight.seq`.\n\n::::\n:::::\n\n###### plot1 (0)\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-chap04-plot-heights-m4-3a}\na: 89% prediction interval for height, as a function of weight by sampling 1e3 times (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-plot-heights-m4-3a_1c89dd0fe7b7622edd061ad9b211dc57'}\n\n```{.r .cell-code}\n## R code 4.61a ##################\n# plot raw data\nplot(height ~ weight, d2_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))\n\n# draw MAP line\ngraphics::lines(weight.seq, mu_46.mean_m4.3a)\n\n# draw PI region for line\n# rethinking::shade(mu_46.PI_m4.3a, weight.seq)\n\n# draw HPDI region for line\nmu_46.HPDI_m4.3a <- apply(mu_46_m4.3a, 2, \n                      rethinking::HPDI, prob = .89)\nrethinking::shade(mu_46.HPDI_m4.3a, weight.seq)\n\n# draw PI region for simulated heights\nrethinking::shade(height.PI_m4.3a, weight.seq)\n```\n\n::: {.cell-output-display}\n![89% prediction interval for height, as a function of weight. The solid line is the average line for the mean height at each weight. The two shaded regions show different 89% plausible regions. The narrow shaded interval around the line is the distribution of μ. The wider shaded region represents the region within which the model expects to find 89% of actual heights in the population, at each weight.](04-geocentric-models_files/figure-html/fig-chap04-plot-heights-m4-3a-1.png){#fig-chap04-plot-heights-m4-3a width=672}\n:::\n:::\n\n\n> “Notice that the outline for the wide shaded interval is a little rough. This is the simulation variance in the tails of the sampled Gaussian values. If it really bothers you, increase the number of samples you take from the posterior distribution. The optional $n$ parameter for `sim.height_m4.3a` controls how many samples are used.” ([McElreath, 2020, p. 109](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=128&annotation=69UVQJZI))\n\nTry for example $1e4$ samples. \n\n> “Run the plotting code again, and you’ll see the shaded boundary smooth out some.” ([McElreath, 2020, p. 109](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=128&annotation=ACYS7GCU)) \n\nSee the next tab 'plot2'.\n\n::::\n:::::\n\n###### plot2 (0)\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-chap04-plot-heights-m4-3a}\na: 89% prediction interval for height, as a function of weight by sampling 1e4 times (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-plot-heights2-m4-3a_662b1b73989b18d28074e568b0cec38b'}\n\n```{.r .cell-code}\n## R code 4.59a adapted ################\nsim2.height_m4.3a <- rethinking::sim(m4.3a, data = list(weight = weight.seq), n = 1e4)\n\n## R code 4.60a adapted ################\nheight2.PI_m4.3a <- apply(sim2.height_m4.3a, 2, rethinking::PI, prob = 0.89)\n\n## R code 4.61a ##################\n\n# plot raw data\nplot(height ~ weight, d2_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))\n\nmu_46.HPDI_m4.3a <- apply(mu_46_m4.3a, 2,\n                      rethinking::HPDI, prob = 0.89)\n\n# draw MAP line\ngraphics::lines(weight.seq, mu_46.mean_m4.3a)\n\n# draw HPDI region for line\nrethinking::shade(mu_46.HPDI_m4.3a, weight.seq)\n\n# draw PI region for simulated heights\nrethinking::shade(height2.PI_m4.3a, weight.seq)\n```\n\n::: {.cell-output-display}\n![89% prediction interval for height, as a function of weight. Shaded boundary smoothed out by sampling 1e4 times instead of the standard value of 1e3](04-geocentric-models_files/figure-html/fig-chap04-plot-heights2-m4-3a-1.png){#fig-chap04-plot-heights2-m4-3a width=672}\n:::\n:::\n\n::::\n:::::\n\n> “With extreme percentiles, it can be very hard to get out all of the roughness. Luckily, it hardly matters, except for aesthetics. Moreover, it serves to remind us that all statistical inference is approximate. The fact that we can compute an expected value to the 10th decimal place does not imply that our inferences are precise to the 10th decimal place” ([McElreath, 2020, p. 109](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=128&annotation=NZMP7RDP))\n\n###### sim2 (0)\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-sim2-heights-m4-3a}\na: Writing you own `rethinking:sim()` function\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/writing-sim-function-a_90cef423326451051e3af17b8219fa46'}\n\n```{.r .cell-code}\n## R code 4.63a ########################################\n\n# post <- extract.samples(m4.3)\n# weight.seq <- 25:70\npost_m4.3a <- rethinking::extract.samples(m4.3a)\nsim.height2_m4.3a <- sapply(weight.seq, function(weight) {\n  rnorm(\n    n = nrow(post_m4.3a),\n    mean = post_m4.3a$a + post_m4.3a$b * (weight - xbar),\n    sd = post_m4.3a$sigma\n  )\n})\nheight2.PI_m4.3a <- apply(sim.height2_m4.3a, 2, rethinking::PI, prob = 0.89)\nhead(height.PI_m4.3a)[ , 1:6]\nhead(height2.PI_m4.3a)[ , 1:6]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>         [,1]     [,2]     [,3]     [,4]     [,5]     [,6]\n#> 5%  128.3733 129.3550 129.9023 131.2129 132.5578 132.3555\n#> 94% 144.7671 145.5569 146.4105 147.6139 147.9741 148.8141\n#>         [,1]     [,2]     [,3]     [,4]     [,5]     [,6]\n#> 5%  128.4286 129.1754 130.1301 131.2844 131.9124 132.9920\n#> 94% 144.7992 145.5424 146.7206 147.4119 148.2944 149.4395\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\nThe small differences are the result of the randomized sampling process.\n\n\n###### predict1 (T)\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-predict-post-m4-3b}\nb: Predict the posterior observations (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-predict-post-m4-3b_62e858ac64c4ec1d5e745483ea5ccd26'}\n\n```{.r .cell-code}\npred_height_m4.3b <-\n  brms:::predict.brmsfit(m4.3b,\n          newdata = weight_seq,\n          probs = c(0.055, 0.945)) |>\n  tibble::as_tibble() |>\n  dplyr::bind_cols(weight_seq)\n\nset.seed(4)\npred_height_m4.3b |>\n  dplyr::slice_sample(n = 6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 6 × 6\n#>   Estimate Est.Error  Q5.5 Q94.5 weight weight_c\n#>      <dbl>     <dbl> <dbl> <dbl>  <int>    <dbl>\n#> 1     145.      5.08  137.  153.     35    -9.99\n#> 2     138.      5.27  130.  147.     27   -18.0 \n#> 3     142.      5.16  134.  150.     31   -14.0 \n#> 4     163.      4.98  155.  171.     54     9.01\n#> 5     170.      5.04  162.  178.     62    17.0 \n#> 6     137.      5.19  129.  146.     26   -19.0\n```\n\n\n:::\n:::\n\n\nThe `predict()` code looks a lot like what we used for the tab in fitted3 of @exm-chap04-generating-predictions-and-intervals-m4-3b.\n\n\n::::\n:::::\n\n:::::{.my-important}\n:::{.my-important-header}\n{**brms**} equivalence of `rethinking::link()` and `rethinking::sim()`\n:::\n::::{.my-important-container}\nMuch as `brms:::fitted.brmsfit()` was our analogue to\n`rethinking::link()`, `brms:::predict.brmsfit()` is our analogue to\n`rethinking::sim()`. ([Kurz](https://bookdown.org/content/4857/geocentric-models.html#prediction-intervals.))\n\n***\n\n- **link**: Computes the value of each linear model at each sample for each case in the data.\n- **sim**: Uses the model definition from a `map` or `map2stan` fit to simulate outcomes that average over the posterior distribution. \n- **fitted.brmsfit**: Compute posterior draws of the expected value of the posterior predictive distribution. Returns an array of predicted *mean* response values.\n- **predict.brmsfit**: Compute posterior draws of the posterior predictive distribution. Returns an array of predicted response values. \n\n\n::::\n:::::\n\n###### plot1 (T)\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-chap04-predict-plot1-m4-3b}\nb: Plot 89% prediction interval for height, as a function of weight with 2e3 iterations (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-predict-plot1-m4-3b_db618ab69bac14ceecfe644ec53f0dbb'}\n\n```{.r .cell-code}\nd2_b |>\n  ggplot2::ggplot(ggplot2::aes(x = weight)) +\n  ggplot2::geom_ribbon(data = pred_height_m4.3b, \n              ggplot2::aes(ymin = Q5.5, ymax = Q94.5),\n              fill = \"grey83\") +\n  ggplot2::geom_smooth(data = mu_summary_m4.3b,\n              ggplot2::aes(y = Estimate, ymin = Q5.5, ymax = Q94.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/2) +\n  ggplot2::geom_point(ggplot2::aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 2/3) +\n  ggplot2::coord_cartesian(xlim = base::range(d2_b$weight),\n                  ylim = base::range(d2_b$height)) +\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![89% prediction interval for height, as a function of weight. The solid line is the average line for the mean height at each weight. The two shaded regions show different 89% plausible regions. The narrow shaded interval around the line is the distribution of μ. The wider shaded region represents the region within which the model expects to find 89% of actual heights in the population, at each weight. (Tidyverse)](04-geocentric-models_files/figure-html/fig-chap04-predict-plot1-m4-3b-1.png){#fig-chap04-predict-plot1-m4-3b width=672}\n:::\n:::\n\nTo smooth out the rough shaded interval we would have in the {**brms**}\nmodel fitting approach to refit `m4.3b` into `m4.3b_smooth` after specifying a larger number of post-warmup iterations with alterations to the `iter` and `warmup`\nparameters.\n\n::::\n:::::\n\n###### plot2 (T)\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-chap04-predict-plot2-m4-3b}\nb: Plot 89% prediction interval for height, as a function of weight with 2e4 iterations (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-predict-plot2-m4-3b_d28c1487fed4ffabca92aa35280f45bc'}\n\n```{.r .cell-code}\nm4.3b_smooth <- \n  brms::brm(data = d2_b, \n      family = gaussian,\n      height ~ 1 + weight_c,\n      prior = c(brms::prior(normal(178, 20), class = Intercept),\n                brms::prior(lognormal(0, 1), class = b, lb = 0),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 20000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"brm_fits/m04.03b_smooth\")\n\nmu_summary_m4.3b_smooth <-\n  brms:::fitted.brmsfit(m4.3b_smooth, \n         newdata = weight_seq,\n         probs = c(0.055, 0.945)) |>\n  tibble::as_tibble() |>\n  dplyr::bind_cols(weight_seq)\n\npred_height_m4.3b_smooth <-\n  brms:::predict.brmsfit(m4.3b_smooth,\n          newdata = weight_seq,\n          probs = c(0.055, 0.945)) |>\n  tibble::as_tibble() |>\n  dplyr::bind_cols(weight_seq)\n\n\nd2_b |>\n  ggplot2::ggplot(ggplot2::aes(x = weight)) +\n  ggplot2::geom_ribbon(data = pred_height_m4.3b_smooth, \n              ggplot2::aes(ymin = Q5.5, ymax = Q94.5),\n              fill = \"grey83\") +\n  ggplot2::geom_smooth(data = mu_summary_m4.3b_smooth,\n              ggplot2::aes(y = Estimate, ymin = Q5.5, ymax = Q94.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/2) +\n  ggplot2::geom_point(ggplot2::aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 2/3) +\n  ggplot2::coord_cartesian(xlim = base::range(d2_b$weight),\n                  ylim = base::range(d2_b$height)) +\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![89% prediction interval for height, as a function of weight. The solid line is the average line for the mean height at each weight. The two shaded regions show different 89% plausible regions. The narrow shaded interval around the line is the distribution of μ. The wider shaded region represents the region within which the model expects to find 89% of actual heights in the population, at each weight. (Tidyverse)](04-geocentric-models_files/figure-html/fig-chap04-predict-plot2-m4-3b-1.png){#fig-chap04-predict-plot2-m4-3b width=672}\n:::\n:::\n\n\n\n::::\n:::::\n\nThis is the same graphic as in tab 'plot1' but with a factor 10 more iterations to smooth out the rough shaded interval.\n\n###### predict2 (T)\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-chap04-predict2-manually-m4-3b}\nb: Write you own predict for the posterior observations: Model-based predictions without {**brms**} and `predict()`: mean with quantiles of 0.055 and .945 (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-predict2-manually-m4-3b_975f6607094f34f6a4a78afc8c7d0426'}\n\n```{.r .cell-code}\nset.seed(4)\n\npost_m4.3b |> \n  tidyr::expand_grid(weight = 25:70) |> \n  dplyr::mutate(weight_c = weight - base::mean(d2_b$weight)) |> \n  dplyr::mutate(sim_height = stats::rnorm(dplyr::n(),\n                            mean = b_Intercept + b_weight_c * weight_c,\n                            sd   = sigma)) |> \n  dplyr::group_by(weight) |> \n  dplyr::summarise(mean = base::mean(sim_height),\n            ll   = stats::quantile(sim_height, prob = .055),\n            ul   = stats::quantile(sim_height, prob = .945)) |> \n  \n  # plot\n  ggplot2::ggplot(ggplot2::aes(x = weight)) +\n  ggplot2::geom_smooth(ggplot2::aes(y = mean, ymin = ll, ymax = ul),\n              stat = \"identity\",\n              fill = \"grey83\", color = \"black\", alpha = 1, linewidth = 1/2) +\n  ggplot2::geom_point(data = d2_b,\n             ggplot2::aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 2/3) +\n  ggplot2::coord_cartesian(xlim = base::range(d2_b$weight),\n                           ylim = base::range(d2_b$height)) +\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Model-based predictions without {brms} and pedict(): mean with quantiles of 0.055 and 0.945](04-geocentric-models_files/figure-html/fig-chap04-predict2-manually-m4-3b-1.png){#fig-chap04-predict2-manually-m4-3b width=672}\n:::\n:::\n\nHere we followed McElreath's example and calculated our model-based predictions \"by\nhand\". Instead of relying on base R `apply()` and `sapply()`, the main\naction in the {**tidyverse**} approach is in `tidyr::expand_grid()`, the second\n`dplyr::mutate()` line with `stats:rnomr()` and the `dplyr::group_by()` + `dplyr::summarise()` combination.\n\nWe specifically left out the `fitted()` intervals to make it more\napparent what we were simulating. \n\n::::\n:::::\n\n###### predict3 (T)\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-chap04-predict3-manually-m4-3b}\nb: Write you own predict for the posterior observations, now using {**tidybayes**} HDI of 89%: mode with quantiles of 0.055 and 0.945\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-predict3-manually-m4-3b_af616e35ca2cf393092a3456a8907d29'}\n\n```{.r .cell-code}\nset.seed(4)\n\npost_m4.3b |> \n  tidyr::expand_grid(weight = 25:70) |> \n  dplyr::mutate(weight_c = weight - base::mean(d2_b$weight)) |> \n  dplyr::mutate(sim_height = stats::rnorm(dplyr::n(),\n                            mean = b_Intercept + b_weight_c * weight_c,\n                            sd   = sigma)) |> \n  # dplyr::group_by(weight) |> \n  # dplyr::summarise(mean = base::mean(sim_height),\n  #           ll   = stats::quantile(sim_height, prob = .055),\n  #           ul   = stats::quantile(sim_height, prob = .945)) |> \n  \n  dplyr::group_by(weight) |> \n  tidybayes::mode_hdi(sim_height, .width = .89) |> \n  \n  # plot\n  ggplot2::ggplot(ggplot2::aes(x = weight)) +\n  ## instead of \"aes(y = mean, ymin = ll, ymax = ul)\"\n  ggplot2::geom_smooth(ggplot2::aes(y = .point, ymin = .lower, ymax = .upper),\n              stat = \"identity\",\n              fill = \"grey83\", color = \"black\", alpha = 1, linewidth = 1/2) +\n  ggplot2::geom_point(data = d2_b,\n             ggplot2::aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 2/3) +\n  ggplot2::coord_cartesian(xlim = base::range(d2_b$weight),\n                           ylim = base::range(d2_b$height)) +\n  ggplot2::labs(y = \"mode\") +\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Model-based predictions without {brms} and predict(): now using mode of HDI of 89% from {**tidybayes**}: mean with quantiles of 0.055 and .945](04-geocentric-models_files/figure-html/fig-chap04-predict3-manually-m4-3b-1.png){#fig-chap04-predict3-manually-m4-3b width=672}\n:::\n:::\n\nHere we replaced that three-line summarize() code with a single line of\n`tidybayes::mean_qi(sim_height)`, or whatever combination of central\ntendency and interval type you wanted (here we used: `tidybayes::mode_hdi(sim_height, .width = .89)`).\n\n\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n\n\n\n\n\n\n## Curves from lines\n\n> “We’ll consider two commonplace methods that use linear regression to build curves. The first is <a class='glossary' title='It is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial in x. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y. Although polynomial regression fits a nonlinear model to the data, as a statistical estimation problem it is linear. What “linear” means in this context is that \\(\\mu_{i}\\) is a linear function of any single parameter. For this reason, polynomial regression is considered to be a special case of multiple linear regression. (Wikipedia) (Chap.4)'>polynomial regression</a>. The second is <a class='glossary' title='The term “spline” refers to a wide class of basis functions that are used in applications requiring data interpolation and/or smoothing. Splines are special function defined piecewise by polynomials, it results to a smooth function built out of smaller, component functions. In interpolating problems, spline interpolation is often preferred to polynomial interpolation because it yields similar results, even when using low degree polynomials, while avoiding big numbers and the problem of oscillation at the edges of intervals of higher degrees. The term “spline” comes from the flexible spline devices used by shipbuilders and draftsmen to draw smooth shapes. They used a long, thin piece of wood or metal that could be anchored in a few places in order to aid drawing curves. | There are many types of splines, especially the common-place ‘B-splines’: The ‘B’ stands for ‘basis,’ which just means ‘component.’ B-splines build up wiggly functions from simpler less-wiggly components. Those components are called basis functions. B-splines force you to make a number of choices that other types of splines automate. (Chap.4)'>b-splines</a>.” ([McElreath, 2020, p. 110](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=129&annotation=TDHV7FFZ))\n\n### Polynomial regression\n\n> “Polynomial regression uses powers of a variable—squares and cubes—as extra predictors.” ([McElreath, 2020, p. 110](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=129&annotation=NBTWLNYX))\n\n\n#### Looking at the full !Kung data (Scatterplot)\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-fig-scatterplot-height-weight}\n: Scatterplot of height against weight\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Original\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-scatterplot-height-weight-a}\na: Height in centimeters (vertical) plotted against weight in kilograms (horizontal) (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-scatterplot-height-weight-a_29432cda033e6274d5ffe862e09e4db1'}\n\n```{.r .cell-code #fig-scatterplot-height-weight-a lst-cap=\"Height in centimeters (vertical) plotted against weight in kilograms (horizontal): rethinking version\"}\nplot(height ~ weight, data = d_a, col = rethinking::rangi2)\n```\n\n::: {.cell-output-display}\n![Height in centimeters (vertical) plotted against weight in kilograms (horizontal). This time with the full data, e.g., the non-adults data.](04-geocentric-models_files/figure-html/fig-scatterplot-height-weight-a-1.png){#fig-scatterplot-height-weight-a width=672}\n:::\n:::\n\n\n::::\n:::::\n\n\n###### Tidyverse\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-scatterplot-height-weight-b}\nb: Height in centimeters (vertical) plotted against weight in kilograms (horizontal) (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-scatterplot-height-weight-b_24fa9742cf22ebd532d8cb6530c744d6'}\n\n```{.r .cell-code}\nd_b |> \n  ggplot2::ggplot(ggplot2::aes(x = weight, y = height)) +\n  ggplot2::geom_point(color = \"navyblue\", shape = 1, size = 1.5, alpha = 2/3) +\n  ggplot2::annotate(geom = \"text\",\n           x = 42, y = 115,\n           label = \"This relation is\\nvisibly curved.\",\n           family = \"Times\") +\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Height in centimeters (vertical) plotted against weight in kilograms (horizontal). This time with the full data, e.g., the non-adults data. (Tidyverse)](04-geocentric-models_files/figure-html/fig-scatterplot-height-weight-b-1.png){#fig-scatterplot-height-weight-b width=672}\n:::\n:::\n\n\n::::\n:::::\n\n:::\n\nThe relationship is visibly curved, now that we've included the\nnon-adult individuals. (Compare with adult data in @fig-raw-data-line-m4-3a).\n\n::::\n:::::\n\n\n\n:::::{.my-note}\n:::{.my-note-header}\nWhat is a quadratic polynomial?\n:::\n::::{.my-note-container}\n> A quadratic function (also called a quadratic, a quadratic polynomial,\n> or a polynomial of degree 2) is special type of polynomial function\n> where the highest-degree term is second degree. ... The graph of a\n> quadratic function is a parabola, a 2-dimensional curve that looks\n> like either a cup(∪) or a cap(∩). ([Statistis How\n> To](https://www.statisticshowto.com/quadratic-function/))\n::::\n:::::\n\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-ID-text}\n: Parabolic and cubic equation for the mean height\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Parabolic\n\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-parabolic-mean}\n: Parabolic equation for the mean height\n::::::\n:::\n::::{.my-theorem-container}\n\n> “The most common polynomial regression is a parabolic model of the mean. Let x be standardized body weight.” ([McElreath, 2020, p. 110](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=129&annotation=SKF6GCYX))\n\n\n$$\n\\mu_{i} = \\alpha + \\beta_{1}x_{i} + \\beta_{2}x_{i}^2\n$$ {#eq-parabolic-mean}\n\n> “The above is a parabolic (second order) polynomial. The $\\alpha + \\beta_{1}x_{i}$ part is the same linear function of $x$ in a linear regression, just with a little '1' subscript added to the parameter name, so we can tell it apart from the new parameter. The additional term uses the square of $x_{i}$ to construct a parabola, rather than a perfectly straight line. The new parameter $\\beta_{2}$ measures the curvature of the relationship.” ([McElreath, 2020, p. 110](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=129&annotation=S6I93I6F))\n\n\n$$\n\\begin{align*}\nh_{i} \\sim \\operatorname{Normal}(μ_{i}, σ) \\space \\space (1) \\\\ \nμ_{i} = \\alpha + \\beta_{1}x_{i} + \\beta_{2}x_{i}^2 \\space \\space (2) \\\\\n\\alpha \\sim \\operatorname{Normal}(178, 20) \\space \\space (3)  \\\\ \n\\beta_{1} \\sim \\operatorname{Log-Normal}(0,10) \\space \\space (4) \\\\\n\\beta_{2} \\sim \\operatorname{Normal}(0,10) \\space \\space (5) \\\\\n\\sigma \\sim \\operatorname{Uniform}(0, 50) \\space \\space (6)      \n\\end{align*}\n$$ {#eq-parabolic-model}\n\n```         \nheight ~ dnorm(mu, sigma)                  # (1)\nmu <- a + b1 * weight.s + b2 * weight.s2^2 # (2)\na ~ dnorm(178, 20)                         # (3)\nb1 ~ dlnorm(0, 10)                         # (4)\nb2 ~ dnorm(0, 10)                          # (5)\nsigma ~ dunif(0, 50)                       # (6)\n```\n\n> “The confusing issue here is assigning a prior for $\\beta_{2}$, the parameter on the squared value of $x$. Unlike $\\beta_{1}$, we don’t want a positive constraint.” ([McElreath, 2020, p. 111](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=130&annotation=6CQMMGIN))\n\n\n::::\n:::::\n\n\n###### Cubic\n\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-cubic-mean}\n: Cubic equation for the mean height\n::::::\n:::\n::::{.my-theorem-container}\n\n\n$$\n\\begin{align*}\nh_{i} \\sim \\operatorname{Normal}(μ_{i}, σ) \\space \\space (1) \\\\ \nμ_{i} = \\alpha + \\beta_{1}x_{i} + \\beta_{2}x_{i}^2 + \\beta_{3}x_{i}^3 \\space \\space (2) \\\\\n\\alpha \\sim \\operatorname{Normal}(178, 20) \\space \\space (3)  \\\\ \n\\beta_{1} \\sim \\operatorname{Log-Normal}(0,10) \\space \\space (4) \\\\\n\\beta_{2} \\sim \\operatorname{Normal}(0,10) \\space \\space (5) \\\\\n\\beta_{3} \\sim \\operatorname{Normal}(0,10) \\space \\space (6) \\\\\n\\sigma \\sim \\operatorname{Uniform}(0, 50) \\space \\space (7)      \n\\end{align*}\n$$ {#eq-cubic-model}\n\n```         \nheight ~ dnorm(mu, sigma)                                      # (1)\nmu <- a + b1 * weight.s +  b2 * weight.s2^2 + b3 * weight.s3^3 # (2)\na ~ dnorm(178, 20)                                             # (3)\nb1 ~ dlnorm(0, 10)                                             # (4)\nb2 ~ dnorm(0, 10)                                              # (5)\nb3 ~ dnorm(0, 10)                                              # (6)\nsigma ~ dunif(0, 50)                                           # (7)\n```\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n\n#### Standardizing the predictor variable\n\n> “The first thing to do is to <a class='glossary' title='In statistics, standardization (also called Normalizing) is the process of putting different variables on the same scale. This process allows you to compare scores between different types of variables. Typically, to standardize variables, you calculate the mean and standard deviation for a variable. Then, for each observed value of the variable, you subtract the mean and divide by the standard deviation. (Statistics by Jim) See scale() in R.  (Chap.4)'>standardize</a> the <a class='glossary' title='Predictor variable – also known sometimes as the independent or explanatory variable – is the counterpart to the response or dependent variable. Predictor variables are used to make predictions for dependent variables. (DeepAI, MiniTab)'>predictor variable</a>. We’ve done this in previous examples. But this is especially helpful for working with polynomial models. When predictor variables have very large values in them, there are sometimes numerical glitches. Even well-known statistical software can suffer from these glitches, leading to mistaken estimates. These problems are very common for polynomial regression, because the square or cube of a large number can be truly massive. Standardizing largely resolves this issue. It should be your default behavior.” ([McElreath, 2020, p. 111](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=130&annotation=MY2LJCW5))\n\n\n\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-find-post-parabolic}\n: Posterior distribution of a parabolic model of height on weight\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Original\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-quap-parabolic-m4-5a}\na: Finding the posterior distribution of a parabolic model of height on weight (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/quap-parabolic-m4-5a_ad6509df0239888d57697b733bfe57b0'}\n\n```{.r .cell-code}\n## R code 4.65a #######################\n\n## standardization ########\nd_a$weight_s <- (d_a$weight - mean(d_a$weight)) / sd(d_a$weight)\nd_a$weight_s2 <- d_a$weight_s^2\n\n## find posterior distribution ########\nm4.5a <- rethinking::quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + b1 * weight_s + b2 * weight_s2,\n    a ~ dnorm(178, 20),\n    b1 ~ dlnorm(0, 1),\n    b2 ~ dnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ),\n  data = d_a\n)\n\n## R code 4.66a ################\n## precis results ###########\nrethinking::precis(m4.5a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>             mean        sd       5.5%      94.5%\n#> a     146.057415 0.3689756 145.467720 146.647109\n#> b1     21.733063 0.2888890  21.271363  22.194764\n#> b2     -7.803268 0.2741839  -8.241467  -7.365069\n#> sigma   5.774474 0.1764651   5.492449   6.056500\n```\n\n\n:::\n:::\n\n\n**$\\alpha$ (a)**: Intercept. Expected value of `height` when `weight` is at its mean value. \n\n> “But it is no longer equal to the mean height in the sample, since there is no guarantee it should in a polynomial regression.” ([McElreath, 2020, p. 112](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=131&annotation=2X62MWHB))\n\n> “The implied definition of α in a parabolic model is $\\alpha = Ey_{i} − \\beta_{1}Ex_{i} − \\beta_{2} Ex_{i}^2$. Now even when the average $x_{i}$ is zero, $Ex_{i} = 0$, the average square will likely not be zero. So $\\alpha$ becomes hard to directly interpret again.” ([McElreath, 2020, p. 562](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=581&annotation=Q4D6XGEU))\n\n**$\\beta_{1}$ and $\\beta_{2}$ (b1 and b2)**: Linear and square components of the curve. But that doesn’t make them transparent.\n\n::::\n:::::\n\n> “Now, since the relationship between the outcome height and the predictor weight depends upon two slopes, $\\beta_{1}$ and $\\beta_{2}$, it isn’t so easy to read the relationship off a table of coefficients.” ([McElreath, 2020, p. 111](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=130&annotation=T7DM6G2P))\n\n> “You have to plot these model fits to understand what they are saying.” ([McElreath, 2020, p. 112](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=131&annotation=6RLW99RK))\n\n\n\n###### Tidyverse\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-brm-parabolic-m4-5b}\nb: Finding the posterior distribution of a parabolic model of height on weight (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/brm-parabolic-m4-5b_19277a4a140cb30d2b3cdbf66e80949c'}\n\n```{.r .cell-code}\n## R code 4.65b #######################\n## standardization ###########\nd3_m4.5b <-\n  d_b |>\n  dplyr::mutate(weight_s = (weight - base::mean(weight)) / stats::sd(weight)) |> \n  dplyr::mutate(weight_s2 = weight_s^2)\n\n## fitting model ############\nm4.5b <- \n  brms::brm(data = d3_m4.5b, \n      family = gaussian,\n      height ~ 1 + weight_s + weight_s2,\n      prior = c(brms::prior(normal(178, 20), class = Intercept),\n                brms::prior(lognormal(0, 1), class = b, coef = \"weight_s\"),\n                brms::prior(normal(0, 1), class = b, coef = \"weight_s2\"),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"brm_fits/m04.05b\")\n\n## R code 4.66a ################\n## print.brmsfit results ###############\nbrms:::print.brmsfit(m4.5b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ 1 + weight_s + weight_s2 \n#>    Data: d3_m4.5b (Number of observations: 544) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   146.05      0.38   145.30   146.77 1.00     3544     2516\n#> weight_s     21.74      0.29    21.15    22.32 1.00     3106     2604\n#> weight_s2    -7.79      0.28    -8.33    -7.25 1.00     3298     2631\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     5.81      0.18     5.47     6.18 1.00     3788     2881\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\nNote our use of the coef argument within our prior statements. Since $\\beta_{1}$\nand $\\beta_{2}$ are both parameters of `class = b` within the {**brms**} set-up,\nwe need to use the `coef` argument when we want their priors to differ.\n\n::::\n:::::\n\n###### Trace plot\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-trace-plot-m4-5b}\n: Display trace plot (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell hash='04-geocentric-models_cache/html/trace-plot-m4-5b_1250d033d786db5f0ea812305e843238'}\n\n```{.r .cell-code}\nbrms:::plot.brmsfit(m4.5b, widths = c(1, 2))\n```\n\n::: {.cell-output-display}\n![](04-geocentric-models_files/figure-html/trace-plot-m4-5b-1.png){width=672}\n:::\n:::\n\n\n::::\n:::::\n\n\n\n:::\n\n::::\n:::::\n\n\n#### Fit linear, parabolic and cubic model\n\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-fig-different-regressions}\n: Different regressions (linear, parabolic and cubic) of height on weight (standardized), for the full !Kung data\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Linear (O)\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-linear-full-data-m4-4a}\na: Posterior distribution of the linear height-weight model with the full !Kung data set (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-linear-full-data-m4-4a_5b2176610c322d86eef19372f03b8e49'}\n\n```{.r .cell-code}\n## R code 4.42a #############################\n# define the average weight, x-bar\nxbar_m4.4a <- mean(d_a$weight_s)\n\n# fit model\nm4.4a <- rethinking::quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + b * (weight_s - xbar_m4.4a),\n    a ~ dnorm(178, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ),\n  data = d_a\n)\n\n\n## R code 4.46a ############################################\nplot(height ~ weight_s, data = d_a, col = rethinking::rangi2)\npost_m4.4a <- rethinking::extract.samples(m4.4a)\na_map_m4.4a <- mean(post_m4.4a$a)\nb_map_m4.4a <- mean(post_m4.4a$b)\ncurve(a_map_m4.4a + b_map_m4.4a * (x - xbar_m4.4a), add = TRUE)\n\n## R code 4.58a ################################\nmu.link_m4.4a <- function(weight) post_m4.4a$a + post_m4.4a$b * (weight - xbar_m4.4a)\nweight.seq_m4.4a <- seq(from = -3, to = 3, by = 1)\nmu_m4.4a <- sapply(weight.seq_m4.4a, mu.link_m4.4a)\nmu.mean_m4.4a <- apply(mu_m4.4a, 2, mean)\nmu.CI_m4.4a <- apply(mu_m4.4a, 2, rethinking::PI, prob = 0.89)\n\n\n## R code 4.59a #######################\nsim.height_m4.4a <- rethinking::sim(m4.4a, data = list(weight_s = weight.seq_m4.4a))\n\n## R code 4.60a ###################\nheight.PI_m4.4a <- apply(sim.height_m4.4a, 2, rethinking::PI, prob = 0.89)\n\n## R code 4.61a ##################\n# plot raw data\n# plot(height ~ weight, d_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))\n\n# draw MAP line\ngraphics::lines(weight.seq_m4.4a, mu.mean_m4.4a)\n\n# draw HPDI region for line\nrethinking::shade(mu.CI_m4.4a, weight.seq_m4.4a)\n\n# draw PI region for simulated heights\nrethinking::shade(height.PI_m4.4a, weight.seq_m4.4a)\n```\n\n::: {.cell-output-display}\n![Posterior distribution of the linear height-weight model with the full !Kung data set (Original)](04-geocentric-models_files/figure-html/fig-linear-full-data-m4-4a-1.png){#fig-linear-full-data-m4-4a width=672}\n:::\n:::\n\n\n\n\n::::\n:::::\n\n> “[The graphic] shows the familiar linear regression from earlier in the chapter, but now with the standardized predictor and full data with both adults and non-adults. The linear model makes some spectacularly poor predictions, at both very low and middle weights.” ([McElreath, 2020, p. 113](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=132&annotation=6FFX8KKA))\n\n###### Parabolic (O)\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-parabolic-regression-m4-5a}\na: Parabolic regression of height on weight (standardized), for the full !Kung data (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-parabolic-regression-m4-5a_f53e6fea33c2f11809342b2e44c722a6'}\n\n```{.r .cell-code}\n## R code 4.67a ################\nweight.seq_m4.5a <- seq(from = -2.2, to = 2, length.out = 30)\npred_dat_m4.5a <- list(weight_s = weight.seq_m4.5a, weight_s2 = weight.seq_m4.5a^2)\nmu_m4.5a <- rethinking::link(m4.5a, data = pred_dat_m4.5a)\nmu.mean_m4.5a <- apply(mu_m4.5a, 2, mean)\nmu.PI_m4.5a <- apply(mu_m4.5a, 2, rethinking::PI, prob = 0.89)\nsim.height_m4.5a <- rethinking::sim(m4.5a, data = pred_dat_m4.5a)\nheight.PI_m4.5a <- apply(sim.height_m4.5a, 2, rethinking::PI, prob = 0.89)\n\n\n## R code 4.68a #################\nplot(height ~ weight_s, d_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))\ngraphics::lines(weight.seq_m4.5a, mu.mean_m4.5a)\nrethinking::shade(mu.PI_m4.5a, weight.seq_m4.5a)\nrethinking::shade(height.PI_m4.5a, weight.seq_m4.5a)\n```\n\n::: {.cell-output-display}\n![A second order polynomial, a parabolic or quadratic regression of height on weight (standardized), for the full !Kung data (Original)](04-geocentric-models_files/figure-html/fig-parabolic-regression-m4-5a-1.png){#fig-parabolic-regression-m4-5a width=672}\n:::\n:::\n\n\n::::\n:::::\n\nThe quadratic regression does a pretty good job. It is much better than\na linear regression for the full `Howell1` data set.\n\n###### Cubic (O)\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-cubic-regression-m4-6a}\na: Cubic regressions of height on weight (standardized), for the full !Kung data (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-cubic-regression-m4-6a_26c9f61eb60018d3321700abb5462e6f'}\n\n```{.r .cell-code}\n## R code 4.69a ####################\nd_a$weight_s3 <- d_a$weight_s^3\nm4.6a <- rethinking::quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + b1 * weight_s + b2 * weight_s2 + b3 * weight_s3,\n    a ~ dnorm(178, 20),\n    b1 ~ dlnorm(0, 1),\n    b2 ~ dnorm(0, 10),\n    b3 ~ dnorm(0, 10),\n    sigma ~ dunif(0, 50)\n  ),\n  data = d_a\n)\n\n## R code 4.67a ################\nweight.seq_m4.6a <- seq(from = -2.2, to = 2, length.out = 30)\npred_dat_m4.6a <- list(weight_s = weight.seq_m4.6a, weight_s2 = weight.seq_m4.6a^2,\n                    weight_s3 = weight.seq_m4.6a^3)\nmu_m4.6a <- rethinking::link(m4.6a, data = pred_dat_m4.6a)\nmu.mean_m4.6a <- apply(mu_m4.6a, 2, mean)\nmu.PI_m4.6a <- apply(mu_m4.6a, 2, rethinking::PI, prob = 0.89)\nsim.height_m4.6a <- rethinking::sim(m4.6a, data = pred_dat_m4.6a)\nheight.PI_m4.6a <- apply(sim.height_m4.6a, 2, rethinking::PI, prob = 0.89)\n\n\n## R code 4.68a #################\nplot(height ~ weight_s, d_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))\ngraphics::lines(weight.seq_m4.6a, mu.mean_m4.6a)\nrethinking::shade(mu.PI_m4.6a, weight.seq_m4.6a)\nrethinking::shade(height.PI_m4.6a, weight.seq_m4.6a)\n```\n\n::: {.cell-output-display}\n![A third order polynomial, a cubic regression of height on weight (standardized), for the full !Kung data (Original)](04-geocentric-models_files/figure-html/fig-cubic-regression-m4-6a-1.png){#fig-cubic-regression-m4-6a width=672}\n:::\n:::\n\n\n::::\n:::::\n\n> “This cubic curve is even more flexible than the parabola, so it fits the data even better.” ([McElreath, 2020, p. 113](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=132&annotation=X335CTFQ))\n\n###### Linear (T)\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-linear-full-data-m4-4b}\nb: Fit a linear regression model of height on weight (standardized), for the full !Kung data (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-linear-full-data-m4-4b_97fdef5b56f0476bee84b28aecbd0aeb'}\n\n```{.r .cell-code}\nd_m4.4b <-\n  d_b |>\n  dplyr::mutate(weight_s = (weight - base::mean(weight)) / stats::sd(weight))\n\n\n## fit model m4.4b #############\nm4.4b <- \n  brms::brm(data = d_m4.4b, \n      family = gaussian,\n      height ~ 1 + weight_s,\n      prior = c(brms::prior(normal(178, 20), class = Intercept),\n                brms::prior(lognormal(0, 1), class = b, coef = \"weight_s\"),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"brm_fits/m04.04b\")\n\nweight_seq_m4.4b <- \n  tibble::tibble(weight_s = base::seq(from = -2.5, to = 2.5, length.out = 30))\n\n\n## data wrangling: fitted, predict #########\nfitd_m4.4b <-\n  brms:::fitted.brmsfit(m4.4b, \n         newdata = weight_seq_m4.4b,\n         probs = c(0.055, 0.945)) |>\n  tibble::as_tibble() |>\n  dplyr::bind_cols(weight_seq_m4.4b)\n\npred_m4.4b <-\n  brms:::predict.brmsfit(m4.4b, \n          newdata = weight_seq_m4.4b,\n         probs = c(0.055, 0.945)) |>\n  tibble::as_tibble() |>\n  dplyr::bind_cols(weight_seq_m4.4b) \n\n\n## plot linear model ############\nggplot2::ggplot(data = d_m4.4b, \n     ggplot2::aes(x = weight_s)) +\nggplot2::geom_ribbon(data = pred_m4.4b, \n            ggplot2::aes(ymin = Q5.5, ymax = Q94.5),\n            fill = \"grey83\") +\nggplot2::geom_smooth(data = fitd_m4.4b,\n            ggplot2::aes(y = Estimate, ymin = Q5.5, ymax = Q94.5),\n            stat = \"identity\",\n            fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/4) +\nggplot2::geom_point(ggplot2::aes(y = height),\n           color = \"navyblue\", shape = 1, size = 1.5, alpha = 1/3) +\nggplot2::labs(subtitle = \"linear\",\n     y = \"height\") +\nggplot2::coord_cartesian(xlim = base::range(d_m4.4b$weight_s),\n                         ylim = base::range(d_m4.4b$height)) +\nggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Fit a linear regression model of height on weight (standardized), for the full !Kung data. The raw data are shown by the circles. The solid curves show the path of μ in each model, and the shaded regions show the 95% interval of the mean (close to the solid curve) and the 95% interval of predictions (wider) (Tidyverse)](04-geocentric-models_files/figure-html/fig-linear-full-data-m4-4b-1.png){#fig-linear-full-data-m4-4b width=672}\n:::\n:::\n\n\n::::\n:::::\n\n> “[The graphic] shows the familiar linear regression from earlier in the chapter, but now with the standardized predictor and full data with both adults and non-adults. The linear model makes some spectacularly poor predictions, at both very low and middle weights.” ([McElreath, 2020, p. 113](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=132&annotation=6FFX8KKA))\n\n###### Parabolic (T)\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-parabolic-regression-m4-5b}\nb: Parabolic regression of height on weight (standardized), for the full !Kung data (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-parabolic-regression-m4-5b_ce2ebcb83117edc490393fd6a4272cea'}\n\n```{.r .cell-code}\n## data wrangling: fitted, predict ##############\nweight_seq_m4.5b <- \n  tibble::tibble(weight_s = base::seq(from = -2.5, to = 2.5, length.out = 30)) |> \n  dplyr::mutate(weight_s2 = weight_s^2)\n\nfitd_m4.5b <-\n  brms:::fitted.brmsfit(m4.5b, \n         newdata = weight_seq_m4.5b,\n         probs = c(0.055, 0.945)) |>\n  tibble::as_tibble() |>\n  dplyr::bind_cols(weight_seq_m4.5b)\n\npred_m4.5b <-\n  brms:::predict.brmsfit(m4.5b, \n          newdata = weight_seq_m4.5b,\n          probs = c(0.055, 0.945)) |>\n  tibble::as_tibble() |>\n  dplyr::bind_cols(weight_seq_m4.5b)  \n\n## plot quadratic model #########\nggplot2::ggplot(data = d3_m4.5b, \n       ggplot2::aes(x = weight_s)) +\nggplot2::geom_ribbon(data = pred_m4.5b, \n            ggplot2::aes(ymin = Q5.5, ymax = Q94.5),\n            fill = \"grey83\") +\nggplot2::geom_smooth(data = fitd_m4.5b,\n            ggplot2::aes(y = Estimate, ymin = Q5.5, ymax = Q94.5),\n            stat = \"identity\",\n            fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/2) +\nggplot2::geom_point(ggplot2::aes(y = height),\n           color = \"navyblue\", shape = 1, size = 1.5, alpha = 1/3) +\nggplot2::labs(subtitle = \"quadratic\",\n     y = \"height\") +\nggplot2::coord_cartesian(xlim = base::range(d3_m4.5b$weight_s),\n                         ylim = base::range(d3_m4.5b$height)) +\nggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Polynomial regressions of height on weight (standardized), for the full !Kung data. The raw data are shown by the circles. The solid curves show the path of μ in each model, and the shaded regions show the 95% interval of the mean (close to the solid curve) and the 95% interval of predictions (wider) (Tidyverse)](04-geocentric-models_files/figure-html/fig-parabolic-regression-m4-5b-1.png){#fig-parabolic-regression-m4-5b width=672}\n:::\n:::\n\n\n::::\n:::::\n\nThe quadratic regression does a pretty good job. It is much better than\na linear regression for the full `Howell1` data set.\n\n###### Cubic (T)\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-cubic-regression-m4-6b}\nb: Cubic regressions of height on weight (standardized), for the full !Kung data (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-cubic-regression-m4-6b_a75e50f0c84f31eb4a14cbe94514cac7'}\n\n```{.r .cell-code}\n## data wrangling: cubic ##############\nd3_m4.6b <-\n  d3_m4.5b |> \n  dplyr::mutate(weight_s3 = weight_s^3)\n\nm4.6b <- \n  brms::brm(data = d3_m4.6b, \n      family = gaussian,\n      height ~ 1 + weight_s + weight_s2 + weight_s3,\n      prior = c(brms::prior(normal(178, 20), class = Intercept),\n                brms::prior(lognormal(0, 1), class = b, coef = \"weight_s\"),\n                brms::prior(normal(0, 1), class = b, coef = \"weight_s2\"),\n                brms::prior(normal(0, 1), class = b, coef = \"weight_s3\"),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"brm_fits/m04.06b\")\n\n## data wrangling: fitted, predict ##############\nweight_seq_m4.6b <- \n  weight_seq_m4.5b |> \n  dplyr::mutate(weight_s3 = weight_s^3)\n\nfitd_m4.6b <-\n  brms:::fitted.brmsfit(m4.6b, \n         newdata = weight_seq_m4.6b,\n          probs = c(0.055, 0.945)) |>\n  tibble::as_tibble() |>\n  dplyr::bind_cols(weight_seq_m4.6b)\n\npred_m4.6b <-\n  brms:::predict.brmsfit(m4.6b, \n          newdata = weight_seq_m4.6b,\n          probs = c(0.055, 0.945)) |>\n  tibble::as_tibble() |>\n  dplyr::bind_cols(weight_seq_m4.6b) \n\n## plot quadratic model #########\nggplot2::ggplot(data = d3_m4.6b, \n     ggplot2::aes(x = weight_s)) +\nggplot2::geom_ribbon(data = pred_m4.6b, \n            ggplot2::aes(ymin = Q5.5, ymax = Q94.5),\n            fill = \"grey83\") +\nggplot2::geom_smooth(data = fitd_m4.6b,\n            ggplot2::aes(y = Estimate, ymin = Q5.5, ymax = Q94.5),\n            stat = \"identity\",\n            fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/4) +\nggplot2::geom_point(aes(y = height),\n           color = \"navyblue\", shape = 1, size = 1.5, alpha = 1/3) +\nggplot2::labs(subtitle = \"cubic\",\n     y = \"height\") +\nggplot2::coord_cartesian(xlim = base::range(d3_m4.6b$weight_s),\n                         ylim = base::range(d3_m4.6b$height)) +\nggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Fit a cubic regression model of height on weight (standardized), for the full !Kung data. The raw data are shown by the circles. The solid curves show the path of μ in each model, and the shaded regions show the 95% interval of the mean (close to the solid curve) and the 95% interval of predictions (wider) (Tidyverse)](04-geocentric-models_files/figure-html/fig-cubic-regression-m4-6b-1.png){#fig-cubic-regression-m4-6b width=672}\n:::\n:::\n\n\n::::\n:::::\n\n> “This cubic curve is even more flexible than the parabola, so it fits the data even better.” ([McElreath, 2020, p. 113](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=132&annotation=X335CTFQ))\n\n:::\n\n::::\n:::::\n\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! Models are just geocentric descriptions, not more!\n:::\n::::{.my-watch-out-container}\n> “… it’s not clear that any of these models make a lot of sense. They are good geocentric descriptions of the sample, yes. But there are two problems. First, a better fit to the sample might not actually be a better model. That’s the subject of @sec-chap07. Second, the model contains no biological information. We aren’t learning any causal relationship between height and weight. We’ll deal with this second problem much later, in @sec-chap16.” ([McElreath, 2020, p. 113](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=132&annotation=IAKJ5CC4))\n::::\n:::::\n\n\n\n#### Converting back to natural scale\n\n\n\n:::::{.my-procedure}\n:::{.my-procedure-header}\nFrom Z-scores to natural scale\n:::\n::::{.my-procedure-container}\n> “The plots in @exm-fig-different-regressions have standard units on the horizontal axis. These units are sometimes called <a class='glossary' title='A z-score (also called a standard score) gives you an idea of how far from the mean a data point is. But more technically it’s a measure of how many standard deviations below or above the population mean a raw score is. (StatisticsHowTo)'>z-scores</a>. But suppose you fit the model using standardized variables, but want to plot the estimates on the original scale.” ([McElreath, 2020, p. 114](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=133&annotation=HZCU5QEE))\n\n***\n\n0.  Turn off the horizontal axis when plotting the raw data.\n1.  Define the location of the labels, in standardized units.\n2.  Takes standardized units and converts them back to the natural\n    scale.\n3.  Explicitly construct and then draw the axis with values from the natural scale.\n::::\n:::::\n\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-ID-text}\n: From Z-scores back to natural scale\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### Original\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-chap04-standardized-natural-scale-a}\na: From Z-scores back to natural scale (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-standardized-natural-scale-a_a25a3ff159734357e8c16897b8c4f93b'}\n\n```{.r .cell-code}\n## R code 4.71a #############\nplot(height ~ weight_s, d_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5), xaxt = \"n\", xlab = \"Weight in kg\", ylab = \"Height in cm\")\n\nat_a <- c(-2, -1, 0, 1, 2)                           # 1\nlabels <- at_a * sd(d_a$weight) + mean(d_a$weight)   # 2\naxis(side = 1, at = at_a, labels = round(labels, 1)) # 3\n```\n\n::: {.cell-output-display}\n![Height against weight: Standarzided but with natural scale (Original)](04-geocentric-models_files/figure-html/fig-chap04-standardized-natural-scale-a-1.png){#fig-chap04-standardized-natural-scale-a width=672}\n:::\n:::\n\n1.  Defines the location of the labels, in standardized units.\n2.  Takes standardized units and converts them back to the original\n    scale.\n3.  Draws the axis.\n\nTake a look at the help `?axis` for more details.\n::::\n:::::\n\n\n###### Tidyverse\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-chap04-standardized-natural-scale-b}\nb: From Z-scores back to natural scale (Tidyverse)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-standardized-natural-scale-b_3580780dee7e71fdcadcbf11347414a3'}\n\n```{.r .cell-code}\nat_b <- c(-2, -1, 0, 1, 2)\n\n## plot linear model ############\nggplot2::ggplot(data = d_m4.4b, \n     ggplot2::aes(x = weight_s)) +\nggplot2::geom_ribbon(data = pred_m4.4b, \n            ggplot2::aes(ymin = Q5.5, ymax = Q94.5),\n            fill = \"grey83\") +\nggplot2::geom_smooth(data = fitd_m4.4b,\n            ggplot2::aes(y = Estimate, ymin = Q5.5, ymax = Q94.5),\n            stat = \"identity\",\n            fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/4) +\nggplot2::geom_point(ggplot2::aes(y = height),\n           color = \"navyblue\", shape = 1, size = 1.5, alpha = 1/3) +\nggplot2::labs(subtitle = \"linear\", \n              y = \"height in cm\", x = \"weight in kg\") +\nggplot2::coord_cartesian(xlim = base::range(d_m4.4b$weight_s),\n                         ylim = base::range(d_m4.4b$height)) +\nggplot2::theme_bw() +\n  \n# here it is!\nggplot2::scale_x_continuous(\"standardized weight converted back: Weight in kg\",\n                   breaks = at_a,\n                   labels = base::round(at_b*sd(d_m4.4b$weight) + \n                            base::mean(d_m4.4b$weight), 1))\n```\n\n::: {.cell-output-display}\n![](04-geocentric-models_files/figure-html/fig-chap04-standardized-natural-scale-b-1.png){#fig-chap04-standardized-natural-scale-b width=672}\n:::\n:::\n\n\n::::\n:::::\n\n:::\n\n::::\n:::::\n\n\n### Splines\n\n#### How do splines work?\n\n> “The second way to introduce a curve is to construct something known as a spline. The word spline originally referred to a long, thin piece of wood or metal that could be anchored in a few places in order to aid drafters or designers in drawing curves. In statistics, a spline is a smooth function built out of smaller, component functions. There are actually many types of splines. The <a class='glossary' title='The term “spline” refers to a wide class of basis functions that are used in applications requiring data interpolation and/or smoothing. Splines are special function defined piecewise by polynomials, it results to a smooth function built out of smaller, component functions. In interpolating problems, spline interpolation is often preferred to polynomial interpolation because it yields similar results, even when using low degree polynomials, while avoiding big numbers and the problem of oscillation at the edges of intervals of higher degrees. The term “spline” comes from the flexible spline devices used by shipbuilders and draftsmen to draw smooth shapes. They used a long, thin piece of wood or metal that could be anchored in a few places in order to aid drawing curves. | There are many types of splines, especially the common-place ‘B-splines’: The ‘B’ stands for ‘basis,’ which just means ‘component.’ B-splines build up wiggly functions from simpler less-wiggly components. Those components are called basis functions. B-splines force you to make a number of choices that other types of splines automate. (Chap.4)'>b-spline</a> we’ll look at here is commonplace. The “B” stands for “basis,” which here just means “component.” B-splines build up wiggly functions from simpler less-wiggly components. Those components are called basis functions.” ([McElreath, 2020, p. 114](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=133&annotation=3RJEARGI))\n\n> “The short explanation of B-splines is that they divide the full range of some predictor variable, like `year`, into parts. Then they assign a parameter to each part. These parameters are gradually turned on and off in a way that makes their sum into a fancy, wiggly curve.” ([McElreath, 2020, p. 115](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=134&annotation=YGZ4PE5G))\n\n> “With B-splines, just like with polynomial regression, we do this by generating new predictor variables and using those in the linear model, $\\mu_{i}$. Unlike polynomial regression, B-splines do not directly transform the predictor by squaring or cubing it. Instead they invent a series of entirely new, synthetic predictor variables. Each of these synthetic variables exists only to gradually turn a specific parameter on and off within a specific range of the real predictor variable. Each of the synthetic variables is called a <a class='glossary' title='B-Splines invent a series of entirely new, synthetic predictor variables. Each of these synthetic variables exists only to gradually turn a specific parameter on and off within a specific range of the real predictor variable. Each of the synthetic variables is called a basis function. (Chap.4)'>basis function</a>.” ([McElreath, 2020, p. 115](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=134&annotation=4HL4NXBW))\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-chap04-b-splines}\n: Linear model with B-splines\n::::::\n:::\n::::{.my-theorem-container}\n$$\n\\mu_{i} = \\alpha + w_{1}B_{i,1} + w_{2}B_{i,2} + w_{3}B_{i,3} + ...\n$$\n> “… $B_{i,n}$ is the $n$-th basis function’s value on row $i$, and the $w$ parameters are corresponding weights for each. The parameters act like slopes, adjusting the influence of each basis function on the mean $\\mu_{i}$. So really this is just another linear regression, but with some fancy, synthetic predictor variables.” ([McElreath, 2020, p. 115](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=134&annotation=H5Y2V6Y3))\n\n:::::{.my-procedure}\n:::{.my-procedure-header}\n:::::: {#prp-chap04-b-splines-m4-7a}\n: How to generate B-splines?\n::::::\n:::\n::::{.my-procedure-container}\n1.  Choose the number and distribution of <a class='glossary' title='B-splines divide the full range of some predictor variable into parts called knots. Knots are cutpoints that defines different regions (or partitions) for a variable. In each regions, a fitting must occurs. The definition of different regions is a way to stay local in the fitting process. (DataCademia | Statistics Knots (Cut Points)) (Chap.4)'>knots</a>\n2.  Choose the <a class='glossary' title='The degree of a polynomial term is the sum of the exponents of the variables that appear in it. (Wikipedia)'>polynomial degree</a>\n3.  Get the parameter weights for each basis function (define the model\n    and make it run)\n::::\n:::::\n\n::::\n:::::\n\n#### Original fit\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap04-fit-cherry-blossoms-m4-7a}\n: Fit Cherry Blossoms data (Original)\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### data\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-load-data-d2-m4-7a}\na: Load Cherry Blossoms data, filter by complete cases and display summary `precis`\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-load-data-d-m4-7a_7c72ad724eaa64c38ff662dd77dd8d3a'}\n\n```{.r .cell-code}\n## R code 4.72 modified ######################\ndata(package = \"rethinking\", list = \"cherry_blossoms\")\nd_m4.7a <- cherry_blossoms\nd2_m4.7a <- d_m4.7a[ complete.cases(d_m4.7a$doy) , ] # complete cases on doy\nrethinking::precis(d2_m4.7a)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>                   mean          sd      5.5%     94.5%       histogram\n#> year       1548.841596 304.1497740 1001.7200 1969.5700   ▁▂▂▃▅▃▇▇▇▇▇▇▁\n#> doy         104.540508   6.4070362   94.4300  115.0000        ▁▂▅▇▇▃▁▁\n#> temp          6.100356   0.6834104    5.1000    7.3662        ▁▃▅▇▃▁▁▁\n#> temp_upper    6.937560   0.8119865    5.8100    8.4177 ▁▃▅▇▅▃▂▁▁▁▁▁▁▁▁\n#> temp_lower    5.263545   0.7621943    4.1923    6.6277     ▁▁▁▂▅▇▃▁▁▁▁\n```\n\n\n:::\n:::\n\nThe original data set has 1,215 observation but only 827 records have values in day-of-year (`doy`, Day of the year of first blossom) column.\n::::\n:::::\n\nSee `?cherry_blossoms` for details and sources.\n\n###### plot\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-chap04-plot-d2-m4-7a}\na: Display raw data for `doy` (Day of the year of first blossom) against the year\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-plot-d2-m4-7a_16df67a683aa7bb737f5426f1b25ead8'}\n\n```{.r .cell-code}\nplot(doy ~ year, data = d2_m4.7a, col = rethinking::rangi2)\n```\n\n::: {.cell-output-display}\n![Display raw data for `doy` (Day of the year of first blossom) against the year (Original)](04-geocentric-models_files/figure-html/fig-chap04-plot-d2-m4-7a-1.png){#fig-chap04-plot-d2-m4-7a width=672}\n:::\n:::\n\n\n\n> “We’re going to work with the historical record of first day of blossom, `doy`, for now. It ranges from 86 (late March) to 124 (early May). The years with recorded blossom dates run from 812 <a class='glossary' title='CE is an abbreviation for Common Era. It means the same as AD (Anno Domini) and represents the time from year 1 and onward. (timeanddate)'>CE</a> to 2015 CE.” ([McElreath, 2020, p. 114](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=133&annotation=QTB8HPTD))\n::::\n:::::\n\nYou can see that the `doy` data are sparse in the early years. Their number increases steadily approaching the year 2000.\n\n###### knots\n\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-knots-m4-7a}\na: Choose number of knots and distribute them over the data points of the `year` variable. (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-knots-m4-7a_b48a91aa62bb9eda44b06c29b6cb3a84'}\n\n```{.r .cell-code}\n## R code 4.73a complete cases on doy ##########\nnum_knots <- 15                                  # (1) number of cutpoints\nknot_list <- quantile(d2_m4.7a$year,             # (2) divide regions for fitting\n      probs = seq(0, 1, length.out = num_knots))\nprint(tibble::enframe(knot_list), n = 15)        # (3) display knot_list\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 15 × 2\n#>    name      value\n#>    <chr>     <dbl>\n#>  1 0%          812\n#>  2 7.142857%  1036\n#>  3 14.28571%  1174\n#>  4 21.42857%  1269\n#>  5 28.57143%  1377\n#>  6 35.71429%  1454\n#>  7 42.85714%  1518\n#>  8 50%        1583\n#>  9 57.14286%  1650\n#> 10 64.28571%  1714\n#> 11 71.42857%  1774\n#> 12 78.57143%  1833\n#> 13 85.71429%  1893\n#> 14 92.85714%  1956\n#> 15 100%       2015\n```\n\n\n:::\n:::\n\n\nThe locations of the knotsare part of the model. Therefore you are responsible for them. We placed the knots at different evenly-spaced quantiles of the predictor variable.\n\n**Explanation of code lines**\n\n(1) **num_knots**: Specify the number of cutpoints that define different regions (or partitions) for a variable. Here were 15 knots chosen.\n(2) **knot_list**: Vector that divides the available rows of variable `year` into 15 parts named after the percentiles. It is important to understand that not the range of the variable `year` was divided but the available data points.\n(3) I have `knot_list` wrapped into a `tibble` and output with `print(tibble::enframe(knot_list), n = 15)`, so that one can inspect the content of the vector more easily.\n\n::::\n:::::\n\nAgain you can see that the `doy` data are sparse in the early years. Starting with a the 16th century the we get similar intervals for the distances between years. This can be inspected better graphically in the next \"parts\" tab.\n\n###### parts\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-chap04-plot-parts-d2-m4-7a}\na: Plot data with equally number of `doy` data points for each segment against `year` \n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-plot-parts-d2-m4-7a_be6d51d200b79df09aa1e70e02aad0d9'}\n\n```{.r .cell-code}\nplot(doy ~ year, data = d2_m4.7a, col = rethinking::rangi2)\nabline(v = knot_list)\n```\n\n::: {.cell-output-display}\n![Display raw data for `doy` (Day of the year of first blossom) against the year with vertical lines at the knots positions (Original)](04-geocentric-models_files/figure-html/fig-chap04-plot-parts-d2-m4-7a-1.png){#fig-chap04-plot-parts-d2-m4-7a width=672}\n:::\n:::\n\n::::\n:::::\n\nStarting with a the 16th century the we get similar intervals for the distances between years, e.g. the number of `doy` data points is approximately evenly distributed (59 - 67 years)\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-knot_list-diff-d2-m4-7a_51f22be1c490b7350de5951940c68886'}\n\n```{.r .cell-code}\nknot_list2 <- tibble::enframe(knot_list) |> \n  dplyr::mutate(diff = round(value - dplyr::lag(value), 0))\nprint(knot_list2, n = 15)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 15 × 3\n#>    name      value  diff\n#>    <chr>     <dbl> <dbl>\n#>  1 0%          812    NA\n#>  2 7.142857%  1036   224\n#>  3 14.28571%  1174   138\n#>  4 21.42857%  1269    95\n#>  5 28.57143%  1377   108\n#>  6 35.71429%  1454    77\n#>  7 42.85714%  1518    64\n#>  8 50%        1583    65\n#>  9 57.14286%  1650    67\n#> 10 64.28571%  1714    64\n#> 11 71.42857%  1774    60\n#> 12 78.57143%  1833    59\n#> 13 85.71429%  1893    60\n#> 14 92.85714%  1956    63\n#> 15 100%       2015    59\n```\n\n\n:::\n:::\n\n\n\n\n###### calc\n\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-str-splines-m4-7a}\na: Calculate basis functions of a cubic spline (degree = 3) with 15 areas (knots) to fit (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-str-splines-m4-7a_110d49fbb9258d37b7622325d9084124'}\n\n```{.r .cell-code}\n## R code 4.74a ################\nB_m4.7a <- splines::bs(d2_m4.7a$year,            # (1) generate B-splines\n  knots = knot_list[-c(1, num_knots)],           # (2) knots without first & last\n  degree = 3,                                    # (3) polynomial (cubic) degree\n  intercept = TRUE)                              # (4) intercept\nstr(B_m4.7a)                                     # (5) show data structure                                   \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  'bs' num [1:827, 1:17] 1 0.96 0.767 0.563 0.545 ...\n#>  - attr(*, \"dimnames\")=List of 2\n#>   ..$ : NULL\n#>   ..$ : chr [1:17] \"1\" \"2\" \"3\" \"4\" ...\n#>  - attr(*, \"degree\")= int 3\n#>  - attr(*, \"knots\")= Named num [1:13] 1036 1174 1269 1377 1454 ...\n#>   ..- attr(*, \"names\")= chr [1:13] \"7.142857%\" \"14.28571%\" \"21.42857%\" \"28.57143%\" ...\n#>  - attr(*, \"Boundary.knots\")= int [1:2] 812 2015\n#>  - attr(*, \"intercept\")= logi TRUE\n```\n\n\n:::\n:::\n\n\n**Explanation of code lines**\n\n(1) The function `splines::bs()` generate the B-spline basis matrix for\n    a polynomial spline:\n(2) `knots` is generated without the two boundary knots (fist and last\n    knot) that are placed at the minimum and maximum of the variable.\n    These two knots are excluded with the tricky code `knot_list[-c(1, num_knots)]` to prevent redundancies as\n    `splines::bs()` places by default knots at the boundaries. So we\n    have 13 internal knots.\n(3) With `degree = 3` a cubic B-spline is chosen. The polynomial degree determines how basis functions combine, which determines how the parameters interact to\nproduce the spline. For degree 1, (=\n    line), two basis functions combine at each point. For degree 2 (=\n    quadratic), three functions combine at each point. For degree 3 (=\n    cubic), there are four basis functions combined. This should give\n    enough flexibility for each region to fit.\n(4) McElreath chose `intercept = TRUE`: \\> \"We'll also have an intercept\n    to capture the average blossom day. This will make it easier to\n    define priors on the basis weights, because then we can just\n    conceive of each as a deviation from the intercept.\" ([McElreath,\n    2020, p. 117](zotero://select/groups/5243560/items/NFUEVASQ))\n    ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=136&annotation=9MA7DU8W))\\\n    Kurz mentioned ominously: \"For reasons I'm not prepared to get into,\n    here, splines don't always include intercept parameters. Indeed, the\n    `bs()` default is `intercept = FALSE`.\"\n    ([Kurz](https://bookdown.org/content/4857/geocentric-models.html#splines.)).\\\n    I do not know exactly what's the result of choosing\n    `intercept = TRUE` as the difference to FALSE is marginal. The first\n    B-spline (third hill) starts on the left edge of the graph with\n    $1.0$ instead of $0$.\n(5) In the data structure you can see in the second to last line, that there are two\n    \"Boundary.knots\" at year $812$ and $2015$. These two years are in\n    fact the first and last value of the `year` variable:\n\n-   `dplyr::first(d2_m4.7a$year)`: 812\n-   `dplyr::last(d2_m4.7a$year)` : 2015\n\n::::\n:::::\n\n\n###### basis\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-chap04-basis-splines-m4-7a}\na: Draw raw basis functions for the year variable for 15 areas (knots) and degree 3 (= cubic polynomial) (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-basis-splines-m4-7a_46f7f9852af04670a0212ddca4ba35e6'}\n\n```{.r .cell-code}\n## R code 4.75a #############\nplot(NULL, xlim = range(d2_m4.7a$year),     \n  ylim = c(0, 1), xlab = \"year\", ylab = \"basis\")  \nfor (i in 1:ncol(B_m4.7a)) \n  lines(d2_m4.7a$year, B_m4.7a[, i])\n```\n\n::: {.cell-output-display}\n![Draw raw basis functions B-splines for the year variable for 15 areas (knots) and degree 3 (cubic polynomial) (Original)](04-geocentric-models_files/figure-html/fig-chap04-basis-splines-m4-7a-1.png){#fig-chap04-basis-splines-m4-7a width=672}\n:::\n:::\n\n\n::::\n:::::\n\n###### formula\n\n\n:::::{.my-theorem}\n:::{.my-theorem-header}\n:::::: {#thm-theorem-text}\na: Linear model for Cherry Blossoms data\n::::::\n:::\n::::{.my-theorem-container}\n\n$$\n\\begin{align*}\n\\text{day of year}_{i} \\sim \\operatorname{Normal}(\\mu_{i}, \\sigma) \\\\\n\\mu_{i} = \\alpha + {\\sum_{k=1}^K w_{k} B_{k, i}} \\\\\n\\alpha \\sim \\operatorname{Normal}(100, 10) \\\\\nw_{j} \\sim \\operatorname{Normal}(0, 10) \\\\\n\\sigma \\sim \\operatorname{Exponential}(1)\n\\end{align*}\n$${#eq-lm-blossoms}\n\n***\n> where $\\alpha$ is the intercept, $B_{k, i}$ is the value of the $k^\\text{th}$ bias function on the $i^\\text{th}$ row of the data, and $w_k$ is the estimated regression weight for the corresponding $k^\\text{th}$ bias function. (](https://bookdown.org/content/4857/geocentric-models.html#splines.))\n\n::::\n:::::\n\n> “[The model] is multiplying each basis value by a corresponding parameter $w_{k}$ and then adding up all $K$ of those products. This is just a compact way of writing a linear model. The rest should be familiar. … the $w$ priors influence how wiggly the spline can be.” ([McElreath, 2020, p. 118](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=137&annotation=Z6NJ85VE))\n\n> “This is also the first time we’ve used an <a class='glossary' title='The exponential distribution is often concerned with the amount of time until some specific event occurs. For example, the amount of time (beginning now) until an earthquake occurs has an exponential distribution. | Values for an exponential random variable occur in the following way: There are fewer large values and more small values. For example, the amount of money customers spend in one trip to the supermarket follows an exponential distribution. There are more people who spend small amounts of money and fewer people who spend large amounts of money. (LibreTexts Statistics)'>exponential distribution</a> as a <a class='glossary' title='The Prior Probability, also called the Prior, is the assumed probability distribution before we have seen the data. (Wikipedia) It quantifies how likely our initial belief is: P(belief). (BF, Chap.8)'>prior</a>. Exponential distributions are useful priors for scale parameters, parameters that must be positive. The prior for $\\sigma$ is exponential with a rate of $1$. The way to read an exponential distribution is to think of it as containing no more information than an average deviation. That average // is the inverse of the rate. So in this case it is $1/1 = 1$. If the rate were $0.5$, the mean would be $1/0.5 = 2$. We’ll use exponential priors for the rest of the book, in place of uniform priors. It is much more common to have a sense of the average deviation than of the maximum.” ([McElreath, 2020, p. 118/119](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=138&annotation=N2XUQTJB))\n\n:::::{.my-resource}\n:::{.my-resource-header}\nExponential distributions in R\n:::\n::::{.my-resource-container}\nTo learn more about the exponential distribution and it's single parameter $\\lambda$ lambda), which is also called the *rate* and how to apply it in R read [Exponential distribution in R](https://r-coder.com/exponential-distribution-r/).\n::::\n:::::\n\n\n###### quap\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-fit-model-precis-m4-7a}\na: Fit model and show summary (Original)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-fit-model-precis-m4-7a_9aab5795fbeddfa077d722a000683e14'}\n\n```{.r .cell-code}\n## R code 4.76a ############\nm4.7a <- rethinking::quap(\n  alist(\n    D ~ dnorm(mu, sigma),\n    mu <- a + B_m4.7a %*% w,\n    a ~ dnorm(100, 10),\n    w ~ dnorm(0, 10),\n    sigma ~ dexp(1)\n  ),\n  data = list(D = d2_m4.7a$doy, B_m4.7a = B_m4.7a),\n  start = list(w = rep(0, ncol(B_m4.7a)))\n)\n\nrethinking::precis(m4.7a, depth = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>               mean        sd         5.5%       94.5%\n#> w[1]   -0.74907825 3.8699046  -6.93393317   5.4357767\n#> w[2]    0.46260929 3.0138007  -4.35402625   5.2792448\n#> w[3]    5.75074086 2.6352108   1.53916511   9.9623166\n#> w[4]    0.29257647 2.4850428  -3.67900190   4.2641548\n#> w[5]    5.33551430 2.6157766   1.15499802   9.5160306\n#> w[6]   -4.24900191 2.4432842  -8.15384200  -0.3441618\n#> w[7]    8.89031186 2.4660377   4.94910731  12.8315164\n#> w[8]    0.05414883 2.5463000  -4.01533036   4.1236280\n#> w[9]    4.08605901 2.5841481  -0.04390883   8.2160269\n#> w[10]   5.71821727 2.5609005   1.62540361   9.8110309\n#> w[11]   0.90260492 2.5360789  -3.15053899   4.9557488\n#> w[12]   6.60934377 2.5575674   2.52185710  10.6968304\n#> w[13]   1.77695187 2.6804345  -2.50690017   6.0608039\n#> w[14]   0.23168278 3.0115420  -4.58134299   5.0447085\n#> w[15]  -5.89934783 3.0966563 -10.84840275  -0.9502929\n#> w[16]  -6.64967713 2.9471305 -11.35976093  -1.9395933\n#> a     102.29008683 1.9474030  99.17776076 105.4024129\n#> sigma   5.87829936 0.1438198   5.64844752   6.1081512\n```\n\n\n:::\n:::\n\n\nThe model conforms to the content in tab \"formula\".\n\n> “To build this model in quap, we just need a way to do that sum. The easiest way is to use matrix multiplication.” ([McElreath, 2020, p. 119](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=138&annotation=3EWHEG6T))\n\n> “Matrix algebra is just a new way to represent ordinary algebra. It is often much more compact. So to make model `m4.7a` easier to program, we used a matrix multiplication of the basis matrix `B_m4.7a` by the vector of parameters $w$: ([McElreath, 2020, p. 120](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=139&annotation=FXMD6NR9\n\n$$B %\\*% w$$. \n\n\n> This notation is just linear algebra shorthand for (1) multiplying each element of the vector w by each value in the corresponding row of B and then (2) summing up each result.” ([McElreath, 2020, p. 120](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=139&annotation=FXMD6NR9))\n\nThe difference between matrix multiplication and traditional style is seen in the line with the basis matrix `B_m4.7a`\n\n$$\n\\begin{align*}\n\\text{mu <- a + B %*% w, (1)} \\\\\n\\text{mu <- a + sapply(1:827, function(i) sum(B[i, ] * w)), (2)} \\\\\n\\end{align*}\n$$\n(1) Matrix multiplication\n(2) Less elegant code but with the same result\n\n\n::::\n:::::\n\nWe looked with `precis(m4.7a, depth = 2)` at the posterior means. We see 17 $w$ parameters. But this didn't help much. We need to plot the posterior prediction (see last tab \"plot2\")\n\n###### weighted\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-chap04-weighted-splines-m4-7a}\na: Draw weighted basis functions (B-splines)\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-weighted-splines-m4-7a_8b302599ad79dfe2da27a8ab612f54c3'}\n\n```{.r .cell-code}\n## R code 4.77a #############\npost_m4.7a <- rethinking::extract.samples(m4.7a)\nw_m4.7a <- apply(post_m4.7a$w, 2, mean)\nplot(NULL,\n  xlim = base::range(d2_m4.7a$year), ylim = c(-6, 6),\n  xlab = \"year\", ylab = \"basis * weight\"\n)\nfor (i in 1:ncol(B_m4.7a)) \n    lines(d2_m4.7a$year, w_m4.7a[i] * B_m4.7a[, i])\n```\n\n::: {.cell-output-display}\n![Draw weighted basis functions (B-splines) for the year variable for 15 areas (knots) and degree 3 (cubic polynomial) (Original)](04-geocentric-models_files/figure-html/fig-chap04-weighted-splines-m4-7a-1.png){#fig-chap04-weighted-splines-m4-7a width=672}\n:::\n:::\n\n\n\nTo get the parameter weights for each basis function, we had to define the model (tab \"formula\") and make it run (tab \"fit\").\n\n::::\n:::::\n\nCompare the weighted basis functions with the raw basis function (tab \"basis\") and with the wiggles in the Cherry Blossoms data (tab \"plot2\").\n\n\n###### plot2\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-chap04-plot-post-m4-7a}\na: Plot the 97% posterior interval for $\\mu$, at each year\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-plot-post-m4-7a_bf5a3d541a5de6abad90aab8b00ac490'}\n\n```{.r .cell-code}\n## R code 4.78a ###############\nmu_m4.7a <- rethinking::link(m4.7a)\nmu_PI_m4.7a <- apply(mu_m4.7a, 2, rethinking::PI, 0.97)\nplot(d2_m4.7a$year, d2_m4.7a$doy, \n     col = rethinking::col.alpha(rethinking::rangi2, 0.3), pch = 16)\nrethinking::shade(mu_PI_m4.7a, d2_m4.7a$year, \n     col = rethinking::col.alpha(\"black\", 0.5))\n```\n\n::: {.cell-output-display}\n![Posterior prediction: The sum of the weighted basis functions, at each point, produces the spline, displayed as a 97% posterior interval of μ](04-geocentric-models_files/figure-html/fig-chap04-plot-post-m4-7a-1.png){#fig-chap04-plot-post-m4-7a width=672}\n:::\n:::\n\n\n::::\n:::::\n\n> “The spline is much wigglier now. Something happened around 1500, for example. If you add more knots, you can make this even wigglier. You might wonder how many knots is correct. We’ll be ready to address that question in a few more chapters” ([McElreath, 2020, p. 119](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=138&annotation=Z5X4FZCM))\n\n:::\n\n::::\n:::::\n\n#### Tidyverse fit\n\n\n:::::{.my-example}\n:::{.my-example-header}\n:::::: {#exm-chap04-fit-cherry-blossoms-m4-7b}\nb: Fit Cherry Blossoms data (Tidyverse)\n::::::\n:::\n::::{.my-example-container}\n\n::: {.panel-tabset}\n\n###### data\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-load-data-d2-m4-7b}\nb: Load Cherry Blossoms data, complete cases and display summary\n::::::\n:::\n::::{.my-r-code-container}\n\n\n\n::: {.cell hash='04-geocentric-models_cache/html/load-cherry-blossoms-data-m4-7b_05c23996feaf1ab15b570871d1780890'}\n\n```{.r .cell-code}\n## R code 4.72b modified ######################\ndata(package = \"rethinking\", list = \"cherry_blossoms\")\nd_m4.7b <- cherry_blossoms\n\nd2_m4.7b <- \n  d_m4.7b |> \n  tidyr::drop_na(doy)\n\n\n# ground-up tidyverse way to summarize\n(\n    d2_m4.7b |> \n      tidyr::gather() |> \n      dplyr::group_by(key) |> \n      dplyr::summarise(mean = base::mean(value, na.rm = T),\n                sd   = stats::sd(value, na.rm = T),\n                ll   = stats::quantile(value, prob = .055, na.rm = T),\n                ul   = stats::quantile(value, prob = .945, na.rm = T)) |> \n      dplyr::mutate(dplyr::across(where(is.double), \n                                  \\(x) round(x, digits = 2)))\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 5 × 5\n#>   key           mean     sd      ll      ul\n#>   <chr>        <dbl>  <dbl>   <dbl>   <dbl>\n#> 1 doy         105.     6.41   94.4   115   \n#> 2 temp          6.1    0.68    5.1     7.37\n#> 3 temp_lower    5.26   0.76    4.19    6.63\n#> 4 temp_upper    6.94   0.81    5.81    8.42\n#> 5 year       1549.   304.   1002.   1970.\n```\n\n\n:::\n:::\n\n\nWithin the tidyverse, we can drop NA's with the `tidyr::drop_na()` function. -- This is a much easier way than my\napproach originally used with `dplyr::filter()` (See also the different ways in\n[StackOverflow](https://stackoverflow.com/questions/70848048/filtering-any-missing-values-in-r/70848085)\n\n::::\n:::::\n\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! `across()` supersedes the family of \"scoped variants\" like `mutate_if()`\n:::\n::::{.my-watch-out-container}\n\nI am using [Kurz](https://bookdown.org/content/4857/geocentric-models.html#splines)’ \"ground-up {**tidyverse**} way\" to summarize the data. But instead of the superseded `dplyr::mutate_if()` function I used the new `dplyr::across()`. At first I wrote `dplyr::mutate(dplyr::across(where(is.double), round, digits = 2))` but after I warning I changed it to an anonymous function:\n\n```\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n```\n\n::::\n:::::\n\n\n###### skim\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-skim-d2-m4-7b}\nb: Display summary with `skimr::skim()`\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/summarize-data-with-skim-m4-7b_881f27c3c5c7320f332e23c2223c6fb0'}\n\n```{.r .cell-code}\nd2_m4.7b |>\n  skimr::skim()\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |         |\n|:------------------------|:--------|\n|Name                     |d2_m4.7b |\n|Number of rows           |827      |\n|Number of columns        |5        |\n|_______________________  |         |\n|Column type frequency:   |         |\n|numeric                  |5        |\n|________________________ |         |\n|Group variables          |None     |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|    mean|     sd|     p0|     p25|     p50|     p75|    p100|hist  |\n|:-------------|---------:|-------------:|-------:|------:|------:|-------:|-------:|-------:|-------:|:-----|\n|year          |         0|          1.00| 1548.84| 304.15| 812.00| 1325.00| 1583.00| 1803.50| 2015.00|▂▅▆▇▇ |\n|doy           |         0|          1.00|  104.54|   6.41|  86.00|  100.00|  105.00|  109.00|  124.00|▁▅▇▅▁ |\n|temp          |        40|          0.95|    6.10|   0.68|   4.69|    5.62|    6.06|    6.46|    8.30|▃▇▇▂▁ |\n|temp_upper    |        40|          0.95|    6.94|   0.81|   5.45|    6.38|    6.80|    7.38|   12.10|▇▇▂▁▁ |\n|temp_lower    |        40|          0.95|    5.26|   0.76|   2.61|    4.77|    5.25|    5.65|    7.74|▁▃▇▂▁ |\n\n\n:::\n:::\n\n\nKurz's version does not have the mini histograms. I added another\nsummary with `skimr::skim()` to add tiny graphics.\n\n::::\n:::::\n\n###### plot\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-ID-text}\nb: Display raw data for `doy` (Day of the year of first blossom) against the year\n::::::\n:::\n::::{.my-r-code-container}\n\n\n\n::: {.cell fig-heights='3' hash='04-geocentric-models_cache/html/fig-chap04-plot-d2-m4-7b_e2ed19dfc20c1a16bb649663346d1d42'}\n\n```{.r .cell-code}\nd2_m4.7b |> \n  ggplot2::ggplot(ggplot2::aes(x = year, y = doy)) +\n  ## color from https://www.colorhexa.com/ffb7c5\n  ggplot2::geom_point(color = \"#ffb7c5\", alpha = 1/2) +\n  ggplot2::theme() +\n  ggplot2::theme(panel.grid = ggplot2::element_blank(),\n    panel.background = ggplot2::element_rect(fill = \"#4f455c\"))\n\n## color from https://www.colordic.org/w/, \n## inspired by \n## https://chichacha.netlify.com/2018/11/29/plotting-traditional-colours-of-japan/\n```\n\n::: {.cell-output-display}\n![Display raw data for `doy` (Day of the year of first blossom) against the year (Tidyverse)](04-geocentric-models_files/figure-html/fig-chap04-plot-d2-m4-7b-1.png){#fig-chap04-plot-d2-m4-7b width=672}\n:::\n:::\n\n\nBy default {**ggplot2**} removes missing data records with a warning. But I had already removed missing data for the `doy` variable (see: tab \"data\").\n\n::::\n:::::\n\n###### knots\n\n::: {.callout-note style=\"color: blue;\"}\nThis is the same code and text as in tab \"knots\" of @exm-chap04-fit-cherry-blossoms-m4-7a.\n:::\n\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-knots-m4-7b}\nb: Choose number of knots and distribute them over the data points of the `year` variable.\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-knots-m4-7b_a37bfcd2f1b5792f846bcc46ca61481c'}\n\n```{.r .cell-code}\n## R code 4.73b complete cases on doy ##########\nnum_knots_b <- 15                                  # (1) number of cutpoints\nknot_list_b <- quantile(d2_m4.7b$year,             # (2) divide regions for fitting\n      probs = seq(0, 1, length.out = num_knots_b))\nprint(tibble::enframe(knot_list_b), n = 15)        # (3) display knot_list\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 15 × 2\n#>    name      value\n#>    <chr>     <dbl>\n#>  1 0%          812\n#>  2 7.142857%  1036\n#>  3 14.28571%  1174\n#>  4 21.42857%  1269\n#>  5 28.57143%  1377\n#>  6 35.71429%  1454\n#>  7 42.85714%  1518\n#>  8 50%        1583\n#>  9 57.14286%  1650\n#> 10 64.28571%  1714\n#> 11 71.42857%  1774\n#> 12 78.57143%  1833\n#> 13 85.71429%  1893\n#> 14 92.85714%  1956\n#> 15 100%       2015\n```\n\n\n:::\n:::\n\n\nThe locations of the knots are part of the model. Therefore you are responsible for them. We placed the knots at different evenly-spaced quantiles of the predictor variable.\n\n**Explanation of code lines**\n\n(1) **num_knots_b**: Specify the number of cutpoints that define different regions (or partitions) for a variable. Here were 15 knots chosen.\n(2) **knot_list_b**: Vector that divides the available rows of variable `year` into 15 parts named after the percentiles. It is important to understand that not the range of the variable `year` was divided but the available data points.\n(3) I have `knot_list` wrapped into a `tibble` and output with `print(tibble::enframe(knot_list_b), n = 15)`, so that one can inspect the content of the vector more easily.\n\n::::\n:::::\n\nAgain you can see that the `doy` data are sparse in the early years. Starting with a the 16th century the we get similar intervals for the distances between years. This can be inspected better graphically in the next \"parts\" tab.\n\n###### parts\n\n::: {.callout-note style=\"color: blue;\"}\nTThe code is different but the text is the same as in tab \"parts\" of @exm-chap04-fit-cherry-blossoms-m4-7a.\n:::\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-chap04-plot-parts-d2-m4-7a}\nb: Plot data with equally number of `doy` data points for each segment against `year` \n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-plot-parts-d2-m4-7b_26dc9cefd85ed9cc8c86158c113dba13'}\n\n```{.r .cell-code}\nd2_m4.7b |> \n  ggplot2::ggplot(ggplot2::aes(x = year, y = doy)) +\n  ggplot2::geom_vline(xintercept = knot_list_b, \n             color = \"white\", alpha = 1/2) +\n  ggplot2::geom_point(color = \"#ffb7c5\", alpha = 1/2) +\n  ggplot2::theme_bw() +\n  ggplot2::theme(panel.background = ggplot2::element_rect(fill = \"#4f455c\"),\n        panel.grid = ggplot2::element_blank())\n```\n\n::: {.cell-output-display}\n![Display raw data for `doy` (Day of the year of first blossom) against the year with vertical lines at the knots positions (Tidyverse)](04-geocentric-models_files/figure-html/fig-chap04-plot-parts-d2-m4-7b-1.png){#fig-chap04-plot-parts-d2-m4-7b width=672}\n:::\n:::\n\n\n::::\n:::::\n\n\n\nStarting with a the 16th century the we get similar intervals for the distances between years, e.g. the number of `doy` data points is approximately evenly distributed (59 - 67 years)\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-knot_list-diff-d2-m4-7b_48ba2f928e44dd3145a73cd2f0bbf8d3'}\n\n```{.r .cell-code}\nknot_list2 <- tibble::enframe(knot_list) |> \n  dplyr::mutate(diff = round(value - dplyr::lag(value), 0))\nprint(knot_list2, n = 15)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 15 × 3\n#>    name      value  diff\n#>    <chr>     <dbl> <dbl>\n#>  1 0%          812    NA\n#>  2 7.142857%  1036   224\n#>  3 14.28571%  1174   138\n#>  4 21.42857%  1269    95\n#>  5 28.57143%  1377   108\n#>  6 35.71429%  1454    77\n#>  7 42.85714%  1518    64\n#>  8 50%        1583    65\n#>  9 57.14286%  1650    67\n#> 10 64.28571%  1714    64\n#> 11 71.42857%  1774    60\n#> 12 78.57143%  1833    59\n#> 13 85.71429%  1893    60\n#> 14 92.85714%  1956    63\n#> 15 100%       2015    59\n```\n\n\n:::\n:::\n\n\n\n\n###### calc\n\n::: {.callout-note style=\"color: blue;\"}\nThis is the same code and text as in tab \"calc\" of @exm-chap04-fit-cherry-blossoms-m4-7a.\n:::\n\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-str-splines-m4-7b}\nb: Calculate basis functions of a cubic spline (degree = 3) with 15 areas (knots) to fit\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-str-splines-m4-7b_8f0bcba1c77e486136b914980925273b'}\n\n```{.r .cell-code}\n## R code 4.74b ################\nB_m4.7b <- splines::bs(d2_m4.7b$year,            # (1) generate B-splines\n  knots = knot_list_b[-c(1, num_knots_b)],       # (2) knots without first & last\n  degree = 3,                                    # (3) polynomial (cubic) degree\n  intercept = TRUE)                              # (4) intercept\nstr(B_m4.7b)                                     # (5) show data structure\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  'bs' num [1:827, 1:17] 1 0.96 0.767 0.563 0.545 ...\n#>  - attr(*, \"dimnames\")=List of 2\n#>   ..$ : NULL\n#>   ..$ : chr [1:17] \"1\" \"2\" \"3\" \"4\" ...\n#>  - attr(*, \"degree\")= int 3\n#>  - attr(*, \"knots\")= Named num [1:13] 1036 1174 1269 1377 1454 ...\n#>   ..- attr(*, \"names\")= chr [1:13] \"7.142857%\" \"14.28571%\" \"21.42857%\" \"28.57143%\" ...\n#>  - attr(*, \"Boundary.knots\")= int [1:2] 812 2015\n#>  - attr(*, \"intercept\")= logi TRUE\n```\n\n\n:::\n:::\n\n\n**Explanation of code lines**\n\n(1) The function `splines::bs()` generate the B-spline basis matrix for\n    a polynomial spline:\n(2) `knots` is generated without the two boundary knots (fist and last\n    knot) that are placed at the minimum and maximum of the variable.\n    These two knots are excluded with the tricky code `knot_list[-c(1, num_knots)]` to prevent redundancies as\n    `splines::bs()` places by default knots at the boundaries. So we\n    have 13 internal knots.\n(3) With `degree = 3` a cubic B-spline is chosen. The polynomial degree determines how basis functions combine, which determines how the parameters interact to\nproduce the spline. For degree 1, (=\n    line), two basis functions combine at each point. For degree 2 (=\n    quadratic), three functions combine at each point. For degree 3 (=\n    cubic), there are four basis functions combined. This should give\n    enough flexibility for each region to fit.\n(4) McElreath chose `intercept = TRUE`: \\> \"We'll also have an intercept\n    to capture the average blossom day. This will make it easier to\n    define priors on the basis weights, because then we can just\n    conceive of each as a deviation from the intercept.\" ([McElreath,\n    2020, p. 117](zotero://select/groups/5243560/items/NFUEVASQ))\n    ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=136&annotation=9MA7DU8W))\\\n    Kurz mentioned ominously: \"For reasons I'm not prepared to get into,\n    here, splines don't always include intercept parameters. Indeed, the\n    `bs()` default is `intercept = FALSE`.\"\n    ([Kurz](https://bookdown.org/content/4857/geocentric-models.html#splines.)).\\\n    I do not know exactly what's the result of choosing\n    `intercept = TRUE` as the difference to FALSE is marginal. The first\n    B-spline (third hill) starts on the left edge of the graph with\n    $1.0$ instead of $0$.\n(5) In the data structure you can see in the second to last line, that there are two\n    \"Boundary.knots\" at year $812$ and $2015$. These two years are in\n    fact the first and last value of the `year` variable:\n\n-   `dplyr::first(d2_m4.7b$year)`: 812\n-   `dplyr::last(d2_m4.7b$year)` : 2015\n\n::::\n:::::\n\n###### basis\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-chap04-basis-splines-m4-7b}\na: Draw raw basis functions for the year variable for 15 areas (knots) and degree 3 (= cubic polynomial)\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-basis-splines-m4-7b_4ec54678429e6a784110c31d63ead0c0'}\n\n```{.r .cell-code}\n# wrangle a bit\nd_B_m4.7b <-\n  B_m4.7b |> \n  base::as.data.frame() |> \n  rlang::set_names(stringr::str_c(0, 1:9), 10:17) |>  \n  dplyr::bind_cols(dplyr::select(d2_m4.7b, year)) |> \n  tidyr::pivot_longer(-year,\n               names_to = \"bias_function\",\n               values_to = \"bias\")\n\n# plot\nd_B_m4.7b |> \n  ggplot2::ggplot(ggplot2::aes(x = year, y = bias, group = bias_function)) +\n  ggplot2::geom_vline(xintercept = knot_list_b, color = \"white\", alpha = 1/2) +\n  ggplot2::geom_line(color = \"#ffb7c5\", alpha = 1/2, linewidth = 1.5) +\n  ggplot2::ylab(\"bias value\") +\n  ggplot2::theme_bw() +\n  ggplot2::theme(panel.background = ggplot2::element_rect(fill = \"#4f455c\"),\n        panel.grid = ggplot2::element_blank())\n```\n\n::: {.cell-output-display}\n![Draw raw basis functions B-splines for the year variable for 15 areas (knots) and degree 3 (cubic polynomial) (Tidyverse)](04-geocentric-models_files/figure-html/fig-chap04-basis-splines-m4-7b-1.png){#fig-chap04-basis-splines-m4-7b width=672}\n:::\n:::\n\n\n\n::::\n:::::\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nWATCH OUT! Warning with `tibble::as_tibble()` instead of `base::as.data.frame()`\n:::\n::::{.my-watch-out-container}\nAt first I used as always `tibble::as_tibble()`. But this generated a warning message.\n\n> Don't know how to automatically pick scale for object of type <bs/basis/matrix>. Defaulting to continuous.\n\nI learned that it has to do with the difference of a variable name versus a function name in `aes()`. For instance \"mean\" versus variable \"Mean\" ([statology](https://www.statology.org/r-dont-know-how-to-automatically-pick-scale-for-object-type-function/)) or \"sample\" versus \"Sample\" ([StackOverflow](https://stackoverflow.com/questions/22058322/ggplot-error-dont-know-how-to-automatically-pick-scale-for-object-of-type-func)).\n\nBut why this is the case with `tibble::as_tibble()` but not with `base::as.data.frame()` I do not know. Perhaps it has to do with the somewhat stricter tibble naming rules? (see: [tibble overview](https://tibble.tidyverse.org/) and [tbl_df class](https://tibble.tidyverse.org/reference/tbl_df-class.html)).\n::::\n:::::\n\n\n\n###### facet\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-chap04-facets-m4-7b}\nb: Basis functions of a cubic spline with 15 knots broken up in different facets\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-facets-m4-7b_5ebf20611242ccca815d5ae8b0b4bd8e'}\n\n```{.r .cell-code}\nd_B_m4.7b |> \n  dplyr::mutate(bias_function = stringr::str_c(\"bias function \", \n                               bias_function)) |> \n  ggplot2::ggplot(ggplot2::aes(x = year, y = bias)) +\n  ggplot2::geom_vline(xintercept = knot_list_b, \n             color = \"white\", alpha = 1/2) +\n  ggplot2::geom_line(color = \"#ffb7c5\", linewidth = 1.5) +\n  ggplot2::ylab(\"bias value\") +\n  ggplot2::theme_bw() +\n  ggplot2::theme(panel.background = ggplot2::element_rect(fill = \"#4f455c\"),\n        panel.grid = ggplot2::element_blank(),\n        strip.background = ggplot2::element_rect(\n          fill = scales::alpha(\"#ffb7c5\", .25), color = \"transparent\"),\n        strip.text = ggplot2::element_text(\n          size = 8, margin = ggplot2::margin(0.1, 0, 0.1, 0, \"cm\"))) +\n  ggplot2::facet_wrap(~ bias_function, ncol = 1)\n```\n\n::: {.cell-output-display}\n![Basis functions of a cubic spline with 15 knots broken up in different facets](04-geocentric-models_files/figure-html/fig-chap04-facets-m4-7b-1.png){#fig-chap04-facets-m4-7b width=672}\n:::\n:::\n\n\nTo see what's going on in that plot, we tool Kurz code to break the graphic up for displaying all basis function in an extra slot with `ggplot2::facet_wrap()`.\n\n::::\n:::::\n\n###### exp\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-chap04-exp-prior-m4-7b}\nb: Using the density of the exponential distribution `dexp()` to show the prior\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell fig.heigt='2.5' hash='04-geocentric-models_cache/html/fig-chap04-exp-prior-m4-7b_05c85f605b2804baa2ef225b3eacbe06'}\n\n```{.r .cell-code}\ntibble::tibble(x = base::seq(from = 0, to = 10, by = 0.1)) |> \n  dplyr::mutate(d = stats::dexp(x, rate = 1)) |> \n  \n  ggplot2::ggplot(ggplot2::aes(x = x, y = d)) +\n  ggplot2::geom_area(fill = \"grey\") +\n  ggplot2::scale_y_continuous(NULL, breaks = NULL) +\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Using the density of the exponential distribution `dexp()` to show the prior](04-geocentric-models_files/figure-html/fig-chap04-exp-prior-m4-7b-1.png){#fig-chap04-exp-prior-m4-7b width=384}\n:::\n:::\n\n\n::::\n:::::\n\nWe used the `dexp()` function for the model (see tab \"formula\" in @exm-chap04-fit-cherry-blossoms-m4-7a). To get a sense of what that prior looks like, we displayed the `dexp()` function.\n\n###### matrix\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-wrangle-d3-m4-7b}\nb: Add B-splines matrix for 15 knots to the data frame by creating a new data frame\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-wrangle-d3-m4-7b_cdc641b7919c9860198fabcd517949e9'}\n\n```{.r .cell-code}\nd3_m4.7b <-\n  d2_m4.7b |> \n  dplyr::mutate(B_m4.7b = B_m4.7b) \n\n# take a look at the structure of the new data frame\nd3_m4.7b |> \n  dplyr::glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Rows: 827\n#> Columns: 6\n#> $ year       <int> 812, 815, 831, 851, 853, 864, 866, 869, 889, 891, 892, 894,…\n#> $ doy        <int> 92, 105, 96, 108, 104, 100, 106, 95, 104, 109, 108, 106, 10…\n#> $ temp       <dbl> NA, NA, NA, 7.38, NA, 6.42, 6.44, NA, 6.83, 6.98, 7.11, 6.9…\n#> $ temp_upper <dbl> NA, NA, NA, 12.10, NA, 8.69, 8.11, NA, 8.48, 8.96, 9.11, 8.…\n#> $ temp_lower <dbl> NA, NA, NA, 2.66, NA, 4.14, 4.77, NA, 5.19, 5.00, 5.11, 5.5…\n#> $ B_m4.7b    <bs[,17]> <bs[26 x 17]>\n```\n\n\n:::\n:::\n\n::::\n:::::\n\nIn R code 4.76a (tab \"quap\" in @exm-chap04-fit-cherry-blossoms-m4-7b), McElreath\ndefined his data in a list (`list(D = d2_m4.7a$doy, B_m4.7a = B_m4.7a)`). The approach with {**brms**} will be a little different. We'll add the `B_m4.7b` matrix to our `d2_m4.7b` data frame and name the results as `d3_m4.7b`.\n\n\n> In the `d3_m4.7b` data, columns `year` through `temp_lower` are all standard\ndata columns. The `B_m4-7b` column is a *matrix column*, which contains the\nsame number of rows as the others, but also smuggled in 17 columns\n*within* that column. Each of those 17 columns corresponds to one of our\nsynthetic $B_{k}$ variables. The advantage of such a data structure is\nwe can simply define our `formula` argument as $doy \\sim 1 + B_m4-7b$, where\n`B_m4-7b` is a stand-in for `B_m4-7b.1 + B_m4-7b.2 + ... + B_m4-7b.17`. ([Kurz](https://bookdown.org/content/4857/geocentric-models.html#splines.)) \n\nSee next tab \"brm\".\n\n###### brm\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-fit-model-m4-7b}\nb: Fit model m4.7b with `brms::brm()`\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-fit-model-m4-7b_b2ba0be4815ff0138f05b497df01c8c2'}\n\n```{.r .cell-code}\nm4.7b <- \n  brms::brm(data = d3_m4.7b,\n      family = gaussian,\n      doy ~ 1 + B_m4.7b,\n      prior = c(brms::prior(normal(100, 10), class = Intercept),\n                brms::prior(normal(0, 10), class = b),\n                brms::prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"brm_fits/m04.07b\")\n\nbrms:::print.brmsfit(m4.7b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: doy ~ 1 + B_m4.7b \n#>    Data: d3_m4.7b (Number of observations: 827) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   103.48      2.44    98.62   108.27 1.00      590      932\n#> B_m4.7b1     -3.14      3.89   -10.68     4.52 1.00     1780     2472\n#> B_m4.7b2     -0.92      4.02    -8.63     6.92 1.00     1331     1862\n#> B_m4.7b3     -1.25      3.61    -8.20     6.12 1.00     1233     1842\n#> B_m4.7b4      4.73      3.02    -1.27    10.59 1.00      875     1345\n#> B_m4.7b5     -0.98      2.93    -6.63     4.90 1.00      792     1394\n#> B_m4.7b6      4.18      2.96    -1.66    10.07 1.00      883     1470\n#> B_m4.7b7     -5.44      2.84   -11.00     0.20 1.00      820     1325\n#> B_m4.7b8      7.71      2.88     2.06    13.19 1.00      719     1430\n#> B_m4.7b9     -1.13      2.94    -6.81     4.63 1.00      812     1371\n#> B_m4.7b10     2.87      2.94    -2.95     8.58 1.00      851     1393\n#> B_m4.7b11     4.53      2.96    -1.15    10.53 1.00      820     1265\n#> B_m4.7b12    -0.31      2.95    -6.20     5.45 1.00      805     1411\n#> B_m4.7b13     5.43      2.98    -0.34    11.27 1.00      849     1315\n#> B_m4.7b14     0.55      3.02    -5.38     6.39 1.00      953     1506\n#> B_m4.7b15    -0.90      3.38    -7.52     5.78 1.00     1012     1742\n#> B_m4.7b16    -7.08      3.42   -13.99    -0.42 1.00     1116     1982\n#> B_m4.7b17    -7.79      3.29   -14.37    -1.20 1.00     1074     1763\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     5.95      0.15     5.66     6.25 1.00     5900     2573\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n> Each of the 17 columns in our `B` matrix was assigned its\nown parameter. If you fit this model using McElreath's rethinking code,\nyou'll see the results are very similar. (Kurz, ibid.)\n\n###### glimpse\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-chap04-glimpse-model-m4-7b}\nb: Glimpse at transformed data of model `m4.7b` to draw objects\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/chap04-glimpse-model-m4-7b_0c3c77dc13ccdcb0f5ca44515e550d4a'}\n\n```{.r .cell-code}\npost_m4.7b <- brms::as_draws_df(m4.7b)\n\nglimpse(post_m4.7b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Rows: 4,000\n#> Columns: 24\n#> $ b_Intercept <dbl> 100.6156, 100.4937, 103.1678, 102.5159, 101.5559, 101.7562…\n#> $ b_B_m4.7b1  <dbl> 2.90164141, 3.32435081, -2.70015340, -0.02253457, -1.97050…\n#> $ b_B_m4.7b2  <dbl> -2.1067230, -0.5311967, 0.6794133, -2.8772090, 3.4878935, …\n#> $ b_B_m4.7b3  <dbl> 2.9992344, 6.5131175, 0.8860770, 1.2321642, -1.3893031, 1.…\n#> $ b_B_m4.7b4  <dbl> 6.1257337, 4.9927835, 5.3287564, 4.6657258, 7.8084901, 4.9…\n#> $ b_B_m4.7b5  <dbl> 1.16796045, 2.69443262, -1.14483788, -0.87995893, 3.187535…\n#> $ b_B_m4.7b6  <dbl> 9.5029937, 5.2401181, 4.1763796, 7.3521813, 3.7853630, 7.5…\n#> $ b_B_m4.7b7  <dbl> -3.8974486, -2.5350548, -3.8448680, -1.7500613, -3.5003478…\n#> $ b_B_m4.7b8  <dbl> 10.234376, 12.795946, 5.555182, 7.698008, 11.170412, 8.328…\n#> $ b_B_m4.7b9  <dbl> -1.516545385, 0.965091340, 3.329924529, 0.450062993, 1.554…\n#> $ b_B_m4.7b10 <dbl> 6.48286406, 6.06286271, 0.64513422, 4.83968514, 3.35499895…\n#> $ b_B_m4.7b11 <dbl> 7.79660448, 7.56857405, 5.66668708, 3.36413271, 9.02955370…\n#> $ b_B_m4.7b12 <dbl> 1.8447173, -0.7395718, 1.8709336, 1.6117838, -0.9294026, 2…\n#> $ b_B_m4.7b13 <dbl> 10.26433473, 10.88433479, 4.55267435, 8.00798110, 6.444273…\n#> $ b_B_m4.7b14 <dbl> -1.8069079, -0.8779897, 5.5164497, -0.4733169, 7.1190998, …\n#> $ b_B_m4.7b15 <dbl> 9.7734694, 8.8519378, -4.2358415, -1.7802159, 0.1933067, 0…\n#> $ b_B_m4.7b16 <dbl> -9.2641966, -14.0515369, -3.1279428, 0.1699908, -7.1740999…\n#> $ b_B_m4.7b17 <dbl> -0.007993781, -1.342456078, -13.852015496, -9.733215118, -…\n#> $ sigma       <dbl> 5.639404, 5.677425, 6.229067, 6.289891, 6.191738, 5.977885…\n#> $ lprior      <dbl> -67.06532, -67.63332, -66.43315, -66.13359, -66.75463, -65…\n#> $ lp__        <dbl> -2723.712, -2724.698, -2719.803, -2721.983, -2720.532, -27…\n#> $ .chain      <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#> $ .iteration  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n#> $ .draw       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\nWe used `brms::as_draws_df()` to transform `m4.7b` to a `draw`\nobject so that it can processed easier by the {**posterior**} package.\n\n###### weighted\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-chap04-weighted-m4-7b}\nb: Weight each basis function by its corresponding parameter\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-weighted-m4-7b_2a1a6d3ede0e577923ca7c94f7254eb4'}\n\n```{.r .cell-code}\npost_m4.7b |> \n  dplyr::select(b_B_m4.7b1:b_B_m4.7b17) |> \n  rlang::set_names(base::c(stringr::str_c(0, 1:9), 10:17)) |> \n  tidyr::pivot_longer(tidyselect::everything(), \n                      names_to = \"bias_function\") |> \n  dplyr::group_by(bias_function) |> \n  dplyr::summarise(weight = base::mean(value)) |> \n  ## add weight column to year & bias via \"bias_function\"\n  dplyr::full_join(d_B_m4.7b, by = \"bias_function\") |> \n  \n  # plot\n  ggplot2::ggplot(ggplot2::aes(x = year, \n                        y = bias * weight, \n                        group = bias_function)) +\n  ggplot2::geom_vline(xintercept = knot_list_b, \n                      color = \"white\", alpha = 1/2) +\n  ggplot2::geom_line(color = \"#ffb7c5\", \n                     alpha = 1/2, linewidth = 1.5) +\n  ggplot2::theme_bw() +\n  ggplot2::theme(panel.background = \n                 ggplot2::element_rect(fill = \"#4f455c\"),\n                 panel.grid = ggplot2::element_blank()) \n```\n\n::: {.cell-output-display}\n![Each basis function weighted by its corresponding parameter](04-geocentric-models_files/figure-html/fig-chap04-weighted-m4-7b-1.png){#fig-chap04-weighted-m4-7b width=672}\n:::\n:::\n\n\n> In case you missed it, the main action in the {**ggplot2**} code was `y = bias * weight`, where we defined the $y$-axis as the product of `bias` and `weight`. This is fulfillment of the $w_k B_{k, i}$ parts of the model. (Kurz, ibid.)\n\n\n::::\n:::::\n\n###### plot2\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-chap04-post-m4-7b}\nb: Expected values of the posterior predictive distribution\n::::::\n:::\n::::{.my-r-code-container}\n\n\n\n::: {.cell hash='04-geocentric-models_cache/html/fig-chap04-post-pred-dist-m4-7b_364fdfdf56772edb863994506acb194c'}\n\n```{.r .cell-code}\nf_m4.7b <- brms:::fitted.brmsfit(m4.7b, probs = c(0.055, 0.945))\n\nf_m4.7b |> \n  base::as.data.frame() |> \n  dplyr::bind_cols(d3_m4.7b) |> \n  \n  ggplot2::ggplot(ggplot2::aes(x = year, y = doy, \n                               ymin = Q5.5, ymax = Q94.5)) + \n  ggplot2::geom_vline(xintercept = knot_list_b, \n                      color = \"white\", alpha = 1/2) +\n  ggplot2::geom_hline(yintercept = \n                      brms::fixef(m4.7b, \n                                  probs = c(0.055, 0.945))[1, 1], \n                      color = \"white\", linetype = 2) +\n  ggplot2::geom_point(color = \"#ffb7c5\", alpha = 1/2) +\n  ggplot2::geom_ribbon(fill = \"white\", alpha = 2/3) +\n  ggplot2::labs(x = \"year\", y = \"day in year\") +\n  ggplot2::theme_bw() +\n  ggplot2::theme(panel.background = \n                 ggplot2::element_rect(fill = \"#4f455c\"),\n                 panel.grid = ggplot2::element_blank())\n```\n\n::: {.cell-output-display}\n![Expected values of the posterior predictive distribution](04-geocentric-models_files/figure-html/fig-chap04-post-pred-dist-m4-7b-1.png){#fig-chap04-post-pred-dist-m4-7b width=672}\n:::\n:::\n\n\n> If it wasn’t clear, the dashed horizontal line intersecting a little above $100$ on the $y$-axis is the posterior mean for the intercept. (Kurz, ibid.)\n\nIn contrast to Kurz’ code I have used `probs = c(0.055, 0.945)` to get the same 89% (Q5.5 and Q94.5) intervals as in the book.\n\n::::\n:::::\n\n\n:::\n\n::::\n:::::\n\n\n### Smooth functions for a rough world\n\n> “The splines in the previous section are just the beginning. A entire class of models, generalized additive models (<a class='glossary' title='A Generalized Additive Model (GAM) is a generalized linear model in which the linear response variable depends linearly on unknown smooth functions of some predictor variables, and interest focuses on inference about these smooth functions. They can be interpreted as the discriminative generalization of the naive Bayes generative model. (Wikipedia). | GAMs relax the restriction that the relationship must be a simple weighted sum, and instead assume that the outcome can be modelled by a sum of arbitrary functions of each feature. (Medium member story) (Chap.4)'>GAM</a>s), focuses on predicting an outcome variable using smooth functions of some predictor variables.” ([McElreath, 2020, p. 120](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=139&annotation=UX53SXQD))\n\n:::::{.my-resource}\n:::{.my-resource-header}\nResources for working with Generalized Additive Models (GAMs)\n:::\n::::{.my-resource-container}\n-   Wood, S. N. (2017). Generalized Additive Models: An Introduction\n    with R, Second Edition (2nd ed.). Taylor & Francis Inc.\n-   SemanticScholar: [Series of paper dedicated to\n    GAMs](https://www.semanticscholar.org/paper/Generalized-Additive-Models%3A-An-Introduction-with-R-G%C3%B3mez%E2%80%90Rubio/025f25133a5c1da746eb7e7719bb715b71a7f518)\n-   Anish Singh Walia: [Generalized Additive\n    Model](https://datascienceplus.com/generalized-additive-models/)\n-   Noam Ross: [GAMs in\n    R](https://noamross.github.io/gams-in-r-course/): A Free,\n    Interactive Course using `mgcv`\n-   Michael Clark: [Generalized Additive\n    Models](https://m-clark.github.io/generalized-additive-models/)\n-   Dheeraj Vaidya: [Generalized Additive\n    Model](https://www.wallstreetmojo.com/generalized-additive-model/)\n\n------------------------------------------------------------------------\n\n-   Adam Shaif: What is Generalised Additive Model? ([Medium member\n    story](https://towardsdatascience.com/generalised-additive-models-6dfbedf1350a))\n-   Eugenio Anello: Generalized Additive Models with R ([Medium member\n    story](https://pub.towardsai.net/generalized-additive-models-with-r-5f01c8e52089))\n\n***\n\nIn the bonus section [Kurz](https://bookdown.org/content/4857/geocentric-models.html#summary-first-bonus-smooth-functions-with-brmss) added some more references for GAMs:\n\n-   For more on the B-splines and smooths, more generally, check out the\n    blog post by the great [Gavin Simpson](https://twitter.com/ucfagls),\n    [Extrapolating with B splines and\n    GAMs](https://fromthebottomoftheheap.net/2020/06/03/extrapolating-with-gams/).\n-   For a high-level introduction to the models you can fit with\n    {**mgcv**}, check out the nice talk by [Noam\n    Ross](https://twitter.com/noamross), [Nonlinear models in R: The\n    wonderful world of mgcv](https://youtu.be/q4_t8jXcQgc), or the\n    equally-nice presentation by Simpson, [Introduction to generalized\n    additive models with R and mgcv](https://youtu.be/sgw4cu8hrZM).\n-   Ross offers a free online course covering {**mgcv**}, called [GAMS\n    in R](https://noamross.github.io/gams-in-r-course/), and he\n    maintains a GitHub repo cataloging other GAM-related resources,\n    called [Resources for learning about and using GAMs in\n    R](https://github.com/noamross/gam-resources).\n-   For specific examples of fitting various GAMS with {**brms**}, check\n    out Simpson's blog post, [Fitting GAMs with brms: part\n    1](https://fromthebottomoftheheap.net/2018/04/21/fitting-gams-with-brms/).\n-   Finally, [Tristan Mahr](https://twitter.com/tjmahr) has a nice blog\n    post called [Random effects and penalized splines are the same\n    thing](https://www.tjmahr.com/random-effects-penalized-splines-same-thing/),\n    where he outlined the connections between penalized smooths, such as\n    you might fit with {**mgcv**}, with the multilevel model, which\n    we'll learn all about starting in @sec-chap13.\n    \n::::\n:::::\n\nIn this reference Kurz announces that in @sec-chap13 we will understand better what's going on with the `s()` function that he described in his first bonus section. As I didn't understand this bonus section I skipped it and will come back to this section when I have understood more on GAMs and related model fitting procedures. \n\nIn the second bonus section Kurz explained that compact syntax to pass a matrix column of predictors into the formula as used in tab \"matrix\" in @exm-chap04-fit-cherry-blossoms-m4-7b was not an isolated trick but is a general approach. He referenced an example of a multiple regression\nmodel in Section 11.2.6 of the book \"Regression and Other Stories\", by Gelman, Hill, and Vehtari. I skip this second bonus section too as I do not have sufficient knowledge to fully understand the procedure.\n\n## Practice\n\nProblems are labeled Easy (E), Medium (M), and Hard (H).\n\n### 4E1\n\nIn the model definition below, which line is the likelihood?\n\n$$\ny_{i} \\sim \\operatorname{Normal}(\\mu, \\sigma) \\\\\n\\mu \\sim \\operatorname{Normal}(0, 10) \\\\\n\\sigma \\sim \\operatorname{Exponential}(1)\n$$ {#eq-4e1}\n\n**My answer**: $y_{i} \\sim \\operatorname{Normal}(\\mu, \\sigma)$, the\nother lines are priors.\n\n### 4E2\n\nIn the model definition just above, how many parameters are in the\nposterior distribution?\n\n**My answer**: Just one.\n\n::: callout-warning\n#### Wrong Answer\n\nIn the definition @eq-4e1 $y_{i}$ is not to be estimated, but represents\nthe data we have at hand and want to understand through parameters.\nErroneously I took this for the parameter. The correct answer is $\\mu$\nand §\\sigma\\$ as both are the parameters which we attempt to estimate.\nSo the correct answer is: Two\n:::\n\n### 4E3\n\nUsing the model definition above, write down the appropriate form of\n<a class='glossary' title='This is the theorem that gives Bayesian data analysis its name. But the theorem itself is a trivial implication of probability theory. The mathematical definition of the posterior distribution arises from Bayes’ Theorem. The key lesson is that the posterior is proportional to the product of the prior and the probability of the data. (Chap.2)'>Bayes’ theorem</a> that includes the proper likelihood and\npriors.\n\n### 4E4\n\nIn the model definition below, which line is the linear model?\n\n$$\ny_{i} \\sim \\operatorname{Normal}(\\mu, \\sigma) \\\\\n\\mu_{i} = \\alpha + \\beta{x_{i}} \\\\\n\\alpha \\sim \\operatorname{Normal}(0, 10) \\\\\n\\beta \\sim \\operatorname{Normal}(0, 1) \\\\\n\\sigma \\sim \\operatorname{Exponential}(1)\n$$ **My answer**: $\\mu_{i} = \\alpha + \\beta{x_{i}}$\n\n### 4E5\n\nIn the model definition just above, how many parameters are in the\nposterior distribution?\n\n**My answer**: There are three parameters.\n\n\n::: {.cell hash='04-geocentric-models_cache/html/session-info_bed34cc5ea71f2833cc2677e314897f8'}\n\n```{.r .cell-code #lst-session-info lst-cap=\"Session info\"}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> R version 4.3.2 (2023-10-31)\n#> Platform: x86_64-apple-darwin20 (64-bit)\n#> Running under: macOS Sonoma 14.1.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n#> \n#> locale:\n#> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n#> \n#> time zone: Europe/Vienna\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] patchwork_1.1.3     lubridate_1.9.3     forcats_1.0.0      \n#>  [4] stringr_1.5.1       dplyr_1.1.4         purrr_1.0.2        \n#>  [7] readr_2.1.4         tidyr_1.3.0         tibble_3.2.1       \n#> [10] ggplot2_3.4.4       tidyverse_2.0.0     glossary_1.0.0.9000\n#> \n#> loaded via a namespace (and not attached):\n#>   [1] tensorA_0.36.2       rstudioapi_0.15.0    jsonlite_1.8.7      \n#>   [4] shape_1.4.6          magrittr_2.0.3       TH.data_1.1-2       \n#>   [7] estimability_1.4.1   farver_2.1.1         rmarkdown_2.25      \n#>  [10] vctrs_0.6.4          base64enc_0.1-3      htmltools_0.5.7     \n#>  [13] distributional_0.3.2 curl_5.1.0           tidybayes_3.0.6     \n#>  [16] StanHeaders_2.26.28  KernSmooth_2.23-22   htmlwidgets_1.6.2   \n#>  [19] plyr_1.8.9           sandwich_3.0-2       emmeans_1.8.9       \n#>  [22] zoo_1.8-12           commonmark_1.9.0     igraph_1.5.1        \n#>  [25] mime_0.12            lifecycle_1.0.4      pkgconfig_2.0.3     \n#>  [28] colourpicker_1.3.0   Matrix_1.6-3         R6_2.5.1            \n#>  [31] fastmap_1.1.1        shiny_1.8.0          digest_0.6.33       \n#>  [34] colorspace_2.1-0     ps_1.7.5             brms_2.20.4         \n#>  [37] crosstalk_1.2.0      labeling_0.4.3       fansi_1.0.5         \n#>  [40] timechange_0.2.0     mgcv_1.9-0           abind_1.4-5         \n#>  [43] compiler_4.3.2       withr_2.5.2          backports_1.4.1     \n#>  [46] inline_0.3.19        shinystan_2.6.0      rethinking_2.40     \n#>  [49] QuickJSR_1.0.7       pkgbuild_1.4.2       MASS_7.3-60         \n#>  [52] gtools_3.9.4         loo_2.6.0            tools_4.3.2         \n#>  [55] httpuv_1.6.12        threejs_0.3.3        glue_1.6.2          \n#>  [58] callr_3.7.3          nlme_3.1-163         promises_1.2.1      \n#>  [61] grid_4.3.2           cmdstanr_0.5.3       checkmate_2.3.0     \n#>  [64] reshape2_1.4.4       generics_0.1.3       isoband_0.2.7       \n#>  [67] gtable_0.3.4         tzdb_0.4.0           hms_1.1.3           \n#>  [70] xml2_1.3.5           utf8_1.2.4           pillar_1.9.0        \n#>  [73] ggdist_3.3.0         markdown_1.11        posterior_1.5.0     \n#>  [76] later_1.3.1          splines_4.3.2        lattice_0.22-5      \n#>  [79] survival_3.5-7       tidyselect_1.2.0     miniUI_0.1.1.1      \n#>  [82] knitr_1.45           arrayhelpers_1.1-0   gridExtra_2.3       \n#>  [85] V8_4.4.0             rversions_2.1.2      stats4_4.3.2        \n#>  [88] xfun_0.41            bridgesampling_1.1-2 skimr_2.1.5         \n#>  [91] matrixStats_1.1.0    DT_0.30              rstan_2.32.3        \n#>  [94] stringi_1.8.1        yaml_2.3.7           evaluate_0.23       \n#>  [97] codetools_0.2-19     cli_3.6.1            RcppParallel_5.1.7  \n#> [100] shinythemes_1.2.0    xtable_1.8-4         repr_1.1.6          \n#> [103] munsell_0.5.0        processx_3.8.2       Rcpp_1.0.11         \n#> [106] coda_0.19-4          svUnit_1.0.6         parallel_4.3.2      \n#> [109] rstantools_2.3.1.1   ellipsis_0.3.2       prettyunits_1.2.0   \n#> [112] dygraphs_1.1.1.6     bayesplot_1.10.0     Brobdingnag_1.2-9   \n#> [115] viridisLite_0.4.2    mvtnorm_1.2-3        scales_1.2.1        \n#> [118] xts_0.13.1           crayon_1.5.2         rlang_1.1.2         \n#> [121] multcomp_1.4-25      shinyjs_2.1.0\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
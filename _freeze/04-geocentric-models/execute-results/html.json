{
  "hash": "b53240b2f976bcf003a2d7a73ed8d047",
  "result": {
    "markdown": "# Geocentric Models\n\n## File setup {.unnumbered}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.3     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.2     \n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\nlibrary(patchwork)\n```\n:::\n\n\n## `ORIGINAL`\n\n### Why normal distributions? {#sec-why-normal-dist-a}\n\nWhy are there so many distribution approximately [normal]{.smallcaps},\nresulting in a Gaussian curve? Because there will be more combinations\nof outcomes that sum up to a \"central\" value, rather than to some\nextreme value.\n\n****\n:::: {#prp-why-normal}\nWhy are normal distribution normal?\n\n::: callout-important\nAny process that adds together random values from the same distribution\nconverges to a normal.\n:::\n\n::::\n***\n\n\n#### Normal by addition\n\nWhatever the average value of the source distribution, each sample from\nit can be thought of as a fluctuation from that average value. When we\nbegin to add these fluctuations together, they also begin to cancel one\nanother out. A large positive fluctuation will cancel a large negative\none. The more terms in the sum, the more chances for each fluctuation to\nbe canceled by another, or by a series of smaller ones in the opposite\ndirection. So eventually the most likely sum, in the sense that there\nare the most ways to realize it, will be a sum in which every\nfluctuation is canceled by another, a sum of zero (relative to the\nmean).\n\nIt doesn't matter what shape the underlying distribution possesses. It\ncould be uniform, like in our example above, or it could be (nearly)\nanything else. Depending upon the underlying distribution, the\nconvergence might be slow, but it will be inevitable.\n\nSee the excellent article [Why is normal distribution so\nubiquitous?](https://ekamperi.github.io/mathematics/2021/01/29/why-is-normal-distribution-so-ubiquitous.html)\nwhich also explains the example of random walks from SR2. See also the\nscientific paper [Why are normal distribution\nnormal?](https://www.journals.uchicago.edu/doi/pdf/10.1093/bjps/axs046)\nof the The British Journal for the Philosophy of Science.\n\n#### Normal by multiplication\n\nThis is not only valid for addition but also for multiplication of small\nvalues: Multiplying small numbers is approximately the same as addition.\n\n#### Normal by log-multipliation\n\nBut even the multiplication of large values tend to produce Gaussian\ndistributions on the log scale.\n\n#### Using Gaussian distributions\n\nThe justifications for using the Gaussian distribution fall into two\nbroad categories:\n\n1.  **Ontological justification**: The world is full of Gaussian\n    distributions, approximately. We're never going to experience a\n    perfect Gaussian distribution. But it is a widespread pattern,\n    appearing again and again at different scales and in different\n    domains. Measurement errors, variations in growth, and the\n    velocities of molecules all tend towards Gaussian distributions.\n\nThere are many other patterns in nature, so make no mistake in assuming\nthat the Gaussian pattern is universal. In later chapters, we'll see how\nother useful and common patterns, like the exponential and gamma and\nPoisson, also arise from natural processes. The Gaussian is a member of\na family of fundamental natural distributions known as the **Exponential\nfamily**. All of the members of this family are important for working\nscience, because they populate our world.\n\n2.  **Epistemological justification**: The Gaussian represents a\n    particular state of ignorance. When all we know or are willing to\n    say about a distribution of measures (measures are continuous values\n    on the real number line) is their mean and variance, then the\n    Gaussian distribution arises as the most consistent with our\n    assumptions. It is the least surprising and least informative\n    assumption to make. --- If you don't think the distribution should\n    be Gaussian, then that implies that you know something else that you\n    should tell your golem about, something that would improve\n    inference.\n\n::: callout-caution\nAlthough the Gaussian distribution is common in nature and has some nice\nproperties, there are some risks in using it as a default data model.\nThe Gaussian distribution has some very thin tails---there is very\nlittle probability in them. Instead most of the mass in the Gaussian\nlies within one standard deviation of the mean. Many natural (and\nunnatural) processes have much heavier tails.\n:::\n\nThe Gaussian is a continuous distribution, unlike the discrete\ndistributions of earlier chapters. Probability distributions with only\ndiscrete outcomes, like the binomial, are called *probability mass*\nfunctions and denoted `Pr`. Continuous ones like the Gaussian are called\n*probability density* functions, denoted with *`p`* or just plain old\n*`f`*, depending upon author and tradition. For mathematical reasons,\nprobability densities can be greater than 1. Try `dnorm(0,0,0.1)`\", for\nexample, which is the way to make R calculate *`p`*`(0|0, 0.1)`. The\nanswer, about 4, is no mistake. Probability *density* is the rate of\nchange in cumulative probability. So where cumulative probability is\nincreasing rapidly, density can easily exceed 1. But if we calculate the\narea under the density function, it will never exceed 1. Such areas are\nalso called *probability mass*.\n\n### Model describing language\n\n1.  First, we recognize a set of variables to work with. Some of these\n    variables are observable. We call these *data.* Others are\n    unobservable things like rates and averages. We call these\n    *parameters*.\n2.  We define each variable either in terms of the other variables or in\n    terms of a *probability distribution*.\n3.  The combination of variables and their probability distributions\n    defines a *joint generative model* that can be used both to simulate\n    hypothetical observations as well as analyze real ones.\n\nThis outline applies to models in every field, from astronomy to art\nhistory. The biggest difficulty usually lies in the subject\nmatter---which variables matter and how does theory tell us to connect\nthem?---not in the mathematics.\n\nThe mathy way to summarize models will be something like: (taken from\nKurz's version as it defines the general approach more clearly.)\n\n------------------------------------------------------------------------\n\n::: {#def-summarize-a-model}\nHow to summarize a model mathematically?\n\n$$\n\\begin{align*}\n\\text{criterion}_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i  & = \\beta \\times \\text{predictor}_i \\\\\n\\beta &  \\sim \\operatorname{Normal}(0, 10) \\\\\n\\sigma & \\sim \\operatorname{Exponential}(1) \\\\\nx_i   &  \\sim \\operatorname{Normal}(0, 1).\n\\end{align*}\n$$ {#eq-summarize-a-model}\n:::\n\n------------------------------------------------------------------------\n\n::: callout-tip\nThe ampersand sign `&` in the code of Kurz' version is used for\nhorizontal alignment of different parts for case statements. It wouldn't\nbe necessary in @eq-summarize-a-model because there is no conditional\nstatement.\n:::\n\n<a class='glossary' title='A statistical model is an expression that attempts to explain patterns in the observed values of a response variable by relating the response variable to a set of predictor variables and parameters. (Monash University)Statistical models are mappings of one set of variables through a probability distribution onto another set of variables. Fundamentally, these models define the ways values of some variables can arise, given values of other variables, because it can be quite hard to anticipate how priors influence the observable variables. (Chap.4)'>Models</a> are mappings of one set of\nvariables through a probability distribution onto another set of\nvariables. Fundamentally, these models define the ways values of some\nvariables can arise, given values of other variables.\n\n#### Re-describing the globe tossing model\n\nRecall the proportion of the water problem from previous chapters. The\nmodel in that case was always:\n\n------------------------------------------------------------------------\n\n::: {#def-glob-tossing-model}\nDescribe the globe tossing model from @sec-sampling-the-imaginary\n\n$$\n\\begin{align*}\nW \\sim \\operatorname{Binomial}(N, p) \\space \\space (1)\\\\\np \\sim \\operatorname{Uniform}(0, 1)  \\space \\space (2)\n\\end{align*}\n$$ {#eq-globe-tossing-model}\n\n-   `W`: observed count of water\n-   `N`: total number of tosses\n-   `p`: proportion of water on the globe\n\nRead the above statement as:\n\n1.  **First line**: The count W is distributed binomially with sample\n    size `N` and probability `p`.\n2.  **Second line**: The prior for `p` is assumed to be uniform between\n    zero and one.\n:::\n\n------------------------------------------------------------------------\n\n::: callout-important\nThe first line in these kind of models always defines the likelihood\nfunction used in <a class='glossary' title='This is the theorem that gives Bayesian data analysis its name. But the theorem itself is a trivial implication of probability theory. The mathematical definition of the posterior distribution arises from Bayes’ Theorem. The key lesson is that the posterior is proportional to the product of the prior and the probability of the data. (Chap.2)'>Bayes’ theorem</a>. The other lines define priors.\n:::\n\nBoth of the lines in the model of @eq-globe-tossing-model are\n<a class='glossary' title='A stochastic relationship is a mapping of a variable or parameter onto a distribution. It is said to be “stochastic” because no single instance of the variable on the left of the equation is known with certainty. Instead, the mapping is probabilistic: Some values are more plausible than others, but very many different values are plausible under any model. (Chap.4)'>stochastic</a>, as indicated by the `~` symbol. A stochastic\nrelationship is just a mapping of a variable or parameter onto a\ndistribution. It is stochastic because no single instance of the\nvariable on the left is known with certainty. Instead, the mapping is\nprobabilistic: Some values are more plausible than others, but very many\ndifferent values are plausible under any model. Later, we'll have models\nwith deterministic definitions in them.\n\n##### From model definition to Bayes’ theorem\n\nTo relate the mathematical format of @eq-globe-tossing-model to <a class='glossary' title='This is the theorem that gives Bayesian data analysis its name. But the theorem itself is a trivial implication of probability theory. The mathematical definition of the posterior distribution arises from Bayes’ Theorem. The key lesson is that the posterior is proportional to the product of the prior and the probability of the data. (Chap.2)'>Bayes’ theorem</a>, you could use the model definition to define the posterior\ndistribution:\n\n------------------------------------------------------------------------\n\n::: {#def-from-model-to-bayes-theorem}\nFrom model definition to Bayes’ theorem\n\n$$\nPr(p|w,n) = \\frac{\\operatorname{Binomial(w|n,p)}\\operatorname{Uniform(p|0,1)}}{\\int\\operatorname{Binomial(w|n,p)}\\operatorname{Uniform(p|0,1)}dp}\n$$ {#eq-from-model-to-bayes-theorem}\n:::\n\n------------------------------------------------------------------------\n\nThat monstrous denominator is just the average likelihood again. It\nstandardizes the posterior to sum to 1. The action is in the numerator,\nwhere the posterior probability of any particular value of `p` is seen\nagain to be proportional to the product of the likelihood and prior. In\nR code form, this is the same grid approximation calculation you've been\nusing all along.\n\nWe will write it in a form that is compatible and therefore better\nrecognizable with the expression in @eq-from-model-to-bayes-theorem:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-model-bayes-theorem-a lst-cap=\"Calculate the posterior distribution in a way that is regognizable with the Bayes theorem\"}\n## R code 4.6 ##############\nw <- 6\nn <- 9\np_grid_a <- seq(from = 0, to = 1, length.out = 100)\nposterior_num <- dbinom(w, n, p_grid_a) * dunif(p_grid_a, 0, 1)\nposterior_a <- posterior_num / sum(posterior_num)\n```\n:::\n\n\nCompare to the calculations in earlier chapters, for example with\n@lst-grid-approx-a.\n\n### Gaussian model of height {#sec-gaussian-model-of-height-a}\n\nIn this section we want a single measurement variable to model as a Gaussian distribution. It is a preparation for the linear regression model in @sec-linear-prediction-a  where we will construct and add a predictor variable to the model. \n\nFor the moment, we want just a single measurement variable to model as a Gaussian distribution. There will be two parameters describing the distribution’s shape, the <a class='glossary' title='The arithmetic mean, also known as “arithmetic average”, is the sum of the values divided by the number of values. If the data set were based on a series of observations obtained by sampling from a statistical population, the arithmetic mean is the sample mean \\(\\overline{x}\\) (pronounced x-bar) to distinguish it from the mean, or expected value, of the underlying distribution, the population mean \\(\\mu\\) (pronounced /’mjuː/). (Wikipedia)'>mean</a> `μ` and the <a class='glossary' title='The standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of its variance. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter \\(\\sigma\\) (sigma), for the population standard deviation, or the Latin letter \\(s\\) for the sample standard deviation. (Wikipedia)'>standard deviation</a> `σ`. <a class='glossary' title='A Bayesian model begins with one set of plausibilities assigned to each of these possibilities. These are the prior plausibilities. Then it updates them in light of the data, to produce the posterior plausibilities. This updating process is a kind of learning. (Chap.2)'>Bayesian updating</a> will allow us to consider every possible combination of values for `μ` and `σ` and to score each combination by its relative plausibility, in light of the data. These relative plausibilities are the <a class='glossary' title='It is the revised or updated probability of an event occurring after taking into consideration new information. (Investopedia). Posterior probability = prior probability + new evidence (called likelihood). (Statistics How To) The posterior distribution will be a distribution of Gaussian distributions. (Chap.4)'>posterior probabilities</a> of each combination of values `μ`, `σ`.\n\nThere are an infinite number of possible Gaussian distributions. Some\nhave small means. Others have large means. Some are wide, with a large\n`σ`. Others are narrow. We want our Bayesian machine to consider every\npossible distribution, each defined by a combination of `μ` and `σ`, and\nrank them by posterior plausibility. Posterior plausibility provides a\nmeasure of the logical compatibility of each possible distribution with\nthe data and model.\n\nKeep in mind that the “estimate” here will be the entire posterior distribution, not any point within it. And as a result, the posterior distribution will be a distribution of Gaussian distributions. Yes, a distribution of distributions. \n\n#### The data\n\nThe data contained in `data(Howell1)` are partial census data for the\nDobe area !Kung San, compiled from interviews conducted by Nancy Howell\nin the late 1960s. Much more raw data is available for download from\nhttps://tspace.library.utoronto.ca/handle/1807/10395.\n\nFor the non-anthropologists reading along, the !Kung San are the most\nfamous foraging population of the twentieth century, largely because of\ndetailed quantitative studies by people like Howell.\n\n::: callout-caution\nLoading data from a package with `data()` is only possible if you have\nalready loaded the package. In our example:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## R code 4.7 #######################\nlibrary(rethinking)\ndata(Howell1)\nd_a <- Howell1\n```\n:::\n\n\nBecause of many function name conflicts with {**brms**} I do not want to\nload {**rethinking**} and will call the function of these conflicted\npackages with `<package name>::<function name>()` Therefore I have to\nuse another, not so usual loading strategy of the data set:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-loading-data-from-package2_a lst-cap=\"Load data `Howell1` from the {**rethinking**} package without loading the package and assign the data to an object. Rethinking\"}\ndata(package = \"rethinking\", list = \"Howell1\")\nd_a <- Howell1\n```\n:::\n\n\nThe advantage of this strategy is that I have not always to detach the\n{**rethinking**} package and to make sure {**rethinking**} is detached\nbefore using {**brms**} as it is necessary in the Kurz's {**tidyverse**}\n/ {**brms**} version.\n:::\n\n##### Show the data\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-show-howell-data-a lst-cap=\"Show and inspect the data: rethinking\"}\n## R code 4.8 ####################\nstr(d_a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> 'data.frame':\t544 obs. of  4 variables:\n#>  $ height: num  152 140 137 157 145 ...\n#>  $ weight: num  47.8 36.5 31.9 53 41.3 ...\n#>  $ age   : num  63 63 65 41 51 35 32 27 19 54 ...\n#>  $ male  : int  1 0 0 1 0 1 0 1 0 1 ...\n```\n:::\n\n```{.r .cell-code #lst-show-howell-data-a lst-cap=\"Show and inspect the data: rethinking\"}\n## R code 4.9 ###################\nrethinking::precis(d_a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>               mean         sd      5.5%     94.5%     histogram\n#> height 138.2635963 27.6024476 81.108550 165.73500 ▁▁▁▁▁▁▁▂▁▇▇▅▁\n#> weight  35.6106176 14.7191782  9.360721  54.50289 ▁▂▃▂▂▂▂▅▇▇▃▂▁\n#> age     29.3443934 20.7468882  1.000000  66.13500     ▇▅▅▃▅▂▂▁▁\n#> male     0.4724265  0.4996986  0.000000   1.00000    ▇▁▁▁▁▁▁▁▁▇\n```\n:::\n:::\n\n\nThis data frame contains four columns. Each column has 544 entries, so\nthere are 544 individuals in these data. Each individual has a recorded\nheight (centimeters), weight (kilograms), age (years), and \"maleness\" (0\nindicating female and 1 indicating male).\n\n##### Select the height data of adults\n\nWe're going to work with just the height column, for the moment. All we\nwant for now are heights of adults in the sample. The reason to filter\nout non-adults for now is that height is strongly correlated with age,\nbefore adulthood.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-select-height-adults-a lst-cap=\"Select the height data of adults (individuals older or equal than 18 years): base R version\"}\n## R code 4.10 ###################\nhead(d_a$height)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 151.765 139.700 136.525 156.845 145.415 163.830\n```\n:::\n\n```{.r .cell-code #lst-select-height-adults-a lst-cap=\"Select the height data of adults (individuals older or equal than 18 years): base R version\"}\n## R code 4.11 ###################\nd2_a <- d_a[d_a$age >= 18, ]\n```\n:::\n\n\nWe'll be working with the data frame d2 now. It should have 352 rows\n(individuals) in it. We will check this with `nrow(d2_a)` =\n352.\n\n#### The model\n\nOur goal is to model the data in `d2_a` using a Gaussian distribution.\n\n##### Plot the distribution of heights\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-dist-heights-a lst-cap=\"Plot the distribution of the heights of adults, overlaid by an ideal Gaussian distribution: rethinking version\"}\nrethinking::dens(d2_a$height, norm.comp = TRUE)\n```\n\n::: {.cell-output-display}\n![The distribution of the heights data, overlaid by an ideal Gaussian distribution: rethinking version](04-geocentric-models_files/figure-html/fig-dist-heights-a-1.png){#fig-dist-heights-a width=672}\n:::\n:::\n\n\nWith the option `norm.comp = TRUE` I have overlaid a Gaussian\ndistribution to see the differences to the actual data. There are some\ndifferences locally, especially on the peak of the distribution. But the\ntails looks nice and we can say that the overall impression of the curve\nis Gaussian.\n\n::: callout-caution\n###### Decisions how to model the data\n\nGawking at the raw data, to try to decide how to model them, is usually\nnot a good idea. The data could be, for example, a mixture of different\nGaussian distributions. Furthermore, the empirical distribution need not\nbe actually Gaussian in order to justify using a Gaussian probability\ndistribution.\n:::\n\nDefine the heights as normally distributed with a mean `μ` and standard\ndeviation `σ`\n\n------------------------------------------------------------------------\n\n::: {#def-height-normal-dist}\nHeights normally distributed\n\n$$\nh_{i} \\sim \\operatorname{Normal}(σ, μ) \n$$ {#eq-height-normal-dist}\n:::\n\n------------------------------------------------------------------------\n\nThe symbol `h` refers to the list of heights, and the subscript `i`\nmeans each individual element of this list. It is conventional to use\n`i` because it stands for index. The index `i` takes on row numbers, and\nso in this example can take any value from 1 to 352 (the number of\nheights in `d2_a$height`). As such, the model above is saying that all\nthe golem knows about each height measurement is defined by the same\nnormal distribution, with mean `μ` and standard deviation `σ`.\n\nThe short model in @def-height-normal-dist assumes that the values\n$h_{i}$ are *independent and identically distributed*, abbreviated\n`i.i.d.`, `iid`, or `IID`.\n\nTo complete the model, we're going to need some priors. The parameters\nto be estimated are both `μ` and `σ`, so we need a prior `Pr(μ, σ)`, the\njoint prior probability for all parameters. In most cases, priors are\nspecified independently for each parameter, which amounts to assuming\n$Pr(μ,σ) = Pr(μ)Pr(σ)$.\n\n------------------------------------------------------------------------\n\n::: {#def-height-linear-model-m4.1}\n###### Linear heights model\n\n$$\n\\begin{align*}\nh_{i} \\sim \\operatorname{Normal}(μ, σ) \\space \\space (1) \\\\ \nμ \\sim \\operatorname{Normal}(178, 20)  \\space \\space (2) \\\\ \nμ \\sim \\operatorname{Uniform}(0, 50)   \\space \\space (3)      \n\\end{align*}\n$$ {#eq-height-linear-model-m4.1}\n\n1.  First line represents the likelihood.\n2.  Second line is the chosen `μ`(mu, mean) prior.\n3.  Third line is the chosen `σ` (sigma, standard deviation) prior.\n:::\n\n------------------------------------------------------------------------\n\nLet's think about the chosen value for the priors more in detail:\n\nThe prior for `μ` is a broad Gaussian prior, centered on 178 cm, with\n95% of probability between 178 ± 40 cm.\n\nWhy 178 cm? Your author is 178 cm tall. And the range from 138 cm to 218\ncm encompasses a huge range of plausible mean heights for human\npopulations. So domain-specific information has gone into this prior.\nEveryone knows something about human height and can set a reasonable and\nvague prior of this kind. But in many regression problems, as you'll see\nlater, using prior information is more subtle, because parameters don't\nalways have such clear physical meaning.\n\nWhatever the prior, it's a very good idea to plot your priors, so you\nhave a sense of the assumption they build into the model.\n\n##### Plot the mean prior (mu)\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-mean-prior-a lst-cap=\"Plot the chosen mean prior: base R version\"}\n## R code 4.12 ###############################\ncurve(dnorm(x, 178, 20), from = 100, to = 250)\n```\n\n::: {.cell-output-display}\n![Plot of the chosen mean prior: base R version](04-geocentric-models_files/figure-html/fig-mean-prior-a-1.png){#fig-mean-prior-a width=672}\n:::\n:::\n\n\nYou can see that the golem is assuming that the average height (not each\nindividual height) is almost certainly between 140 cm and 220 cm. So\nthis prior carries a little information, but not a lot.\n\n##### Plot the prior of the standard deviation (sigma)\n\nA standard deviation like `σ` must be positive, so bounding it at zero\nmakes sense. How should we pick the upper bound? In this case, a\nstandard deviation of 50 cm would imply that 95% of individual heights\nlie within 100 cm of the average height. That's a very large range.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-sd-prior-a lst-cap=\"Plot chosen prior for the standard deviation: base R version\"}\n## R code 4.13 ###########################\ncurve(dunif(x, 0, 50), from = -10, to = 60)\n```\n\n::: {.cell-output-display}\n![Plot the chosen prior for the standard deviation: base R version](04-geocentric-models_files/figure-html/fig-sd-prior-a-1.png){#fig-sd-prior-a width=672}\n:::\n:::\n\n\n##### Prior predictive simulation\n\n> Once you've chosen priors for `h`, `μ`, and `σ`, these imply a joint\n> prior distribution of individual heights. By simulating from this\n> distribution, you can see what your choices imply about observable\n> height. This helps you diagnose bad choices.\n\nOkay, so how to do this? You can quickly simulate heights by sampling\nfrom the prior, like you sampled from the posterior back in\n@sec-sampling-the-imaginary. Remember, every posterior is also\npotentially a prior for a subsequent analysis, so you can process priors\njust like posteriors.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-prior-predictive-sim-a lst-cap=\"Heights by sampling from the prior: rethinking version\"}\nset.seed(4) # to make example reproducible\n## R code 4.14 #######################################\nsample_mu_a <- rnorm(1e4, 178, 20)\nsample_sigma_a <- runif(1e4, 0, 50)\nprior_h_a <- rnorm(1e4, sample_mu_a, sample_sigma_a)\nrethinking::dens(prior_h_a, norm.comp = TRUE)\n```\n\n::: {.cell-output-display}\n![Simulate heights by sampling from the prior: rethinking version](04-geocentric-models_files/figure-html/fig-prior-predictive-sim-a-1.png){#fig-prior-predictive-sim-a width=672}\n:::\n:::\n\n\n> It displays a vaguely bell-shaped density with thick tails. It is the\n> expected distribution of heights, averaged over the prior. Notice that\n> the prior probability distribution of height is not itself Gaussian.\n> This is okay. The distribution you see is not an empirical\n> expectation, but rather the distribution of relative plausibilities of\n> different heights, before seeing the data.\n\nThis comment is strange for me as in my point of view the distribution\n*is* Gaussian. It is true that the tails are (a little bit?) thicker\nthan in the standard Gaussian distribution. But in my view\n@fig-prior-predictive-sim-a is more Gaussian than @fig-dist-heights-a.\nOK, in @fig-dist-heights-a we have just 352 data and in\n@fig-prior-predictive-sim-a we sampled 10,000 times. But this is not a\ncounter argument for @fig-prior-predictive-sim-a not being a a bell\nshaped distribution.\n\n##### Simulate heights from priors with large sd\n\n<a class='glossary' title='It is an essential part of modeling. Once you’ve chosen priors for all variables these priors imply a joint prior distribution. By simulating from this distribution, you can see what your choices imply about the observable variable. This helps to diagnose bad choices. Prior predictive simulation is therefore very useful for assigning sensible priors. (Chap.4)'>Prior predictive simulation</a> is very useful for assigning sensible\npriors, because it can be quite hard to anticipate how priors influence\nthe observable variables. As an example, consider a much flatter and\nless informative prior for `μ`, like $μ \\sim Normal(178, 100)$. Priors\nwith such large standard deviations are quite common in Bayesian models,\nbut they are hardly ever sensible.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-prior-predictive-sim2-a lst-cap=\"Simulate heights from priors with a large standard deviation: rethinking version\"}\nset.seed(4) # to make example reproducible\n## R code 4.15 ############################\nsample_mu2_a <- rnorm(1e4, 178, 100)\nprior_h2_a <- rnorm(1e4, sample_mu2_a, sample_sigma_a)\nrethinking::dens(prior_h2_a)\n```\n\n::: {.cell-output-display}\n![Simulate heights from priors with a large standard deviation: rethinking version](04-geocentric-models_files/figure-html/fig-prior-predictive-sim2-a-1.png){#fig-prior-predictive-sim2-a width=672}\n:::\n:::\n\n\nThe results of @fig-prior-predictive-sim2-a contradicts our scientific\nknowledge --- but also our common sense --- about possible height values\nof humans. Now the model, before seeing the data, expects people to have\nnegative height. It also expects some giants. One of the tallest people\nin recorded history, [Robert Pershing\nWadlow](https://en.wikipedia.org/wiki/Robert_Wadlow) (1918--1940) stood\n272 cm tall. In our prior predictive simulation many people are taller\nthan this.\n\nDoes this matter? In this case, we have so much data that the silly\nprior is harmless. But that won't always be the case. There are plenty\nof inference problems for which the data alone are not sufficient, no\nmatter how numerous. Bayes lets us proceed in these cases. But only if\nwe use our scientific knowledge to construct sensible priors. Using\nscientific knowledge to build priors is not cheating. The important\nthing is that your prior not be based on the values in the data, but\nonly on what you know about the data before you see it.\n\n#### Grid approximation of the posterior distribution\n\nWe are going to map out the posterior distribution through brute force\ncalculations.\n\nThis is not recommended because it is\n\n-   laborious and computationally expensive\n-   usually so impractical as to be essentially impossible.\n\nTherefor the grid approximation technique has limited relevance. Later\non we will use the quadratic approximation with `rethinking::quap()`.\n\nThe strategy is the same grid approximation strategy as before in @lst-grid-approx-base-demo. But now there are two dimensions, and so there is a geometric (literally) increase in bother. The next code chunk is complex, so I will explain it line by line:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-grid-approx-posterior-a lst-cap=\"Grid approximation of the posterior distribution: rethinking version\"}\n## R code 4.16 ##################################\n\n# establish range of μ and σ values, respectively, to calculate over \n# as well as how many points to calculate in-between. \nmu.list_a <- seq(from = 150, to = 160, length.out = 100)  # <1>\nsigma.list_a <- seq(from = 7, to = 9, length.out = 100)   # <1>\n\n# expands μ & σ values into a matrix of all of the combinations\npost_a <- expand.grid(mu_a = mu.list_a, sigma_a = sigma.list_a) # <2>\n\n# compute the log-likelihood at each combination of μ and σ\npost_a$LL <- sapply(1:nrow(post_a), function(i) {                 # <3>\n  sum(                                                            # <3>\n    dnorm(d2_a$height, post_a$mu[i], post_a$sigma[i], log = TRUE) # <3>\n  )                                                               # <3>\n})                                                                # <3>\n\n# multiply the prior by the likelihood\n# as the priors are on the log scale adding = multiplying\npost_a$prod <- post_a$LL + dnorm(post_a$mu_a, 178, 20, TRUE) +    # <4>\n  dunif(post_a$sigma_a, 0, 50, TRUE)                              # <4>\n\n# getting back on the probability scale without rounding error \npost_a$prob <- exp(post_a$prod - max(post_a$prod))                # <5>\n```\n:::\n\n\n1. Establish the range of `μ` and `σ` values, respectively, to calculate over, as well as how many points to calculate in-between. \n2. Expand those chosen `μ` and `σ` values into a matrix of all of the combinations of `μ` and `σ`. This matrix is stored in a data frame, `post_a`. \n3. Compute the log-likelihood at each combination of `μ` and `σ`. This line looks so awful, because we have to be careful here to do everything on the log scale. Otherwise rounding error will quickly make all of the posterior probabilities zero. So what `sapply()` does is pass the unique combination of `μ` and `σ` on each row of post to a function that computes the log-likelihood of each observed height, and adds all of these log-likelihoods together (`sum()`). \n4. Multiply the prior by the likelihood to get the product that is proportional to the posterior density. The priors are also on the log scale, and so we add them to the log-likelihood, which is equivalent to multiplying the raw densities by the likelihood. \n5. Finally, the obstacle for getting back on the probability scale is that rounding error is always a threat when moving from log-probability to probability. If you use the obvious approach, like `exp(post_a$prod)`, you’ll get a vector full of zeros, which isn’t very helpful. This is a result of R’s rounding very small probabilities to zero. Remember, in large samples, all unique samples are unlikely. This is why you have to work with log-probability. The code in the box dodges this problem by scaling all of the log-products by the maximum log-product. As a result, the values in `post_a$prob` are not all zero, but they also aren’t exactly probabilities. Instead they are relative posterior probabilities. But that’s good enough for what we wish to do with these values.\n\n\n**Plot contour lines**\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-contour-plot-a lst-cap=\"Draw a contour plot: rethinking version\"}\n## R code 4.17 ##################################\nrethinking::contour_xyz(post_a$mu_a, post_a$sigma_a, post_a$prob)\n```\n\n::: {.cell-output-display}\n![Draw a contour plot: rethinking version](04-geocentric-models_files/figure-html/fig-contour-plot-a-1.png){#fig-contour-plot-a width=672}\n:::\n:::\n\n\nYou can inspect this posterior distribution, now residing in\n`post_a$prob`, using a variety of plotting commands.\n\n**Plot heat map**\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-heat-map-a lst-cap=\"Draw a heat map: rethinking version\"}\n## R code 4.18 ##################################\nrethinking::image_xyz(post_a$mu_a, post_a$sigma_a, post_a$prob)\n```\n\n::: {.cell-output-display}\n![Draw a heat map: rethinking version](04-geocentric-models_files/figure-html/fig-heat-map-a-1.png){#fig-heat-map-a width=672}\n:::\n:::\n\n\n#### Sampling from the posterior\n\nTo study this posterior distribution in more detail, again I'll push the\nflexible approach of sampling parameter values from it. This works just\nlike it did in @sec-sampling-to-summarize, when you sampled values of\n`p` from the posterior distribution for the globe tossing example. The\nonly new trick is that since there are two parameters, and we want to\nsample combinations of them, we first randomly sample row numbers in\n`post_a` in proportion to the values in `post_a$prob`. Then we pull out\nthe parameter values on those randomly sampled rows.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-posterior-sample-a lst-cap=\"Samples from the posterior distribution for the heights data\"}\n## R code 4.19 ###########################\n\n# randomly sample row numbers in post_a \n# in proportion to the values in post_a$prob. \nsample.rows <- sample(1:nrow(post_a),\n  size = 1e4, replace = TRUE,\n  prob = post_a$prob\n)\n\n# pull out the parameter values\nsample.mu_a <- post_a$mu[sample.rows]\nsample.sigma_a <- post_a$sigma[sample.rows]\n\n## R code 4.20 ###########################\nplot(sample.mu_a, sample.sigma_a, cex = 0.8, pch = 21, col = rethinking::col.alpha(rethinking:::rangi2, 0.1))\n```\n\n::: {.cell-output-display}\n![Samples from the posterior distribution for the heights data. The density of points is highest in the center, reflecting the most plausible combinations of μ and σ. There are many more ways for these parameter values to produce the data, conditional on the model. (rethinking version)](04-geocentric-models_files/figure-html/fig-posterior-sample-a-1.png){#fig-posterior-sample-a width=672}\n:::\n:::\n\n\nThe function `col.alpha()` is part of the {**rethinking**} R package.\nAll it does is make colors transparent, which helps the plot in FIGURE\n4.4 (here: @fig-posterior-sample-a) more easily show density, where\nsamples overlap. `rangi2` itself is just the [definition of a hex color\ncode](https://github.com/rmcelreath/rethinking/blob/2f01a9c5dac4bc6e9a6f95eec7cae268200a8181/R/colors.r#L22)\n(\"#8080FF\") specifying the shade of blue.\n\nAdjust the plot to your tastes by playing around with `cex` (character\nexpansion, the size of the points), `pch` (plot character), and the 0.1\ntransparency value.\n\n**Marginal Posterior Density**\n\nNow that you have these samples, you can describe the distribution of\nconfidence in each combination of `μ` and `σ` by summarizing the\nsamples. Think of them like data and describe them, just like in\n@sec-sampling-to-summarize. For example, to characterize the shapes of\nthe marginal posterior densities of `μ` and `σ`, all we need to do is:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-marg-post-density-a lst-cap=\"Plot shapes of the marginal posterior densities of μ and σ\"}\n## R code 4.21 #########################\nrethinking::dens(sample.mu_a)\nrethinking::dens(sample.sigma_a)\n```\n\n::: {.cell-output-display}\n![Shapes of the marginal posterior densities of μ and σ: rethinking version](04-geocentric-models_files/figure-html/fig-marg-post-density-a-1.png){#fig-marg-post-density-a-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Shapes of the marginal posterior densities of μ and σ: rethinking version](04-geocentric-models_files/figure-html/fig-marg-post-density-a-2.png){#fig-marg-post-density-a-2 width=672}\n:::\n:::\n\n\nThe jargon \"marginal\" here means \"averaging over the other parameters.\"\nExecute the above code and inspect the plots. These densities are very\nclose to being normal distributions. And this is quite typical. As\nsample size increases, posterior densities approach the normal\ndistribution. If you look closely, though, you'll notice that the\ndensity for σ has a longer right-hand tail. I'll exaggerate this\ntendency a bit later, to show you that this condition is very common for\nstandard deviation parameters.\n\n**Posterior Compatibility Intervals (PIs)**\n\nTo summarize the widths of these densities with posterior compatibility\nintervals we use:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-post-comp-intervals-a lst-cap=\"Posterior Compatibility Intervals (PIs): rethinking version\"}\n## R code 4.22 ####################\nrethinking::PI(sample.mu_a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>       5%      94% \n#> 153.9394 155.2525\n```\n:::\n\n```{.r .cell-code #lst-post-comp-intervals-a lst-cap=\"Posterior Compatibility Intervals (PIs): rethinking version\"}\nrethinking::PI(sample.sigma_a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>       5%      94% \n#> 7.323232 8.252525\n```\n:::\n:::\n\n\nSince these samples are just vectors of numbers, you can compute any\nstatistic from them that you could from ordinary data: `mean`, `median`,\nor `quantile`, for example.\n\n**Sample size and the normality of sigmas posterior**\n\nBefore moving on to using quadratic approximation `rethinking::quap()`\nas shortcut to all of this inference, it is worth repeating the analysis\nof the height data above, but now with only a fraction of the original\ndata. The reason to do this is to demonstrate that, in principle, the\nposterior is not always so Gaussian in shape. There's no trouble with\nthe mean, `μ`. For a Gaussian likelihood and a Gaussian prior on `μ`,\nthe posterior distribution is always Gaussian as well, regardless of\nsample size. It is the standard deviation `σ` that causes problems. So\nif you care about `σ`---often people do not---you do need to be careful\nof abusing the quadratic approximation.\n\nThe deep reasons for the posterior of `σ` tending to have a long\nright-hand tail are complex. But a useful way to conceive of the problem\nis that variances must be positive. As a result, there must be more\nuncertainty about how big the variance (or standard deviation) is than\nabout how small it is. For example, if the variance is estimated to be\nnear zero, then you know for sure that it can't be much smaller. But it\ncould be a lot bigger.\n\nLet's quickly analyze only 20 of the heights from the height data to\nreveal this issue. To sample 20 random heights from the original list:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-sample-only-20-a lst-cap=\"Sample 20 heights: rethinking version\"}\n## R code 4.23 ######################################\nd3_a <- sample(d2_a$height, size = 20)\n\n## R code 4.24 ######################################\nmu2_a.list <- seq(from = 150, to = 170, length.out = 200)\nsigma2_a.list <- seq(from = 4, to = 20, length.out = 200)\npost2_a <- expand.grid(mu = mu2_a.list, sigma = sigma2_a.list)\npost2_a$LL <- sapply(1:nrow(post2_a), function(i) {\n  sum(dnorm(d3_a,\n    mean = post2_a$mu[i], sd = post2_a$sigma[i],\n    log = TRUE\n  ))\n})\npost2_a$prod <- post2_a$LL + dnorm(post2_a$mu, 178, 20, TRUE) +\n  dunif(post2_a$sigma, 0, 50, TRUE)\npost2_a$prob <- exp(post2_a$prod - max(post2_a$prod))\nsample2_a.rows <- sample(1:nrow(post2_a),\n  size = 1e4, replace = TRUE,\n  prob = post2_a$prob\n)\nsample2_a.mu <- post2_a$mu[sample2_a.rows]\nsample2_a.sigma <- post2_a$sigma[sample2_a.rows]\nplot(sample2_a.mu, sample2_a.sigma,\n  cex = 0.5,\n  col = rethinking::col.alpha(rethinking:::rangi2, 0.1),\n  xlab = \"mu\", ylab = \"sigma\", pch = 16\n)\n```\n\n::: {.cell-output-display}\n![Sample 20 heights: rethinking version](04-geocentric-models_files/figure-html/fig-sample-only-20-a-1.png){#fig-sample-only-20-a width=672}\n:::\n:::\n\n\nyou'll see another scatter plot of the samples from the posterior\ndensity, but this time you'll notice a distinctly longer tail at the top\nof the cloud of points.\n\n**Marginal Posterior Density with only 20 rows**\n\nYou should also inspect the marginal posterior density for σ, averaging\nover μ, produced with:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-marg-post-density-a2 lst-cap=\"Marginal posterior density for σ, averaging over μ: rethinking version\"}\n## R code 4.25 ############\nrethinking::dens(sample2_a.sigma, norm.comp = TRUE)\n```\n\n::: {.cell-output-display}\n![Marginal posterior density for σ, averaging over μ: rethinking version](04-geocentric-models_files/figure-html/fig-marg-post-density-a2-1.png){#fig-marg-post-density-a2 width=672}\n:::\n:::\n\n\n#### Finding the posterior distribution with quap()\n\n> To build the **quadratic approximation**, we'll use quap, a command in\n> the `rethinking` package. The `quap` function works by using the model\n> definition you were introduced to earlier in this chapter. Each line\n> in the definition has a corresponding definition in the form of R\n> code. The engine inside quap then uses these definitions to define the\n> posterior probability at each combination of parameter values. Then it\n> can climb the posterior distribution and find the peak, its <a class='glossary' title='In Bayesian statistics a Maximum A Posteriori probability or MAP estimate is an estimate of an unknown quantity, that equals the mode of the posterior distribution. The MAP can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data. (Wikipedia) (Chap.4)'>MAP</a>\n> (**Maximum A Posteriori** estimate). Finally, it estimates the\n> quadratic curvature at the MAP to produce an approximation of the\n> posterior distribution. (parenthesis and emphasis are mine)\n\n::: callout-note\nThe procedure used by `rethinking:quap()` is very similar to what many\nnon-Bayesian procedures do, just without any priors.\n:::\n\n1.  We start with the Howell1 data frame for adults `d2_a` (age \\>= 18).\n    We will place the R code equivalents into an `alist()` We are going\n    to use the @def-height-linear-model-m4.1. (Code 4.27).\n2.  Then we fit the model with `rethinking::quap()` to the data in the\n    data frame `d2_a` (Code 4.28) to `m4.1`.\n3.  Now we can have a look with `rethinking::precis()` at the posterior\n    distribution (Code 4.29).\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-post-dist-quap-m4-1 lst-cap=\"Finding the posterior distribution with rethinking::quap(): Model m4.1\"}\n## R code 4.27 ######################\nflist <- alist(\n  height ~ dnorm(mu, sigma),\n  mu ~ dnorm(178, 20),\n  sigma ~ dunif(0, 50)\n)\n\n## R code 4.28 ######################\nm4.1 <- rethinking::quap(flist, data = d2_a)\n\n## R code 4.29 ######################\nrethinking::precis(m4.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>             mean        sd       5.5%      94.5%\n#> mu    154.607024 0.4119947 153.948577 155.265471\n#> sigma   7.731333 0.2913860   7.265642   8.197024\n```\n:::\n:::\n\n\n> These numbers provide Gaussian approximations for each parameter's\n> <a class='glossary' title='It is the probability distribution of each of the individual variables. A marginal distribution gets it’s name because it appears in the margins of a probability distribution table. (Statology, Statistics How To) (Chap.4)'>marginal distribution</a>. This means the plausibility of\n> each value of `_μ_`, after averaging over the plausibilities of each\n> value of `_σ_`, is given by a Gaussian distribution with mean 154.6\n> and standard deviation 0.4.\n>\n> The 5.5% and 94.5% quantiles are percentile interval boundaries,\n> corresponding to an 89% compatibility interval. Why 89%? It's just the\n> default. It displays a quite wide interval, so it shows a\n> high-probability range of parameter values. If you want another\n> interval, such as the conventional and mindless 95%, you can use\n> `precis(m4.1, prob=0.95)`. But I don't recommend 95% intervals,\n> because readers will have a hard time not viewing them as significance\n> tests. 89 is also a prime number, so if someone asks you to justify\n> it, you can stare at them meaningfully and incant, \"Because it is\n> prime.\" That's no worse justification than the conventional\n> justification for 95%.\n\n> I encourage you to compare these 89% boundaries to the compatibility\nintervals from the grid approximation in @lst-post-comp-intervals-a\nearlier. You'll find that they are almost identical. When the posterior\nis approximately Gaussian, then this is what you should expect.\n\n##### Start values for `rethinking::quap()` {#sec-start-values-rethinking}\n\nMean and standard deviation are good values to start values for hill\nclimbing. If you don't specify `rethinking::quap()` will use a random\nvalue.\n\n\n::: {.cell}\n\n```{.r .cell-code #start-values-quap lst-cap=\"Define start values for rethinking::quap()\"}\n## R code 4.30 ######################\nstart <- list(\n  mu = mean(d2_a$height),\n  sigma = sd(d2_a$height)\n)\nm4.1_2 <- rethinking::quap(flist, data = d2_a, start = start)\nrethinking::precis(m4.1_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>             mean        sd       5.5%      94.5%\n#> mu    154.607024 0.4119947 153.948576 155.265471\n#> sigma   7.731333 0.2913860   7.265642   8.197024\n```\n:::\n:::\n\n\n::: callout-note\n###### list() and alist()\n\nNote that the list of start values is a regular `list`, not an `alist`\nlike the formula list is. The two functions `alist` and `list` do the\nsame basic thing: allow you to make a collection of arbitrary R objects.\nThey differ in one important respect: `list` evaluates the code you\nembed inside it, while `alist` does not. So when you define a list of\nformulas, you should use `alist`, so the code isn't executed. But when\nyou define a list of start values for parameters, you should use `list`,\nso that code like `mean(d2_a$height)` will be evaluated to a numeric\nvalue.\n:::\n\n**Slicing in more information**\n\n> The priors we used before are very weak, both because they are nearly\n> flat and because there is so much data. So I'll splice in a more\n> informative prior for `*μ*`, so you can see the effect. All I'm going\n> to do is change the standard deviation of the prior to 0.1, so it's a\n> very narrow prior. I'll also build the formula right into the call to\n> `quap` this time.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-post-dist-quap-m4.2 lst-cap=\"Finding the posterior distribution with a narrower prior rethinking::quap(): Model m4.2\"}\n## R code 4.31 ###########################\nm4.2 <- rethinking::quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu ~ dnorm(178, 0.1),\n    sigma ~ dunif(0, 50)\n  ),\n  data = d2_a\n)\nrethinking::precis(m4.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>            mean        sd      5.5%    94.5%\n#> mu    177.86376 0.1002354 177.70356 178.0240\n#> sigma  24.51768 0.9289344  23.03307  26.0023\n```\n:::\n:::\n\n\n> Notice that the estimate for `*μ*` has hardly moved off the prior. The\n> prior was very concentrated around 178. So this is not surprising. But\n> also notice that the estimate for `*σ*` has changed quite a lot, even\n> though we didn't change its prior at all. Once the golem is certain\n> that the mean is near 178---as the prior insists---then the golem has\n> to estimate `*σ*` conditional on that fact. This results in a\n> different posterior for `*σ*`, even though all we changed is prior\n> information about the other parameter.\n\n::: callout-caution\n###### `μ` has hardly moved off the prior\n\nAt first I did not understand \"that the estimate for `*μ*` has hardly\nmoved off the prior\". I thought this assertion refers to the value of\n`*μ*` in both calculation. *μ* has changed considerably from 154.61 to\n177.86 and under that assumption the above quote does not make sense.\n\nBut in contrast to my wrong assumption the assertion refers to the\ndifference between the chosen prior (178) and the resulting value of\n`*μ*` (177.86).\n:::\n\n#### Sampling from a quap\n\nThe above explains how to get a quadratic approximation of the\nposterior, using `rethinking::quap()`. But how do we then get samples\nfrom the quadratic approximate posterior distribution? --- When R\nconstructs a quadratic approximation, it calculates not only standard\ndeviations for all parameters, but also the covariances among all pairs\nof parameters. Just like a mean and standard deviation (or its square, a\nvariance) are sufficient to describe a one-dimensional Gaussian\ndistribution, a list of means and a matrix of variances and covariances\nare sufficient to describe a multi-dimensional Gaussian distribution.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-vcov-matrix-m4.1 lst-cap=\"Calculation of the variance-covariance matrix: rethinking version\"}\n## R code 4.32 ###################\nrethinking::vcov(m4.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>                 mu        sigma\n#> mu    0.1697396028 0.0002180414\n#> sigma 0.0002180414 0.0849058146\n```\n:::\n:::\n\n\n`vcov()` returns the variance-covariance matrix of the main parameters\nof a fitted model object. In the above {**rethinking**} version is uses\nthe class `map2stan` for a fitted Stan model as `m4.1` is of class\n`map`.\n\n***\n:::: {#prp-vcov-function)\nTwo different `vcov()` functions\n\n::: callout-warning\nIn @lst-vcov-matrix-m4.1 I am explicitly using the package\n{**rethinking**} for the `vcov()` function. The same function is also\navailable as a base R function with `stats::vcov()`. But this generates\nan error because there is no method known for an object of class `map`\nfrom the rethinking package. The help file for `stats::vcov()` only says\nthat the `vcov` object is an S3 method for classes `lm`, `glm`, `mlm`\nand `aov` but not for `map`.\n\n> Error in UseMethod(\"vcov\") : no applicable method for 'vcov' applied\n> to an object of class \"map\"\n\nI could have used only `vcov()`. But this only works when the\n{**rethinking**} package is already loaded. In that case R knows because\nof the class of the object which `vcov()` version to use. In this case:\nclass of object = `class(m4.1)` map.\n:::\n\n::::\n***\n\n@lst-vcov-matrix-m4.1 results in a variance-covariance matrix. It is\nthe multi-dimensional glue of a quadratic approximation, because it\ntells us how each parameter relates to every other parameter in the\nposterior distribution. A variance-covariance matrix can be factored\ninto two elements: (1) a vector of variances for the parameters and (2)\na correlation matrix that tells us how changes in any parameter lead to\ncorrelated changes in the others.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-vcov-decomp-m4.1 lst-cap=\"Variance-Covariance Matrix of model m4.1 decomposed\"}\n## R code 4.33 #######################\nbase::diag(rethinking::vcov(m4.1))      # <1>\nstats::cov2cor(rethinking::vcov(m4.1))  # <2>\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>         mu      sigma \n#> 0.16973960 0.08490581 \n#>                mu       sigma\n#> mu    1.000000000 0.001816262\n#> sigma 0.001816262 1.000000000\n```\n:::\n:::\n\n\n1.  `base::diag()` extracts the diagonal of the (variance-covariance)\n    matrix. The two-element vector in the output is the list of\n    variances. If you take the square root of this vector, you get the\n    standard deviations that are shown in the `rethinking::precis()` output of\n    @lst-post-dist-quap-m4-1.\n2.  `stats::cov2cor()` scales a covariance matrix into the corresponding\n    correlation matrix. The two-by-two matrix in the output is this\n    correlation matrix. Each entry shows the correlation, bounded\n    between −1 and +1, for each pair of parameters. The 1's indicate a\n    parameter's correlation with itself. If these values were anything\n    except 1, we would be worried. The other entries are typically\n    closer to zero, and they are very close to zero in this example.\n    This indicates that learning `μ` tells us nothing about `σ` and likewise\n    that learning `σ` tells us nothing about `μ`. This is typical of simple\n    Gaussian models of this kind. But it is quite rare more generally,\n    as you'll see in later chapters.\n\n***\n:::: {#prp-why-cor-almost-0}\n\nConfusing m4.1 (Gaussian distribution) with m4.3 (height-weight against each other)\n\n::: callout-warning\n\nFrankly speaking I had troubles to understand why the correlation is almost 0. It turned out that I had unconsciously in mind a correlation between height and weight, an issue that is raised later in this chapter with `m4.3`. \n\nBut here there is only a Gaussian distribution of height under discussion, and not the correlation between height and weight.\n:::\n::::\n\n***\n\nBut wait, there is another problem in my understanding:\n\n***\n:::: {#prp-from-vcov-to-cor}\n\nHow to compute correlation from variance-covariance matrix?\n\n::: {.callout-warning}\n\nI wonder how to compute the correlation matrix by hand form the covariance-variance matrix. I thought that I have to use `sqrt()`, but it didn't work. After I inspected the  code of the `cov2cor()` function I noticed that it uses the expression `sqrt(1/diag(V))`. I don't understand why it is using the inverse value and not just `sqrt()`. Maybe it has to do with matrix calculation, where I am not firm about it?\n:::\n::::\n\n***\n\nLet's check the assertion that the square root of the variance vector, will get us the standard deviations that are shown in the `rethinking::precis()` output of @lst-post-dist-quap-m4-1 (0.41 for $\\mu$ and 0.29 for $\\sigma$):\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-sqrt-var-m41 lst-cap=\"Square root of variance vector of model m4.1\"}\nsqrt(base::diag(rethinking::vcov(m4.1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>        mu     sigma \n#> 0.4119947 0.2913860\n```\n:::\n:::\n\n\nIn fact: The square roots of the variance vector results in the standard deviation and therefore in the same output as in @lst-post-dist-quap-m4-1.\n\n> Scaling a covariance matrix into a correlation one can be achieved in many ways, mathematically most appealing by multiplication with a diagonal matrix from left and right, or more efficiently by using `base::sweep(.., FUN = \"/\")` twice. The `stats::cov2cor()` function is even a bit more efficient, and provided mostly for didactical reasons.\n\nFor computing the covariance matrix with `base::sweep()` see the answer in  [StackOverflow](https://stats.stackexchange.com/a/407954/207389).\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-compute-vcov-with-sweep lst-cap=\"Compute covariance matrix using sweep()\"}\nR <- stats::cov2cor(rethinking::vcov(m4.1))\nS <- sqrt(base::diag(rethinking::vcov(m4.1)))\n\nsweep(sweep(R, 1, S, \"*\"), 2, S, \"*\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>                 mu        sigma\n#> mu    0.1697396028 0.0002180414\n#> sigma 0.0002180414 0.0849058146\n```\n:::\n:::\n\n\n\n\n:::: {#prp-vcov-interpretation}\nHow to interpret covariances?\n\n::: callout-caution\nA large covariance can mean a strong relationship between variables. However, you can’t compare variances over data sets with different scales (like pounds and inches).\n\nThe main problem with interpretation is that the wide range of results that it takes on makes it hard to interpret. For example, your data set could return a value of 3, or 3,000. This wide range of values is cause by a simple fact: *The larger the X and Y values, the larger the covariance*. A value of 300 tells us that the variables are correlated, but unlike the correlation coefficient, that number doesn’t tell us exactly how strong that relationship is. The problem can be fixed by dividing the covariance by the standard deviation to get the correlation coefficient. ([Statistics How To](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/covariance/))\n\n:::\n\n::::\n***\n\nOkay, so how do we get samples from this multi-dimensional posterior?\nNow instead of sampling single values from a simple Gaussian\ndistribution, we sample vectors of values from a multi-dimensional\nGaussian distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-extract-samples-m4.1-a lst-cap=\"Extract sample vectors of values from the multi-dimensional Gaussian distribution of m4.1: rethinking version\"}\n## R code 4.34 #######################\npost3_a <- rethinking::extract.samples(m4.1, n = 1e4)\nhead(post3_a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>         mu    sigma\n#> 1 153.8952 7.377465\n#> 2 154.8740 8.041744\n#> 3 155.2567 8.030947\n#> 4 154.3546 7.841502\n#> 5 154.5708 7.646268\n#> 6 154.6600 7.662197\n```\n:::\n:::\n\n\nYou end up with a data frame, post, with 10,000 (1e4) rows and two\ncolumns, one column for `_μ_` and one for `_σ_`. Each value is a sample\nfrom the posterior, so the mean and standard deviation of each column\nwill be very close to the <a class='glossary' title='In Bayesian statistics a Maximum A Posteriori probability or MAP estimate is an estimate of an unknown quantity, that equals the mode of the posterior distribution. The MAP can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data. (Wikipedia) (Chap.4)'>MAP</a> values from before. You can confirm this by summarizing the samples:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-summary-samples-m4.1-a lst-cap=\"Summary the extracted samples: rethinking version\"}\n## R code 4.35 ##################\nrethinking::precis(post3_a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>             mean        sd       5.5%      94.5%     histogram\n#> mu    154.612933 0.4113513 153.952250 155.260706       ▁▁▅▇▂▁▁\n#> sigma   7.734397 0.2917627   7.269182   8.200406 ▁▁▁▁▂▅▇▇▃▁▁▁▁\n```\n:::\n:::\n\n\nCompare these values to the output from @lst-post-dist-quap-m4-1. And\nyou can use `plot(post)` to see how much they resemble the samples from\nthe grid approximation in FIGURE 4.4 (here @fig-posterior-sample-a).\nThese samples also preserve the covariance between `_μ_` and `_σ_`. This\nhardly matters right now, because `_μ_` and `_σ_` don't co-vary at all\nin this model. But once you add a predictor variable to your model,\ncovariance will matter a lot.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-posterior-sample-vectors-a lst-cap=\"Samples from the vectors of values from a multi-dimensional Gaussian distribution\"}\nbase::plot(post3_a)\n```\n\n::: {.cell-output-display}\n![Samples from the vectors of values from a multi-dimensional Gaussian distribution. The density of points is highest in the center, reflecting the most plausible combinations of μ and σ. It is very similar to @fig-posterior-sample-a (rethinking version)](04-geocentric-models_files/figure-html/fig-posterior-sample-vectors-a-1.png){#fig-posterior-sample-vectors-a width=672}\n:::\n:::\n\n\n##### Under the hood with multivariate sampling {#sec-under-the-hood-multivariate-sampling-a}\n\nThe function `rethinking::extract.samples()` is for convenience. It is\njust running a simple simulation of the sort you conducted near the end\nof @sec-sampling-the-imaginary with @lst-sim-pred-samples-a. Here's a\npeak at the motor. The work is done by a multi-dimensional version of\n`stats::rnorm()`, `MASS::mvrnorm()`. The function `stats::rnorm()`\nsimulates random Gaussian values, while `MASS::mvrnorm()` simulates\nrandom vectors of multivariate Gaussian values. Here's how to use it the\n{**MASS**} function to do what `rethinking::extract.samples()` does:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-posterior-sample-vectors-MASS-a lst-cap=\"Extract samples from the vectors of values from a multi-dimensional Gaussian distribution (rethinking version)\"}\n## R code 4.36 ######################\npost4_a <- MASS::mvrnorm(n = 1e4, mu = rethinking::coef(m4.1), \n                      Sigma = rethinking::vcov(m4.1))\nplot(post4_a)\n```\n\n::: {.cell-output-display}\n![Extract samples from the vectors of values from a multi-dimensional Gaussian distribution using the {**MASS**} package. It is the same calculation as in @fig-posterior-sample-a (rethinking version)](04-geocentric-models_files/figure-html/fig-posterior-sample-vectors-MASS-a-1.png){#fig-posterior-sample-vectors-MASS-a width=672}\n:::\n:::\n\n\n### Linear prediction {#sec-linear-prediction-a}\n\nWhat we've done until now is just a Gaussian model of height in a\npopulation of adults. But it doesn’t really have the usual feel of \"regression\" to it. Typically, we are interested in modeling how\nan outcome is related to some other variable, a <a class='glossary' title='Predictor variable – also known sometimes as the independent or explanatory variable – is the counterpart to the response or dependent variable. Predictor variables are used to make predictions for dependent variables. (DeepAI, MiniTab)'>predictor variable</a>. If the predictor variable has any statistical association with the outcome\nvariable, then we can use it to predict the outcome. When the predictor\nvariable is built inside the model in a particular way, we'll have\nlinear regression.\n\nLet's look at how height in these Kalahari foragers (the outcome\nvariable) co-varies with weight (the predictor variable).\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-height-against-weight-a lst-cap=\"Scatterplot of height versus weight (Base R version)\"}\n## R code 4.37 #####################\nplot(d2_a$height ~ d2_a$weight)\n```\n\n::: {.cell-output-display}\n![Adult height and weight against one another](04-geocentric-models_files/figure-html/fig-height-against-weight-a-1.png){#fig-height-against-weight-a width=672}\n:::\n:::\n\n\nThere's obviously a relationship: Knowing a person's weight helps you\npredict height. To make this vague observation into a more precise\nquantitative model that relates values of `weight` to plausible values\nof `height`, we need some more technology. How do we take our Gaussian\nmodel from @sec-gaussian-model-of-height-a and incorporate predictor\nvariables?\n\n#### The linear model strategy\n\n##### Model definition\n\nThe strategy is to make the parameter for the mean of a Gaussian distribution, `μ`, into a linear function of the predictor variable and other, new parameters that we invent. This strategy is often simply called the <a class='glossary' title='A linear model specifies a linear relationship between a dependent variable and \\(n\\) independent variables. It conforms to a mathematical model represented by a linear equation of the form \\(Y = b_{1}X_{1} + b_{2}X_{2} + … + b_{n}X_{n}\\). (Oxford Reference)'>linear model</a>. The linear model strategy instructs the golem to assume that the predictor variable has a constant and additive relationship to the mean of the outcome. We ask the golem: “Consider all the lines that relate one variable to the other. Rank all of these lines by plausibility, given these data.” The golem answers with a posterior distribution.\n\nRecall @def-height-linear-model-m4.1 for the Gaussian height model. How do we\nget `weight` into this model? Let $x$ be the name for the column of\nweight measurements, `d2_a$weight`. Let the average of the $x$ values\nbe $\\overline{x}$, \"ex bar\". Now we have a predictor variable `$x$,\nwhich is a list of measures of the same length as $h$. To get weight\ninto the model, we define the mean $\\mu$ as a function of the values in\n$x$.\n\n------------------------------------------------------------------------\n\n::: {#def-height-weight-linear-model1-m4.3}\n###### Linear model height against weight (Version 1 of m4.3)\n\n$$\n\\begin{align*}\nh_{i} \\sim \\operatorname{Normal}(μ_{i}, σ) \\space \\space (1) \\\\ \nμ_{i} = \\alpha + \\beta(x_{i}-\\overline{x}) \\space \\space (2) \\\\\n\\alpha \\sim \\operatorname{Normal}(178, 20) \\space \\space (3)  \\\\ \n\\beta \\sim \\operatorname{Normal}(0,10) \\space \\space (4) \\\\\n\\sigma \\sim \\operatorname{Uniform}(0, 50) \\space \\space (5)      \n\\end{align*}\n$$ {#eq-height-weight-linear-model1-m4.3}\n\n(1) **Likelihood (Probability of the data)**: The first line is nearly\n    identical to before, except now there is a little index $i$ on the\n    $μ$ as well as on the $h$. You can read $h_{i}$ as \"each height\" and\n    $\\mu_{i}$ as \"each $μ$\" The mean $μ$ now depends upon unique values\n    on each row $i$. So the little $i$ on $\\mu_{i}$ indicates that *the\n    mean depends upon the row*.\n\n(2) **Linear model**: The mean $μ$ is no longer a parameter to be\n    estimated. Rather, as seen in the second line of the model, $\\mu{i}$\n    is constructed from other parameters, $\\alpha$ and $\\beta$, and the\n    observed variable $x$. This line is not a stochastic relationship\n    ----- there is no `~` in it, but rather an `=` in it ----- because\n    the definition of $\\mu{i}$ is deterministic. That is to say that,\n    once we know $\\alpha$ and $\\beta$ and $x_{i}$, we know $\\mu{i}$ with\n    certainty. (More details in @sec-linear-model-a.)\n\n(3) **includes (3),(4) and(5) with** $\\alpha, \\beta, \\sigma$ priors: The\n    remaining lines in the model define distributions for the unobserved\n    variables. These variables are commonly known as parameters, and\n    their distributions as priors. There are three parameters:\n    $\\alpha, \\beta, \\sigma$. You've seen priors for $\\alpha$ and $\\sigma$\n    before, although $\\sigma$ was called $\\mu$ back then. (More details\n    in @sec-priors-a)\n:::\n\n------------------------------------------------------------------------\n\n##### Linear model {#sec-linear-model-a}\n\nThe mean $\\mu$ is no longer a parameter to be estimated. Rather, as seen in the second line of the model, $\\mu_{i}$ is constructed from other parameters, $\\alpha$ and $\\beta$, and the observed variable $x$. This line is not a stochastic relationship—there is no `~` in it, but rather an `=` in it—because the definition of $\\mu_{i}$ is deterministic. That is to say that, once we know $\\alpha$ and $\\beta$ and $x_{i}$, we know $\\mu_{i}$ with certainty.\n\nThe value $x_{i}$ in the second line of @def-height-weight-linear-model1-m4.3\nis just the weight value on row $i$. It refers to the same individual as\nthe height value, $h_{i}$, on the same row. The parameters $\\alpha$ and\n$\\beta$ are more mysterious. Where did they come from? We made them up.\nThe parameters $\\mu$ and $\\sigma$ are necessary and sufficient to\ndescribe a Gaussian distribution. But $\\alpha$ and $\\beta$ are instead\ndevices we invent for manipulating $\\mu$, allowing it to vary\nsystematically across cases in the data.\n\nYou'll be making up all manner of parameters as your skills improve. One\nway to understand these made-up parameters is to think of them as\ntargets of learning. Each parameter is something that must be described\nin the posterior distribution. So when you want to know something about\nthe data, you ask your golem by inventing a parameter for it. This will\nmake more and more sense as you progress.\n\nWhat does the second line of @def-height-weight-linear-model1-m4.3? It tells\nthe regression golem that you are asking two questions about the mean of\nthe outcome.\n\n1.  What is the expected height when $x_{i} = \\overline{x}$? The\n    parameter $\\alpha$ answers this question, because when\n    $x_{i} = \\overline{x}$, $\\mu_{i} = \\alpha$. For this reason,\n    $\\alpha$ is often called the **intercept**. But we should think not\n    in terms of some abstract line, but rather in terms of the meaning\n    with respect to the observable variables.\n2.  What is the change in expected height, when $x_{i}$ changes by 1\n    unit? The parameter $\\beta$ answers this question. It is often\n    called a **slope**, again because of the abstract line. Better to\n    think of it as a rate of change in expectation.\n\nJointly these two parameters ask the golem to find a line that relates\n$x$ to $h$, a line that passes through $\\alpha$ when\n$x_{i} = \\overline{x}$ and has slope $\\beta$. That is a task that golems\nare very good at. It's up to you, though, to be sure it's a good\nquestion.\n\n##### Priors {#sec-priors-a}\n\nThe prior for $\\beta$ in @def-height-weight-linear-model1-m4.3 deserves\nexplanation. Why have a Gaussian prior with mean zero? This prior places\njust as much probability below zero as it does above zero, and when\n$\\beta = 0$, weight has no relationship to height. To figure out what\nthis prior implies, we have to simulate the prior predictive\ndistribution.\n\nThe goal is to simulate heights from the model, using only the priors.\nFirst, let's consider a range of weight values to simulate over. The\nrange of observed weights will do fine. Then we need to simulate a bunch\nof lines, the lines implied by the priors for $\\alpha$ and $\\beta$.\nHere's how to do it, setting a seed so you can reproduce it exactly:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-sim-heights-only-with-priors-a lst-cap=\"Simulating heights from the model, using only the priors: rethinking version\"}\n## R code 4.38 #####################\nset.seed(2971)\nN_100_a <- 100 # 100 lines\na <- rnorm(N_100_a, 178, 20)\nb <- rnorm(N_100_a, 0, 10)\n\n\n## R code 4.39 #####################\nplot(NULL,\n  xlim = range(d2_a$weight), ylim = c(-100, 400),\n  xlab = \"weight\", ylab = \"height\"\n)\nabline(h = 0, lty = 2)\nabline(h = 272, lty = 1, lwd = 0.5)\nmtext(\"b ~ dnorm(0,10)\")\nxbar <- mean(d2_a$weight)\nfor (i in 1:N_100_a) {\n  curve(a[i] + b[i] * (x - xbar),\n    from = min(d2_a$weight), to = max(d2_a$weight), add = TRUE,\n    col = rethinking::col.alpha(\"black\", 0.2)\n  )\n}\n```\n\n::: {.cell-output-display}\n![Simulating heights from the model, using only the priors: rethinking version](04-geocentric-models_files/figure-html/fig-sim-heights-only-with-priors-a-1.png){#fig-sim-heights-only-with-priors-a width=672}\n:::\n:::\n\n\nFor reference, I've added a dashed line at zero---no one is shorter than\nzero---and the \"Wadlow\" line at 272 cm for the world's tallest person.\nThe pattern doesn't look like any human population at all. It\nessentially says that the relationship between weight and height could\nbe absurdly positive or negative. Before we've even seen the data, this\nis a bad model. Can we do better?\n\nWe can do better immediately. We know that average height increases with\naverage weight, at least up to a point. Let's try restricting it to\npositive values. The easiest way to do this is to define the prior as\nLog-Normal instead. Defining $\\beta$ as `Log-Normal(0,1)` means to claim\nthat the logarithm of $\\beta$ has a Normal(0,1) distribution.\n\n------------------------------------------------------------------------\n\n::: {#def-prior-log-normal-dist}\nDefining the prior as Log-Normal distribution\n\n$$\n\\beta \\sim \\operatorname{Log-Normal}(0,1)\n$$ {#eq-prior-log-normal-dist}\n:::\n\n------------------------------------------------------------------------\n\nBase R provides the `dlnorm()` and `rlnorm()` densities for working with\nlog-normal distributions. You can simulate this relationship to see what\nthis means for $\\beta$:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-ig-log-normal-a lst-cap=\"Log-Normal distribution: rethinking version\"}\nset.seed(4) # to reproduce with tidyverse version\n\n## R code 4.40 ####################\nb <- rlnorm(1e4, 0, 1)\nrethinking::dens(b, xlim = c(0, 5), adj = 0.1)\n```\n\n::: {.cell-output-display}\n![Log-Normal distribution: rethinking version](04-geocentric-models_files/figure-html/fig-log-normal-a-1.png){#fig-log-normal-a width=672}\n:::\n:::\n\n\nIf the logarithm of $\\beta$ is normal, then $\\beta$ itself is strictly\npositive. The reason is that `exp(x)` is greater than zero for any real\nnumber `x`. This is the reason that Log-Normal priors are commonplace.\nThey are an easy way to enforce positive relationships.\n\nSo what does this earn us? Do the prior predictive simulation again, now\nwith the Log-Normal prior:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-prior-pred-sim-a lst-cap=\"Prior predictive simulation with Log-Normal prior: rethinking version\"}\n## R code 4.41 ###################\nset.seed(2971)\nN_100_a <- 100 # 100 lines\na <- rnorm(N_100_a, 178, 20)\nb <- rlnorm(N_100_a, 0, 1)\n\n## R code 4.39 ###################\nplot(NULL,\n  xlim = range(d2_a$weight), ylim = c(-100, 400),\n  xlab = \"weight\", ylab = \"height\"\n)\nabline(h = 0, lty = 2)\nabline(h = 272, lty = 1, lwd = 0.5)\nmtext(\"b ~ dnorm(0,10)\")\nxbar <- mean(d2_a$weight)\nfor (i in 1:N_100_a) {\n  curve(a[i] + b[i] * (x - xbar),\n    from = min(d2_a$weight), to = max(d2_a$weight), add = TRUE,\n    col = rethinking::col.alpha(\"black\", 0.2)\n  )\n}\n```\n\n::: {.cell-output-display}\n![Prior predictive simulation again, now with the Log-Normal prior: rethinking version](04-geocentric-models_files/figure-html/fig-prior-pred-sim-a-1.png){#fig-prior-pred-sim-a width=672}\n:::\n:::\n\n\nThis is much more sensible. There is still a rare impossible\nrelationship. But nearly all lines in the joint prior for $\\alpha$ and\n$\\beta$ are now within human reason.\n\n::: callout-note\n###### What is the correct prior?\n\nThere is no more a uniquely correct prior than there is a uniquely\ncorrect likelihood. Statistical models are machines for inference. Many\nmachines will work, but some work better than others. Priors can be\nwrong, but only in the same sense that a kind of hammer can be wrong for\nbuilding a table.\n\nPriors encode states of information before seeing data. So priors allow\nus to explore the consequences of beginning with different information.\nIn cases in which we have good prior information that discounts the\nplausibility of some parameter values, like negative associations\nbetween height and weight, we can encode that information directly into\npriors. When we don't have such information, we still usually know\nenough about the plausible range of values. And you can vary the priors\nand repeat the analysis in order to study how different states of\ninitial information influence inference. Frequently, there are many\nreasonable choices for a prior, and all of them produce the same\ninference.\n:::\n\n::: callout-note\n###### Prior predictive simulation and p-hacking\n\nWhen the model is adjusted in light of the observed data, then p-values\nno longer retain their original meaning. False results are to be\nexpected. This is valid for Bayesian and Non-Bayesian statistics. Even\nif Bayesian statistics don't pay any attention to p-values, the danger\nremains. We could choose our priors conditional on the observed sample,\njust to get some desired (wrong) result. It is therefore important to\nchoose priors conditional on pre-data knowledge of the variables---their\nconstraints, ranges, and theoretical relationships. We should always\njudge our priors against general facts, not the sample.\n:::\n\n#### Finding the posterior distribution\n\nThe code needed to approximate the posterior is a straightforward\nmodification of the kind of code you've already seen. All we have to do\nis incorporate our new model for the mean into the model specification\ninside `rethinking::quap()` and be sure to add a prior for the new\nparameter, `β`. Let's repeat the model definition, now with the\ncorresponding R code as footnotes:\n\n------------------------------------------------------------------------\n\n::: {#def-height-weight-linear-model2-m4.3}\n###### Linear model height against weight (Version 2 of m4.3)\n\n$$\n\\begin{align*}\nh_{i} \\sim \\operatorname{Normal}(μ_{i}, σ) \\space \\space (1) \\\\ \nμ_{i} = \\alpha + \\beta(x_{i}-\\overline{x}) \\space \\space (2) \\\\\n\\alpha \\sim \\operatorname{Normal}(178, 20) \\space \\space (3)  \\\\ \n\\beta \\sim \\operatorname{Log-Normal}(0,10) \\space \\space (4) \\\\\n\\sigma \\sim \\operatorname{Uniform}(0, 50)  \\space \\space (5) \\\\    \n\\end{align*} \n$$ {#eq-height-weight-linear-model2-m4.3}\n\n```         \nheight ~ dnorm(mu, sigma)     # <1>\nmu <- a + b * (weight - xbar) # <2>\na ~ dnorm(178, 20)            # <3>\nb ~ dlnorm(0, 10)             # <4>\nsigma ~ dunif(0, 50)          # <5>\n```\n:::\n\n------------------------------------------------------------------------\n\nNotice that the linear model, in the R code on the right-hand side, uses\nthe R assignment operator, `<-` instead of the symbol `=`.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-find-post-dist-m4.3 lst-cap=\"Find the posterior distribution of the linear height-weight model: rethinking version\"}\n## R code 4.42 #############################\n\n# define the average weight, x-bar\nxbar_a <- mean(d2_a$weight)\n\n# fit model\nm4.3 <- rethinking::quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + b * (weight - xbar_a),\n    a ~ dnorm(178, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ),\n  data = d2_a\n)\n\n# summary result\n## R code 4.44 ############################\nrethinking::precis(m4.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>              mean         sd        5.5%       94.5%\n#> a     154.6013671 0.27030766 154.1693633 155.0333710\n#> b       0.9032807 0.04192363   0.8362787   0.9702828\n#> sigma   5.0718809 0.19115478   4.7663786   5.3773831\n```\n:::\n:::\n\n\nYou can usefully think of as assigning to $y = log(x)$ the order of\nmagnitude of $x = exp(y)$. The function is the reverse, turning a\nmagnitude into a value. These definitions will make a mathematician\nshriek. But much of our computational work relies only on these\nintuitions.\n\nThese definitions allow the Log-Normal prior for `β` to be coded another\nway. Instead of defining a parameter `β`, we define a parameter that is\nthe logarithm of `β` and then assign it a normal distribution. Then we\ncan reverse the logarithm inside the linear model. It looks like this:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-find-post-dist2-a lst-cap=\"Find the posterior distribution of the linear height-weight model (log version): rethinking version\"}\n## R code 4.43 ############################\nm4.3b <- rethinking::quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + exp(log_b) * (weight - xbar),\n    a ~ dnorm(178, 20),\n    log_b ~ dnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ),\n  data = d2_a\n)\n\nrethinking::precis(m4.3b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>               mean         sd        5.5%        94.5%\n#> a     154.60260508 0.27034420 154.1705428 155.03466732\n#> log_b  -0.09955739 0.04626836  -0.1735032  -0.02561161\n#> sigma   5.07256420 0.19121896   4.7669594   5.37816904\n```\n:::\n:::\n\n\nNote the `exp(log_b)` in the definition of `mu`. This is the same model\nas `m4.3`. It will make the same predictions. But instead of `β` in the\nposterior distribution, you get `log((β)`. It is easy to translate\nbetween the two, because $\\beta = exp(log(\\beta))$. In code form:\n`b <- exp(log_b)`.\n\n#### Interpretating the posterior distribution\n\nStatistical models are hard to interpret. There are two broad categories\nof interpretation: - reading tables - plotting simulation\n\nPlotting posterior distributions and posterior predictions is better\nthan attempting to understand a table.\n\n##### Table of marginal distributions\n\nI have already included the summary (`precis()`) for the `m4.3` model in\n@lst-find-post-dist-m4.3. But I will repeat it to inspect the marginal\nposterior distributions of the parameters in detail:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-table-summary-m4.3 lst-cap=\"Display the marginal posterior distributions of the parameters: rethinking version\"}\n## R code 4.44 ################\nrethinking::precis(m4.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>              mean         sd        5.5%       94.5%\n#> a     154.6013671 0.27030766 154.1693633 155.0333710\n#> b       0.9032807 0.04192363   0.8362787   0.9702828\n#> sigma   5.0718809 0.19115478   4.7663786   5.3773831\n```\n:::\n:::\n\n\n**Interpreting the result of `rethinking::precis(m4.3)`**\n\n1.  First row: quadratic approximation for $\\alpha$\n2.  Second row: quadratic approximation for $\\beta$\n3.  Third row: quadratic approximation for $\\sigma$\n\nLet's focus on b ($\\beta$), because it's the new parameter. Since\n($\\beta$) is a slope, the value 0.90 can be read as *a person 1 kg\nheavier is expected to be 0.90 cm taller*. 89% of the posterior\nprobability ($94.5-5.5$) lies between 0.84 and 0.97. That suggests that\n($\\beta$) values close to zero or greatly above one are highly\nincompatible with these data and this model. It is most certainly not\nevidence that the relationship between weight and height is linear,\nbecause the model only considered lines. It just says that, if you are\ncommitted to a line, then lines with a slope around 0.9 are plausible\nones.\n\nRemember, the numbers in the default precis output aren't sufficient to\ndescribe the quadratic posterior completely. For that, we also require\nthe variance-covariance matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-var-cov-matrix-m4.3 lst-cap=\"Calculate the variance-covariance matrix for model m4.3\"}\n## R code 4.45 ######################\nround(rethinking::vcov(m4.3), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>           a     b sigma\n#> a     0.073 0.000 0.000\n#> b     0.000 0.002 0.000\n#> sigma 0.000 0.000 0.037\n```\n:::\n:::\n\n\nThere is very little covariation among the parameters in this case. The\nlack of covariance among the parameters results from\n<a class='glossary' title='Substracting the means leads to a lack of covariance among the parameters. In centering, you are changing the values but not the scale.  So a predictor that is centered at the mean has new values–the entire scale has shifted so that the mean now has a value of 0, but one unit is still one unit.  The intercept will change, but the regression coefficient for that variable will not.  Since the regression coefficient is interpreted as the effect on the mean of Y for each one unit difference in X, it doesn’t change when X is centered.(The Analysis Factor) (Chap.4)'>centering</a>.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-var-cov-matrix-m4.3 lst-cap=\"Show the marginal posteriors and covariance matrix for model m4.3\"}\nrethinking::pairs(m4.3)\n```\n\n::: {.cell-output-display}\n![The marginal posteriors and the covariance matrix for model m4.3](04-geocentric-models_files/figure-html/fig-marg-post-cov-m4.3-1.png){#fig-marg-post-cov-m4.3 width=672}\n:::\n:::\n\n\n##### Plotting posterior inference against the data\n\nIt's almost always much more useful to plot the posterior inference\nagainst the data. Not only does plotting help in interpreting the\nposterior, but it also provides an informal check on model assumptions.\n\nWe're going to start with a simple version of that task, superimposing\njust the posterior mean values over the height and weight data. Then\nwe'll slowly add more and more information to the prediction plots,\nuntil we've used the entire posterior distribution.\n\nWe'll start with just the raw data and a single line. The code below\nplots the raw data, computes the posterior mean values for `a` and `b`,\nthen draws the implied line:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-raw-data-line-m4.3 lst-cap=\"Height against weight with linear regression\"}\n## R code 4.46 ############################################\nplot(height ~ weight, data = d2_a, col = rethinking::rangi2)\npost_m4.3 <- rethinking::extract.samples(m4.3)\na_map <- mean(post_m4.3$a)\nb_map <- mean(post_m4.3$b)\ncurve(a_map + b_map * (x - xbar), add = TRUE)\n```\n\n::: {.cell-output-display}\n![Height in centimeters (vertical) plotted against weight in kilograms (horizontal), with the line at the posterior mean plotted in black: rethinking version](04-geocentric-models_files/figure-html/fig-raw-data-line-m4.3-1.png){#fig-raw-data-line-m4.3 width=672}\n:::\n:::\n\n\nEach point in this plot is a single individual. The black line is\ndefined by the mean slope `β` and mean intercept `α`. This is not a bad\nline. It certainly looks highly plausible. But there are an infinite\nnumber of other highly plausible lines near it. Let's draw those too.\n\n##### Adding uncertainty around the mean\n\nPlots of the average line, like in @fig-raw-data-line-m4.3, are useful\nfor getting an impression of the magnitude of the estimated influence of\na variable. But they do a poor job of communicating uncertainty.\nRemember, the posterior distribution considers every possible regression\nline connecting height to weight. It assigns a relative plausibility to\neach. This means that each combination of `α` and `β` has a posterior\nprobability. It could be that there are many lines with nearly the same\nposterior probability as the average line. Or it could be instead that\nthe posterior distribution is rather narrow near the average line.\n\nSo how can we get that uncertainty onto the plot? Together, a\ncombination of `α` and `β` define a line. And so we could sample a bunch\nof lines from the posterior distribution. Then we could display those\nlines on the plot, to visualize the uncertainty in the regression\nrelationship.\n\nTo better appreciate how the posterior distribution contains lines, we\nwork with all of the samples from the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## R code 4.47 ################\n# post_m4.3 <- rethinking::extract.samples(m4.3) # already in previous listing\npost_m4.3[1:5, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>          a         b    sigma\n#> 1 154.5208 0.8393668 4.995964\n#> 2 154.8405 0.8407996 5.016383\n#> 3 154.5813 0.9485057 5.143586\n#> 4 154.0805 0.8992872 5.100492\n#> 5 154.8228 0.9488178 5.146869\n```\n:::\n:::\n\n\nEach row is a correlated random sample from the joint posterior of all\nthree parameters, using the covariances provided by\n`rethinking::vcov(m4.3)` (@lst-var-cov-matrix-m4.3). The paired values\nof `a` and `b` on each row define a line. The average of very many of\nthese lines is the posterior mean line. But the scatter around that\naverage is meaningful, because it alters our confidence in the\nrelationship between the predictor and the outcome.\n\nSo now let's display a bunch of these lines, so you can see the scatter.\nThis lesson will be easier to appreciate, if we use only some of the\ndata to begin. Then you can see how adding in more data changes the\nscatter of the lines. So we'll begin with just the first 10 cases in\n`d2_a`. The following code extracts the first 10 cases and re-estimates\nthe model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## R code 4.48 ##########################\nN10_a <- 10\ndN10_a <- d2_a[1:N10_a, ]\nmN10_a <- rethinking::quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + b * (weight - mean(weight)),\n    a ~ dnorm(178, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ),\n  data = dN10_a\n)\n```\n:::\n\n\nNow let's plot 20 of these lines for the 10 data points, to see what the\nuncertainty looks like.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-plot-lines-10-points-a lst-cap=\"20 samples from the quadratic approximate posterior distribution for m4.3 with 10 data points\"}\n## R code 4.49 ##############################\n# extract 20 samples from the posterior\npost_20_m4.3 <- rethinking::extract.samples(mN10_a, n = 20)\n\n# display raw data and sample size\nplot(dN10_a$weight, dN10_a$height,\n  xlim = range(d2_a$weight), ylim = range(d2_a$height),\n  col = rethinking::rangi2, xlab = \"weight\", ylab = \"height\"\n)\nmtext(rethinking:::concat(\"N = \", N10_a))\n\n# plot the lines, with transparency\nfor (i in 1:20) {\n  curve(post_20_m4.3$a[i] + post_20_m4.3$b[i] * (x - mean(dN10_a$weight)),\n    col = rethinking::col.alpha(\"black\", 0.3), add = TRUE\n  )\n}\n```\n\n::: {.cell-output-display}\n![Samples from the quadratic approximate posterior distribution for the height/weight model, m4.3. 20 lines sampled from 10 data points of the posterior distribution, showing the uncertainty in the regression relationship: rethinking version](04-geocentric-models_files/figure-html/fig-plot-lines-10-points-a-1.png){#fig-plot-lines-10-points-a width=672}\n:::\n:::\n\n\nThe result is shown in the upper-left plot in FIGURE 4.7. By plotting\nmultiple regression lines, sampled from the posterior, it is easy to see\nboth the highly confident aspects of the relationship and the less\nconfident aspects. The cloud of regression lines displays greater\nuncertainty at extreme values for weight.\n\nThe other plots in FIGURE 4.7 show the same relationships, but for\nincreasing amounts of data. Just re-use the code from before, but change\nN \\<- 10 to some other value. Notice that the cloud of regression lines\ngrows more compact as the sample size increases. This is a result of the\nmodel growing more confident about the location of the mean.\n\nPlot 20 lines sampled from 50 data points of the posterior distribution\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-plot-lines-50-points-a lst-cap=\"20 samples from the quadratic approximate posterior distribution for m4.3 with 50 data points\"}\n## R code 4.48, 4.49 ######################\nN50_a <- 50\ndN50_a <- d2_a[1:N50_a, ]\nmN50_a <- rethinking::quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + b * (weight - mean(weight)),\n    a ~ dnorm(178, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ),\n  data = dN50_a\n)\n\n# extract 20 samples from the posterior\npost_50_m4.3 <- rethinking::extract.samples(mN50_a, n = 20)\n\n# display raw data and sample size\nplot(dN50_a$weight, dN50_a$height,\n  xlim = range(d2_a$weight), ylim = range(d2_a$height),\n  col = rethinking::rangi2, xlab = \"weight\", ylab = \"height\"\n)\nmtext(rethinking:::concat(\"N = \", N50_a))\n\n# plot the lines, with transparency\nfor (i in 1:20) {\n  curve(post_50_m4.3$a[i] + post_50_m4.3$b[i] * (x - mean(dN50_a$weight)),\n    col = rethinking::col.alpha(\"black\", 0.3), add = TRUE\n  )\n}\n```\n\n::: {.cell-output-display}\n![Samples from the quadratic approximate posterior distribution for the height/weight model, m4.3. 20 lines sampled from 50 data points of the posterior distribution, showing the uncertainty in the regression relationship: rethinking version](04-geocentric-models_files/figure-html/fig-plot-lines-50-points-a-1.png){#fig-plot-lines-50-points-a width=672}\n:::\n:::\n\n\nPlot 20 lines sampled from 352 data points of the posterior distribution\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-plot-lines-all-352-points-a lst-cap=\"20 Samples from the quadratic approximate posterior distribution for m4.3 with all data\"}\n## R code 4.48, 4.49 ###########################\nN352_a <- 352\ndN352_a <- d2_a[1:N352_a, ]\nmN352_a <- rethinking::quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + b * (weight - mean(weight)),\n    a ~ dnorm(178, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ),\n  data = dN352_a\n)\n\n# extract 20 samples from the posterior\npost_352_m4.3 <- rethinking::extract.samples(mN352_a, n = 20)\n\n# display raw data and sample size\nplot(dN352_a$weight, dN352_a$height,\n  xlim = range(d2_a$weight), ylim = range(d2_a$height),\n  col = rethinking::rangi2, xlab = \"weight\", ylab = \"height\"\n)\nmtext(rethinking:::concat(\"N = \", N352_a))\n\n# plot the lines, with transparency\nfor (i in 1:20) {\n  curve(post_352_m4.3$a[i] + post_352_m4.3$b[i] * (x - mean(dN352_a$weight)),\n    col = rethinking::col.alpha(\"black\", 0.3), add = TRUE\n  )\n}\n```\n\n::: {.cell-output-display}\n![Samples from the quadratic approximate posterior distribution for the height/weight model, m4.3. 20 lines sampled from all 352 data points of the posterior distribution, showing the uncertainty in the regression relationship.](04-geocentric-models_files/figure-html/fig-plot-lines-all-352-points-a-1.png){#fig-plot-lines-all-352-points-a width=672}\n:::\n:::\n\n\n##### Plotting regression intervals and contours\n\nThe cloud of regression lines in @fig-plot-lines-10-points-a,\n@fig-plot-lines-50-points-a and @fig-plot-lines-all-352-points-a is an\nappealing display, because it communicates uncertainty about the\nrelationship in a way that many people find intuitive. But it's more\ncommon, and often much clearer, to see the uncertainty displayed by\nplotting an interval or contour around the average regression line.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-calc-PI-around-regr-line-a lst-cap=\"Calculate uncertainty around the average regression line\"}\n## R code 4.50 ##########################\n\npost_m4.3 <- rethinking::extract.samples(m4.3)      # <1>\nmu_at_50_a <- post_m4.3$a + post_m4.3$b * (50 - xbar) # <2>\nhead(mu_at_50_a)                                      # <3>\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 159.5583 159.4304 159.0339 159.5448 159.3048 159.0473\n```\n:::\n:::\n\n\n1.  Repeating the code for drawing (extracting and collecting) from the\n    fitted model m4.3 (already done in @fig-raw-data-line-m4.3)\n2.  The code to the right of the `<-` takes its form from the equation\n    for $\\mu_{i} = \\alpha + \\beta(x_{i} - \\overline{x})$. The value of\n    $x_{i}$ in this case is 50.\n3.  The result is a vector of predicted means, one for each random\n    sample from the posterior. Since joint `a` and `b` went into\n    computing each, the variation across those means incorporates the\n    uncertainty in and correlation between both parameters.\n\nIt might be helpful at this point to actually plot the density for this\nvector of means:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-ig-density-vector-mean-50-a fig-cap=\"Quadratic approximation of the posterior distribution for mean height, μ, when weight is 50 kg\"}\n## R code 4.51 ##################\nrethinking::dens(mu_at_50_a, col = rethinking::rangi2, lwd = 2, xlab = \"mu|weight=50\")\n```\n\n::: {.cell-output-display}\n![The quadratic approximate posterior distribution of the mean height, μ, when weight is 50 kg. This distribution represents the relative plausibility of different values of the mean. Rethinking version](04-geocentric-models_files/figure-html/fig-density-vector-mean-50-a-1.png){#fig-density-vector-mean-50-a width=672}\n:::\n:::\n\n\nSince the components of `μ` have distributions, so too does `μ`. And\nsince the distributions of `α` and `β` are Gaussian, so too is the\ndistribution of `μ` (adding Gaussian distributions always produces a\nGaussian distribution).\n\nSince the posterior for `μ` is a distribution, you can find intervals\nfor it, just like for any posterior distribution. To find the 89%\ncompatibility interval of `μ` at 50 kg, just use the `PI()` command as\nusual:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-PI-mu-at-50-a lst-cap=\"89% compatibility interval of μ at 50 kg\"}\n## R code 4.52 ##############\nrethinking::PI(mu_at_50_a, prob = 0.89)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>       5%      94% \n#> 158.5863 159.6690\n```\n:::\n:::\n\n\nWhat these numbers mean is that the central 89% of the ways for the\nmodel to produce the data place the average height between about 159 cm\nand 160 cm (conditional on the model and data), assuming the weight is\n50 kg.\n\nThat's good so far, but we need to repeat the above calculation for\nevery weight value on the horizontal axis, not just when it is 50 kg. We\nwant to draw 89% intervals around the average slope in\n@fig-raw-data-line-m4.3.\n\nThis is made simple by strategic use of the `rethinking::link()`\nfunction, a part of the {**rethinking**} package.\nWhat`rethinking::link()` will do is take your`rethinking::quap()`\napproximation, sample from the posterior distribution, and then compute\n`μ\\` for each case in the data and sample from the posterior\ndistribution. Here's what it looks like for the data you used to fit the\nmodel:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-calc-mu-with-link-a lst-cap=\"Calculate μ for each case in the data and sample from the posterior distribution: Rethinking version\"}\n## R code 4.53 ##############\nmu_a <- rethinking::link(m4.3)\nstr(mu_a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  num [1:1000, 1:352] 157 157 157 157 157 ...\n```\n:::\n:::\n\n\nYou end up with a big matrix of values of `μ`. Each row is a sample from\nthe posterior distribution. The default is 1000 samples, but you can use\nas many or as few as you like. Each column is a case (row) in the data.\nThere are 352 rows in `d2_a`, corresponding to 352 individuals. So there\nare 352 columns in the matrix mu above.\n\nThe function `rethinking::link()` provides a posterior distribution of\n`μ` for each case we feed it. So above we have a distribution of `μ` for\neach individual in the original data. We actually want something\nslightly different: a distribution of `μ` for each unique weight value\non the horizontal axis. It's only slightly harder to compute that, by\njust passing `rethinking::link()` some new data:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-calc-dist-mu-unique-with-link-a lst-cap=\"Calculate a distribution of μ for each unique weight value on the horizontal axis: rethinking version\"}\n## R code 4.54 ###############################\n# define sequence of weights to compute predictions for\n# these values will be on the horizontal axis\nweight.seq <- seq(from = 25, to = 70, by = 1)\n\n# use link to compute mu\n# for each sample from posterior\n# and for each weight in weight.seq\nmu2_a <- rethinking::link(m4.3, data = data.frame(weight = weight.seq))\nstr(mu2_a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  num [1:1000, 1:46] 137 137 137 136 136 ...\n```\n:::\n:::\n\n\nAnd now there are only 46 columns in mu, because we fed it 46 different\nvalues for weight. To visualize what you've got here, let's plot the\ndistribution of `μ` values at each height.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-dist-mu-height-100-a lst-cap=\"The first 100 values in the distribution of μ at each weight value. Rethinking version\"}\n## R code 4.55 ##################\n# use type=\"n\" to hide raw data\nbase::plot(height ~ weight, d2_a, type = \"n\")\n\n# loop over samples and plot each mu value\nfor (i in 1:100) {\n  graphics::points(weight.seq, mu2_a[i, ], pch = 16, col = rethinking::col.alpha(rethinking::rangi2, 0.1))\n}\n```\n\n::: {.cell-output-display}\n![The first 100 values in the distribution of μ at each weight value. Rethinking version](04-geocentric-models_files/figure-html/fig-dist-mu-height-100-a-1.png){#fig-dist-mu-height-100-a width=672}\n:::\n:::\n\n\nAt each weight value in `weight.seq`, a pile of computed `μ` values are\nshown. Each of these piles is a Gaussian distribution, like that in\n@fig-density-vector-mean-50-a. You can see now that the amount of\nuncertainty in `μ` depends upon the value of weight. And this is the\nsame fact you saw in @fig-plot-lines-10-points-a,\n@fig-plot-lines-50-points-a and @fig-plot-lines-all-352-points-a.\n\nThe final step is to summarize the distribution for each weight value.\nWe'll use `base::apply()`, which applies a function of your choice to a\nmatrix.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-sum-dist-weight-a lst-cap=\"Summary of the distribution for each weight value. Rethinking version\"}\n## R code 4.56 #####################\n# summarize the distribution of mu\nmu2.mean <- apply(mu2_a, 2, mean)                      # <1>\nmu2.PI <- apply(mu2_a, 2, rethinking::PI, prob = 0.89) # <2>\nmu2.mean                                               # <3>\nmu2.PI                                                 # <4>\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  [1] 136.5424 137.4466 138.3508 139.2550 140.1592 141.0634 141.9676 142.8718\n#>  [9] 143.7760 144.6802 145.5844 146.4886 147.3928 148.2970 149.2012 150.1054\n#> [17] 151.0096 151.9138 152.8180 153.7222 154.6264 155.5306 156.4348 157.3390\n#> [25] 158.2432 159.1474 160.0516 160.9558 161.8600 162.7642 163.6684 164.5726\n#> [33] 165.4768 166.3810 167.2852 168.1894 169.0936 169.9978 170.9020 171.8062\n#> [41] 172.7104 173.6146 174.5188 175.4230 176.3272 177.2314\n#>         [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n#> 5%  135.1820 136.1427 137.1090 138.0732 139.0427 140.0060 140.9769 141.9403\n#> 94% 137.9566 138.7957 139.6341 140.4685 141.3171 142.1576 142.9919 143.8282\n#>         [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]\n#> 5%  142.8926 143.8438 144.8002 145.7557 146.7248 147.6974 148.6586 149.5927\n#> 94% 144.6804 145.5304 146.3826 147.2280 148.0705 148.9315 149.7833 150.6489\n#>        [,17]    [,18]    [,19]    [,20]    [,21]    [,22]    [,23]    [,24]\n#> 5%  150.5291 151.4449 152.3822 153.2993 154.2077 155.0976 155.9761 156.8787\n#> 94% 151.5048 152.3802 153.2758 154.1675 155.0723 155.9922 156.9208 157.8639\n#>        [,25]    [,26]    [,27]    [,28]    [,29]    [,30]    [,31]    [,32]\n#> 5%  157.7496 158.6081 159.4745 160.3296 161.1675 162.0240 162.8661 163.6933\n#> 94% 158.8037 159.7456 160.7019 161.6398 162.6004 163.5497 164.5186 165.4686\n#>        [,33]    [,34]    [,35]    [,36]    [,37]    [,38]    [,39]    [,40]\n#> 5%  164.5345 165.3878 166.2443 167.0871 167.9170 168.7482 169.5905 170.4328\n#> 94% 166.4382 167.4100 168.3530 169.3173 170.2682 171.2195 172.1905 173.1432\n#>        [,41]    [,42]    [,43]    [,44]    [,45]    [,46]\n#> 5%  171.2669 172.1074 172.9446 173.7725 174.6008 175.4384\n#> 94% 174.1118 175.0804 176.0486 177.0111 177.9592 178.9288\n```\n:::\n:::\n\n\n1.  Read `apply(mu2,2,mean)` as \"compute the mean of each column\n    (dimension '2') of the matrix mu\". Now `mu2.mean` contains the\n    average `μ` at each weight value.\n2.  `mu2.PI` contains 89% lower and upper bounds for each weight value.\n3.  `mu2.mean` and `mu2.PI` are just different kinds of summaries of the\n    distributions in `mu2_a`, with each column being for a different\n    weight value. These summaries are only summaries. The \"estimate\" is\n    the entire distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-summaries-on-data-top-a lst-cap=\"Plot of the summaries on top of the !Kung height data with 89% compatibility interval\"}\n## R code 4.57 ###########################\n# plot raw data\n# fading out points to make line and interval more visible\nplot(height ~ weight, data = d2_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))\n\n# plot the MAP line, aka the mean mu for each weight\ngraphics::lines(weight.seq, mu2.mean)\n\n# plot a shaded region for 89% PI\nrethinking::shade(mu2.PI, weight.seq)\n```\n\n::: {.cell-output-display}\n![Plot of the summaries on top of the !Kung height data again, now with 89% compatibility interval of the mean indicated by the shaded region. Compare this region to the distributions of blue points in @fig-dist-mu-height-100-a.](04-geocentric-models_files/figure-html/fig-summaries-on-data-top-a-1.png){#fig-summaries-on-data-top-a width=672}\n:::\n:::\n\n\n::: callout-caution\nThere is very little uncertainty about the average height as a function\nof average weight. But keep in mind that these inferences are always\nconditional on the model. Think of the regression line in\n@fig-summaries-on-data-top-a as saying: *Conditional on the assumption\nthat height and weight are related by a straight line, then this is the\nmost plausible line, and these are its plausible bounds.*\n:::\n\nUsing this approach, you can derive and plot posterior prediction means\nand intervals for quite complicated models, for any data you choose. As\nlong as you know the structure of the model ----- how parameters relate\nto the data ----- you can use samples from the posterior to describe any\naspect of the model's behavior.\n\nSummarizing the three steps for generating predictions and intervals\nfrom the posterior of a fit model:\n\n1.  Use `rethinking::link()` to generate distributions of posterior\n    values for `μ`. The default behavior of `rethinking::link()` is to\n    use the original data, so you have to pass it a list of new\n    horizontal axis values you want to plot posterior predictions\n    across.\n2.  Use summary functions like `mean` or `PI` to find averages and lower\n    and upper bounds of `μ` for each value of the predictor variable.\n3.  Finally, use plotting functions like `graphics::lines()` and\n    `rethinking::shade()` to draw the lines and intervals. Or you might\n    plot the distributions of the predictions, or do further numerical\n    calculations with them. It's really up to you.\n\n::: callout-note\nYou could write a function that accomplish the same thing as\n`rethinking::link()`:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-writing-link-function-a lst-cap=\"Code to perform the same steps as the rethinking::link() function\"}\n## R code 4.58 ################################\npost_m4.3 <- rethinking::extract.samples(m4.3)\nmu.link <- function(weight) post_m4.3$a + post_m4.3$b * (weight - xbar)\nweight.seq <- seq(from = 25, to = 70, by = 1)\nmu3_a <- sapply(weight.seq, mu.link)\nmu3.mean <- apply(mu3_a, 2, mean)\nmu3.CI <- apply(mu3_a, 2, rethinking::PI, prob = 0.89)\nmu3.mean\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  [1] 136.5433 137.4467 138.3501 139.2535 140.1569 141.0603 141.9637 142.8671\n#>  [9] 143.7705 144.6740 145.5774 146.4808 147.3842 148.2876 149.1910 150.0944\n#> [17] 150.9978 151.9012 152.8046 153.7080 154.6114 155.5149 156.4183 157.3217\n#> [25] 158.2251 159.1285 160.0319 160.9353 161.8387 162.7421 163.6455 164.5489\n#> [33] 165.4523 166.3558 167.2592 168.1626 169.0660 169.9694 170.8728 171.7762\n#> [41] 172.6796 173.5830 174.4864 175.3898 176.2932 177.1967\n```\n:::\n\n```{.r .cell-code #lst-writing-link-function-a lst-cap=\"Code to perform the same steps as the rethinking::link() function\"}\nmu3.CI\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>         [,1]     [,2]     [,3]     [,4]    [,5]     [,6]     [,7]     [,8]\n#> 5%  135.1346 136.1023 137.0692 138.0347 139.002 139.9674 140.9323 141.8988\n#> 94% 137.9574 138.7923 139.6301 140.4712 141.312 142.1543 142.9959 143.8406\n#>         [,9]    [,10]    [,11]    [,12]    [,13]    [,14]    [,15]    [,16]\n#> 5%  142.8629 143.8221 144.7844 145.7364 146.6898 147.6428 148.5936 149.5419\n#> 94% 144.6846 145.5325 146.3752 147.2210 148.0732 148.9256 149.7815 150.6415\n#>        [,17]    [,18]    [,19]    [,20]    [,21]    [,22]    [,23]    [,24]\n#> 5%  150.4843 151.4178 152.3481 153.2667 154.1750 155.0751 155.9637 156.8450\n#> 94% 151.5069 152.3764 153.2562 154.1457 155.0463 155.9524 156.8687 157.7983\n#>        [,25]    [,26]    [,27]    [,28]    [,29]    [,30]    [,31]    [,32]\n#> 5%  157.7134 158.5802 159.4383 160.2892 161.1380 161.9875 162.8366 163.6822\n#> 94% 158.7350 159.6771 160.6250 161.5771 162.5288 163.4906 164.4549 165.4137\n#>        [,33]    [,34]    [,35]    [,36]    [,37]    [,38]    [,39]    [,40]\n#> 5%  164.5318 165.3705 166.2136 167.0608 167.9028 168.7440 169.5778 170.4147\n#> 94% 166.3832 167.3498 168.3145 169.2823 170.2496 171.2195 172.1922 173.1626\n#>        [,41]    [,42]    [,43]    [,44]    [,45]    [,46]\n#> 5%  171.2495 172.0902 172.9308 173.7712 174.6080 175.4499\n#> 94% 174.1271 175.0931 176.0607 177.0282 177.9955 178.9654\n```\n:::\n:::\n\n\nAnd the values in `mu3.mean` and `mu3.CI` should be very similar\n(allowing for simulation variance) to what you got the automated way,\nusing `rethinking::link()` in @lst-sum-dist-weight-a.\n\nKnowing this manual method is useful both for (1) understanding and (2)\nsheer power. Whatever the model you find yourself with, this approach\ncan be used to generate posterior predictions for any component of it.\nAutomated tools like link save effort, but they are never as flexible as\nthe code you can write yourself.\n:::\n\n##### Prediction intervals\n\nNow let's walk through generating an 89% prediction interval for actual\nheights, not just the average height, `μ`. This means we'll incorporate\nthe standard deviation `σ` and its uncertainty as well. Remember, the\nfirst line of the statistical model in @eq-height-weight-linear-model2-m4.3 is:\n\n------------------------------------------------------------------------\n\n::: {#def-height-weight-linear-model-first-line-m4.3}\n###### Remember the first line of @eq-height-weight-linear-model2-m4.3\n\n$$\nh_{i} \\sim \\operatorname{Normal}(μ_{i}, σ)\n$$ {#eq-height-weight-linear-model-first-line-m4.3}\n:::\n\n------------------------------------------------------------------------\n\nWhat you've done so far is just use samples from the posterior to\nvisualize the uncertainty in $μ_{i}$, the linear model of the mean. But\nactual predictions of heights depend also upon the distribution in the\nfirst line. The Gaussian distribution on the first line tells us that\nthe model expects observed heights to be distributed around `μ`, not\nright on top of it. And the spread around `μ` is governed by `σ`. All of\nthis suggests we need to incorporate `σ` in the predictions.\n\nImagine simulating heights. For any unique weight value, you sample from\na Gaussian distribution with the correct mean `μ` for that weight, using\nthe correct value of σ sampled from the same posterior distribution. If\nyou do this for every sample from the posterior, for every weight value\nof interest, you end up with a collection of simulated heights that\nembody the uncertainty in the posterior as well as the uncertainty in\nthe Gaussian distribution of heights.\n\nThere is a tool called `rethinking::sim()` which does this simulation of\nthe posterior observations:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-simulate-post-dist-a lst-cap=\"Simulation of the posterior observations\"}\n## R code 4.59 #######################\nsim.height <- rethinking::sim(m4.3, data = list(weight = weight.seq))\nstr(sim.height)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  num [1:1000, 1:46] 138 137 138 127 141 ...\n```\n:::\n:::\n\n\nThis matrix is much like the earlier one, `mu`, but it contains\nsimulated heights, not distributions of plausible average height, `μ`.\n\nWe can summarize these simulated heights in the same way we summarized\nthe distributions of `μ`, by using apply:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-sum-sim-heights-a lst-cap=\"Summarize simulted heights\"}\n## R code 4.60 ###################\nheight.PI <- apply(sim.height, 2, rethinking::PI, prob = 0.89)\n```\n:::\n\n\nNow `height.PI` contains the 89% posterior prediction interval of\nobservable (according to the model) heights, across the values of weight\nin `weight.seq`.\n\nLet's plot everything we've built up: (1) the average line, (2) the\nshaded region of 89% plausible `μ`, and (3) the boundaries of the\nsimulated heights the model expects.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-pred-interval-height-a lst-cap=\"89% prediction interval for height, as a function of weight\"}\n## R code 4.61 ##################\n# plot raw data\nplot(height ~ weight, d2_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))\n\n# draw MAP line\ngraphics::lines(weight.seq, mu3.mean)\n\n# draw HPDI region for line\nrethinking::shade(mu3.CI, weight.seq)\n\n# draw PI region for simulated heights\nrethinking::shade(height.PI, weight.seq)\n```\n\n::: {.cell-output-display}\n![89% prediction interval for height, as a function of weight. The solid line is the average line for the mean height at each weight. The two shaded regions show different 89% plausible regions. The narrow shaded interval around the line is the distribution of μ. The wider shaded region represents the region within which the model expects to find 89% of actual heights in the population, at each weight.](04-geocentric-models_files/figure-html/fig-pred-interval-height-a-1.png){#fig-pred-interval-height-a width=672}\n:::\n:::\n\n\nNotice that the outline for the wide shaded interval is a little rough.\nThis is the simulation variance in the tails of the sampled Gaussian\nvalues. If it really bothers you, increase the number of samples you\ntake from the posterior distribution. The optional n parameter for\n`sim.height` controls how many samples are used. Try for example 1e4\nsamples and run the plotting code again. You'll see the shaded boundary\nsmooth out some.\n\nWith extreme percentiles, it can be very hard to get out all of the\nroughness. Luckily, it hardly matters, except for aesthetics. Moreover,\nit serves to remind us that all statistical inference is approximate.\nThe fact that we can compute an expected value to the 10th decimal place\ndoes not imply that our inferences are precise to the 10th decimal\nplace.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-pred-interval-height2-a lst-cap=\"89% prediction interval for height, as a function of weight\"}\n## R code 4.62 (4.59 & 4.60) ###################\n\n## R code 4.59 ################\nsim2.height <- rethinking::sim(m4.3, data = list(weight = weight.seq), n = 1e4)\nheight2.PI <- apply(sim2.height, 2, rethinking::PI, prob = 0.89)\n\n## R code 4.61 ##################\n# plot raw data\nplot(height ~ weight, d2_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))\n\n# draw MAP line\ngraphics::lines(weight.seq, mu3.mean)\n\n# draw HPDI region for line\nrethinking::shade(mu3.CI, weight.seq)\n\n# draw PI region for simulated heights\nrethinking::shade(height2.PI, weight.seq)\n```\n\n::: {.cell-output-display}\n![89% prediction interval for height, as a function of weight. Shaded boundary smoothed out by sampling 1e4 times instead of the standard value of 1e3](04-geocentric-models_files/figure-html/fig-pred-interval-height2-a-1.png){#fig-pred-interval-height2-a width=672}\n:::\n:::\n\n\nJust like with `rethinking::link()`, it's useful to know a little about\nhow `rethinking::sim()` operates. For every distribution like `dnorm`,\nthere is a companion simulation function. For the Gaussian distribution,\nthe companion is `rnorm`, and it simulates sampling from a Gaussian\ndistribution. What we want R to do is simulate a height for each set of\nsamples, and to do this for each value of weight. The following will do\nit:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-writing-sim-function-a lst-cap=\"Writing you own rethinking:sim() function\"}\n## R code 4.63 ########################################\n\npost_m4.3 <- rethinking::extract.samples(m4.3)\n# post <- extract.samples(m4.3)\n# weight.seq <- 25:70\nsim3.height <- sapply(weight.seq, function(weight) {\n  rnorm(\n    n = nrow(post_m4.3),\n    mean = post_m4.3$a + post_m4.3$b * (weight - xbar),\n    sd = post_m4.3$sigma\n  )\n})\nheight3.PI <- apply(sim3.height, 2, rethinking::PI, prob = 0.89)\n```\n:::\n\n\nThe values in `height.PI` will be practically identical to the ones\ncomputed in the main text and displayed in FIGURE 4.10.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-compare-height-PIs-str-a lst-cap=\"Compare height3.PI with height.PI using str()\"}\nstr(height.PI)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  num [1:2, 1:46] 128 145 129 146 130 ...\n#>  - attr(*, \"dimnames\")=List of 2\n#>   ..$ : chr [1:2] \"5%\" \"94%\"\n#>   ..$ : NULL\n```\n:::\n\n```{.r .cell-code #lst-compare-height-PIs-str-a lst-cap=\"Compare height3.PI with height.PI using str()\"}\nstr(height3.PI)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  num [1:2, 1:46] 128 145 129 146 130 ...\n#>  - attr(*, \"dimnames\")=List of 2\n#>   ..$ : chr [1:2] \"5%\" \"94%\"\n#>   ..$ : NULL\n```\n:::\n:::\n\n\nSmall differences are the result of the randomized sampling process:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-compare-height-PIs-head-a lst-cap=\"Compare height3.PI with height.PI using head()\"}\nhead(height.PI)[ , 1:6]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>        [,1]     [,2]     [,3]     [,4]     [,5]     [,6]\n#> 5%  127.640 128.7990 129.7844 131.8654 132.3867 132.6271\n#> 94% 144.615 145.7259 146.0200 147.1918 147.9160 149.6606\n```\n:::\n\n```{.r .cell-code #lst-compare-height-PIs-head-a lst-cap=\"Compare height3.PI with height.PI using head()\"}\nhead(height3.PI)[ , 1:6]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>         [,1]     [,2]     [,3]     [,4]     [,5]     [,6]\n#> 5%  128.1037 129.0345 130.0564 130.8573 132.0527 132.8835\n#> 94% 144.7898 145.5673 146.4939 147.5685 148.4106 149.3849\n```\n:::\n:::\n\n\n### Curves from lines\n\nWe'll consider two commonplace methods that use linear regression to\nbuild curves. The first is <a class='glossary' title='It is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial in x. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y. Although polynomial regression fits a nonlinear model to the data, as a statistical estimation problem it is linear. What “linear” means in this context is that \\(\\mu_{i}\\) is a linear function of any single parameter. For this reason, polynomial regression is considered to be a special case of multiple linear regression. (Wikipedia) (Chap.4)'>polynomial regression</a>. The\nsecond is <a class='glossary' title='The term “spline” refers to a wide class of basis functions that are used in applications requiring data interpolation and/or smoothing. Splines are special function defined piecewise by polynomials, it results to a smooth function built out of smaller, component functions. In interpolating problems, spline interpolation is often preferred to polynomial interpolation because it yields similar results, even when using low degree polynomials, while avoiding big numbers and the problem of oscillation at the edges of intervals of higher degrees. The term “spline” comes from the flexible spline devices used by shipbuilders and draftsmen to draw smooth shapes. They used a long, thin piece of wood or metal that could be anchored in a few places in order to aid drawing curves. | There are many types of splines, especially the common-place ‘B-splines’: The ‘B’ stands for ‘basis,’ which just means ‘component.’ B-splines build up wiggly functions from simpler less-wiggly components. Those components are called basis functions. B-splines force you to make a number of choices that other types of splines automate. (Chap.4)'>spline</a>. Both approaches work by transforming a\nsingle predictor variable into several synthetic variables. But splines\nhave some clear advantages.\n\n#### Polynomial regression\n\nPolynomial regression uses powers of a variable -- squares and cubes --\nas extra predictors. This is an easy way to build curved associations.\nPolynomial regressions are very common, and understanding how they work\nwill help scaffold later models.\n\n\n::: {.cell}\n\n```{.r .cell-code #fig-scatterplot-height-weight-a lst-cap=\"Height in centimeters (vertical) plotted against weight in kilograms (horizontal): rethinking version\"}\nplot(height ~ weight, data = d_a, col = rethinking::rangi2)\n```\n\n::: {.cell-output-display}\n![Height in centimeters (vertical) plotted against weight in kilograms (horizontal). This time with the full data, e.g., the non-adults data.](04-geocentric-models_files/figure-html/fig-scatterplot-height-weight-a-1.png){#fig-scatterplot-height-weight-a width=672}\n:::\n:::\n\n\nThe relationship is visibly curved, now that we've included the\nnon-adult individuals. (Compare with adult data in\n@fig-raw-data-line-m4.3)\n\nThe most common polynomial regression is a parabolic model of the mean.\nLet `x` be standardized body weight. Then the parabolic equation for the\nmean height is:\n\n------------------------------------------------------------------------\n\n::: {#def-parabolic-mean}\nParabolic equation for the mean height\n\n$$\n\\mu_{i} = \\alpha + \\beta_{1}x_{i} + \\beta_{2}x_{i}^2\n$$ {#eq-parabolic-mean}\n:::\n\n------------------------------------------------------------------------\n\n@eq-parabolic-mean is a parabolic (second order) polynomial. The\n$\\alpha + \\beta_{1}x_{i}$ part is the same linear function of x in a\nlinear regression, just with a little \"1\" subscript added to the\nparameter name, so we can tell it apart from the new parameter. The\nadditional term uses the square of $x_{i}$ to construct a parabola,\nrather than a perfectly straight line. The new parameter $\\beta_{2}$\nmeasures the curvature of the relationship.\n\n> Fitting these models to data is easy. Interpreting them can be hard.\n\n::: callout-tip\nPolynomial parameters are in general very difficult to understand. But\nprior predictive simulation does help a lot.\n:::\n\n**Fitting a parabolic model of height on weight**\n\nThe first thing to do is to\n<a class='glossary' title='In statistics, standardization (also called Normalizing) is the process of putting different variables on the same scale. This process allows you to compare scores between different types of variables. Typically, to standardize variables, you calculate the mean and standard deviation for a variable. Then, for each observed value of the variable, you subtract the mean and divide by the standard deviation. (Statistics by Jim) See scale() in R.  (Chap.4)'>standardize</a> the predictor variable.\nWe've done this in previous examples. But this is especially helpful for\nworking with polynomial models. When predictor variables have very large\nvalues in them, there are sometimes numerical glitches. Even well-known\nstatistical software can suffer from these glitches, leading to mistaken\nestimates. These problems are very common for polynomial regression,\nbecause the square or cube of a large number can be truly massive.\nStandardizing largely resolves this issue. It should be your default\nbehavior.\n\n> A quadratic function (also called a quadratic, a quadratic polynomial,\n> or a polynomial of degree 2) is special type of polynomial function\n> where the highest-degree term is second degree. ... The graph of a\n> quadratic function is a parabola, a 2-dimensional curve that looks\n> like either a cup(∪) or a cap(∩). ([Statistis How\n> To](https://www.statisticshowto.com/quadratic-function/))\n\nTo define the parabolic model, just modify the definition of $\\mu_{i}$.\n\n------------------------------------------------------------------------\n\n::: {#def-parabolic-model}\nFitting a parabolic model of height on weight\n\n$$\n\\begin{align*}\nh_{i} \\sim \\operatorname{Normal}(μ_{i}, σ) \\space \\space (1) \\\\ \nμ_{i} = \\alpha + \\beta_{1}x_{i} + \\beta_{2}x_{i}^2 \\space \\space (2) \\\\\n\\alpha \\sim \\operatorname{Normal}(178, 20) \\space \\space (3)  \\\\ \n\\beta_{1} \\sim \\operatorname{Log-Normal}(0,10) \\space \\space (4) \\\\\n\\beta_{2} \\sim \\operatorname{Normal}(0,10) \\space \\space (5) \\\\\n\\sigma \\sim \\operatorname{Uniform}(0, 50) \\space \\space (6)      \n\\end{align*}\n$$ {#eq-parabolic-model}\n\n```         \nheight ~ dnorm(mu, sigma)                  # <1>\nmu <- a + b1 * weight.s + b2 * weight.s2^2 # <2>\na ~ dnorm(178, 20)                         # <3>\nb1 ~ dlnorm(0, 10)                         # <4>\nb2 ~ dnorm(0, 10)                          # <5>\nsigma ~ dunif(0, 50)                       # <6>\n```\n:::\n\n------------------------------------------------------------------------\n\nThe confusing issue here is assigning a prior for $\\beta_{2}$, the\nparameter on the squared value of `x`. Unlike $\\beta_{1}$, we don't want\na positive constraint.\n\nApproximating the posterior is straightforward. Just modify the\ndefinition of `mu` so that it contains both the linear and quadratic\nterms. But in general it is better to pre-process any variable\ntransformations -- you don't need the computer to recalculate the\ntransformations on every iteration of the fitting procedure. So I'll\nalso build the square of `weight_s` as a separate variable:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-quap-parabolic lst-cap=\"Finding the posterior distribution of a parabolic model of height on weight with rethinking::quap()\"}\n## R code 4.65 #######################\nd_a$weight_s <- (d_a$weight - mean(d_a$weight)) / sd(d_a$weight)\nd_a$weight_s2 <- d_a$weight_s^2\nm4.5 <- rethinking::quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + b1 * weight_s + b2 * weight_s2,\n    a ~ dnorm(178, 20),\n    b1 ~ dlnorm(0, 1),\n    b2 ~ dnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ),\n  data = d_a\n)\n```\n:::\n\n\nNow, since the relationship between the outcome height and the predictor\nweight depends upon two slopes, b1 and b2, it isn't so easy to read the\nrelationship off a table of coefficients:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-show-result-m4.5 lst-cap=\"Show table of coefficients for model m4.5\"}\n## R code 4.66 ################\nrethinking::precis(m4.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>             mean        sd       5.5%      94.5%\n#> a     146.057364 0.3689765 145.467668 146.647060\n#> b1     21.733104 0.2888897  21.271402  22.194805\n#> b2     -7.803217 0.2741847  -8.241417  -7.365017\n#> sigma   5.774486 0.1764661   5.492459   6.056513\n```\n:::\n:::\n\n\nThe parameter $\\alpha$ (a) is still the intercept, so it tells us the\nexpected value of height when weight is at its mean value. But it is no\nlonger equal to the mean height in the sample, since there is no\nguarantee it should in a polynomial regression. And those $\\beta_{1}$\nand $\\beta_{2}$ parameters are the linear and square components of the\ncurve. But that doesn't make them transparent.\n\nYou have to plot these model fits to understand what they are saying. So\nlet's do that. We'll calculate the mean relationship and the 89%\nintervals of the mean and the predictions, like in the previous section.\nHere's the working code:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-polynomial-regression-a lst-cap=\"Polynomial regressions of height on weight (standardized), for the full !Kung data.\"}\n## R code 4.67 ################\nweight5.seq <- seq(from = -2.2, to = 2, length.out = 30)\npred_dat_a <- list(weight_s = weight5.seq, weight_s2 = weight5.seq^2)\nmu5_a <- rethinking::link(m4.5, data = pred_dat_a)\nmu5.mean <- apply(mu5_a, 2, mean)\nmu5.PI <- apply(mu5_a, 2, rethinking::PI, prob = 0.89)\nsim5.height <- rethinking::sim(m4.5, data = pred_dat_a)\nheight5.PI <- apply(sim5.height, 2, rethinking::PI, prob = 0.89)\n\n\n## R code 4.68 #################\nplot(height ~ weight_s, d_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))\ngraphics::lines(weight5.seq, mu5.mean)\nrethinking::shade(mu5.PI, weight5.seq)\nrethinking::shade(height5.PI, weight5.seq)\n```\n\n::: {.cell-output-display}\n![Polynomial regressions of height on weight (standardized), for the full !Kung data.](04-geocentric-models_files/figure-html/fig-polynomial-regression-a-1.png){#fig-polynomial-regression-a width=672}\n:::\n:::\n\n\nThe quadratic regression does a pretty good job. It is much better than\na linear regression for the full Howell1 data set.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-find-post-full-data-a lst-cap=\"Posterior distribution of the linear height-weight model: rethinking version\"}\n## R code 4.42 #############################\n\n# define the average weight, x-bar\nxbar_full <- mean(d_a$weight)\n\n# fit model\nm4.3_full <- rethinking::quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + b * (weight - xbar_full),\n    a ~ dnorm(178, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ),\n  data = d_a\n)\n\n## R code 4.46 ############################################\nplot(height ~ weight, data = d_a, col = rethinking::rangi2)\npost_m4.3_full <- rethinking::extract.samples(m4.3_full)\na_map_full <- mean(post_m4.3_full$a)\nb_map_full <- mean(post_m4.3_full$b)\ncurve(a_map_full + b_map_full * (x - xbar_full), add = TRUE)\n\n\n\n## R code 4.58 ################################\nmu.link_full <- function(weight) post_m4.3_full$a + post_m4.3_full$b * (weight - xbar_full)\nweight.seq_full <- seq(from = 0, to = 70, by = 1)\nmu3_a_full <- sapply(weight.seq_full, mu.link_full)\nmu3.mean_full <- apply(mu3_a_full, 2, mean)\nmu3.CI_full <- apply(mu3_a_full, 2, rethinking::PI, prob = 0.89)\n\n## R code 4.59 #######################\nsim.height_full <- rethinking::sim(m4.3_full, data = list(weight = weight.seq_full))\n\n## R code 4.60 ###################\nheight.PI_full <- apply(sim.height_full, 2, rethinking::PI, prob = 0.89)\n\n## R code 4.61 ##################\n# plot raw data\n# plot(height ~ weight, d2_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))\n\n# draw MAP line\ngraphics::lines(weight.seq_full, mu3.mean_full)\n\n# draw HPDI region for line\nrethinking::shade(mu3.CI_full, weight.seq_full)\n\n# draw PI region for simulated heights\nrethinking::shade(height.PI_full, weight.seq_full)\n```\n\n::: {.cell-output-display}\n![Find the posterior distribution of the linear height-weight model (full data set): rethinking version](04-geocentric-models_files/figure-html/fig-find-post-full-data-a-1.png){#fig-find-post-full-data-a width=672}\n:::\n:::\n\n\nI had `xbar` to recalculate with the new data set. All the other code is\nthe same as in @lst-find-post-dist-m4.3 and @lst-fig-raw-data-line-m4.3.\n\n@fig-find-post-full-data-a shows the familiar linear regression from\nearlier in the chapter, but now with the standardized predictor and full\ndata with both adults and non-adults. The linear model makes some\nspectacularly poor predictions, at both very low and middle weights.\nCompare this to @fig-polynomial-regression-a, the new quadratic\nregression. The curve does a better job of finding a central path\nthrough the data.\n\nLet's see what will happen if we are using a higher-order polynomial\nregression, a cubic regression on weight. The model is:\n\n------------------------------------------------------------------------\n\n::: {#def-cubic-regression}\nCubic regression on weigth\n\n$$\n\\begin{align*}\nh_{i} \\sim \\operatorname{Normal}(μ_{i}, σ) \\space \\space (1) \\\\ \nμ_{i} = \\alpha + \\beta_{1}x_{i} + \\beta_{2}x_{i}^2 + \\beta_{3}x_{i}^3 \\space \\space (2) \\\\\n\\alpha \\sim \\operatorname{Normal}(178, 20) \\space \\space (3)  \\\\ \n\\beta_{1} \\sim \\operatorname{Log-Normal}(0,10) \\space \\space (4) \\\\\n\\beta_{2} \\sim \\operatorname{Normal}(0,10) \\space \\space (5) \\\\\n\\beta_{3} \\sim \\operatorname{Normal}(0,10) \\space \\space (6) \\\\\n\\sigma \\sim \\operatorname{Uniform}(0, 50) \\space \\space (7)      \n\\end{align*}\n$$ {#eq-cubic-regression}\n\n```         \nheight ~ dnorm(mu, sigma)               # <1>\nmu <- a + b1 * weight.s + b2 * weight.s2^2 + b3 * weight.s3^3 # <2>\na ~ dnorm(178, 20)                      # <3>\nb1 ~ dlnorm(0, 10)                      # <4>\nb2 ~ dnorm(0, 10)                       # <5>\nb3 ~ dnorm(0, 10)                       # <6>\nsigma ~ dunif(0, 50)                    # <7>\n```\n:::\n\n------------------------------------------------------------------------\n\nFit the model accordingly. It is only a slight modification of the\nparabolic model's code:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-cubic-regression-a lst-cap=\"Cubic regressions of height on weight (standardized), for the full !Kung data.\"}\n## R code 4.69 ####################\nd_a$weight_s3 <- d_a$weight_s^3\nm4.6 <- rethinking::quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + b1 * weight_s + b2 * weight_s2 + b3 * weight_s3,\n    a ~ dnorm(178, 20),\n    b1 ~ dlnorm(0, 1),\n    b2 ~ dnorm(0, 10),\n    b3 ~ dnorm(0, 10),\n    sigma ~ dunif(0, 50)\n  ),\n  data = d_a\n)\n\n## R code 4.67 ################\nweight6.seq <- seq(from = -2.2, to = 2, length.out = 30)\npred_dat6_a <- list(weight_s = weight6.seq, weight_s2 = weight6.seq^2,\n                    weight_s3 = weight6.seq^3)\nmu6_a <- rethinking::link(m4.6, data = pred_dat6_a)\nmu6.mean <- apply(mu6_a, 2, mean)\nmu6.PI <- apply(mu6_a, 2, rethinking::PI, prob = 0.89)\nsim5.height <- rethinking::sim(m4.6, data = pred_dat6_a)\nheight6.PI <- apply(sim5.height, 2, rethinking::PI, prob = 0.89)\n\n\n## R code 4.68 #################\nplot(height ~ weight_s, d_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5))\ngraphics::lines(weight6.seq, mu6.mean)\nrethinking::shade(mu6.PI, weight6.seq)\nrethinking::shade(height6.PI, weight6.seq)\n```\n\n::: {.cell-output-display}\n![Cubic regressions of height on weight (standardized), for the full !Kung data.](04-geocentric-models_files/figure-html/fig-cubic-regression-a-1.png){#fig-cubic-regression-a width=672}\n:::\n:::\n\n\nThis cubic curve is even more flexible than the parabola, so it fits the\ndata even better.\n\nBut it's not clear that any of these models make a lot of sense. They\nare good geocentric descriptions of the sample, yes. But there are two\nproblems. First, a better fit to the sample might not actually be a\nbetter model. That's the subject of @sec-chap07-ulisses-compass. Second, the\nmodel contains no biological information. We aren't learning any causal\nrelationship between height and weight. We'll deal with this second\nproblem much later, in @sec-chap16-genealized-linear-madness.\n\n**Converting back to natural scale**\n\nThe plots @fig-find-post-full-data-a, @fig-polynomial-regression-a and\n@fig-cubic-regression-a have standard units on the horizontal axis.\nThese units are sometimes called `z-scores`. But suppose you fit the\nmodel using standardized variables, but want to plot the estimates on\nthe original scale. All that's really needed is first to turn off the\nhorizontal axis when you plot the raw data:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-x-axis-turned-off-a lst-cap=\"Cubic regression with x-axis turned off: Rethinking version\"}\n## R code 4.70 ###########\nplot(height ~ weight_s, d_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5), xaxt = \"n\")\n```\n\n::: {.cell-output-display}\n![Cubic regression with x-axis turned off: Rethinking version](04-geocentric-models_files/figure-html/fig-x-axis-turned-off-a-1.png){#fig-x-axis-turned-off-a width=672}\n:::\n:::\n\n\nThe `xaxt` at the end there turns off the horizontal axis. Then you\nexplicitly construct the axis, using the `graphics::axis()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-cubic-regression-natural-scale-a lst-cap=\"Cubic regression with x-axis in natural scale: Rehtinking version\"}\n## R code 4.71 #############\n\nplot(height ~ weight_s, d_a, col = rethinking::col.alpha(rethinking::rangi2, 0.5), xaxt = \"n\")\n\nat_a <- c(-2, -1, 0, 1, 2)                           # <1>\nlabels <- at_a * sd(d_a$weight) + mean(d_a$weight)   # <2>\naxis(side = 1, at = at_a, labels = round(labels, 1)) # <3>\n```\n\n::: {.cell-output-display}\n![Cubic regression with x-axis in natural scale: Rehtinking version](04-geocentric-models_files/figure-html/fig-cubic-regression-natural-scale-a-1.png){#fig-cubic-regression-natural-scale-a width=672}\n:::\n:::\n\n\n1.  Defines the location of the labels, in standardized units.\n2.  Takes standardized units and converts them back to the original\n    scale.\n3.  Draws the axis.\n\nTake a look at the help `?axis` for more details.\n\n#### Splines\n\nThe second way to introduce a curve is to construct something known as a\n<a class='glossary' title='The term “spline” refers to a wide class of basis functions that are used in applications requiring data interpolation and/or smoothing. Splines are special function defined piecewise by polynomials, it results to a smooth function built out of smaller, component functions. In interpolating problems, spline interpolation is often preferred to polynomial interpolation because it yields similar results, even when using low degree polynomials, while avoiding big numbers and the problem of oscillation at the edges of intervals of higher degrees. The term “spline” comes from the flexible spline devices used by shipbuilders and draftsmen to draw smooth shapes. They used a long, thin piece of wood or metal that could be anchored in a few places in order to aid drawing curves. | There are many types of splines, especially the common-place ‘B-splines’: The ‘B’ stands for ‘basis,’ which just means ‘component.’ B-splines build up wiggly functions from simpler less-wiggly components. Those components are called basis functions. B-splines force you to make a number of choices that other types of splines automate. (Chap.4)'>spline</a>. The word spline originally referred\nto a long, thin piece of wood or metal that could be anchored in a few\nplaces in order to aid drafters or designers in drawing curves. In\nstatistics, a spline is a smooth function built out of smaller,\ncomponent functions. There are actually many types of splines. The\n<a class='glossary' title='The term “spline” refers to a wide class of basis functions that are used in applications requiring data interpolation and/or smoothing. Splines are special function defined piecewise by polynomials, it results to a smooth function built out of smaller, component functions. In interpolating problems, spline interpolation is often preferred to polynomial interpolation because it yields similar results, even when using low degree polynomials, while avoiding big numbers and the problem of oscillation at the edges of intervals of higher degrees. The term “spline” comes from the flexible spline devices used by shipbuilders and draftsmen to draw smooth shapes. They used a long, thin piece of wood or metal that could be anchored in a few places in order to aid drawing curves. | There are many types of splines, especially the common-place ‘B-splines’: The ‘B’ stands for ‘basis,’ which just means ‘component.’ B-splines build up wiggly functions from simpler less-wiggly components. Those components are called basis functions. B-splines force you to make a number of choices that other types of splines automate. (Chap.4)'>B-spline</a> we'll look at here is commonplace. The \"B\"\nstands for \"basis,\" which here just means \"component.\" B-splines build\nup wiggly functions from simpler less-wiggly components. Those\ncomponents are called basis functions. While there are fancier splines,\nwe want to start B-splines because they force you to make a number of\nchoices that other types of splines automate. You'll need to understand\nB-splines before you can understand fancier splines.\n\nTo see how B-splines work, we'll need an example that is much\nwigglier---that's a scientific term---than the !Kung stature data.\nCherry trees blossom all over Japan in the spring each year, and the\ntradition of flower viewing follows. The timing of the blossoms can vary\na lot by year and century. Let's load a thousand years of blossom dates:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-load-cherry-blossoms-data-a lst-cap=\"Load Cherry Blossoms data and display summary (rethinking version)\"}\n## R code 4.72 modified ######################\ndata(package = \"rethinking\", list = \"cherry_blossoms\")\nd4_a <- cherry_blossoms\nrethinking::precis(d4_a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>                   mean          sd      5.5%      94.5%       histogram\n#> year       1408.000000 350.8845964 867.77000 1948.23000   ▇▇▇▇▇▇▇▇▇▇▇▇▁\n#> doy         104.540508   6.4070362  94.43000  115.00000        ▁▂▅▇▇▃▁▁\n#> temp          6.141886   0.6636479   5.15000    7.29470        ▁▃▅▇▃▂▁▁\n#> temp_upper    7.185151   0.9929206   5.89765    8.90235 ▁▂▅▇▇▅▂▂▁▁▁▁▁▁▁\n#> temp_lower    5.098941   0.8503496   3.78765    6.37000 ▁▁▁▁▁▁▁▃▅▇▃▂▁▁▁\n```\n:::\n:::\n\n\nSee `?cherry_blossoms` for details and sources. We're going to work with\nthe historical record of the **d**ay **o**f **y**ear of first bloom,\n`doy`, for now. It ranges from 86 (late March) to 124 (early May). The\nyears with recorded blossom dates run from 812 CE to 2015 CE. You should\ngo ahead and plot `doy` against year to see. There might be some wiggly\ntrend in that cloud. It's hard to tell. (For the abbreviation CE see:\n[Common Era (CE) and Before Common Era\n(BCE)](https://www.timeanddate.com/calendar/ce-bce-what-do-they-mean.html))\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-scatterplot-cbl-a lst-cap=\"Display raw data for `doy` against the year: base R version\"}\nplot(doy ~ year, data = d4_a)\n```\n\n::: {.cell-output-display}\n![Display raw data for `doy` (Day of the year of first blossom) against the year: base R version](04-geocentric-models_files/figure-html/fig-scatterplot-cbl-a-1.png){#fig-scatterplot-cbl-a width=672}\n:::\n:::\n\n\nThere might be some wiggly trend in that cloud. It's hard to tell.\n\nLet's try extracting a trend with a <a class='glossary' title='The term “spline” refers to a wide class of basis functions that are used in applications requiring data interpolation and/or smoothing. Splines are special function defined piecewise by polynomials, it results to a smooth function built out of smaller, component functions. In interpolating problems, spline interpolation is often preferred to polynomial interpolation because it yields similar results, even when using low degree polynomials, while avoiding big numbers and the problem of oscillation at the edges of intervals of higher degrees. The term “spline” comes from the flexible spline devices used by shipbuilders and draftsmen to draw smooth shapes. They used a long, thin piece of wood or metal that could be anchored in a few places in order to aid drawing curves. | There are many types of splines, especially the common-place ‘B-splines’: The ‘B’ stands for ‘basis,’ which just means ‘component.’ B-splines build up wiggly functions from simpler less-wiggly components. Those components are called basis functions. B-splines force you to make a number of choices that other types of splines automate. (Chap.4)'>B-spline</a>. B-splines\ndivide the full range of some predictor variable, like `year`, into\nparts. Then they assign a parameter to each part. These parameters are\ngradually turned on and off in a way that makes their sum into a fancy,\nwiggly curve.\n\n***\n:::: {#prp-procedure-for-generating-b-splines}\nProcedure for generating B-splines\n\n::: callout-important\n\n\n1.  Choose the number and distribution of knots\n2.  Choose the polynomial degree\n3.  Get the parameter weights for each basis function (define the model\n    and make it run)\n:::\n::::\n***\n\n##### Choice of knots\n\nRemember, the knots are just values of year that serve as pivots for our\nspline. Where should the knots go? There are different ways to answer\nthis question. You can, in principle, put the knots wherever you like.\nTheir locations are part of the model, and you are responsible for them.\nLet's do what we did in the simple example above, place the knots at\ndifferent evenly-spaced quantiles of the predictor variable. This gives\nyou more knots where there are more observations. We used only 5 knots\nin the first example. Now let's go for 15:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-choose-knots-a lst-cap=\"Choose the knots that serve as pivots for the spline: rethinking version\"}\n## R code 4.73 ####################\nd5_a <- d4_a[complete.cases(d4_a$doy), ] # complete cases on doy\nnum_knots15_a <- 15\n(knot_list15_a <- quantile(d5_a$year, \n                           probs = seq(0, 1, length.out = num_knots15_a)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>        0% 7.142857% 14.28571% 21.42857% 28.57143% 35.71429% 42.85714%       50% \n#>       812      1036      1174      1269      1377      1454      1518      1583 \n#> 57.14286% 64.28571% 71.42857% 78.57143% 85.71429% 92.85714%      100% \n#>      1650      1714      1774      1833      1893      1956      2015\n```\n:::\n:::\n\n\n##### Choice of polynomial degree\n\nThe next choice is polynomial degree. This determines how basis\nfunctions combine, which determines how the parameters interact to\nproduce the spline. For degree 1, as in FIGURE 4.12, two basis functions\ncombine at each point. For degree 2, three functions combine at each\npoint. For degree 3, four combine. R already has a nice function that\nwill build basis functions for any list of knots and degree. This code\nwill construct the necessary basis functions for a degree 3 (cubic)\nspline:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-compute-b-spline-matrix-a lst-cap=\"Compute the B-spline basis matrix for a cubic spline (degree 3): rethinking version\"}\n## R code 4.74 #######################\nB_a <- splines::bs(d5_a$year,\n  knots = knot_list15_a[-c(1, num_knots15_a)],\n  degree = 3, intercept = TRUE\n)\n```\n:::\n\n\nThe B-spline basis matrix B_a should have 827 rows and 17 columns. Lets\ncheck it:\n\n-   Number of rows = 827\n-   Number of columns = 17\n\nEach row is a year, corresponding to the rows in the d5_a data frame.\nEach column is a basis function, one of our synthetic variables defining\na span of years within which a corresponding parameter will influence\nprediction.\n\nTo display the basis functions, just plot each column against year:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-b-spline-matrix-a lst-cap=\"B-spline basis matrix for a cubic spline (degree 3): rethinking version\"}\n## R code 4.75 #############################\nplot(NULL, xlim = range(d5_a$year), ylim = c(0, 1), xlab = \"year\", ylab = \"basis\")\nfor (i in 1:ncol(B_a)) lines(d5_a$year, B_a[, i])\n```\n\n::: {.cell-output-display}\n![B-spline basis matrix for a cubic spline (degree 3): rethinking version](04-geocentric-models_files/figure-html/fig-b-spline-matrix-a-1.png){#fig-b-spline-matrix-a width=672}\n:::\n:::\n\n\nThe plot shows, just like the top in FIGURE 4.12., the basis functions.\nHowever now more of these functions overlap.\n\n##### Parameter weights for each basis function\n\nNow to get the parameter weights for each basis function, we need to\nactually define the model and make it run. The model is just a linear\nregression. The synthetic basis functions do all the work. We'll use\neach column of the matrix `B_a` as a variable. We'll also have an\nintercept to capture the average blossom day. This will make it easier\nto define priors on the basis weights, because then we can just conceive\nof each as a deviation from the intercept.\n\nThis is also the first time we've used an\n<a class='glossary' title='The exponential distribution is often concerned with the amount of time until some specific event occurs. For example, the amount of time (beginning now) until an earthquake occurs has an exponential distribution. | Values for an exponential random variable occur in the following way: There are fewer large values and more small values. For example, the amount of money customers spend in one trip to the supermarket follows an exponential distribution. There are more people who spend small amounts of money and fewer people who spend large amounts of money. (LibreTexts Statistics)'>exponential distribution</a> as a prior. Exponential\ndistributions are useful priors for scale parameters, parameters that\nmust be positive. The prior for `σ` is exponential with a rate of 1. The\nway to read an exponential distribution is to think of it as containing\nno more information than an average deviation. That average is the\ninverse of the rate. So in this case it is $1/1 = 2$. If the rate were\n0.5, the mean would be$1/0.5 = 2$. We'll use exponential priors for the\nrest of the book, in place of uniform priors. It is much more common to\nhave a sense of the average deviation than of the maximum.\n\nTo build this model in `rethinking::quap()`, we just need a way to do\nthat sum. The easiest way is to use matrix multiplication.\n@sec-matrix-multiplication has more details about why this works. The\nonly other trick is to use a start list for the weights to tell\n`rethinking::quap()` how many there are.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fit-model-m4.7 lst-cap=\"Fit the model of B-spline basis matrix m4.7\"}\n## R code 4.76 ################################\nm4.7 <- rethinking::quap(\n  alist(\n    D ~ dnorm(mu, sigma),\n    mu <- a + B %*% w,\n    a ~ dnorm(100, 10),\n    w ~ dnorm(0, 10),\n    sigma ~ dexp(1)\n  ),\n  data = list(D = d5_a$doy, B = B_a),\n  start = list(w = rep(0, ncol(B_a)))\n)\n```\n:::\n\n\nA summary with `rethinking::precis` won't reveal much.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-summarize-model-m4.7 lst-cap=\"Summarize model m4.7\"}\nrethinking::precis(m4.7,depth = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>              mean        sd          5.5%       94.5%\n#> w[1]   -3.0453747 3.8612529  -9.216402543   3.1256532\n#> w[2]   -0.8185823 3.8702407  -7.003974475   5.3668098\n#> w[3]   -1.0897932 3.5850049  -6.819323486   4.6397371\n#> w[4]    4.8335852 2.8771531   0.235338802   9.4318315\n#> w[5]   -0.8644966 2.8743676  -5.458291221   3.7292981\n#> w[6]    4.3260429 2.9148874  -0.332510156   8.9845960\n#> w[7]   -5.3512317 2.8002406  -9.826557053  -0.8759064\n#> w[8]    7.8376220 2.8021032   3.359319965  12.3159241\n#> w[9]   -1.0260825 2.8810804  -5.630605493   3.5784405\n#> w[10]   3.0344462 2.9101387  -1.616517566   7.6854100\n#> w[11]   4.6223975 2.8917548   0.000814766   9.2439802\n#> w[12]  -0.1341159 2.8694507  -4.720052362   4.4518205\n#> w[13]   5.5098688 2.8874619   0.895147044  10.1245905\n#> w[14]   0.7377923 2.9993670  -4.055775445   5.5313600\n#> w[15]  -0.8731566 3.2935603  -6.136902110   4.3905890\n#> w[16]  -6.9245111 3.3758377 -12.319751724  -1.5292705\n#> w[17]  -7.7226695 3.2227957 -12.873319461  -2.5720196\n#> a     103.3647814 2.3697353  99.577486720 107.1520762\n#> sigma   5.8767936 0.1437643   5.647030472   6.1065568\n```\n:::\n:::\n\n\nYou should see 17 `w` parameters. But you can't tell what the model\nthinks from the parameter summaries. Instead we need to plot the\nposterior predictions. First, here are the weighted basis functions:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-weighted-basis-function lst-cap=\"Weighted basis functions\"}\n## R code 4.77 #################\npost5_a <- rethinking::extract.samples(m4.7)\nw <- apply(post5_a$w, 2, mean)\nplot(NULL,\n  xlim = range(d5_a$year), ylim = c(-6, 6),\n  xlab = \"year\", ylab = \"basis * weight\"\n)\nfor (i in 1:ncol(B_a)) lines(d5_a$year, w[i] * B_a[, i])\n```\n\n::: {.cell-output-display}\n![Weighted basis functions](04-geocentric-models_files/figure-html/fig-weighted-basis-function-1.png){#fig-weighted-basis-function width=672}\n:::\n:::\n\n\nAnd finally we will plot the 97% posterior interval for `μ`, at each\nyear:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-PI-cbl-a lst-cap=\"97% posterior interval for μ, at each year\"}\n## R code 4.78 ######################\nmu_cbl_a <- rethinking::link(m4.7)\nmu_PI_cbl_a <- apply(mu_cbl_a, 2, rethinking::PI, 0.97)\nplot(d5_a$year, d5_a$doy, \n     col = rethinking::col.alpha(rethinking::rangi2, 0.3), pch = 16)\nrethinking::shade(mu_PI_cbl_a, d5_a$year, \n                  col = rethinking::col.alpha(\"black\", 0.5))\n```\n\n::: {.cell-output-display}\n![97% posterior interval for μ, at each year](04-geocentric-models_files/figure-html/fig-PI-cbl-a-1.png){#fig-PI-cbl-a width=672}\n:::\n:::\n\n\nThe spline is much wigglier now. Something happened around 1500, for\nexample. If you add more knots, you can make this even wigglier. You\nmight wonder how many knots is correct. We'll be ready to (re)address\nthat question in a few more chapters.\n\nDistilling the trend across years provides a lot of information. But\nyear is not really a causal variable, only a proxy for features of each\nyear. In the practice problems below, you'll compare this trend to the\ntemperature record, in an attempt to explain those wiggles.\n\n##### Matrix multiplication in the spline model {#sec-matrix-multiplication}\n\nMatrix algebra is a new way to represent ordinary algebra. It is often\nmuch more compact. So to make model m4.7 easier to program, we used a\nmatrix multiplication of the basis matrix `B_a` by the vector of\nparameters w: `B %*% w`. This notation is just linear algebra shorthand\nfor (1) multiplying each element of the vector `w` by each value in the\ncorresponding row of `B_a` and then (2) summing up each result. You\ncould also fit the same model with the following less-elegant code:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fit-model-m4.7alt-a lst-cap=\"Fit model m4.7 with less-elegant code in matrix multiplication\"}\n## R code 4.79 ################################\nm4.7alt <- rethinking::quap(\n  alist(\n    D ~ dnorm(mu, sigma),\n    mu <- a + sapply(1:827, function(i) sum(B[i, ] * w)),\n    a ~ dnorm(100, 1),\n    w ~ dnorm(0, 10),\n    sigma ~ dexp(1)\n  ),\n  data = list(D = d5_a$doy, B = B_a),\n  start = list(w = rep(0, ncol(B_a)))\n)\nrethinking::precis(m4.7alt, depth = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>              mean        sd       5.5%        94.5%\n#> w[1]   -0.3905537 3.2864884 -5.6429970   4.86188961\n#> w[2]    2.0221955 3.1929667 -3.0807820   7.12517297\n#> w[3]    1.7280202 2.8739462 -2.8651009   6.32114123\n#> w[4]    7.6850522 1.8720299  4.6931870  10.67691753\n#> w[5]    1.9756245 1.8942848 -1.0518085   5.00305756\n#> w[6]    7.1448755 1.9490608  4.0298999  10.25985114\n#> w[7]   -2.4973425 1.7679000 -5.3227881   0.32810308\n#> w[8]   10.6699437 1.7722563  7.8375358  13.50235160\n#> w[9]    1.8184510 1.8978573 -1.2146916   4.85159361\n#> w[10]   5.8532999 1.9401979  2.7524890   8.95411079\n#> w[11]   7.4907580 1.9134166  4.4327486  10.54876735\n#> w[12]   2.6682864 1.8763608 -0.3305006   5.66707342\n#> w[13]   8.3829738 1.9095535  5.3311384  11.43480911\n#> w[14]   3.5413913 2.0645629  0.2418211   6.84096152\n#> w[15]   2.0076920 2.4897358 -1.9713867   5.98677070\n#> w[16]  -4.1447099 2.5832180 -8.2731913  -0.01622861\n#> w[17]  -4.9213291 2.4375734 -8.8170422  -1.02561601\n#> a     100.5104862 0.9252623 99.0317383 101.98923411\n#> sigma   5.8765766 0.1437509  5.6468349   6.10631823\n```\n:::\n:::\n\nBesides that @lst-fit-model-m4.7alt-a is not elegant you will notice that it need also more time to compile. Although all models are cached it seems that @lst-fit-model-m4.7alt-a needs more time to check that it is the same code.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-PI-cbl-a lst-cap=\"97% posterior interval for μ, at each year with matrix multiplication manually\"}\n## R code 4.78 ######################\nmu_cbl2_a <- rethinking::link(m4.7alt)\nmu_PI_cbl2_a <- apply(mu_cbl2_a, 2, rethinking::PI, 0.97)\nplot(d5_a$year, d5_a$doy, \n     col = rethinking::col.alpha(rethinking::rangi2, 0.3), pch = 16)\nrethinking::shade(mu_PI_cbl2_a, d5_a$year, \n                  col = rethinking::col.alpha(\"black\", 0.5))\n```\n\n::: {.cell-output-display}\n![97% posterior interval for μ, at each year: Alternate matrix multiplication](04-geocentric-models_files/figure-html/fig-PI-cbl2-a-1.png){#fig-PI-cbl2-a width=672}\n:::\n:::\n\n\n#### Smooth functions for a rough world\n\nThe splines in the previous section are just the beginning. A entire\nclass of models, <a class='glossary' title='A Generalized Additive Model or GAM is a generalized linear model in which the linear response variable depends linearly on unknown smooth functions of some predictor variables, and interest focuses on inference about these smooth functions. They can be interpreted as the discriminative generalization of the naive Bayes generative model. (Wikipedia). | GAMs relax the restriction that the relationship must be a simple weighted sum, and instead assume that the outcome can be modelled by a sum of arbitrary functions of each feature. (Medium member story) (Chap.4)'>generalized additive model</a>, focuses on\npredicting an outcome variable using smooth functions of some predictor\nvariables.\n\n***\n:::: {#prp-resources-for-gams}\nResources for GAMs\n\n::: callout-tip\n\n-   Wood, S. N. (2017). Generalized Additive Models: An Introduction\n    with R, Second Edition (2nd ed.). Taylor & Francis Inc.\n-   SemanticScholar: [Series of paper dedicated to\n    GAMs](https://www.semanticscholar.org/paper/Generalized-Additive-Models%3A-An-Introduction-with-R-G%C3%B3mez%E2%80%90Rubio/025f25133a5c1da746eb7e7719bb715b71a7f518)\n-   Anish Singh Walia: [Generalized Additive\n    Model](https://datascienceplus.com/generalized-additive-models/)\n-   Noam Ross: [GAMs in\n    R](https://noamross.github.io/gams-in-r-course/): A Free,\n    Interactive Course using `mgcv`\n-   Michael Clark: [Generalized Additive\n    Models](https://m-clark.github.io/generalized-additive-models/)\n-   Dheeraj Vaidya: [Generalized Additive\n    Model](https://www.wallstreetmojo.com/generalized-additive-model/)\n\n------------------------------------------------------------------------\n\n-   Adam Shaif: What is Gneralised Additive Model? ([Medium member\n    story](https://towardsdatascience.com/generalised-additive-models-6dfbedf1350a))\n-   Eugenio Anello: Generalized Additive Models with R ([Medium member\n    story](https://pub.towardsai.net/generalized-additive-models-with-r-5f01c8e52089))\n:::\n\n::::\n***\n\n## `TIDYVERSE`\n\n### Why normal distributions?\n\nI am not going to replicate the examples of @sec-why-normal-dist-a. They\ndo not help conceptually about the main issues why normal distributions\nare wide-spread. But I will mention even the empty sub-chapter to get a\nsymmetry in the table of content.\n\n#### EMPTY: Normal by addition\n\n#### EMPTY: Normal by multiplication\n\n#### EMPTY: Normal by log-multiplication\n\n#### EMPTY: Using Gaussian distributions\n\n### Model describing language\n\n#### From model definition to Bayes’ theorem\n\nWe can use grid approximation to work through our globe tossing model.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-model-bayes-b lst-cap=\"Calculate globe tossing model with syntax from Bayes theorem\"}\n# how many `p_grid` points would you like?\nn_points <- 100\n\nd_bayes_b <-\n  tibble(p_grid = seq(from = 0, to = 1, length.out = n_points),\n         w      = 6, \n         n      = 9) %>% \n  mutate(prior      = dunif(p_grid, min = 0, max = 1),\n         likelihood = dbinom(w, size = n, prob = p_grid)) %>%\n  mutate(posterior = likelihood * prior / sum(likelihood * prior))\n\nhead(d_bayes_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 6 × 6\n#>   p_grid     w     n prior likelihood posterior\n#>    <dbl> <dbl> <dbl> <dbl>      <dbl>     <dbl>\n#> 1 0          6     9     1   0         0       \n#> 2 0.0101     6     9     1   8.65e-11  8.74e-12\n#> 3 0.0202     6     9     1   5.37e- 9  5.43e-10\n#> 4 0.0303     6     9     1   5.93e- 8  5.99e- 9\n#> 5 0.0404     6     9     1   3.23e- 7  3.26e- 8\n#> 6 0.0505     6     9     1   1.19e- 6  1.21e- 7\n```\n:::\n:::\n\n\nIn case you were curious, here's what they look like.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-prior-likelihood-posterior-b lst-cap=\"Prior, likelihood and posterior distribution for globe tossing model calculated with syntax accoridng to Bayes theorem\"}\nd_bayes_b %>% \n  pivot_longer(prior:posterior) %>% \n  \n  # dictate the order in which the panels will appear\n  mutate(name = factor(name, levels = c(\"prior\", \"likelihood\", \"posterior\"))) %>% \n    \n  ggplot(aes(x = p_grid, y = value, fill = name)) +\n  geom_area() +\n  scale_fill_manual(values = c(\"blue\", \"red\", \"purple\")) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(legend.position = \"none\") +\n  facet_wrap(~ name, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![Prior, likelihood and posterior distribution for globe tossing model calculated with syntax accoridng to Bayes' theorem](04-geocentric-models_files/figure-html/fig-prior-likelihood-posterior-b-1.png){#fig-prior-likelihood-posterior-b width=672}\n:::\n:::\n\n\nThe posterior is a combination of the prior and the likelihood. When the\nprior is flat across the parameter space, the posterior is just the\nlikelihood re-expressed as a probability. As we go along, you'll see we\nalmost never use flat priors in practice. Be warned that eschewing flat\npriors is a recent development, however. You only have to look at the\nliterature from a couple decades ago to see mounds and mounds of flat\npriors.\n\n### Gaussian model of height\n\n#### The data\n\n##### Show the data\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-loading-data-from-package_b lst-cap=\"Load data `Howell1` from the {**rethinking**} package without loading the package and assign the data to an object. Tidyverse\"}\ndata(package = \"rethinking\", list = \"Howell1\")\nd_b <- Howell1\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd_b |>\n    glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Rows: 544\n#> Columns: 4\n#> $ height <dbl> 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149…\n#> $ weight <dbl> 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99259, 38.…\n#> $ age    <dbl> 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 54.0, 47.…\n#> $ male   <int> 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, …\n```\n:::\n:::\n\n\n`glimpse()` is the tidyverse analogue for `str()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_b |> \n    summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>      height           weight            age             male       \n#>  Min.   : 53.98   Min.   : 4.252   Min.   : 0.00   Min.   :0.0000  \n#>  1st Qu.:125.09   1st Qu.:22.008   1st Qu.:12.00   1st Qu.:0.0000  \n#>  Median :148.59   Median :40.058   Median :27.00   Median :0.0000  \n#>  Mean   :138.26   Mean   :35.611   Mean   :29.34   Mean   :0.4724  \n#>  3rd Qu.:157.48   3rd Qu.:47.209   3rd Qu.:43.00   3rd Qu.:1.0000  \n#>  Max.   :179.07   Max.   :62.993   Max.   :88.00   Max.   :1.0000\n```\n:::\n:::\n\n\nKurz tells us that the {**brms**} package does not have a function that\nworks like `rethinking::precis()` for providing numeric and graphical\nsummaries of variables, as in the second part of\n@lst-show-howell-data-a. Kurz suggests `base::summary()` to get some of\nthe information from `rethinking::precis()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_b |>            \n    skimr::skim() \n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |     |\n|:------------------------|:----|\n|Name                     |d_b  |\n|Number of rows           |544  |\n|Number of columns        |4    |\n|_______________________  |     |\n|Column type frequency:   |     |\n|numeric                  |4    |\n|________________________ |     |\n|Group variables          |None |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|    sd|    p0|    p25|    p50|    p75|   p100|hist  |\n|:-------------|---------:|-------------:|------:|-----:|-----:|------:|------:|------:|------:|:-----|\n|height        |         0|             1| 138.26| 27.60| 53.98| 125.10| 148.59| 157.48| 179.07|▁▂▂▇▇ |\n|weight        |         0|             1|  35.61| 14.72|  4.25|  22.01|  40.06|  47.21|  62.99|▃▂▃▇▂ |\n|age           |         0|             1|  29.34| 20.75|  0.00|  12.00|  27.00|  43.00|  88.00|▇▆▅▂▁ |\n|male          |         0|             1|   0.47|  0.50|  0.00|   0.00|   0.00|   1.00|   1.00|▇▁▁▁▇ |\n:::\n:::\n\n\nI think `skimr::skim()` is a better option as an alternative to\n`rethinking::precis()` as `base::summary()` because it also has a\ngraphical summary of the variables. {**skimr**} has many other useful\nfunctions and is very adaptable. I propose to install and to try it out.\n\n##### Select the height data of adults\n\nWith {**tidyverse**} we can isolate height values with the\n`dplyr::select()` function and we are using the `dplyr::filter()`\nfunction to make an adults-only data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-select-height-adults-b lst-cap=\"Select the height data of adults (individuals older or equal than 18 years): tidyverse version\"}\nd_b %>%\n  select(height) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Rows: 544\n#> Columns: 1\n#> $ height <dbl> 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149…\n```\n:::\n\n```{.r .cell-code #lst-select-height-adults-b lst-cap=\"Select the height data of adults (individuals older or equal than 18 years): tidyverse version\"}\nd2_b <- \n  d_b %>%\n  filter(age >= 18) \n \nglimpse(d2_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Rows: 352\n#> Columns: 4\n#> $ height <dbl> 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149…\n#> $ weight <dbl> 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99259, 38.…\n#> $ age    <dbl> 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 54.0, 47.…\n#> $ male   <int> 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, …\n```\n:::\n:::\n\n\nThe two functions of @lst-select-height-adults-b are much more readable\nand understandable as the weird base R syntax in\n@lst-select-height-adults-a.\n\n#### The model\n\n##### Plot the distribution of heights\n\nThe plot of the heights distribution compared with the standard Gaussian\ndistribution is missing in Kurz's version. I added this plot by using\nthe last example of [How to Plot a Normal Distribution in\nR](https://www.statology.org/plot-normal-distribution-r/). It uses the `ggplot2::stat_function()` to compute and draw a function as a continuous curve. This makes it easy to superimpose a function on top of an existing plot.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-dist-heights-b lst-cap=\"Plot the distribution of the heights of adults, overlaid by an ideal Gaussian distribution: tidyverse version\"}\np0 <- \n    d2_b |> \n    ggplot(aes(height)) +\n    geom_density() +\n\n    stat_function(\n        fun = dnorm,\n        args = with(d2_b, c(mean = mean(height), sd = sd(height)))\n        ) +\n    scale_x_continuous(\"Height in cm\")\n\np0\n```\n\n::: {.cell-output-display}\n![The distribution of the heights data, overlaid by an ideal Gaussian distribution: tidyverse version](04-geocentric-models_files/figure-html/fig-dist-heights-b-1.png){#fig-dist-heights-b width=672}\n:::\n:::\n\n\n##### Plot the mean prior (mu)\n\nHere is the shape for the prior $μ \\sim Normal(178, 20)$.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-mean-prior-b lst-cap=\"Plot of the chosen mean prior: tidyverse version\"}\np1 <-\n  tibble(x = seq(from = 100, to = 250, by = .1)) %>% \n    \n  ggplot(aes(x = x, y = dnorm(x, mean = 178, sd = 20))) +\n  geom_line() +\n  scale_x_continuous(breaks = seq(from = 100, to = 250, by = 25)) +\n  labs(title = \"mu ~ dnorm(178, 20)\",\n       y = \"density\")\n\np1\n```\n\n::: {.cell-output-display}\n![Plot of the chosen mean prior: tidyverse version](04-geocentric-models_files/figure-html/fig-mean-prior-b-1.png){#fig-mean-prior-b width=672}\n:::\n:::\n\n\nAs there is only one variable y (= `dnorm(x, mean = 178, sd = 20)`) we need to specify x as a sequence of 1501 points t provide a x and y aesthetic for the plot.\n\n\n##### Plot the prior of the standard deviation (sigma)\n\nAnd here's the ggplot2 code for our prior for `σ`, a uniform\ndistribution with a minimum value of 0 and a maximum value of 50. We\ndon't really need the `y`-axis when looking at the shapes of a density,\nso we'll just remove it with `scale_y_continuous()`.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-sd-prior-b lst-cap=\"Plot the chosen prior for the standard deviation: tidyverse version\"}\np2 <-\n  tibble(x = seq(from = -10, to = 60, by = .1)) %>%\n  \n  ggplot(aes(x = x, y = dunif(x, min = 0, max = 50))) +\n  geom_line() +\n  scale_x_continuous(breaks = c(0, 50)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  ggtitle(\"sigma ~ dunif(0, 50)\")\n\np2\n```\n\n::: {.cell-output-display}\n![Plot the chosen prior for the standard deviation: tidyverse version](04-geocentric-models_files/figure-html/fig-sd-prior-b-1.png){#fig-sd-prior-b width=672}\n:::\n:::\n\n\n##### Prior predictive simulation\n\n<a class='glossary' title='It is an essential part of modeling. Once you’ve chosen priors for all variables these priors imply a joint prior distribution. By simulating from this distribution, you can see what your choices imply about the observable variable. This helps to diagnose bad choices. Prior predictive simulation is therefore very useful for assigning sensible priors. (Chap.4)'>Prior predictive simulation</a> is very useful for assigning sensible priors, because it can be quite hard to anticipate how priors influence the observable variables. \n\nWe can simulate from both priors at once to get a prior probability\ndistribution of `height`.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-prior-predictive-sim-b lst-cap=\"Simulate heights by sampling from the prior: tidyverse version\"}\nn <- 1e4\nset.seed(4)\n\nsim <-\n  tibble(sample_mu_b    = rnorm(n, mean = 178, sd  = 20),\n         sample_sigma_b = runif(n, min = 0, max = 50)) %>% \n  mutate(height = rnorm(n, mean = sample_mu_b, sd = sample_sigma_b))\n  \np3 <- sim %>% \n  ggplot(aes(x = height)) +\n  geom_density(fill = \"deepskyblue\") +\n  scale_x_continuous(breaks = c(0, 73, 178, 283)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  ggtitle(\"height ~ dnorm(mu, sigma)\") +\n  theme(panel.grid = element_blank())\n\np3\n```\n\n::: {.cell-output-display}\n![Simulate heights by sampling from the prior: tidyverse version](04-geocentric-models_files/figure-html/fig-prior-predictive-sim-b-1.png){#fig-prior-predictive-sim-b width=672}\n:::\n:::\n\n\n`ggplot2::geom_density()` computes and draws kernel density estimates,  which is a smoothed version of the histogram. Note that there is no data mentioned explicitly in the call of `ggplot2::geom_density()`. When this is the case (data = `NULL`) then the data will be inherited from the plot data as specified in the call to `ggplot2::ggplot()`. Otherwise the function needs a data frame to override the plot data or a function with a single argument, the plot data. ([geom_density() help file](https://ggplot2.tidyverse.org/reference/geom_density.html)).\n\n\nIf you look at the `x`-axis breaks on the plot in McElreath's lower left\npanel in Figure 4.3, you'll notice they're intentional. To compute the\nmean and 3 standard deviations above and below, you might do this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim %>% \n  summarise(ll   = mean(height) - sd(height) * 3,\n            mean = mean(height),\n            ul   = mean(height) + sd(height) * 3) %>% \n  mutate_all(round, digits = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 1 × 3\n#>      ll  mean    ul\n#>   <dbl> <dbl> <dbl>\n#> 1  73.9  177.  281.\n```\n:::\n:::\n\n\nHere's the work to make the lower right panel of Figure 4.3.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-reproduce-4.3-low-right lst-cap=\"Reproduce lower right panels of Figure 4.3\"}\n# simulate\nset.seed(4)\n\nsim <-\n  tibble(sample_mu_b    = rnorm(n, mean = 178, sd = 100),\n         sample_sigma_b = runif(n, min = 0, max = 50)) %>% \n  mutate(height = rnorm(n, mean = sample_mu_b, sd = sample_sigma_b))\n\n# compute the values we'll use to break on our x axis\nbreaks <-\n  c(mean(sim$height) - 3 * sd(sim$height), 0, mean(sim$height), mean(sim$height) + 3 * sd(sim$height)) %>% \n  round(digits = 0)\n\n# this is just for aesthetics\ntext <-\n  tibble(height = 272 - 25,\n         y      = .0013,\n         label  = \"tallest man\",\n         angle  = 90)\n\n# plot\np4 <-\n  sim %>% \n  ggplot(aes(x = height)) +\n  geom_density(fill = \"deepskyblue\", color = \"black\") +\n  geom_vline(xintercept = 0, color = \"black\") +\n  geom_vline(xintercept = 272, color = \"black\", linetype = 3) +\n  geom_text(data = text,\n            aes(y = y, label = label, angle = angle),\n            color = \"black\") +\n  scale_x_continuous(breaks = breaks) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  ggtitle(\"height ~ dnorm(mu, sigma)\\nmu ~ dnorm(178, 100)\") +\n  theme(panel.grid = element_blank())\n\np4\n```\n\n::: {.cell-output-display}\n![Reproduce lower right panels of Figure 4.3](04-geocentric-models_files/figure-html/fig-reproduce-4.3-low-right-1.png){#fig-reproduce-4.3-low-right width=672}\n:::\n:::\n\n\nLet's combine the four to make our version of McElreath's Figure 4.3.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-reproduce-3.4 lst-cap=\"Reproduction of Figure 3.4\"}\n# library(patchwork) ## already in setup chunk\n(p1 + xlab(\"mu\") | p2 + xlab(\"sigma\")) / (p3 | p4)\n```\n\n::: {.cell-output-display}\n![Reproduction of Figure 3.4](04-geocentric-models_files/figure-html/fig-reproduce-3.4-1.png){#fig-reproduce-3.4 width=672}\n:::\n:::\n\n\nOn page 84, McElreath said his prior simulation indicated 4% of the\nheights would be below zero. He also drew the break down compared to the\ntallest man on record, [Robert Pershing\nWadlow](https://en.wikipedia.org/wiki/Robert_Wadlow) (1918--1940).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim %>% \n  count(height < 0) %>% \n  mutate(percent = 100 * n / sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 2 × 3\n#>   `height < 0`     n percent\n#>   <lgl>        <int>   <dbl>\n#> 1 FALSE         9571   95.7 \n#> 2 TRUE           429    4.29\n```\n:::\n\n```{.r .cell-code}\nsim %>% \n  count(height < 272) %>% \n  mutate(percent = 100 * n / sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 2 × 3\n#>   `height < 272`     n percent\n#>   <lgl>          <int>   <dbl>\n#> 1 FALSE           1761    17.6\n#> 2 TRUE            8239    82.4\n```\n:::\n:::\n\n\n#### Grid approximation of the posterior distribution\n\nWith grid approximation we are going to use the brute force method for\nthe calculation of the posterior distribution. This technique has\nlimited relevance. Later on we will use the quadratic approximation with\n`brms::brm()`.\n\nIt is the same technique we have used in\n@sec-sampling-from-a-grid-approximate-posterior respectively in the\ntidyverse version in @sec-grid-approximation-b. As there is no\nconceptually new information to learn, I am not going into the details\nof the following code. (It combines several code chunk from Kurz's\nversion.) But I am going to foreshadow the most important differences in\nthe tidyverse approach of the grid approximation technique:\n\nInstead of `base::grid_expand()` we will use `tidyr::crossing()` Instead\nof `base::sapply()` we will use `purr::map2()`\n\nThe produced tibble contains data frames in its cells, so that we have\nto use the `tidyr::unnest()` function to expand the list-column\ncontaining data frames into rows and columns.\n\nReferring to the plots:\n\n-   Instead of `rethinking::contour_xyz()` we will use\n    `ggplot2::geom_contour()`\n-   Instead of `rethinking::image_xyz()` we will use\n    `ggplot2::geom_raster()`\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-grid-approx-posterior-b lst-cap=\"Grid Approximation of the posterior distribution: tidyverse version\"}\nn <- 200\n\nd_grid_b <-\n  # we'll accomplish with `tidyr::crossing()` what McElreath did with base R `expand.grid()`\n  crossing(mu_b    = seq(from = 140, to = 160, length.out = n),\n           sigma_b = seq(from = 4, to = 9, length.out = n))\n\nglimpse(d_grid_b)\n\ngrid_function <- function(mu, sigma) {\n  \n  dnorm(d2_b$height, mean = mu, sd = sigma, log = TRUE) %>% \n    sum()\n  \n}\n\nd_grid2_b <-\n  d_grid_b %>% \n  mutate(log_likelihood_b = map2(mu_b, sigma_b, grid_function)) %>%\n  unnest(log_likelihood_b) %>% \n  mutate(prior_mu_b    = dnorm(mu_b, mean = 178, sd = 20, log = T),\n         prior_sigma_b = dunif(sigma_b, min = 0, max = 50, log = T)) %>% \n  mutate(product_b = log_likelihood_b + prior_mu_b + prior_sigma_b) %>% \n  mutate(probability_b = exp(product_b - max(product_b)))\n  \nhead(d_grid2_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Rows: 40,000\n#> Columns: 2\n#> $ mu_b    <dbl> 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 14…\n#> $ sigma_b <dbl> 4.000000, 4.025126, 4.050251, 4.075377, 4.100503, 4.125628, 4.…\n#> # A tibble: 6 × 7\n#>    mu_b sigma_b log_likelihood_b prior_mu_b prior_sigma_b product_b\n#>   <dbl>   <dbl>            <dbl>      <dbl>         <dbl>     <dbl>\n#> 1   140    4              -3813.      -5.72         -3.91    -3822.\n#> 2   140    4.03           -3778.      -5.72         -3.91    -3787.\n#> 3   140    4.05           -3743.      -5.72         -3.91    -3753.\n#> 4   140    4.08           -3709.      -5.72         -3.91    -3719.\n#> 5   140    4.10           -3676.      -5.72         -3.91    -3686.\n#> 6   140    4.13           -3644.      -5.72         -3.91    -3653.\n#> # ℹ 1 more variable: probability_b <dbl>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-contour-b lst-cap=\"Draw 2D contours of a 3D surface: tidyverse version\"}\nd_grid2_b %>% \n  ggplot(aes(x = mu_b, y = sigma_b, z = probability_b)) + \n  geom_contour() +\n  labs(x = expression(mu),\n       y = expression(sigma)) +\n  coord_cartesian(xlim = range(d_grid2_b$mu_b),\n                  ylim = range(d_grid2_b$sigma_b)) +\n  theme(panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![Draw 2D contours of a 3D surface](04-geocentric-models_files/figure-html/fig-contour-b-1.png){#fig-contour-b width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-heatmap-b lst-cap=\"Draw heat map: tidyverse version\"}\nd_grid2_b %>% \n  ggplot(aes(x = mu_b, y = sigma_b, fill = probability_b)) + \n  geom_raster(interpolate = TRUE) +\n  scale_fill_viridis_c(option = \"B\") +\n  labs(x = expression(mu),\n       y = expression(sigma)) +\n  theme(panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![Draw heat map](04-geocentric-models_files/figure-html/fig-heatmap-b-1.png){#fig-heatmap-b width=672}\n:::\n:::\n\n\n#### Sampling from the posterior\n\nWe can use `dplyr::sample_n()` to sample rows, with replacement, from\n`d_grid2_b`.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-posterior-sample-b lst-cap=\"Samples from the posterior distribution for the heights data\"}\nset.seed(4)\n\nd_grid_samples_b <- \n  d_grid2_b %>% \n  sample_n(size = 1e4, replace = T, weight = probability_b)\n\nd_grid_samples_b %>% \n  ggplot(aes(x = mu_b, y = sigma_b)) + \n  geom_point(size = .9, alpha = 1/15) +\n  scale_fill_viridis_c() +\n  labs(x = expression(mu[samples]),\n       y = expression(sigma[samples])) +\n  theme(panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![Samples from the posterior distribution for the heights data. The density of points is highest in the center, reflecting the most plausible combinations of μ and σ. There are many more ways for these parameter values to produce the data, conditional on the model. (tidyverse version)](04-geocentric-models_files/figure-html/fig-posterior-sample-b-1.png){#fig-posterior-sample-b width=672}\n:::\n:::\n\n\nWe can use `tidyr::pivot_longer()` and then `ggplot2::facet_wrap()` to\nplot the densities for both `mu` and `sigma` at once.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-densities-mu-sigma lst-cap=\"Plot shapes of the marginal posterior densities of μ and σ: tidyverse version\"}\nd_grid_samples_b %>% \n  pivot_longer(mu_b:sigma_b) %>% \n\n  ggplot(aes(x = value)) + \n  geom_density(fill = \"deepskyblue\", color = \"black\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free\", labeller = label_parsed)\n```\n\n::: {.cell-output-display}\n![Shapes of the marginal posterior densities of μ and σ: tidyverse version](04-geocentric-models_files/figure-html/fig-densities-mu-sigma-1.png){#fig-densities-mu-sigma width=672}\n:::\n:::\n\n\nWe'll use the {**tidybayes**} package to compute their posterior modes\nand 95% HDIs.\n\n::: callout-important\nThere is a companion package {**ggdist**} which is imported completely\nby {**tidybayes**}. Whenever you cannot find the function in\n{**tidybayes**} then look at the documentation of {**ggdist**}. This is\nalso the case for the `tidybayes::mode_hdi()` function. In the help\nfiles of {**tidybayes**} you will just find notes about a deprecated\n`tidybayes::mode_hdih()` function but not the arguments of its new\nversion without the last `h` (for horizontal) `tidybayes::mode_hdi()`.\nBut you can look up these details in the {**ggdist**} documentation.\nThis observation is valid for many families of deprecated functions.\n\nThere is a division of functionality between {**tidybayes**} and\n{**ggdist**}:\n\n-   {**tidybayes**}: Tidy Data and 'Geoms' for Bayesian Models: Compose\n    data for and extract, manipulate, and visualize posterior draws from\n    Bayesian models in a tidy data format. Functions are provided to\n    help extract tidy data frames of draws from Bayesian models and that\n    generate point summaries and intervals in a tidy format.\n-   {**ggdist**}: Visualizations of Distributions and Uncertainty:\n    Provides primitives for visualizing distributions using\n    {**ggplot2**} that are particularly tuned for visualizing\n    uncertainty in either a frequentist or Bayesian mode. Both\n    analytical distributions (such as frequentist confidence\n    distributions or Bayesian priors) and distributions represented as\n    samples (such as bootstrap distributions or Bayesian posterior\n    samples) are easily visualized.\n:::\n\n::: callout-todo\n###### TODO: ggdist \\<-\\> tidybayes {.unnumbered}\n\nReplace `tidyverse::` with `ggdist::` where appropriate. This is\nimportant when you want to find the related help file.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_grid_samples_b %>% \n  pivot_longer(mu_b:sigma_b) %>% \n  group_by(name) %>% \n  tidybayes::mode_hdi(value) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 2 × 7\n#>   name     value .lower .upper .width .point .interval\n#>   <chr>    <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n#> 1 mu_b    155.   154.   155.     0.95 mode   hdi      \n#> 2 sigma_b   7.82   7.19   8.35   0.95 mode   hdi\n```\n:::\n:::\n\n\nLet's say you wanted their posterior medians and 50% quantile-based\nintervals, instead. Just switch out the last line for\n`tidybayes::median_qi(value, .width = .5)`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_grid_samples_b %>% \n  pivot_longer(mu_b:sigma_b) %>% \n  group_by(name) %>% \n  tidybayes::median_qi(value, .width = .5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 2 × 7\n#>   name     value .lower .upper .width .point .interval\n#>   <chr>    <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n#> 1 mu_b    155.   154.   155.      0.5 median qi       \n#> 2 sigma_b   7.77   7.57   7.97    0.5 median qi\n```\n:::\n:::\n\n\n**Sample size and the normality of σ's posterior**\n\nI will skip this part as there is nothing conceptually new in this\nsection.\n\n#### Finding the posterior distribution with brm()\n\n> In the text, McElreath indexed his models with names like `m4.1`. I\n> will largely follow that convention, but will replace the *m* with a\n> *b* to stand for the **`brms`** package.\n\nHere's how to fit the first model for this chapter.\n\n::: callout-tip\n###### Code slightly changed\n\nI have worked my way through the `brms::brm()` help file. From its more than 40 (!) arguments I have concentrated my learning on those arguments, that are used explicitly by Kurz.\n\nBut to understand the syntax I had made some minor changes:\n\n1. Instead of `height ~ 1` I wrote the argument with it argument name `formula = height ~ 1\" and changed put it as first argument.\n2. Instead of `family = gaussian` I wrote `family = gaussian()`as in the help file mentioned.\n\nTo understand the syntax it is also quite revealing to look at @tbl-mirror-rethinking-tidyverse.\n\nTo learn more read [Defining statistical models; formulae](https://cran.r-project.org/doc/manuals/R-intro.html#Formulae-for-statistical-models) and Chapter 11 of [Introduction to R](https://cran.r-project.org/doc/manuals/R-intro.pdf).\n:::\n\nThe following table I have inserted here from Kurz's explanation of model `b4.3`. It is the first time that I used brms::brm() and therefore I need slow down here and go into the gritty details.\n\n> Unlike with McElreath's {**rethinking**} package, the conventional\n`brms::brm()` syntax doesn't mirror the statistical notation. But here\nare the analogues to the exposition at the bottom of page 97:\n\n| {**rethinking**} package                                    | {**brms**} package: `brms::brm()`          |\n|------------------------------------|------------------------------------|\n| $\\text{height}_i \\sim \\operatorname{Normal}(\\mu_i, \\sigma)$ | `family = gaussian`                        |\n| $\\mu_i = \\alpha + \\beta \\text{weight}_i$                    | `height ~ 1 + weight_c`                 |\n| $\\alpha \\sim \\operatorname{Normal}(178, 20)$                | `prior(normal(178, 20), class = Intercept` |\n| $\\beta \\sim \\operatorname{Log-Normal}(0, 1)$                | `prior(lognormal(0, 1), class = b)`        |\n| $\\sigma \\sim \\operatorname{Uniform}(0, 50)$                 | `prior(uniform(0, 50), class = sigma)`     |\n\n: Mirror the statistical notation of the rethinking package with the\ntidyverse approach using the brms package {#tbl-mirror-rethinking-tidyverse}\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-post-dist-brms-b4.1 lst-cap=\"Finding the posterior distribution with brms::brm()\"}\nb4.1 <- \n  brms::brm(\n      formula = height ~ 1,                                           # <1>\n      data = d2_b,                                                    # <2>\n      family = gaussian(),                                            # <3>\n      prior = c(brms::prior(normal(178, 20), class = Intercept),      # <4>\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)), # <4>\n      iter = 2000,               # <5>\n      warmup = 1000,             # <6>\n      chains = 4,                # <7>\n      cores = 4,                 # <8>  \n      seed = 4,                  # <9>\n      file = \"fits/b04.01\")      # <10>\n```\n:::\n\n\n1. **formula** describes the relation between dependent and independent variables in the form of a linear model. The left hand side are the dependent variables, the right hand side the independent. The independent variables are used to calculate the trend component of the linear model, the residuals are then assumed to have some kind of distribution. When the independent are equal to one `~ 1`, the trend component is a single value, e.g. the mean value of the data, i.e. the linear model only has an intercept. ([StackOverflow](https://stackoverflow.com/a/13366973/7322615)) In other words, it is the value the dependent variable is expected to have when the independent variables are zero or have no influence.  ([StackOverflow](https://stackoverflow.com/a/13367260/7322615)). The formula `y ~ 1` is just a model with a constant (intercept) and no regressor ([StackOverflow](https://stackoverflow.com/questions/53812741/tilde-operator-in-r)).\n2. **data**: A data frame that contains all the variables used in the model.\n3. **family**: A description of the response distribution and link function to be used in the model. This can be a family function, a call to a family function or a character string naming the family. By default a linear `gaussian` model is applied. So this line would not have been necessary. There are [standard family functions `stats::family()`](https://www.stat.ethz.ch/R-manual/R-devel/library/stats/html/family.html)that will work with {**brms**}, but there are also [special family functions `brms::brmsfamily()`](https://paul-buerkner.github.io/brms/reference/brmsfamily.html) that work only for {**brms**} models. Additionally you can [specify custom families](https://paul-buerkner.github.io/brms/reference/custom_family.html) for use in brms with the `brms::custom_family()` function.\n4. **prior**: The next two lines specify priors for the normal and the uniform distribution. As you can see this is another place where parts for the formula are provided for the `brms::brm()` function work. --- `class` specifies the parameter class. It defaults to \"b\" ((i.e. population-level -- 'fixed' -- effects)). (There is also the argument `group` for grouping of factors for group-level effects. Not used in this code example.) --- Besides the \"b\" class there are other classes for the \"Intercept\" and the standard deviation \"Sigma\" on the population level: There is also a \"sd\" class for the standard deviation of group-level effects. Finally there is the special case of `class = \"cor\"` to set the same prior on every correlation matrix. --- `ub = 50` sets the upper bound to 50. There is also a `lb` (lower bound). Both bounds are for parameter restriction, so that population-level effects must fall within a certain interval using the `lb` and `ub` arguments. `lb` and `ub` default to `NULL`, i.e. there is no restriction.\n5. **iter**: Number of total iterations per chain (including `warmup`; defaults to 2000).\n6. **warmup**: A positive integer specifying number of warmup iterations. This also specifies the number of iterations used for stepsize adaptation, so warmup draws should not be used for inference. The number of warmup should not be larger than iter and the default is iter/2.\n7. **chains**: Number of <a class='glossary' title='A Markov chain or Markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. Informally, this may be thought of as, “What happens next depends only on the state of affairs now.” (Wikipedia) For example, if you made a Markov chain model of a baby’s behavior, you might include “playing,” “eating”, “sleeping,” and “crying” as states, which together with other behaviors could form a ‘state space’: a list of all possible states. In addition, on top of the state space, a Markov chain tells you the probabilitiy of hopping, or “transitioning,” from one state to any other state—e.g., the chance that a baby currently playing will fall asleep in the next five minutes without crying first. (Explained visually)'>Markov chains</a> (defaults to 4). \n8. **cores**: Number of cores to use when executing the chains in parallel, which defaults to 1 but we recommend setting the `mc.cores` option to be as many processors as the hardware and RAM allow (up to the number of chains).\n9. **seed**: The seed for random number generation to make results reproducible. Kurz has always used for `set.seed()` in other code chunks the chapter number. If `NA` (the default), Stan will set the seed randomly.\n10. **file**: Either `NULL` or a character string. In the latter case, the fitted model object is saved via `base::saveRDS()` in a file named after the string supplied in file. The `.rds` extension is added automatically. If the file already exists, `brms::brm()` will load and return the saved model object instead of refitting the model. Unless you specify the `file_refit` argument as well, the existing files won't be overwritten, you have to manually remove the file in order to refit and save the model under an existing file name. The file name is stored in the brmsfit object for later usage.\n\nIf you want detailed diagnostics for the HMC chains, call\n`brms::launch_shinystan(b4.1)`. That'll keep you busy for a while. See\nthe [ShinyStan website](https://mc-stan.org/users/interfaces/shinystan)\nfor more information.\n\n***\n:::: {#prp-shinystan}\nLaunch of shinystan turned off\n\n::: callout-warning\n\nI turned off the evaluation of the following chunk that should launch {*shinystan*). Besides that the evaluation takes time\nI am not yet ready to understand the many configurable options programmed with a {**shiny**) interface. \n::::\n\n:::\n***\n\n\nA short investigation showed that there are several pages of documentation of different packages that I would need to study. These include:\n\n:::: {prp-stan-literature}\n\nPackage documentation of Stan and friends\n\n::: callout-tip\n\n- [Interface to shinystan](https://paul-buerkner.github.io/brms/reference/launch_shinystan.brmsfit.html) (`brms::launch_shinystan`)\n- I believe that it is also very important to understand [RStan: the R interface to Stan](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html).\n- Possibly I should read [Using the ShinyStan GUI with rstanarm models](https://mc-stan.org/rstanarm/reference/launch_shinystan.stanreg.html)\n- And maybe it could be also helpful to read selected chapters from the [rstanarm documentation](https://mc-stan.org/rstanarm/index.html), from the [{**rstan**} documentation](https://mc-stan.org/rstan/) or generally from [Stan User’s Guide](https://mc-stan.org/docs/stan-users-guide/index.html) resp. [Stan Language Reference Manual](https://mc-stan.org/docs/reference-manual/index.html. \n\nOoops, this opens up Pandora's box!\n\nI do not even understand completely what the different packages do. What follows is a first try where I copied from the documentation pages:\n\n> {**rstanarm**} is an R package that emulates other R model-fitting functions but uses Stan (via the {**rstan**} package) for the back-end estimation. The primary target audience is people who would be open to Bayesian inference if using Bayesian software were easier but would use frequentist software otherwise.\n\n> RStan is the R interface to Stan. It is distributed on CRAN as the {**rstan**} package and its source code is hosted on GitHub.\n\n> Stan is a state-of-the-art platform for statistical modeling and high-performance statistical computation.\n\n:::\n\n::::\n***\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-detailed-diganostic-chains-brms-b4.1 lst-cap=\"Detailed diagnostic using {**shinystan**}\"}\n# brms::launch_shinystan(b4.1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbrms:::print.brmsfit(b4.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ 1 \n#>    Data: d2_b (Number of observations: 352) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   154.60      0.41   153.81   155.42 1.00     2763     2635\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     7.77      0.29     7.21     8.39 1.00     3400     2561\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\nFor the interpretation of this output I am going to use the explication in the [How to use brms](https://github.com/paul-buerkner/brms#how-to-use-brms) section of the {**brms**} GitHup page.\n\n1. **Top**: On the top of the output, some general information on the model is given, such as family, formula, number of iterations and chains.\n2. **Upper Middle**: If the data were grouped the next part would display group-level effects separately for each grouping factor in terms of standard deviations and (in case of more than one group-level effect per grouping factor) correlations between group-level effects. (This part is absent above as there are no grouping factors.)\n3. **Lower Middle: here Middle**: Next follow the display of the population-level effects (i.e. regression coefficients). If incorporated, autocorrelation effects and family specific parameters (e.g., the residual standard deviation ‘sigma’ in normal models) are also given.\n    In general, every parameter is summarized using the mean (`Estimate`) and the standard deviation (`Est.Error`) of the posterior distribution as well as two-sided 95% credible intervals (`l-95% CI` and `u-95% CI`) based on quantiles. The last three values (`ESS_bulk`, `ESS_tail`, and `Rhat`) provide information on how well the algorithm could estimate the posterior distribution of this parameter. If `Rhat` is considerably greater than 1, the algorithm has not yet converged and it is necessary to run more iterations and / or set stronger priors.\n4. **Bottom**: The last part is some short explanation of the sampling procedure. <a class='glossary' title='The abbreviation “NUTS” stands for No U-Turn Sampler and is a Hamiltonian Monte Carlo (HMC) Method. This means that it is not a Markov Chain method and thus, this algorithm avoids the random walk part, which is often deemed as inefficient and slow to converge. Instead of doing the random walk, NUTS does jumps of length x. Each jump doubles as the algorithm continues to run. This happens until the trajectory reaches a point where it wants to return to the starting point. (CrossValidated) (Chap.4 in my notes)'>NUTS</a> stands for **No U-Turn Sampler** and is a Hamiltonian Monte Carlo (HMC) Method. This means that it is not a <a class='glossary' title='A Markov chain or Markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. Informally, this may be thought of as, “What happens next depends only on the state of affairs now.” (Wikipedia) For example, if you made a Markov chain model of a baby’s behavior, you might include “playing,” “eating”, “sleeping,” and “crying” as states, which together with other behaviors could form a ‘state space’: a list of all possible states. In addition, on top of the state space, a Markov chain tells you the probabilitiy of hopping, or “transitioning,” from one state to any other state—e.g., the chance that a baby currently playing will fall asleep in the next five minutes without crying first. (Explained visually)'>Markov Chain</a> method and thus, this algorithm avoids the random walk part, which is often deemed as inefficient and slow to converge.\n    Instead of doing the random walk, NUTS does jumps of length x. Each jump doubles as the algorithm continues to run. This happens until the trajectory reaches a point where it wants to return to the starting point.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb4.1$fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Inference for Stan model: anon_model.\n#> 4 chains, each with iter=2000; warmup=1000; thin=1; \n#> post-warmup draws per chain=1000, total post-warmup draws=4000.\n#> \n#>                 mean se_mean   sd     2.5%      25%      50%      75%    97.5%\n#> b_Intercept   154.60    0.01 0.41   153.81   154.31   154.59   154.88   155.42\n#> sigma           7.77    0.01 0.29     7.21     7.57     7.76     7.97     8.39\n#> lprior         -8.51    0.00 0.02    -8.56    -8.53    -8.51    -8.50    -8.46\n#> lp__        -1227.04    0.02 0.97 -1229.63 -1227.44 -1226.75 -1226.33 -1226.06\n#>             n_eff Rhat\n#> b_Intercept  2778    1\n#> sigma        3404    1\n#> lprior       2773    1\n#> lp__         1798    1\n#> \n#> Samples were drawn using NUTS(diag_e) at Sat Aug  5 13:01:11 2023.\n#> For each parameter, n_eff is a crude measure of effective sample size,\n#> and Rhat is the potential scale reduction factor on split chains (at \n#> convergence, Rhat=1).\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-plot-b4.1 lst-cap=\"Plot model b4.1\"}\nbrms:::plot.brmsfit(b4.1)\n```\n\n::: {.cell-output-display}\n![Plot model b4.1](04-geocentric-models_files/figure-html/fig-plot-b4.1-1.png){#fig-plot-b4.1 width=672}\n:::\n:::\n\n\nWhereas rethinking defaults to 89% intervals, using `print()` or\n`summary()` with {**brms**} models defaults to 95% intervals.\n\n::: callout-note\nAs I have learned shortly: `print()` or `summary()` are generic\nfunctions where one can add new printing methods with new classes. In\nthis case `class(b4.1)` = brmsfit. This means I do not need to\nadd `brms::` to secure that I will get the {**brms**} printing or\nsummary method as I didn't load the {**brms**} package. Quite the\ncontrary: Adding `brms::` would result into the message: \"Error:\n'summary' is not an exported object from 'namespace:brms'\".\n\nAs I really want to specify explicitly the method these generic\nfunctions should use, I need to use the syntax with *three* columns, like `brms:::print.brmsfit()` or `brms:::summary.brmsfit()` respectively.\n\nIn this respect I have to learn more about S3 classes. There are many\nimportant web resources about this subject that I have found with the\nsearch string \"r what is s3 class\". Maybe I should start with the [S3\nchapter in Advanced R](https://adv-r.hadley.nz/s3.html).\n:::\n\nUnless otherwise specified, Kurz will stick with 95% intervals\nthroughout. To get those 89% intervals or McElreath approach, one could\nuse the `prob` argument within `summary()` or `print()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrms:::summary.brmsfit(b4.1, prob = .89)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ 1 \n#>    Data: d2_b (Number of observations: 352) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   154.60      0.41   153.94   155.25 1.00     2763     2635\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     7.77      0.29     7.33     8.26 1.00     3400     2561\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\nHere's the `brms::brm()` code for the model with the very narrow `μ`\nprior corresponding to the `rethinking::quap()` code in\n@lst-post-dist-quap-m4.2.\n\n\n::: {.cell att-source='#lst-post-dist-brms-b4.2 lst-cap=\"Finding the posterior distribution with a narrower prior using brms::brm()\"'}\n\n```{.r .cell-code}\nb4.2 <- \n  brms::brm(data = d2_b, \n      family = gaussian,\n      height ~ 1,\n      prior = c(brms::prior(normal(178, 0.1), class = Intercept),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.02\")\n\nbrms:::plot.brmsfit(b4.2, widths = c(1, 2))\n```\n\n::: {.cell-output-display}\n![Finding the posterior distribution with a narrower prior using brms::brm()](04-geocentric-models_files/figure-html/fig-post-dist-brms-b4.2-1.png){#fig-post-dist-brms-b4.2 width=672}\n:::\n:::\n\n\nAnd here's the model `summary()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrms:::summary.brmsfit(b4.2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ 1 \n#>    Data: d2_b (Number of observations: 352) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   177.86      0.11   177.66   178.07 1.00     2721     2586\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma    24.60      0.91    22.87    26.47 1.00     3400     2729\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\nSubsetting the `summary()` output with `$fixed` provides a convenient\nway to compare the Intercept summaries between `b4.1` and `b4.2`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrbind(brms:::summary.brmsfit(b4.1)$fixed,\n      brms:::summary.brmsfit(b4.2)$fixed)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>            Estimate Est.Error l-95% CI u-95% CI     Rhat Bulk_ESS Tail_ESS\n#> Intercept  154.5959 0.4144632 153.8123 155.4221 1.000708 2762.982 2635.159\n#> Intercept1 177.8647 0.1052528 177.6577 178.0705 1.001224 2720.796 2586.491\n```\n:::\n:::\n\n\n#### Sampling from a ~~quap()~~ `brm()` fit\n\n{**brms**} doesn't seem to have a convenience function that works the\nway `vcov()` does for {**rethinking**}.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-calc-var-cov-m4.1-b lst-cap=\"Calculation of vcov(): tidyverse version.\"}\nbrms:::vcov.brmsfit(b4.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>           Intercept\n#> Intercept 0.1717797\n```\n:::\n:::\n\n\nThis only returns the first element in the matrix it did for\n{**rethinking**}. That is, it appears `brms::vcov()` only returns the\nvariance/covariance matrix for the single-level `_β_` parameters.\n\n::: callout-caution\n###### brms::vcov()\n\nReferring to a similar situation with `rethinking::vcov()` in\n@lst-vcov-matrix-m4.1 I cannot write `brms::vcov()`, but have to use\neither `brms:::vcov.brmsfit(b4.1)` or just `vcov(b4.1)`. The weird thing\nis that the first time it also works with `brms::vcov()` but only the\nfirst time!\n:::\n\nHowever, if you really wanted this information, you could get it after\nputting the Hamilton Monte Carlo (HMC) chains in a data frame. We do\nthat with the `brms::as_draws_df()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-put-hmc-into-df-b lst-cap=\"Extract the iteration of the Hamiliton Monte Carlo (HMC) chains into a data frame\"}\npost_b <- brms::as_draws_df(b4.1)\n\nhead(post_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A draws_df: 6 iterations, 1 chains, and 4 variables\n#>   b_Intercept sigma lprior  lp__\n#> 1         155   7.5   -8.5 -1227\n#> 2         155   7.0   -8.5 -1230\n#> 3         154   7.6   -8.5 -1226\n#> 4         154   8.0   -8.5 -1226\n#> 5         155   7.6   -8.5 -1226\n#> 6         155   7.4   -8.5 -1227\n#> # ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n```\n:::\n:::\n\n\n::: callout-tip\n###### draws object\n\nThe functions of the family `as_draws()` transform `brmsfit` objects to\n`draws` objects, a format supported by the {**posterior**} package.\n{**brms**} currently imports the family of `as_draws()`functions from the\n{**posterior**} package, a tool for working with posterior\ndistributions, i.e. for fitting Bayesian models or working with output from Bayesian models. (See as an introduction [The posterior R package](https://mc-stan.org/posterior/articles/posterior.html))\n\n@lst-put-hmc-into-df-b produced the {**brms**} version of what McElreath\nachieved with `extract.samples()` in @lst-extract-samples-m4.1-a.\nHowever, what happened under the hood was different. Whereas {**rethinking**} used the `mvnorm()` function from the {**MASS**} package with {**brms**} we just extracted the iterations of the <a class='glossary' title='Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) method that uses the derivatives of the density function being sampled to generate efficient transitions spanning the posterior. It uses an approximate Hamiltonian dynamics simulation based on numerical integration which is then corrected by performing a Metropolis acceptance step. (Stan Reference Manual)'>HMC</a> chains and put them in a data frame. \n\nIt’s also noteworthy that the `as_draws_df()` is part of a larger class of `as_draws()` functions {**brms**} currently imports from the {**posterior**} package. \n\n\n::: {.cell}\n\n```{.r .cell-code #lst-show-class-post-dist lst-cap=\"Show classes of the posterior distribution post_b, an object created by as_draws() function\"}\nclass(post_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] \"draws_df\"   \"draws\"      \"tbl_df\"     \"tbl\"        \"data.frame\"\n```\n:::\n:::\n\n\nBesides of class `tbl_df` and `tbl`, [subclasses of data.frame with different behavior](https://tibble.tidyverse.org/reference/tbl_df-class.html) the `as_draws_df()` function has created the `draws` class, the parent class of all supported [draws formats](https://mc-stan.org/posterior/articles/posterior.html#draws-formats). \n:::\n\nNow `select()` the columns containing the draws from the desired\nparameters `b_Intercept` and `sigma` and feed them into `cov()`.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-calc-cov-post-b lst-cap=\"Calculate the vector of variances and correlation matrix for b_Intercept and sigma\"}\nselect(post_b, b_Intercept:sigma) %>% \n  stats::cov()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n#>               b_Intercept         sigma\n#> b_Intercept  0.1717797087 -0.0005450089\n#> sigma       -0.0005450089  0.0868161627\n```\n:::\n:::\n\n\n@lst-calc-cov-post-b displays \"(1) a vector of variances for the\nparameters and (2) a correlation matrix\" for them (p. 90). Here are just\nthe variances (i.e., the diagonal elements) and the correlation matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-calc-var-post-b lst-cap=\"Calculate only variances (the diagonal values)\"}\nselect(post_b, b_Intercept:sigma) %>%\n  stats::cov() %>%\n  base::diag()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n#> b_Intercept       sigma \n#>  0.17177971  0.08681616\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code #lst-calc-corr-matrix-post-b lst-cap=\"Calculate only crrelation\"}\n# correlation\npost_b %>%\n  select(b_Intercept, sigma) %>%\n  stats::cor()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n#>              b_Intercept        sigma\n#> b_Intercept  1.000000000 -0.004462902\n#> sigma       -0.004462902  1.000000000\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(post_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> draws_df [4,000 × 7] (S3: draws_df/draws/tbl_df/tbl/data.frame)\n#>  $ b_Intercept: num [1:4000] 155 155 154 154 155 ...\n#>  $ sigma      : num [1:4000] 7.55 7.02 7.58 7.95 7.63 ...\n#>  $ lprior     : num [1:4000] -8.49 -8.49 -8.52 -8.52 -8.52 ...\n#>  $ lp__       : num [1:4000] -1227 -1230 -1226 -1226 -1226 ...\n#>  $ .chain     : int [1:4000] 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ .iteration : int [1:4000] 1 2 3 4 5 6 7 8 9 10 ...\n#>  $ .draw      : int [1:4000] 1 2 3 4 5 6 7 8 9 10 ...\n```\n:::\n:::\n\n\nThe `post_b` object is not just a data frame, but also of class\n`draws_df`, which means it contains three metadata variables ----\n`.chain`, `.iteration`, and `.draw` --- which are often hidden from\nview, but are there in the background when needed. As you'll see, we'll\nmake good use of the `.draw` variable in the future. Notice how our post\ndata frame also includes a vector named `lp__`. That's the log\nposterior.\n\nFor details, see: - The [Log-Posterior (function and\ngradient)](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html#the-log-posterior-function-and-gradient)\nsection of the Stan Development Team's (2023) vignette [RStan: the R\ninterface to\nStan](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html)\nand - Stephen Martin's [explanation of the log\nposterior](https://discourse.mc-stan.org/t/basic-question-what-is-lp-in-posterior-samples-of-a-brms-regression/17567/2)\non the Stan Forums.\n\n::: callout-caution\n###### Summaries of {brms} and {posterior} packages\n\nKurz claims that `summary()` function doesn't work for {**brms**}\nposterior data frames quite the way `rethinking::precis()` does for\nposterior data frames from the {**rethinking**} package. But I think his\nobservation is somewhat misleading.\n\nThe posterior data frame `post_b` is not of class `brms`. Let's check\nthis:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(post_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] \"draws_df\"   \"draws\"      \"tbl_df\"     \"tbl\"        \"data.frame\"\n```\n:::\n:::\n\n\nThe class `draws_df`and `draws` refers to the {**posterior**} and not to\nthe {**brms**} package. Remember: In @lst-put-hmc-into-df-b we\ntransformed with the function `as_draws_df` the `brms` object into a\n`draws_df` and `draws` object.\n\nTherefore Kurz's claim should be read: The `summary()` function doesn't\nwork for {**posterior**} posterior data frames quite the way\n`rethinking::precis()` does for posterior data frames from the\n{**rethinking**} package. Instead of calling `brms:::summary.brmsfit()`\nI will use `posterior:::summary.draws()`.\n\nI wouldn't have noticed this difference if I hadn't mentioned explicitly\nthe name of the packages in front of the function, because in that case\nR would have used `base::summary()` as in Kurz's text.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-base-summary-samples-b4.1-b lst-cap=\"Summary the extracted iterations of the HMC chains: base version\"}\nbase::summary(post_b[, 1:2])\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n#>   b_Intercept        sigma      \n#>  Min.   :153.3   Min.   :6.841  \n#>  1st Qu.:154.3   1st Qu.:7.567  \n#>  Median :154.6   Median :7.757  \n#>  Mean   :154.6   Mean   :7.771  \n#>  3rd Qu.:154.9   3rd Qu.:7.971  \n#>  Max.   :156.2   Max.   :8.864\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code #lst-posterior-summary-samples-b4.1-b lst-cap=\"Summary the extracted iterations of the HMC chains: posterior version\"}\nposterior:::summary.draws(post_b[, 1:2])\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 2 × 10\n#>   variable      mean median    sd   mad     q5    q95  rhat ess_bulk ess_tail\n#>   <chr>        <num>  <num> <num> <num>  <num>  <num> <num>    <num>    <num>\n#> 1 b_Intercept 155.   155.   0.414 0.421 154.   155.    1.00    2722.    2624.\n#> 2 sigma         7.77   7.76 0.295 0.301   7.31   8.27  1.00    3369.    2537.\n```\n:::\n:::\n\n\nTo get a similar summary with tiny histograms Kurz offers different\nsolutions:\n\n-   A base R approach by using the transpose of a `stats::quantile()`\n    call nested within `base::apply()`\n-   A {**tidyverse**} approach\n-   A {**brms**} approach by just putting the `brm()` fit object into\n    `posterior_summary()`\n-   A {**tidybayes**} approach using `tidybayes::mean_hdi()` if you're\n    willing to drop the posterior `sd` and\n-   Using additionally the [function\n    `histospark()`](https://github.com/hadley/precis/blob/master/R/histospark.R)\n    (from the unfinished {**precis**} package by Hadley Wickham supposed\n    to replace `base::summary()`) to get the tiny histograms and to add\n    them into the tidyverse approach.\n\nAdditionally I will propose using the {**skimr**} packages:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-skim-summary-samples-b4.1-b lst-cap=\"Summary the extracted iterations of the HMC chains: skimr version\"}\nskimr::skim(post_b[, 1:2])\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n```\n:::\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |              |\n|:------------------------|:-------------|\n|Name                     |post_b[, 1:2] |\n|Number of rows           |4000          |\n|Number of columns        |2             |\n|_______________________  |              |\n|Column type frequency:   |              |\n|numeric                  |2             |\n|________________________ |              |\n|Group variables          |None          |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|   sd|     p0|    p25|    p50|    p75|   p100|hist  |\n|:-------------|---------:|-------------:|------:|----:|------:|------:|------:|------:|------:|:-----|\n|b_Intercept   |         0|             1| 154.60| 0.41| 153.29| 154.31| 154.59| 154.88| 156.22|▁▆▇▂▁ |\n|sigma         |         0|             1|   7.77| 0.29|   6.84|   7.57|   7.76|   7.97|   8.86|▁▆▇▂▁ |\n:::\n:::\n\n\nKurz refers only shortly to both `overthinking` blocks of this section:\n\n-   Start values for `rethinking::quap()` resp. `brms::brm()` (See\n    @sec-start-values-rethinking): Within the `brm()` function, you use\n    the `init` argument fpr the start values.\n-   Under the hood with multivariate sampling (See\n    @sec-under-the-hood-multivariate-sampling-a): Again Kurz remarked\n    that `brms::as_draws_df()` is not the same as\n    `rethinking::extract.samples()`. What this exactly means will\n    (hopefully) explained later in @sec-chap09-markov-chain-monte-carlo.\n\n### Linear prediction\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-height-against-weight-b lst-cap=\"Scatterplot of adult height and weight\"}\nd2_b |> \n    ggplot(aes(height, weight)) + \n    geom_point()\n```\n\n::: {.cell-output-display}\n![Adult height and weight against one another](04-geocentric-models_files/figure-html/fig-height-against-weight-b-1.png){#fig-height-against-weight-b width=672}\n:::\n:::\n\n\n#### The linear model strategy\n\n##### EMPTY: Model definition\n\n##### EMPTY: Linear model\n\n##### Priors {#sec-priors-b}\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-sim-heights-only-with-priors-b lst-cap=\"Simulate heights from the model, using only the priors: tidyverse version\"}\nset.seed(2971)\n# how many lines would you like?\nn_lines <- 100\n\nlines <-\n  tibble(n = 1:n_lines,\n         a = rnorm(n_lines, mean = 178, sd = 20),\n         b = rnorm(n_lines, mean = 0, sd = 10)) %>% \n  expand_grid(weight = range(d2_b$weight)) %>% \n  mutate(height = a + b * (weight - mean(d2_b$weight)))\n\n\nlines %>% \n  ggplot(aes(x = weight, y = height, group = n)) +\n  geom_hline(yintercept = c(0, 272), linetype = 2:1, linewidth = 1/3) +\n  geom_line(alpha = 1/10) +\n  coord_cartesian(ylim = c(-100, 400)) +\n  ggtitle(\"b ~ dnorm(0, 10)\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![Simulating heights from the model, using only the priors: tidyverse version](04-geocentric-models_files/figure-html/fig-sim-heights-only-with-priors-b-1.png){#fig-sim-heights-only-with-priors-b width=672}\n:::\n:::\n\n\nUsing the Log-Normal distribution prohibits negative values. This is an\nimportant constraint for height and weight as these variables cannot be\nunder 0.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(4)\n\ntibble(b = rlnorm(1e4, meanlog = 0, sdlog = 1)) %>% \n  ggplot(aes(x = b)) +\n  geom_density(fill = \"grey92\") +\n  coord_cartesian(xlim = c(0, 5)) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![Log-Normal distribution: tidyverse version](04-geocentric-models_files/figure-html/fig-log-normal-b-1.png){#fig-log-normal-b width=672}\n:::\n:::\n\n\n::: callout-note\nKurz wrote just `mean` and `sd` instead of `meanlog` and `sdlog.` These shorter argument names work because of the [partial matching feature in argument evaluation](https://cran.r-project.org/doc/manuals/R-lang.html#Argument-matching) of R functions. But for educational reason (misunderstanding, clashing with other matching arguments and less readable code) I apply this technique only sometimes in interactive use.\n:::\n\nI am not very skilled with the Log-Normal distribution, and so I am\nhappy that Kurz added some explanations:\n\n> If you're unfamiliar with the log-normal distribution, it is the\n> distribution whose logarithm is normally distributed. For example,\n> here's what happens when we compare Normal(0,1) with\n> log(Log-Normal(0,1)).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(4)\n\ntibble(rnorm           = rnorm(1e5, mean = 0, sd = 1),\n       `log(rlognorm)` = log(rlnorm(1e5, meanlog = 0, sdlog = 1))) %>% \n  pivot_longer(everything()) %>% \n\n  ggplot(aes(x = value)) +\n  geom_density(fill = \"grey92\") +\n  coord_cartesian(xlim = c(-3, 3)) +\n  theme_classic() +\n  facet_wrap(~ name, nrow = 2)\n```\n\n::: {.cell-output-display}\n![Compare Normal(0,1) with log(Log-Normal(0,1))](04-geocentric-models_files/figure-html/fig-normal-log-normal-1.png){#fig-normal-log-normal width=672}\n:::\n:::\n\n\n> Those values are ~~what~~ the mean and standard deviation of the\n> output from the `rlnorm()` function **after** they are log\n> transformed. The formulas for the actual mean and standard deviation\n> for the log-normal distribution itself are complicated (see\n> [Wikipedia](https://en.wikipedia.org/wiki/Log-normal_distribution)).\n\n------------------------------------------------------------------------\n\n::: {#def-mean-sd}\nCalculate mean and standard deviation\n\n$$\n\\begin{align*}\n\\text{mean}               & = \\exp \\left (\\mu + \\frac{\\sigma^2}{2} \\right) \\\\\n\\text{standard deviation} & = \\sqrt{[\\exp(\\sigma ^{2})-1] \\; \\exp(2\\mu +\\sigma ^{2})}\n\\end{align*}\n$$ {#eq-formula-mean-sd}\n:::\n\n------------------------------------------------------------------------\n\nLet's try our hand at those formulas and compute the mean and standard\ndeviation for Log-Normal(0,1):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu    <- 0\nsigma <- 1\n\n# mean\nexp(mu + (sigma^2) / 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 1.648721\n```\n:::\n\n```{.r .cell-code}\n# sd\nsqrt((exp(sigma^2) - 1) * exp(2 * mu + sigma^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 2.161197\n```\n:::\n:::\n\n\nLet's confirm with simulated draws from `rlnorm()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(4)\n\ntibble(x = rlnorm(1e7, meanlog = 0, sdlog = 1)) %>% \n  summarise(mean = mean(x),\n            sd   = sd(x))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 1 × 2\n#>    mean    sd\n#>   <dbl> <dbl>\n#> 1  1.65  2.17\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# make a tibble to annotate the plot\ntext <-\n  tibble(weight = c(34, 43),\n         height = c(0 - 25, 272 + 25),\n         label  = c(\"Embryo\", \"World's tallest person (272 cm)\"))\n\n# simulate\nset.seed(2971)\n\ntibble(n = 1:n_lines,\n       a = rnorm(n_lines, mean = 178, sd = 20),\n       b = rlnorm(n_lines, mean = 0, sd = 1)) %>% \n  expand_grid(weight = range(d2_b$weight)) %>% \n  mutate(height = a + b * (weight - mean(d2_b$weight))) %>%\n  \n  # plot\n  ggplot(aes(x = weight, y = height, group = n)) +\n  geom_hline(yintercept = c(0, 272), linetype = 2:1, linewidth = 1/3) +\n  geom_line(alpha = 1/10) +\n  geom_text(data = text,\n            aes(label = label),\n            size = 3) +\n  coord_cartesian(ylim = c(-100, 400)) +\n  ggtitle(\"log(b) ~ dnorm(0, 1)\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![Prior predictive simulation again, now with the Log-Normal prior: tidyverse version](04-geocentric-models_files/figure-html/fig-prior-pred-sim-b-1.png){#fig-prior-pred-sim-b width=672}\n:::\n:::\n\n***\n:::: {#prp-p-value-problem}\nTorturing data until they fit your expectations / desires\n\n::: callout-tip\nThe paper by Simmons, Nelson and Simonsohn (2011), [False-positive\npsychology: Undisclosed flexibility in data collection and analysis\nallows presenting anything as\nsignificant](https://journals.sagepub.com/doi/10.1177/0956797611417632),\nis often cited as an introduction to the problem.\n:::\n::::\n***\n\n#### Finding the posterior distribution\n\nUnlike with McElreath's `rethinking::quap()` formula syntax, Kurz is not\naware if we can just specify something like `weight – xbar` in the\n`formula` argument in `brms::brm()`.\n\nHowever, the alternative is easy: Just make a new variable in the data\nthat is equivalent to `weight – mean(weight)`. We'll call it `weight_c`.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-create-weight-diff-var-b lst-cap=\"Create a new variable in the data equivalent to weight - mean(height): tidyverse version\"}\nd2_b <-\n  d2_b %>% \n  mutate(weight_c = weight - mean(weight))\n```\n:::\n\n\nRemember: The detailed explication of the syntax for the following `brms::brm()` function are in @tbl-mirror-rethinking-tidyverse and @lst-post-dist-brms-b4.1.\n\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-find-and-summarize-post-dist-b lst-cap=\"Find the posterior distribution of the linear height-weight model: tidyverse version\"}\nb4.3 <- \n  brms::brm(data = d2_b, \n      family = gaussian,\n      height ~ 1 + weight_c,\n      prior = c(brms::prior(normal(178, 20), class = Intercept),\n                brms::prior(lognormal(0, 1), class = b, lb = 0),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.03\")\n\n# brms:::summary.brmsfit(b4.3)\n```\n:::\n\n\nHere are the trace plots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(b4.3, widths = c(1, 2))\n```\n\n::: {.cell-output-display}\n![Find the posterior distribution of the linear height-weight model: tidyverse version](04-geocentric-models_files/figure-html/fig-find-post-dist-b-1.png){#fig-find-post-dist-b width=672}\n:::\n:::\n\n\n{**brms**} does not allow users to insert coefficients into functions\nlike exp() within the conventional `formula` syntax. We can fit a\n{**brms**} model like McElreath's `m4.3b` if we adopt what's called the\n[non-linear\nsyntax](https://cran.r-project.org/web/packages/brms/vignettes/brms_nonlinear.html).\nThe non-linear syntax is a lot like the syntax McElreath uses in\n{**rethinking**} in that it typically includes both predictor and\nvariable names in the `formula`. Since this is so early in the book, I\nwon't go into a full-blown explanation, here. There will be many more\nopportunities to practice with the non-linear syntax in the chapters to\ncome.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-find-post-dist2-b lst-cap=\"Find the posterior distribution of the linear height-weight model (log version): tidyverse version\"}\nb4.3b <- \n  brms::brm(data = d2_b, \n      family = gaussian,\n      brms::bf(height ~ a + exp(lb) * weight_c,\n         a ~ 1,\n         lb ~ 1,\n         nl = TRUE),\n      prior = c(brms::prior(normal(178, 20), class = b, nlpar = a),\n                brms::prior(normal(0, 1), class = b, nlpar = lb),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.03b\")\n```\n:::\n\n\nIf you execute `summary(b4.3b)`, you'll see the intercept and `σ`\nsummaries for this model are about the same as those for `b4.3`, above.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-summarize-post-dist2-b lst-cap=\"Summarize the posterior distribution of the linear height-weight model (log version): tidyverse version\"}\nbrms:::summary.brmsfit(b4.3b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ a + exp(lb) * weight_c \n#>          a ~ 1\n#>          lb ~ 1\n#>    Data: d2_b (Number of observations: 352) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> a_Intercept    154.61      0.27   154.08   155.12 1.00     3524     2646\n#> lb_Intercept    -0.10      0.05    -0.20    -0.01 1.00     4412     2848\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     5.11      0.19     4.75     5.51 1.00     3697     2629\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\nThe difference is for the β parameter, which we called `lb` in the\n`b4.3b` model. If we term that parameter from `b4.3` as $\\beta^{b4.3}$\nand the one from our new model $\\beta^{b4.3b}$, it turns out that\n$\\beta^{b4.3} = exp(\\beta^{b4.3b})$.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-extract-foxed-effects-b lst-cap=\"Extract and compare the population-level (fixed) effects from object b4.3 and b4.3b\"}\nbrms::fixef(b4.3)[\"weight_c\", \"Estimate\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 0.9031995\n```\n:::\n\n```{.r .cell-code #lst-extract-foxed-effects-b lst-cap=\"Extract and compare the population-level (fixed) effects from object b4.3 and b4.3b\"}\nbrms::fixef(b4.3b)[\"lb_Intercept\", \"Estimate\"] %>% exp()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 0.9037418\n```\n:::\n:::\n\n\nThey're the same within simulation variance.\n\n#### Interpreting the posterior distribution\n\n##### Tables of marginal distribution\n\nWith a little `[]` subsetting we can exclude the log posterior from our\nsummary for `b4.3`.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-table-summary-b4.3 lst-cap=\"Display the marginal posterior distributions of the parameters: brms version\"}\nbrms::posterior_summary(b4.3)[1:3, ] %>% \n  round(digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>             Estimate Est.Error   Q2.5  Q97.5\n#> b_Intercept   154.61      0.27 154.09 155.14\n#> b_weight_c      0.90      0.04   0.82   0.99\n#> sigma           5.11      0.20   4.72   5.51\n```\n:::\n:::\n\n\nWithout the subsetting we will get 2 more lines:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-table-summary2-b4.3 lst-cap=\"Display the marginal posterior distributions of the parameters: brms version\"}\nbrms::posterior_summary(b4.3) %>% \n  round(digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>             Estimate Est.Error     Q2.5    Q97.5\n#> b_Intercept   154.61      0.27   154.09   155.14\n#> b_weight_c      0.90      0.04     0.82     0.99\n#> sigma           5.11      0.20     4.72     5.51\n#> lprior         -9.33      0.05    -9.42    -9.25\n#> lp__        -1080.46      1.28 -1083.84 -1079.03\n```\n:::\n:::\n\n\n::: callout-note\n###### TODO: Interpret printout {.unnumbered}\n\n-   `b_Intercept` represents `a` in the rethinking version.\n-   `b_weight_c` represents `b` in the rethinking version. But why did\n    we have to calculate it different?\n-   `sigma` is the quadratic approximation of the standard deviation.\n-   `lprior` is -- I assume -- the log prior.\n-   `l__` is what??\n:::\n\nLooking up `brms::posterior_summary()` I learned that the \"function\nmainly exists to retain backwards compatibility. It will eventually be\nreplaced by functions of the {**posterior**} package\".\n\nOne of the following examples suggests to convert the `brmsfit` object\ninto a `draws` object and then to use `posterior::summarise_draws()`.\nBut reading the help file of `posterior::summarise_draws()` it turned\nout that it \"will convert an object to a draws object if it isn't\nalready\".\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-table-summary3-b4.3 lst-cap=\"Display the marginal posterior distributions of the parameters: posterior version\"}\nposterior::summarize_draws(b4.3, posterior::default_summary_measures(), \n                           .num_args = list(sigfig = 2))[1:3, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 3 × 7\n#>   variable       mean  median      sd     mad      q5     q95\n#>   <chr>       <num:2> <num:2> <num:2> <num:2> <num:2> <num:2>\n#> 1 b_Intercept  155.    155.     0.27    0.28   154.    155.  \n#> 2 b_weight_c     0.90    0.90   0.042   0.041    0.83    0.97\n#> 3 sigma          5.1     5.1    0.20    0.19     4.8     5.4\n```\n:::\n:::\n\n\n@lst-table-summary3-b4.3 is the default call using\n`posterior::default_summary_measures()`. This returns the default\nmeasures. But we can adapt this standard summary to get a very similar\nresult a in @lst-table-summary-m4.3.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-table-summary4-b4.3 lst-cap=\"Display the marginal posterior distributions of the parameters: posterior version\"}\nposterior::summarize_draws(b4.3, \"mean\", \"median\", \"sd\", \n                           ~quantile(., probs = c(0.055, 0.945)),\n                           .num_args = list(sigfig = 2))[1:3, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 3 × 6\n#>   variable       mean  median      sd  `5.5%` `94.5%`\n#>   <chr>       <num:2> <num:2> <num:2> <num:2> <num:2>\n#> 1 b_Intercept  155.    155.     0.27   154.    155.  \n#> 2 b_weight_c     0.90    0.90   0.042    0.84    0.97\n#> 3 sigma          5.1     5.1    0.20     4.8     5.4\n```\n:::\n:::\n\n\nBut don't forget that there exists also `summary()` as an alias either\nfor `brmsfit` or `draws` objects which can be used for a standardized\noutput:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-table-summary5-b4.3 lst-cap=\"Display and compare the marginal posterior distributions of the parameters of the brms and the posterior version\"}\n## summary for brmsfit object\nsummary(b4.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ 1 + weight_c \n#>    Data: d2_b (Number of observations: 352) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   154.61      0.27   154.09   155.14 1.00     4546     2895\n#> weight_c      0.90      0.04     0.82     0.99 1.00     4278     2978\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     5.11      0.20     4.72     5.51 1.00     4439     2773\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n```{.r .cell-code #lst-table-summary5-b4.3 lst-cap=\"Display and compare the marginal posterior distributions of the parameters of the brms and the posterior version\"}\n## summary for draws object\nsummary(brms::as_draws_array(b4.3))[1:3, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 3 × 10\n#>   variable    mean  median     sd    mad      q5     q95  rhat ess_bulk ess_tail\n#>   <chr>      <num>   <num>  <num>  <num>   <num>   <num> <num>    <num>    <num>\n#> 1 b_Inter… 155.    155.    0.272  0.275  154.    155.     1.00    4546.    2895.\n#> 2 b_weigh…   0.903   0.903 0.0424 0.0414   0.834   0.973  1.00    4278.    2978.\n#> 3 sigma      5.11    5.10  0.198  0.190    4.79    5.44   1.00    4439.    2773.\n```\n:::\n:::\n\n\nIf we put our {**brms**} fit into the `brms:::vcov.brmsfit()` function,\nwe'll get the variance/covariance matrix of the `intercept` and\n`weight_c` coefficient.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-var-cov-matrix-b4.3 lst-cap=\"Calculate the variance-covariance matrix for model b4.3\"}\nbrms:::vcov.brmsfit(b4.3) %>% \n  round(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>           Intercept weight_c\n#> Intercept     0.074    0.000\n#> weight_c      0.000    0.002\n```\n:::\n:::\n\n\nNo `σ`, however. To get that, we'll have to extract the posterior draws\nand use the `cov()` function, instead.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-var-cov-matrix2-b4.3 lst-cap=\"Calculate the variance-covariance matrix for model b4.3 with {posterior} draws objects\"}\nbrms::as_draws_df(b4.3) %>%\n  select(b_Intercept:sigma) %>%\n  cov() %>%\n  round(digits = 3)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n#>             b_Intercept b_weight_c sigma\n#> b_Intercept       0.074      0.000 0.000\n#> b_weight_c        0.000      0.002 0.000\n#> sigma             0.000      0.000 0.039\n```\n:::\n:::\n\n\nThe `pairs()` function will work for a brms fit much like it would one\nfrom rethinking. It will show \"both the marginal posteriors and the\ncovariance\".\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-vfig-marg-post-cov-b4.3 lst-cap=\"Show the marginal posteriors and covariance matrix for model m4.3\"}\nbrms:::pairs.brmsfit(b4.3)\n```\n\n::: {.cell-output-display}\n![The marginal posteriors and the covariance matrix for model b4.3](04-geocentric-models_files/figure-html/fig-marg-post-cov-b4.3-1.png){#fig-marg-post-cov-b4.3 width=672}\n:::\n:::\n\n\n##### Plotting posterior inference around the mean\n\nHere is the code for Figure 4.6.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd2_b %>%\n  ggplot(aes(x = weight_c, y = height)) +\n  geom_abline(intercept = brms:::fixef.brmsfit(b4.3)[1], \n              slope     = brms:::fixef.brmsfit(b4.3)[2]) +\n  geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![Height in centimeters (vertical) plotted against weight_c (horizontal), with the line at the posterior mean plotted in black: tidyverse version](04-geocentric-models_files/figure-html/fig-raw-data-line-b4.3-1.png){#fig-raw-data-line-b4.3 width=672}\n:::\n:::\n\n\nNote how the breaks on our `x`-axis look off. That's because we fit the\nmodel with `weight_c` and we plotted the points in that metric, too.\nSince we computed `weight_c` by subtracting the mean of weight from the\ndata, we can adjust the `x`-axis break point labels by simply adding\nthat value back.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-raw-data-line2-b4.3 lst-cap=\"Reproducing Figure 4.6 of SR2 with adjusted x-axis\"}\nlabels <-\n  c(-10, 0, 10) + mean(d2_b$weight) %>% \n  round(digits = 0)\n\nd2_b %>%\n  ggplot(aes(x = weight_c, y = height)) +\n  geom_abline(intercept = brms::fixef(b4.3, probs = c(0.055, 0.945))[[1]], \n              slope     = brms::fixef(b4.3, probs = c(0.055, 0.945))[[2]]) +\n  geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  scale_x_continuous(\"weight\",\n                     breaks = c(-10, 0, 10),\n                     labels = labels) +\n  theme_bw() +\n  theme(panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![Height in centimeters (vertical) plotted against weight in kilograms (horizontal), with the line at the posterior mean plotted in black: tidyverse version](04-geocentric-models_files/figure-html/fig-raw-data-line2-b4.3-1.png){#fig-raw-data-line2-b4.3 width=672}\n:::\n:::\n\n\nFurthermore note the use of the `brms:::fixef.brmsfit()` function within\n`ggplot2::geom_abline()`. The function extracts the population-level\n('fixed') effects from a `brmsfit` object. \n\n::: callout-note\n###### Extract population-level (\"fixed\") effects {#sec-fixed-effects}\n\nI do not know what it means exactly that `brms::fixef()` extracts the population-level ('fixed') effect from a `brmsfit` object. After I googled it turned out that there is a great discussion about [Fixed Effects in Linear Regression](https://statisticsglobe.com/fixed-effects-linear-regression) and generally about fixed, random and mixec models (See Cross Validated [here](https://stats.stackexchange.com/questions/4700/what-is-the-difference-between-fixed-effect-random-effect-and-mixed-effect-mode) and [here](https://stats.stackexchange.com/questions/21760/what-is-a-difference-between-random-effects-fixed-effects-and-marginal-model)).\n\nAt the moment I do not understand background and essence of this differentiation. What follows is a quote and summary of short glossary entry in [statistics.com](https://www.statistics.com/glossary/fixed-effects/).\n\n> The term “fixed effects” (as contrasted with “random effects”) is related to how particular coefficients in a model are treated – as fixed or random values. Which approach to choose depends on both the nature of the data and the objective of the study. A fixed effect approach can be used for both random and non-random samples. Random effect models are usually applied only to random samples. \n\nFollowing the rest of the glossary entry in statistics.com:\n\nSuppose the data at hand are values of the annual income of 100 school teachers – T1: $N_{1}=30$ males and T2: $N_{2}=70$ females.\n\n1. Suppose the 100 individuals has been drawn randomly from a population, for example, from all school teachers of New York.\n    a. If the question of interest is the average income of New York school teachers, then the random effects approach is reasonable. We treat $T_{i}$ as values of a random variable taking on two values – $T_{1}$ and $T_{2}$ . For example, we simply use the mean of the 100 values as an estimate of the average income of New York teachers.\n    b. If the question of interest is the average income of female and male teachers separately, then we treat $T_{1}$ and $T_{2}$ as two fixed values.\n2. Suppose a researcher decided to pick up $N_{1}=30$ male teachers randomly from all male teachers of New York, and $N_{2}=70$ female teachers from all female teachers. In this case, only the fixed effect approach is reasonable – because the $N_{1}$ values $T_{1}$ and the $N_{2}$ values of $T_{1}$ in the sample of 100 have not been drawn randomly from the population of interest.\n\nThere are also academic papers on this topic ([Let's Talk About Fixed Effects](https://link.springer.com/article/10.1007/s11577-020-00699-8)), tutorials ([Fixed Effects Regression](https://www.econometrics-with-r.org/10-3-fixed-effects-regression.html), [Fixed or Random Effects](https://bookdown.org/Yuleng/polimethod/fixed.html)) and R packages ([fixest]https://lrberge.github.io/fixest/index.html, see it's [introduction](https://lrberge.github.io/fixest/articles/fixest_walkthrough.html)) dedicated especially to this subject. \n\n:::\n\nLet's try and see what `brms::fixef()` produces:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-using-fixef-b lst-cap=\"Extract population-level estimates from a brmsfit object\" code-summary=\"Extract population-level estimates from a brmsfit object\"}\nbrms::fixef(b4.3, probs = c(0.055, 0.945))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>              Estimate  Est.Error        Q5.5       Q94.5\n#> Intercept 154.6087522 0.27158394 154.1768663 155.0464348\n#> weight_c    0.9031995 0.04243563   0.8354987   0.9715501\n```\n:::\n:::\n\n\nSo the function `ggplot2::geom_abline()` has used the intercept (`a`)\nand the slope (`b`) of the estimate column.\n\n::: callout-note\n###### My code changes in using the `brms::fixef()` in @lst-fig-raw-data-line2-b4.3\n\n1. Slightly different use of the extracting operator `[`\n\nInstead of using `[1]` and `[2]` I have used `[[1]]` and `[[2]]`.\n\n> `[` selects sub-lists: it always returns a list. If you use it with a\n> single positive integer, it returns a list of length one. `[[` selects\n> an element within a list. (from [Advanced\n> R](https://adv-r.hadley.nz/subsetting.html#subsetting-answers), 2nd\n> ed.)\n\n2. Different percentiles\n\nMcElreath uses a 89% probability mass as CI (<a class='glossary' title='Two parameter values that contain between them a specified amount of posterior probability, a probability mass, is usually know as confidence interval that may instead be called a credible interval. We’re going to call it a compatibility interval instead, in order to avoid the unwarranted implications of “confidence”” and “credibility.” What the interval indicates is a range of parameter values compatible with the model and data. The model and data themselves may not inspire confidence, in which case the interval will not either. (Chap.3)'>compatibility interval</a>, aka credible interval or confidence interval). But the default value for the `probs` arguments is `c(0.025, 0.975)`, a 95% interval. To reproduce the Figure 4.6 I have therefore used `probs = c(0.055, 0.945)`.\n\n3. Direct function call\n\n`brms::fixef(x)` is equivalent to `brms:::fixef.brmsfit(x)` if\n`x` is a `brmsfit` object.\n:::\n\n\n\n\n##### Adding uncertainty around the mean\n\nInstead of `rethinking::extract.samples()` the {**brms**} packages\nextract all the posterior draws with `brms::as_draws_df()`. We have\nalready done this with @lst-put-hmc-into-df-b. We just repeat this code\nhere using `dplyr::slice(1:6)` instead of `utils::head()`\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-put-hmc-into-df2-b lst-cap=\"Extract the iteration of the Hamiliton Monte Carlo (HMC) chains into a data frame\"}\npost_b <- brms::as_draws_df(b4.3)\npost_b %>%\n  slice(1:6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A draws_df: 6 iterations, 1 chains, and 5 variables\n#>   b_Intercept b_weight_c sigma lprior  lp__\n#> 1         155       0.86   5.3   -9.3 -1080\n#> 2         154       0.85   5.1   -9.3 -1080\n#> 3         155       0.91   5.3   -9.3 -1079\n#> 4         155       0.90   5.2   -9.3 -1079\n#> 5         155       0.89   5.3   -9.3 -1080\n#> 6         154       0.90   5.5   -9.4 -1082\n#> # ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n```\n:::\n:::\n\n\nHere are the four models leading up to Figure 4.7:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-calc-all-four-models-b lst-cap=\"Calculate all four models\"}\ndN10_b <- 10\n\nb4.3_010 <- \n  brms::brm(data = d2_b %>%\n        slice(1:dN10_b),  # note our tricky use of `N` and `slice()`\n      family = gaussian,\n      height ~ 1 + weight_c,\n      prior = c(brms::prior(normal(178, 20), class = Intercept),\n                brms::prior(lognormal(0, 1), class = b),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.03_010\")\n\ndN50_b <- 50\n\nb4.3_050 <- \n  brms::brm(data = d2_b %>%\n        slice(1:dN50_b), \n      family = gaussian,\n      height ~ 1 + weight_c,\n      prior = c(brms::prior(normal(178, 20), class = Intercept),\n                brms::prior(lognormal(0, 1), class = b),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.03_050\")\n\ndN150_b <- 150\n\nb4.3_150 <- \n  brms::brm(data = d2_b %>%\n        slice(1:dN150_b), \n      family = gaussian,\n      height ~ 1 + weight_c,\n      prior = c(brms::prior(normal(178, 20), class = Intercept),\n                brms::prior(lognormal(0, 1), class = b),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.03_150\")\n\ndN352_b <- 352\n\nb4.3_352 <- \n  brms::brm(data = d2_b %>%\n        slice(1:dN352_b), \n      family = gaussian,\n      height ~ 1 + weight_c,\n      prior = c(brms::prior(normal(178, 20), class = Intercept),\n                brms::prior(lognormal(0, 1), class = b),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.03_352\")\n```\n:::\n\n\nHere are the trace plots and coefficient summaries from these four\nmodels.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-trace-plots-cov-sum-b lst-cap=\"Trace plots and coefficient summaries from all four models\"}\nplot(b4.3_010)\n```\n\n::: {.cell-output-display}\n![](04-geocentric-models_files/figure-html/trace-plots-cov-sum-b-1.png){width=672}\n:::\n\n```{.r .cell-code #lst-trace-plots-cov-sum-b lst-cap=\"Trace plots and coefficient summaries from all four models\"}\nprint(b4.3_010)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning: There were 1 divergent transitions after warmup. Increasing\n#> adapt_delta above 0.8 may help. See\n#> http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ 1 + weight_c \n#>    Data: d2_b %>% slice(1:dN10_b) (Number of observations: 10) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   152.12      1.99   148.31   156.28 1.00     2464     1901\n#> weight_c      0.91      0.20     0.49     1.29 1.00     2702     2078\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     5.88      1.91     3.40    10.62 1.00     1911     1879\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n```{.r .cell-code #lst-trace-plots-cov-sum-b lst-cap=\"Trace plots and coefficient summaries from all four models\"}\nplot(b4.3_050)\n```\n\n::: {.cell-output-display}\n![](04-geocentric-models_files/figure-html/trace-plots-cov-sum-b-2.png){width=672}\n:::\n\n```{.r .cell-code #lst-trace-plots-cov-sum-b lst-cap=\"Trace plots and coefficient summaries from all four models\"}\nprint(b4.3_050)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ 1 + weight_c \n#>    Data: d2_b %>% slice(1:dN50_b) (Number of observations: 50) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   152.92      0.71   151.51   154.30 1.00     3989     2783\n#> weight_c      0.88      0.10     0.69     1.07 1.00     3776     3110\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     5.04      0.52     4.13     6.20 1.00     3637     2433\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n```{.r .cell-code #lst-trace-plots-cov-sum-b lst-cap=\"Trace plots and coefficient summaries from all four models\"}\nplot(b4.3_150)\n```\n\n::: {.cell-output-display}\n![](04-geocentric-models_files/figure-html/trace-plots-cov-sum-b-3.png){width=672}\n:::\n\n```{.r .cell-code #lst-trace-plots-cov-sum-b lst-cap=\"Trace plots and coefficient summaries from all four models\"}\nprint(b4.3_150)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ 1 + weight_c \n#>    Data: d2_b %>% slice(1:dN150_b) (Number of observations: 150) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   153.85      0.46   152.94   154.74 1.00     3479     2849\n#> weight_c      0.90      0.06     0.77     1.02 1.00     3910     2652\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     5.51      0.32     4.92     6.19 1.00     3576     2896\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n```{.r .cell-code #lst-trace-plots-cov-sum-b lst-cap=\"Trace plots and coefficient summaries from all four models\"}\nplot(b4.3_352)\n```\n\n::: {.cell-output-display}\n![](04-geocentric-models_files/figure-html/trace-plots-cov-sum-b-4.png){width=672}\n:::\n\n```{.r .cell-code #lst-trace-plots-cov-sum-b lst-cap=\"Trace plots and coefficient summaries from all four models\"}\nprint(b4.3_352)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ 1 + weight_c \n#>    Data: d2_b %>% slice(1:dN352_b) (Number of observations: 352) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   154.60      0.27   154.08   155.14 1.00     4445     2954\n#> weight_c      0.90      0.04     0.82     0.98 1.00     4035     2903\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     5.11      0.19     4.74     5.51 1.00     4349     3157\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\n::: callout-note\n###### Checking MCMC chains with trace plots and trank plots\n\nAt the moment I didn't learn how to interpret these types of graphic output. McElreath's explains in later video lectures that it is important that <a class='glossary' title='A trace plot is a chain visualization that plots the samples in sequential order, joined by a line. A trace plot isn’t the last thing analysts do to inspect MCMC output. But it’s often the first. A healty trace plot is not a guarantee that you have a functioning Markov chain. (Chap.9)'>trace plot</a> cover the same location in the vertical axis (e.g. they do not jump around) and show that all different chains alternate in their (top) positions. I mentioned here the top position because this is the place where irregularities can be detected more easily. \n\nIn the above example it is difficult to decide if this is the case because the color differences of the different chains are weak. But it is general difficult to inspect trace plot, therefore McElreath proposes trace rank plots or <a class='glossary' title='Trace Rank Plot or as McElreath’s suggest a Trank Plot visualizes the chains as a distribution of the ranked samples. What this means is to take all the samples for each individual parameter and rank them. The lowest sample gets rank 1. The largest gets the maximum rank (the number of samples across all chains). Then we draw a histogram of these ranks for each individual chain. Why do this? Because if the chains are exploring the same space efficiently, the histograms should be similar to one another and largely overlapping. (Chap.9)'>trank plot</a> (his terminus) in @sec-checking-the-chain.\n:::\n\n\nWe'll need to put the chains of each model into data frames.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npost010_b4.3 <- brms::as_draws_df(b4.3_010)\npost050_b4.3 <- brms::as_draws_df(b4.3_050)\npost150_b4.3 <- brms::as_draws_df(b4.3_150)\npost352_b4.3 <- brms::as_draws_df(b4.3_352)\n```\n:::\n\n\nHere is the code for the four individual plots:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-calc-code-for-plots-b lst-cap=\"Prepare data for four individual plots\"}\np5 <- \n  ggplot(data =  d2_b[1:10, ], \n         aes(x = weight_c, y = height)) +\n  geom_abline(data = post010_b4.3 %>% slice(1:20),\n              aes(intercept = b_Intercept, slope = b_weight_c),\n              linewidth = 1/3, alpha = .3) +\n  geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  coord_cartesian(xlim = range(d2_b$weight_c),\n                  ylim = range(d2_b$height)) +\n  labs(subtitle = \"N = 10\")\n\np6 <-\n  ggplot(data =  d2_b[1:50, ], \n         aes(x = weight_c, y = height)) +\n  geom_abline(data = post050_b4.3 %>% slice(1:20),\n              aes(intercept = b_Intercept, slope = b_weight_c),\n              linewidth = 1/3, alpha = .3) +\n  geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  coord_cartesian(xlim = range(d2_b$weight_c),\n                  ylim = range(d2_b$height)) +\n  labs(subtitle = \"N = 50\")\n\np7 <-\n  ggplot(data =  d2_b[1:150, ], \n         aes(x = weight_c, y = height)) +\n  geom_abline(data = post150_b4.3 %>% slice(1:20),\n              aes(intercept = b_Intercept, slope = b_weight_c),\n              linewidth = 1/3, alpha = .3) +\n  geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  coord_cartesian(xlim = range(d2_b$weight_c),\n                  ylim = range(d2_b$height)) +\n  labs(subtitle = \"N = 150\")\n\np8 <- \n  ggplot(data =  d2_b[1:352, ], \n         aes(x = weight_c, y = height)) +\n  geom_abline(data = post352_b4.3 %>% slice(1:20),\n              aes(intercept = b_Intercept, slope = b_weight_c),\n              linewidth = 1/3, alpha = .3) +\n  geom_point(shape = 1, size = 2, color = \"royalblue\") +\n  coord_cartesian(xlim = range(d2_b$weight_c),\n                  ylim = range(d2_b$height)) +\n  labs(subtitle = \"N = 352\")\n```\n:::\n\n\nNow we can combine the ggplots with patchwork syntax to make the full\nversion of Figure 4.7.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# library(patchwork) ## already in setup chunk\n\n(p5 + p6 + p7 + p8) &\n  scale_x_continuous(\"weight\",\n                     breaks = c(-10, 0, 10),\n                     labels = labels) &\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![Samples from the quadratic approximate posterior distribution for the height/weight model, b4.3, with increasing amounts of data. In each plot, 20 lines sampled from the posterior distribution, showing the uncertainty in the regression relationship. Tidyverse version.](04-geocentric-models_files/figure-html/fig-draw-plots-figure-4.7-b-1.png){#fig-draw-plots-figure-4.7-b width=672}\n:::\n:::\n\n\n##### Plotting regression intervals and contours\n\nSince we used `weight_c` to fit our model, we might first want to\nunderstand what exactly the mean value is for weight.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-calc-mean-weight-b lst-cap=\"Calculate mean of weight\"}\nmean(d2_b$weight)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 44.99049\n```\n:::\n:::\n\n\nJust a hair under 45. If we're interested in $\\mu$ at `weight` = 50,\nthat implies we're also interested in $\\mu$ at `weight_c` + 5.01. Within\nthe context of our model, we compute this with\n$\\alpha + \\beta \\cdot 5.01$. Here's what that looks like with `post_b`.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-calc-mean-weight-at-50-b lst-cap=\"Calculate the mean at weight 50 kg\"}\nmu_at_50_b <- \n  post_b %>% \n  transmute(mu_at_50_b = b_Intercept + b_weight_c * 5.01)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n```\n:::\n\n```{.r .cell-code #lst-calc-mean-weight-at-50-b lst-cap=\"Calculate the mean at weight 50 kg\"}\nhead(mu_at_50_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 6 × 1\n#>   mu_at_50_b\n#>        <dbl>\n#> 1       159.\n#> 2       159.\n#> 3       159.\n#> 4       159.\n#> 5       159.\n#> 6       159.\n```\n:::\n:::\n\n\nAnd here is a version McElreath's Figure 4.8 density plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu_at_50_b %>%\n  ggplot(aes(x = mu_at_50_b)) +\n  geom_density(linewidth = 0, fill = \"deepskyblue\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(expression(mu[\"height | weight = 50\"])) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![The quadratic approximate posterior distribution of the mean height, μ, when weight is 50 kg. This distribution represents the relative plausibility of different values of the mean. Tidyverse version](04-geocentric-models_files/figure-html/fig-density-vector-mean-50-b-1.png){#fig-density-vector-mean-50-b width=672}\n:::\n:::\n\n\nWe'll use `tidybayes::mean_hdi()` to get both 89% and 95% HPDIs along\nwith the mean.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-calc-mean-and-HPDI-b lst-cap=\"Calculate both 89% and 95% Highest Priority Intensity Intervals (HPDIs) along with the mean.\"}\ntidybayes::mean_hdi(mu_at_50_b[, 1], .width = c(.89, .95))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 2 × 6\n#>   mu_at_50_b .lower .upper .width .point .interval\n#>        <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n#> 1       159.   159.   160.   0.89 mean   hdi      \n#> 2       159.   158.   160.   0.95 mean   hdi\n```\n:::\n:::\n\n\nIf you wanted to express those sweet 95% HPDIs on your density plot, you\nmight use `tidybayes::stat_halfeye()`. Since `tidybayes::stat_halfeye()`\nalso returns a point estimate, we'll throw in the mode.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu_at_50_b %>%\n  ggplot(aes(x = mu_at_50_b, y = 0)) +\n  tidybayes::stat_halfeye(point_interval = tidybayes::mode_hdi, .width = .95,\n               fill = \"deepskyblue\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(expression(mu[\"height | weight = 50\"])) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![Plot of half-eye (density + interval) geometry](04-geocentric-models_files/figure-html/fig-half-eye-b-1.png){#fig-half-eye-b width=672}\n:::\n:::\n\n\nWith {**brms**}, you would use fitted() to do what McElreath\naccomplished with `rethinking::link()`.\n\n::: callout-caution\nKurz applies the function `fitted()` in the code, but in the text he\nuses twice `brms::fitted()` which doesn't exist. I used both\n`brms:::fitted.brmsfit()` and `stats::fitted()` to get the same results.\n\nThe object `b4.3` is of class `brmsfit` but in the help file of\n`stats::fitted()` you can read: \"`fitted` is a generic function which\nextracts fitted values from objects returned by modeling functions.\n**All object classes which are returned by model fitting functions\nshould provide a `fitted` method.** (emphasis is mine)\n\nMy interpretation therefore is that `stats::fitted()` is using\n`brms:::fitted.brmsfit()`. Thts why the results are identical.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-calc-mu-with-fitted-b lst-cap=\"Calculate μ for each case in the data and sample from the posterior distribution: Tidyverse version\"}\nmu2_b <- brms:::fitted.brmsfit(b4.3, summary = F)\nmu2.1_b <- stats::fitted(b4.3, summary = F)\nstr(mu2_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  num [1:4000, 1:352] 157 157 157 157 157 ...\n```\n:::\n\n```{.r .cell-code #lst-calc-mu-with-fitted-b lst-cap=\"Calculate μ for each case in the data and sample from the posterior distribution: Tidyverse version\"}\nstr(mu2.1_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  num [1:4000, 1:352] 157 157 157 157 157 ...\n```\n:::\n:::\n\n\nWhen you specify `summary = F`, `brms:::fitted.brmsfit()` returns a\nmatrix of values with as many rows as there were post-warmup draws\nacross your Hamilton Monte Carlo (HMC) chains and as many columns as\nthere were cases in your analysis. Because we had 4,000 post-warmup\ndraws and $n=352$, `brms:::fitted.brmsfit()` returned a matrix of 4,000\nrows and 352 vectors. If you omitted the `summary = F` argument, the\ndefault is TRUE and `brms:::fitted.brmsfit()` will return summary\ninformation instead.\n\nMuch like `rethinking::link()`, `brms:::fitted.brmsfit()` can\naccommodate custom predictor values with its `newdata` argument.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-calc-dist-mu-unique-with-fitted.brmsfit-b lst-cap=\"Calculate a distribution of μ for each unique weight value on the horizontal axis: tidyverse version\"}\nweight_seq <- \n  tibble(weight = 25:70) %>% \n  mutate(weight_c = weight - mean(d2_b$weight))\n\nmu3_b <-\n  brms:::fitted.brmsfit(b4.3,\n         summary = F,\n         newdata = weight_seq) %>%\n  data.frame() %>%\n  # here we name the columns after the `weight` values from which they were computed\n  set_names(25:70) %>% \n  mutate(iter = 1:n())\n\nhead(mu3_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>         25       26       27       28       29       30       31       32\n#> 1 137.4019 138.2602 139.1185 139.9768 140.8351 141.6933 142.5516 143.4099\n#> 2 137.4518 138.3044 139.1569 140.0095 140.8620 141.7145 142.5671 143.4196\n#> 3 136.4257 137.3386 138.2515 139.1644 140.0773 140.9902 141.9032 142.8161\n#> 4 136.7604 137.6626 138.5649 139.4672 140.3695 141.2718 142.1741 143.0764\n#> 5 137.0289 137.9148 138.8008 139.6868 140.5727 141.4587 142.3447 143.2306\n#> 6 136.2401 137.1407 138.0413 138.9419 139.8425 140.7431 141.6437 142.5442\n#>         33       34       35       36       37       38       39       40\n#> 1 144.2682 145.1264 145.9847 146.8430 147.7013 148.5596 149.4178 150.2761\n#> 2 144.2722 145.1247 145.9772 146.8298 147.6823 148.5349 149.3874 150.2399\n#> 3 143.7290 144.6419 145.5548 146.4677 147.3806 148.2935 149.2064 150.1193\n#> 4 143.9787 144.8810 145.7833 146.6855 147.5878 148.4901 149.3924 150.2947\n#> 5 144.1166 145.0026 145.8885 146.7745 147.6604 148.5464 149.4324 150.3183\n#> 6 143.4448 144.3454 145.2460 146.1466 147.0472 147.9478 148.8483 149.7489\n#>         41       42       43       44       45       46       47       48\n#> 1 151.1344 151.9927 152.8510 153.7092 154.5675 155.4258 156.2841 157.1423\n#> 2 151.0925 151.9450 152.7976 153.6501 154.5026 155.3552 156.2077 157.0603\n#> 3 151.0322 151.9451 152.8580 153.7709 154.6838 155.5967 156.5096 157.4225\n#> 4 151.1970 152.0993 153.0016 153.9039 154.8062 155.7084 156.6107 157.5130\n#> 5 151.2043 152.0903 152.9762 153.8622 154.7482 155.6341 156.5201 157.4061\n#> 6 150.6495 151.5501 152.4507 153.3513 154.2519 155.1525 156.0530 156.9536\n#>         49       50       51       52       53       54       55       56\n#> 1 158.0006 158.8589 159.7172 160.5755 161.4337 162.2920 163.1503 164.0086\n#> 2 157.9128 158.7653 159.6179 160.4704 161.3230 162.1755 163.0280 163.8806\n#> 3 158.3354 159.2483 160.1612 161.0741 161.9871 162.9000 163.8129 164.7258\n#> 4 158.4153 159.3176 160.2199 161.1222 162.0245 162.9268 163.8290 164.7313\n#> 5 158.2920 159.1780 160.0640 160.9499 161.8359 162.7219 163.6078 164.4938\n#> 6 157.8542 158.7548 159.6554 160.5560 161.4566 162.3571 163.2577 164.1583\n#>         57       58       59       60       61       62       63       64\n#> 1 164.8669 165.7251 166.5834 167.4417 168.3000 169.1582 170.0165 170.8748\n#> 2 164.7331 165.5857 166.4382 167.2907 168.1433 168.9958 169.8484 170.7009\n#> 3 165.6387 166.5516 167.4645 168.3774 169.2903 170.2032 171.1161 172.0290\n#> 4 165.6336 166.5359 167.4382 168.3405 169.2428 170.1451 171.0474 171.9497\n#> 5 165.3798 166.2657 167.1517 168.0377 168.9236 169.8096 170.6956 171.5815\n#> 6 165.0589 165.9595 166.8601 167.7607 168.6612 169.5618 170.4624 171.3630\n#>         65       66       67       68       69       70 iter\n#> 1 171.7331 172.5914 173.4496 174.3079 175.1662 176.0245    1\n#> 2 171.5534 172.4060 173.2585 174.1111 174.9636 175.8161    2\n#> 3 172.9419 173.8548 174.7677 175.6806 176.5935 177.5064    3\n#> 4 172.8519 173.7542 174.6565 175.5588 176.4611 177.3634    4\n#> 5 172.4675 173.3535 174.2394 175.1254 176.0114 176.8973    5\n#> 6 172.2636 173.1642 174.0648 174.9653 175.8659 176.7665    6\n```\n:::\n:::\n\n\n::: callout-caution\nThe {rethinking} version uses the variable `weight.seq` whereas the\ntidyverse version uses `weight_seq`.\n:::\n\nAnticipating {**ggplot2**}, we went ahead and converted the output to a\ndata frame. But we might do a little more data processing with the aid\nof `tidyr::pivot_longer()`, which will convert the data from the wide\nformat to the long format.\n\n------------------------------------------------------------------------\n\n::: callout-tip\n###### Literature references\n\nIf you are new to the distinction between wide and long data, you can\nlearn more from the [Pivot data from wide to\nlong](https://tidyr.tidyverse.org/reference/pivot_longer.html) vignette\nfrom the tidyverse team (2020); Simon Ejdemyr's blog post, [Wide & long\ndata](https://sejdemyr.github.io/r-tutorials/basics/wide-and-long/); or\nKaren Grace-Martin's blog post, [The wide and long data format for\nrepeated measures\ndata](https://www.theanalysisfactor.com/wide-and-long-data/).\n:::\n\n------------------------------------------------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-convert-wide-to-long-b lst-cap=\"Data processing: Convert data from wide to long format: tidyverse version\"}\nmu4_b <- \n  mu3_b %>%\n  pivot_longer(-iter,\n               names_to = \"weight\",\n               values_to = \"height\") %>% \n  # we might reformat `weight` to numerals\n  mutate(weight = as.numeric(weight))\n\nhead(mu4_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 6 × 3\n#>    iter weight height\n#>   <int>  <dbl>  <dbl>\n#> 1     1     25   137.\n#> 2     1     26   138.\n#> 3     1     27   139.\n#> 4     1     28   140.\n#> 5     1     29   141.\n#> 6     1     30   142.\n```\n:::\n:::\n\n\nNow our data processing is done, here we reproduce McElreath's Figure\n4.9.a.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd2_b %>%\n  ggplot(aes(x = weight, y = height)) +\n  geom_point(data = mu4_b %>% filter(iter < 101), \n             color = \"navyblue\", alpha = .05) +\n  coord_cartesian(xlim = c(30, 65)) +\n  theme(panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![The first 100 values in the distribution of μ at each weight value. Tidyverse version](04-geocentric-models_files/figure-html/fig-dist-mu-height-100-b-1.png){#fig-dist-mu-height-100-b width=672}\n:::\n:::\n\n\nWith `brms:::fitted.brmsfit()`, it's quite easy to plot a regression\nline and its intervals. Just omit the `summary = T` argument.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-sum-dist-weight-b lst-cap=\"Summary of the distribution for each weight value. Tidyverse version\"}\nmu_summary <-\n  brms:::fitted.brmsfit(b4.3, \n         newdata = weight_seq) %>%\n  data.frame() %>%\n  bind_cols(weight_seq)\n\nhead(mu_summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>   Estimate Est.Error     Q2.5    Q97.5 weight  weight_c\n#> 1 136.5534 0.8932012 134.7466 138.2918     25 -19.99049\n#> 2 137.4566 0.8528721 135.7315 139.1110     26 -18.99049\n#> 3 138.3598 0.8127576 136.7219 139.9316     27 -17.99049\n#> 4 139.2630 0.7728911 137.7061 140.7597     28 -16.99049\n#> 5 140.1662 0.7333129 138.6946 141.5920     29 -15.99049\n#> 6 141.0694 0.6940725 139.6817 142.4203     30 -14.99049\n```\n:::\n:::\n\n\nHere it is, our analogue to Figure 4.9.b.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd2_b %>%\n  ggplot(aes(x = weight, y = height)) +\n  geom_smooth(data = mu_summary,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/2) +\n  geom_point(color = \"navyblue\", shape = 1, size = 1.5, alpha = 2/3) +\n  coord_cartesian(xlim = range(d2_b$weight)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![Plot of the summaries on top of the !Kung height data again, now with 89% compatibility interval of the mean indicated by the shaded region. Compare this region to the distributions of blue points in @fig-dist-mu-height-100-b.](04-geocentric-models_files/figure-html/fig-summaries-on-data-top-b-1.png){#fig-summaries-on-data-top-b width=672}\n:::\n:::\n\n\nIf you wanted to use intervals other than the default 95% ones, you'd\ninclude the probs argument like this:\n`brms:::fitted.brmsfit(b4.3, newdata = weight.seq, probs = c(.25, .75))`.\nThe resulting third and fourth vectors from the `fitted()` object would\nbe named `Q25` and `Q75` instead of the default `Q2.5` and `Q97.5`. The\n[Q prefix](https://github.com/paul-buerkner/brms/issues/425) stands for\nquantile.\n\nSimilar to `rethinking::link()`, `brms:::fitted.brmsfit()` uses the\nformula from your model to compute the model expectations for a given\nset of predictor values. I use it a lot in this project. If you follow\nalong, you'll get a good handle on it. But to dive deeper, you can [go\nhere for the\ndocumentation](https://rdrr.io/cran/brms/man/fitted.brmsfit.html).\nThough we won't be using it in this project, {**brms**} users might want\nto know that `fitted()` is also an alias for the\n`brms::posterior_epred()` function, about which you might [learn more\nhere](https://rdrr.io/cran/brms/man/posterior_epred.brmsfit.html). Users\ncan always learn more about them and other functions in the [{**brms**}\nreference\nmanual](https://cran.r-project.org/web/packages/brms/brms.pdf).\n\n##### Prediction intervals\n\nWe've only been plotting the `μ` part. In order to bring in the\nvariability expressed by `σ`, we'll have to switch to the `predict()`\nfunction. Much as `brms:::fitted.brmsfit()` was our analogue to\n`rethinking::link()`, `brms:::predict.brmsfit()` is our analogue to\n`rethinking::sim()`.\n\nWe can reuse our `weight_seq` data from before. The `predict()` code\nlooks a lot like what we used for `fitted()`. Compare\n@lst-calc-dist-mu-unique-with-fitted.brmsfit-b with\n@lst-calc-pred-height-with-predict.brmsfit-b.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-calc-pred-height-with-predict.brmsfit-b lst-cap=\"Calculate the prediction of heights: tidyverse version\"}\npred_height <-\n  brms:::predict.brmsfit(b4.3,\n          newdata = weight_seq) %>%\n  data.frame() %>%\n  bind_cols(weight_seq)\n  \npred_height %>%\n  slice(1:6)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>   Estimate Est.Error     Q2.5    Q97.5 weight  weight_c\n#> 1 136.6131  5.141393 126.4551 146.3092     25 -19.99049\n#> 2 137.4095  5.183946 127.2941 147.5712     26 -18.99049\n#> 3 138.5051  5.156565 128.2284 148.4984     27 -17.99049\n#> 4 139.0661  5.241010 128.9475 149.3429     28 -16.99049\n#> 5 140.1788  5.115921 130.3428 150.1606     29 -15.99049\n#> 6 140.9486  5.167109 130.8277 151.0707     30 -14.99049\n```\n:::\n:::\n\n\nThis time the summary information in our data frame is for, as McElreath\nput it, \"simulated heights, not distributions of plausible average\nheight, `μ`\" (p. 108). Another way of saying that is that these\nsimulations are the joint consequence of both `μ` and `σ`, unlike the\nresults of `fitted()`, which only reflect `μ`.\n\nFigure 4.10 shows how you might visualize them:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd2_b %>%\n  ggplot(aes(x = weight)) +\n  geom_ribbon(data = pred_height, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_smooth(data = mu_summary,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/2) +\n  geom_point(aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 2/3) +\n  coord_cartesian(xlim = range(d2_b$weight),\n                  ylim = range(d2_b$height)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![89% prediction interval for height, as a function of weight. The solid line is the average line for the mean height at each weight. The two shaded regions show different 89% plausible regions. The narrow shaded interval around the line is the distribution of μ. The wider shaded region represents the region within which the model expects to find 89% of actual heights in the population, at each weight. (tidyverse version)](04-geocentric-models_files/figure-html/fig-reproduce-figure-4.10-1.png){#fig-reproduce-figure-4.10 width=672}\n:::\n:::\n\n\nTo smooth out the rough shaded interval we would have in the {**brms**}\nmodel fitting approach to refit `b4.3` after specifying a larger number\nof post-warmup iterations with alterations to the `iter` and `warmup`\nparameters.\n\n::: callout-note\n###### TODO: Smooth boundary {.unnumbered}\n\nI should try to smooth out the grey boundary, because it would give me\nmore experiences how to use the different parameters to fit models with\nthe {**brms**} approach.\n\nThis experiment would also an occasion to change the intervals from the\ndefault 95% ones to the 89% McElreath's is using. See his hint in the\nparagraph before displaying Figure 10 how to change it in the\n{**rethinking**} version.\n\nIn {**brms**} I would have to change the `probs` argument to\nbrms:::fitted.brmsfit(b4.3, newdata = weight.seq, probs = c(0.055,\n0.945)). The resulting third and fourth vectors from the fitted() object\nwould be named Q5.5 and Q94.5 instead of the default Q2.5 and Q97.5. The\nQ prefix stands for quantile. See [Rename summary columns of predict()\nand related methods](https://github.com/paul-buerkner/brms/issues/425).\n:::\n\nNext we follow McElreath's example and do our model-based predictions by\nhand. Instead of relying on base R `apply()` and `sapply()`, the main\naction in the tidyverse approach is in `expand_grid()`, the second\n`mutate()` line and the `group_by()` + `summarise()` combination.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-predict-manually-b lst-cap=\"Model-based predictions without {brms} and pedict(): mean with quantiles of 0.25 and .975\"}\nset.seed(4)\n\npost_b %>% \n  tidyr::expand_grid(weight = 25:70) %>% \n  mutate(weight_c = weight - mean(d2_b$weight)) %>% \n  mutate(sim_height = rnorm(n(),\n                            mean = b_Intercept + b_weight_c * weight_c,\n                            sd   = sigma)) %>% \n  group_by(weight) %>% \n  summarise(mean = mean(sim_height),\n            ll   = quantile(sim_height, prob = .025),\n            ul   = quantile(sim_height, prob = .975)) %>% \n  \n  # plot\n  ggplot(aes(x = weight)) +\n  geom_smooth(aes(y = mean, ymin = ll, ymax = ul),\n              stat = \"identity\",\n              fill = \"grey83\", color = \"black\", alpha = 1, linewidth = 1/2) +\n  geom_point(data = d2_b,\n             aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 2/3) +\n  coord_cartesian(xlim = range(d2_b$weight),\n                  ylim = range(d2_b$height)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![Model-based predictions without {brms} and pedict(): mean with quantiles of 0.25 and .975](04-geocentric-models_files/figure-html/fig-predict-manually-b-1.png){#fig-predict-manually-b width=672}\n:::\n:::\n\n\nWe specifically left out the `fitted()` intervals to make it more\napparent what we were simulating. You might also note that we could have\neasily replaced that three-line summarize() code with a single line of\n`tidybayes::mean_qi(sim_height)`, or whatever combination of central\ntendency and interval type you wanted (e.g.,\n`tidybayes::mode_hdi(sim_height, .width = .89)`)\n\nLet's try this out:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-predict-manually2-b lst-cap=\"Model-based predictions without {brms} and pedict(): mean with width .89\"}\nset.seed(4)\n\npost_b %>% \n  tidyr::expand_grid(weight = 25:70) %>% \n  mutate(weight_c = weight - mean(d2_b$weight)) %>% \n  mutate(sim_height = rnorm(n(),\n                            mean = b_Intercept + b_weight_c * weight_c,\n                            sd   = sigma)) %>% \n  group_by(weight) %>% \n  tidybayes::mean_qi(sim_height, .width = .89) %>% \n  \n  # plot\n  ggplot(aes(x = weight)) +\n  geom_smooth(aes(y = .point, ymin = .lower, ymax = .upper),\n              stat = \"identity\",\n              fill = \"grey83\", color = \"black\", alpha = 1, linewidth = 1/2) +\n  geom_point(data = d2_b,\n             aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 2/3) +\n  coord_cartesian(xlim = range(d2_b$weight),\n                  ylim = range(d2_b$height)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![](04-geocentric-models_files/figure-html/fig-predict-manually2-b-1.png){#fig-predict-manually2-b width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-predict-manually3-b lst-cap=\"Model-based predictions without {brms} and pedict(): mode with width .89\"}\nset.seed(4)\n\npost_b %>% \n  tidyr::expand_grid(weight = 25:70) %>% \n  mutate(weight_c = weight - mean(d2_b$weight)) %>% \n  mutate(sim_height = rnorm(n(),\n                            mean = b_Intercept + b_weight_c * weight_c,\n                            sd   = sigma)) %>% \n  group_by(weight) %>% \n  tidybayes::mode_hdi(sim_height, .width = .89) %>% \n  \n  # plot\n  ggplot(aes(x = weight)) +\n  geom_smooth(aes(y = .point, ymin = .lower, ymax = .upper),\n              stat = \"identity\",\n              fill = \"grey83\", color = \"black\", alpha = 1, linewidth = 1/2) +\n  geom_point(data = d2_b,\n             aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 2/3) +\n  coord_cartesian(xlim = range(d2_b$weight),\n                  ylim = range(d2_b$height)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![](04-geocentric-models_files/figure-html/fig-predict-manually3-b-1.png){#fig-predict-manually3-b width=672}\n:::\n:::\n\n\n::: callout-note\n###### TODO: .width vs. ll/ul {.unnumbered}\n:::\n\nRevise @sec-sampling-to-summarize to understand better the difference\nbetween width (= defined probability mass, for example .89) and ll/ul (=\ndefined boundaries, for example .025 and .975).\n\nWhat is the equivalent of width .89 (defined in probability mass) in\ndefined boundaries under the assumption of a Gaussian distribution?\nSolution: .055 and.945!\n\n### Curves from lines\n\n#### Polynomial regression\n\nTo see an application for fitting curves instead of lines we are going\nto use the `Howell1` data, but this time the full data set. The reason\nis that in the non-adult years there is a steeper slope than in the\nadult years.\n\n\n::: {.cell}\n\n```{.r .cell-code #fig-scatterplot-height-weight-b lst-cap=\"Height in centimeters (vertical) plotted against weight in kilograms (horizontal): tidyverse version\"}\nd_b %>% \n  ggplot(aes(x = weight, y = height)) +\n  geom_point(color = \"navyblue\", shape = 1, size = 1.5, alpha = 2/3) +\n  annotate(geom = \"text\",\n           x = 42, y = 115,\n           label = \"This relation is\\nvisibly curved.\",\n           family = \"Times\") +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![Height in centimeters (vertical) plotted against weight in kilograms (horizontal). This time with the full data, e.g., the non-adults data.](04-geocentric-models_files/figure-html/fig-scatterplot-height-weight-b-1.png){#fig-scatterplot-height-weight-b width=672}\n:::\n:::\n\n\nStandardizing will help `brms::brm()` fit the model. We might\nstandardize our weight variable like so:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-standardize-weight-b lst-cap=\"Stadardize the weight variable\"}\nd3_b <-\n  d_b %>%\n  mutate(weight_s = (weight - mean(weight)) / sd(weight)) %>% \n  mutate(weight_s2 = weight_s^2)\n```\n:::\n\n\nWe fit the quadratic model (se @eq-parabolic-model) with {**brms**}:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-brm-parabolic lst-cap=\"Finding the posterior distribution of a parabolic model of height on weight with brms::brm()\"}\nb4.5 <- \n  brms::brm(data = d3_b, \n      family = gaussian,\n      height ~ 1 + weight_s + weight_s2,\n      prior = c(brms::prior(normal(178, 20), class = Intercept),\n                brms::prior(lognormal(0, 1), class = b, coef = \"weight_s\"),\n                brms::prior(normal(0, 1), class = b, coef = \"weight_s2\"),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.05\")\n```\n:::\n\n\nNote our use of the coef argument within our prior statements. Since β1\nand β2 are both parameters of `class = b` within the {**brms**} set-up,\nwe need to use the `coef` argument when we want their priors to differ.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-brm-parabolic lst-cap=\"Plot the posterior distribution of a parabolic model of height on weight calculated with brms::brm()\"}\nplot(b4.5, widths = c(1, 2))\n```\n\n::: {.cell-output-display}\n![](04-geocentric-models_files/figure-html/fig-brm-parabolic-1.png){#fig-brm-parabolic width=672}\n:::\n:::\n\n\n::: callout-note\n###### TODO: Interpret graphic\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-print-brm-parabolic lst-cap=\"Pint the result of the posterior distribution of a parabolic model of height on weight calculated with brms::brm()\"}\nprint(b4.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ 1 + weight_s + weight_s2 \n#>    Data: d3_b (Number of observations: 544) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   146.05      0.38   145.30   146.77 1.00     3544     2516\n#> weight_s     21.74      0.29    21.15    22.32 1.00     3106     2604\n#> weight_s2    -7.79      0.28    -8.33    -7.25 1.00     3298     2631\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     5.81      0.18     5.47     6.18 1.00     3788     2881\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\n::: callout-note\n###### TODO: Interpret printout\n:::\n\nOur quadratic plot requires new `fitted()`- and `predict()`-oriented\ndata wrangling.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-wrangling-for-parabolic-model-b lst-cap=\"Data wrangling as a preparation for the parabolic (quadratic) model\"}\nweight_seq_full <- \n  tibble(weight_s = seq(from = -2.5, to = 2.5, length.out = 30)) %>% \n  mutate(weight_s2 = weight_s^2)\n\nfitd_quad <-\n  fitted(b4.5, \n         newdata = weight_seq_full) %>%\n  data.frame() %>%\n  bind_cols(weight_seq_full)\n\npred_quad <-\n  predict(b4.5, \n          newdata = weight_seq_full) %>%\n  data.frame() %>%\n  bind_cols(weight_seq_full)  \n```\n:::\n\n\nReplicating Figure 4.11.b:\n\n\n::: {.cell}\n\n```{.r .cell-code}\np10 <-\n  ggplot(data = d3_b, \n         aes(x = weight_s)) +\n  geom_ribbon(data = pred_quad, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_smooth(data = fitd_quad,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/2) +\n  geom_point(aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 1/3) +\n  labs(subtitle = \"quadratic\",\n       y = \"height\") +\n  coord_cartesian(xlim = range(d3_b$weight_s),\n                  ylim = range(d3_b$height)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n\np10\n```\n\n::: {.cell-output-display}\n![Replicate Figure 4.11.b: Polynomial regressions of height on weight (standardized), for the full !Kung data. The raw data are shown by the circles. The solid curves show the path of μ in each model, and the shaded regions show the 95% interval of the mean (close to the solid curve) and the 95% interval of predictions (wider). (Note: This is slightly different to the original version with a width of .89 and qunatiles at .055 and .945 resp. Q5.5 and Q94.5)](04-geocentric-models_files/figure-html/fig-replicate-4.11.b-1.png){#fig-replicate-4.11.b width=672}\n:::\n:::\n\n\nFrom a formula perspective, the cubic model is a simple extension of the\nquadratic (compare @eq-parabolic-model with @eq-cubic-regression). --\nBefore we fit the model, we need to wrangle the data again.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-wrangling-for-cubic-model-b lst-cap=\"Data wrangling as a preparation for the cubic model.\"}\nd3_b <-\n  d3_b %>% \n  mutate(weight_s3 = weight_s^3)\n```\n:::\n\n\nNow fit the model:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fit-cubic-regression-model-b lst-cap=\"Fit a cubic regression model of height on weight (standardized), for the full !Kung data.\"}\nb4.6 <- \n  brms::brm(data = d3_b, \n      family = gaussian,\n      height ~ 1 + weight_s + weight_s2 + weight_s3,\n      prior = c(brms::prior(normal(178, 20), class = Intercept),\n                brms::prior(lognormal(0, 1), class = b, coef = \"weight_s\"),\n                brms::prior(normal(0, 1), class = b, coef = \"weight_s2\"),\n                brms::prior(normal(0, 1), class = b, coef = \"weight_s3\"),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.06\")\n```\n:::\n\n\nHere's the `fitted()`, `predict()`, and {**ggplot2**} code for Figure\n4.11.c, the cubic model.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-cubic-regression-model-b lst-cap=\"Fit a cubic regression model of height on weight (standardized), for the full !Kung data: tidyverse version\"}\nweight_seq_full <- \n  weight_seq_full %>% \n  mutate(weight_s3 = weight_s^3)\n\nfitd_cub <-\n  fitted(b4.6, \n         newdata = weight_seq_full) %>%\n  as_tibble() %>%\n  bind_cols(weight_seq_full)\n\npred_cub <-\n  predict(b4.6, \n          newdata = weight_seq_full) %>%\n  as_tibble() %>%\n  bind_cols(weight_seq_full) \n\np11 <-\n  ggplot(data = d3_b, \n       aes(x = weight_s)) +\n  geom_ribbon(data = pred_cub, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_smooth(data = fitd_cub,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/4) +\n  geom_point(aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 1/3) +\n  labs(subtitle = \"cubic\",\n       y = \"height\") +\n  coord_cartesian(xlim = range(d3_b$weight_s),\n                  ylim = range(d3_b$height)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n\np11\n```\n\n::: {.cell-output-display}\n![Fit a cubic regression model of height on weight (standardized), for the full !Kung data](04-geocentric-models_files/figure-html/fig-cubic-regression-model-b-1.png){#fig-cubic-regression-model-b width=672}\n:::\n:::\n\n\nAnd now we'll fit the good old linear model.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fit-linear-regression-model-b lst-cap=\"Fit a linear regression model of height on weight (standardized), for the full !Kung data.\"}\nb4.7 <- \n  brms::brm(data = d3_b, \n      family = gaussian,\n      height ~ 1 + weight_s,\n      prior = c(brms::prior(normal(178, 20), class = Intercept),\n                brms::prior(lognormal(0, 1), class = b, coef = \"weight_s\"),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.07\")\n```\n:::\n\n\nAnd here's the `fitted()`, `predict()`, and {**ggplot2**} code for\nFigure 4.11.a, the linear model.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-linear-regression-model-b lst-cap=\"Fit a linear regression model of height on weight (standardized), for the full !Kung data: tidyverse version\"}\nfitd_line <-\n  fitted(b4.7, \n         newdata = weight_seq_full) %>%\n  as_tibble() %>%\n  bind_cols(weight_seq_full)\n\npred_line <-\n  predict(b4.7, \n          newdata = weight_seq_full) %>%\n  as_tibble() %>%\n  bind_cols(weight_seq_full) \n\np9 <-\n  ggplot(data = d3_b, \n       aes(x = weight_s)) +\n  geom_ribbon(data = pred_line, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_smooth(data = fitd_line,\n              aes(y = Estimate, ymin = Q2.5, ymax = Q97.5),\n              stat = \"identity\",\n              fill = \"grey70\", color = \"black\", alpha = 1, linewidth = 1/4) +\n  geom_point(aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 1/3) +\n  labs(subtitle = \"linear\",\n       y = \"height\") +\n  coord_cartesian(xlim = range(d3_b$weight_s),\n                  ylim = range(d3_b$height)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank())\n\np9\n```\n\n::: {.cell-output-display}\n![Fit a linear regression model of height on weight (standardized), for the full !Kung data](04-geocentric-models_files/figure-html/fig-linear-regression-model-b-1.png){#fig-linear-regression-model-b width=672}\n:::\n:::\n\n\nDid you notice how we labeled each of the past three plots as `p1`,\n`p2`, and `p3`? Here we use those names to plot them all together with\n{**patchwork**} syntax.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-plot-all-together-b lst-cap=\"Plot all three models (linear, quadratic and cubic) together, using the {**patchwork**} syntax\"}\n# library(patchwork) ## already in setup chunk\np9 | p10 | p11\n```\n\n::: {.cell-output-display}\n![Plot all three models (linear, quadratic and cubic) together](04-geocentric-models_files/figure-html/fig-plot-all-together-b-1.png){#fig-plot-all-together-b width=672}\n:::\n:::\n\n\n**Converting back to natural scale** You can apply McElreath's\nconversion trick within the {**ggplot2**} environment, too. Here it is\nwith the cubic model.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-cubic-regression-natural-scale-b lst-cap=\"Cubic regression with x-axis in natural scale: tidyverse version\"}\nat_b <- c(-2, -1, 0, 1, 2)\n\nggplot(data = d3_b, \n       aes(x = weight_s)) +\n  geom_ribbon(data = pred_cub, \n              aes(ymin = Q2.5, ymax = Q97.5),\n              fill = \"grey83\") +\n  geom_point(aes(y = height),\n             color = \"navyblue\", shape = 1, size = 1.5, alpha = 1/3) +\n  coord_cartesian(xlim = range(d3_b$weight_s)) +\n  theme(text = element_text(family = \"Times\"),\n        panel.grid = element_blank()) +\n  \n  # here it is!\n  scale_x_continuous(\"standardized weight converted back\",\n                     breaks = at_a,\n                     labels = round(at_b*sd(d3_b$weight) + mean(d3_b$weight), 1))\n```\n\n::: {.cell-output-display}\n![](04-geocentric-models_files/figure-html/fig-cubic-regression-natural-scale-b-1.png){#fig-cubic-regression-natural-scale-b width=672}\n:::\n:::\n\n\n#### Splines\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-load-cherry-blossoms-data-b lst-cap=\"Load Cherry Blossoms data and display summary (tidyverse version)\"}\n## R code 4.72 modified ######################\ndata(package = \"rethinking\", list = \"cherry_blossoms\")\nd4_b <- cherry_blossoms\n\n\n# ground-up tidyverse way to summarize\n(\nd4.2_b <- \n    d4_b %>% \n      gather() %>% \n      group_by(key) %>% \n      summarise(mean = mean(value, na.rm = T),\n                sd   = sd(value, na.rm = T),\n                ll   = quantile(value, prob = .055, na.rm = T),\n                ul   = quantile(value, prob = .945, na.rm = T)) %>% \n      mutate_if(is.double, round, digits = 2) \n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A tibble: 5 × 5\n#>   key           mean     sd     ll      ul\n#>   <chr>        <dbl>  <dbl>  <dbl>   <dbl>\n#> 1 doy         105.     6.41  94.4   115   \n#> 2 temp          6.14   0.66   5.15    7.29\n#> 3 temp_lower    5.1    0.85   3.79    6.37\n#> 4 temp_upper    7.19   0.99   5.9     8.9 \n#> 5 year       1408    351.   868.   1948.\n```\n:::\n\n```{.r .cell-code #lst-load-cherry-blossoms-data-b lst-cap=\"Load Cherry Blossoms data and display summary (tidyverse version)\"}\nd4.2_b |> \n  skimr::skim()\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |       |\n|:------------------------|:------|\n|Name                     |d4.2_b |\n|Number of rows           |5      |\n|Number of columns        |5      |\n|_______________________  |       |\n|Column type frequency:   |       |\n|character                |1      |\n|numeric                  |4      |\n|________________________ |       |\n|Group variables          |None   |\n\n\n**Variable type: character**\n\n|skim_variable | n_missing| complete_rate| min| max| empty| n_unique| whitespace|\n|:-------------|---------:|-------------:|---:|---:|-----:|--------:|----------:|\n|key           |         0|             1|   3|  10|     0|        5|          0|\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|     sd|   p0|  p25|  p50|    p75|    p100|hist  |\n|:-------------|---------:|-------------:|------:|------:|----:|----:|----:|------:|-------:|:-----|\n|mean          |         0|             1| 306.19| 617.40| 5.10| 6.14| 7.19| 104.54| 1408.00|▇▁▁▁▂ |\n|sd            |         0|             1|  71.96| 155.94| 0.66| 0.85| 0.99|   6.41|  350.88|▇▁▁▁▂ |\n|ll            |         0|             1| 195.41| 377.85| 3.79| 5.15| 5.90|  94.43|  867.77|▇▁▁▁▂ |\n|ul            |         0|             1| 417.16| 857.16| 6.37| 7.29| 8.90| 115.00| 1948.23|▇▁▁▁▂ |\n:::\n:::\n\n\nKurz's version does not have the mini histograms. I added another\nsummary with `skimr::skim()` to add tiny graphics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd4_b %>% \n  ggplot(aes(x = year, y = doy)) +\n  # color from here: https://www.colorhexa.com/ffb7c5\n  geom_point(color = \"#ffb7c5\", alpha = 1/2) +\n  theme_bw() +\n  theme(panel.grid = element_blank(),\n        # color from here: https://www.colordic.org/w/, inspired by https://chichacha.netlify.com/2018/11/29/plotting-traditional-colours-of-japan/\n        panel.background = element_rect(fill = \"#4f455c\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning: Removed 388 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![Display raw data for `doy` (Day of the year of first blossom) against the year: tidyverse version](04-geocentric-models_files/figure-html/fig-scatterplot-cbl-b-1.png){#fig-scatterplot-cbl-b width=672}\n:::\n:::\n\n\nBy default {**ggplots**} removes missing data records with a warning. It\nturns out there are cases with missing data for the `doy` variable.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-doy-missing-data lst-cap=\"Display missing data of the day of the year variable `doy`\"}\nd4_b %>% \n  count(is.na(doy)) %>% \n  mutate(percent = 100 * n / sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>   is.na(doy)   n  percent\n#> 1      FALSE 827 68.06584\n#> 2       TRUE 388 31.93416\n```\n:::\n:::\n\n\nLet's follow McElreath and make a subset of the data that excludes cases\nwith missing data in `doy.` Within the tidyverse, we might do so with\nthe `tidyr::drop_na()` function. -- This is a much easier way than my\napproach using `dplyr::filter()` (See\n[StackOverflow](https://stackoverflow.com/a/70848085/7322615):\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-remove-missing-data-records lst-cap=\"Remove records of missing data for the two interesting variable `doy` and `year`\"}\nd5_b <- \n    d4_b %>% \n        filter(if_all(.col = c(year, doy), .fns = Negate(is.na)))\nd5_b\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>     year doy temp temp_upper temp_lower\n#> 1    812  92   NA         NA         NA\n#> 2    815 105   NA         NA         NA\n#> 3    831  96   NA         NA         NA\n#> 4    851 108 7.38      12.10       2.66\n#> 5    853 104   NA         NA         NA\n#> 6    864 100 6.42       8.69       4.14\n#> 7    866 106 6.44       8.11       4.77\n#> 8    869  95   NA         NA         NA\n#> 9    889 104 6.83       8.48       5.19\n#> 10   891 109 6.98       8.96       5.00\n#> 11   892 108 7.11       9.11       5.11\n#> 12   894 106 6.98       8.40       5.55\n#> 13   895 104 7.08       8.57       5.58\n#> 14   896 104 7.20       8.69       5.72\n#> 15   902 102 7.50       8.95       6.06\n#> 16   908  98 7.26       8.89       5.62\n#> 17   912  95 6.78       8.83       4.74\n#> 18   913 110 6.61       8.59       4.63\n#> 19   917  95 6.48      10.21       2.76\n#> 20   923 104 6.76       8.50       5.01\n#> 21   926  98 7.45      10.01       4.90\n#> 22   930  97 6.81       8.63       4.98\n#> 23   933 106 6.72       9.41       4.03\n#> 24   941 109 7.25       8.82       5.67\n#> 25   949 107 7.10       8.35       5.85\n#> 26   950  95 7.20       8.65       5.75\n#> 27   955  89 7.07       8.37       5.77\n#> 28   957 112 6.93       8.21       5.66\n#> 29   958 109 6.96       8.34       5.58\n#> 30   959  97 7.14       8.43       5.85\n#> 31   960 101 7.07       8.45       5.69\n#> 32   961  87 7.06       8.44       5.69\n#> 33   963  94 6.82       7.88       5.76\n#> 34   965 104 6.74       7.83       5.66\n#> 35   966  98 6.76       7.97       5.55\n#> 36   967 103 6.75       8.16       5.34\n#> 37   969  98 6.84       8.61       5.07\n#> 38   972 107 6.57       8.17       4.98\n#> 39   974 108 5.91       7.78       4.05\n#> 40   975 101 5.98       8.32       3.64\n#> 41   977 112 6.16       7.85       4.48\n#> 42   979 104 6.48       8.77       4.20\n#> 43   985  94 5.92       9.23       2.61\n#> 44   989 100 5.80       7.31       4.28\n#> 45   991 104 5.53       7.11       3.94\n#> 46  1000 105 5.10       6.80       3.41\n#> 47  1004 117 5.62       7.22       4.03\n#> 48  1006 100 5.92       7.27       4.57\n#> 49  1007 106 5.73       7.32       4.14\n#> 50  1014 115 6.09       7.77       4.41\n#> 51  1016 111 6.26       7.64       4.89\n#> 52  1017  99 6.39       7.78       5.01\n#> 53  1018 113 6.04       7.45       4.64\n#> 54  1019  98 6.32       7.93       4.71\n#> 55  1029 116 5.99       7.81       4.18\n#> 56  1031  97 6.63       7.97       5.29\n#> 57  1032 111 6.53       8.01       5.04\n#> 58  1033 102 6.68       8.18       5.19\n#> 59  1034  96 6.67       8.42       4.91\n#> 60  1036 104 6.34       7.98       4.70\n#> 61  1041 103 6.01       7.82       4.21\n#> 62  1046 103 6.03       7.82       4.24\n#> 63  1048 112 5.98       8.20       3.75\n#> 64  1050 105 6.36       8.38       4.34\n#> 65  1060 108 6.15       7.97       4.34\n#> 66  1061  92 6.33       8.59       4.07\n#> 67  1066 105 6.04       7.69       4.39\n#> 68  1071 113 6.23       7.79       4.68\n#> 69  1073 103 6.49       7.72       5.26\n#> 70  1080 102 6.19       7.29       5.09\n#> 71  1083  96 5.99       7.09       4.90\n#> 72  1084 120 5.90       6.91       4.89\n#> 73  1088 104 6.16       6.98       5.35\n#> 74  1090 105 6.16       6.97       5.36\n#> 75  1093 105 5.99       6.96       5.02\n#> 76  1095 100 6.06       7.00       5.12\n#> 77  1096  96 5.97       6.87       5.06\n#> 78  1098 100 5.70       6.45       4.96\n#> 79  1099 103 5.59       6.27       4.91\n#> 80  1102 104 5.60       6.24       4.95\n#> 81  1104 108 5.56       6.22       4.90\n#> 82  1105 114 5.58       6.28       4.88\n#> 83  1107 105 5.74       6.38       5.09\n#> 84  1111 113 5.58       6.24       4.92\n#> 85  1112 104 5.67       6.41       4.93\n#> 86  1114 100 5.56       6.32       4.79\n#> 87  1117 109 5.42       6.09       4.75\n#> 88  1118 102 5.42       6.17       4.67\n#> 89  1120 104 5.32       6.08       4.55\n#> 90  1123 112 5.23       6.00       4.46\n#> 91  1124  98 5.67       6.49       4.85\n#> 92  1125 111 5.61       6.48       4.74\n#> 93  1126 108 6.11       7.49       4.73\n#> 94  1127 107 6.18       7.67       4.69\n#> 95  1128 111 6.25       7.89       4.61\n#> 96  1129 106 6.26       7.69       4.82\n#> 97  1130 103 6.30       7.89       4.71\n#> 98  1131 103 5.97       7.45       4.49\n#> 99  1134 106 5.84       7.26       4.42\n#> 100 1139 113 5.83       7.02       4.65\n#> 101 1140 103 5.89       7.05       4.72\n#> 102 1141 111 5.82       7.08       4.56\n#> 103 1143 110 5.92       7.27       4.57\n#> 104 1145 107 6.02       7.59       4.45\n#> 105 1151 109 5.80       7.04       4.56\n#> 106 1154  96 5.82       7.00       4.65\n#> 107 1156  90 5.52       6.64       4.39\n#> 108 1159 103 5.28       6.08       4.48\n#> 109 1161 112 5.19       5.95       4.43\n#> 110 1163 112 5.23       6.03       4.42\n#> 111 1164 100 5.10       6.01       4.19\n#> 112 1165 106 5.00       5.89       4.11\n#> 113 1166  95 4.96       5.90       4.02\n#> 114 1167 111 4.83       5.69       3.98\n#> 115 1169 112 4.86       5.71       4.00\n#> 116 1170 113 4.98       5.84       4.13\n#> 117 1172 103 5.28       6.22       4.35\n#> 118 1173 117 5.24       6.22       4.27\n#> 119 1174 112 5.30       6.15       4.46\n#> 120 1175 113 5.39       6.21       4.56\n#> 121 1176 113 5.48       6.31       4.64\n#> 122 1179 103 5.61       6.43       4.80\n#> 123 1180  98 5.48       6.30       4.66\n#> 124 1182 104 5.52       6.42       4.61\n#> 125 1183 101 5.44       6.36       4.53\n#> 126 1184 115 5.38       6.43       4.33\n#> 127 1185 115 5.46       6.36       4.56\n#> 128 1187 104 5.70       6.53       4.87\n#> 129 1188 107 5.66       6.59       4.73\n#> 130 1190 112 5.69       6.58       4.79\n#> 131 1191 106 5.84       6.74       4.93\n#> 132 1194 119 5.79       6.65       4.93\n#> 133 1199 109 6.01       6.67       5.35\n#> 134 1200 104 6.10       6.76       5.45\n#> 135 1201  96 6.15       6.81       5.48\n#> 136 1202 106 6.18       7.01       5.36\n#> 137 1203 105 6.18       6.98       5.37\n#> 138 1204 110 6.20       7.05       5.34\n#> 139 1205 105 6.20       6.98       5.42\n#> 140 1207 104 6.45       7.50       5.40\n#> 141 1210 112 6.51       7.53       5.49\n#> 142 1212  98 6.67       7.69       5.65\n#> 143 1213 107 6.66       7.80       5.53\n#> 144 1214 102 6.55       7.58       5.51\n#> 145 1215 108 6.42       7.44       5.41\n#> 146 1216 101 6.56       7.58       5.54\n#> 147 1219 101 6.41       7.38       5.45\n#> 148 1220 109 6.19       7.23       5.15\n#> 149 1221  99 6.33       7.36       5.29\n#> 150 1223 116 6.24       7.32       5.16\n#> 151 1225 105 6.25       7.38       5.11\n#> 152 1226 106 6.23       7.47       4.99\n#> 153 1227 111 6.51       7.70       5.32\n#> 154 1229  97 6.74       7.92       5.56\n#> 155 1230 102 6.66       7.88       5.44\n#> 156 1231 101 6.64       7.96       5.32\n#> 157 1232  92 6.76       8.05       5.47\n#> 158 1233 105 6.27       7.64       4.91\n#> 159 1235 110 6.41       7.69       5.12\n#> 160 1236  87 6.55       7.92       5.17\n#> 161 1240 106 6.24       7.32       5.16\n#> 162 1244 108 6.46       7.57       5.34\n#> 163 1245 108 6.40       7.44       5.37\n#> 164 1246  98 6.47       7.54       5.39\n#> 165 1247 106 6.40       7.59       5.22\n#> 166 1249 108 6.29       7.38       5.19\n#> 167 1250 116 6.38       7.46       5.30\n#> 168 1251 100 6.61       7.71       5.51\n#> 169 1254 110 6.57       7.56       5.59\n#> 170 1257  94 6.46       7.57       5.35\n#> 171 1259  96 6.21       7.15       5.28\n#> 172 1262  97 5.98       6.80       5.16\n#> 173 1263 120 5.86       6.63       5.08\n#> 174 1264 103 6.15       6.88       5.42\n#> 175 1265  98 6.19       6.87       5.51\n#> 176 1267 105 6.05       6.67       5.42\n#> 177 1268 103 6.04       6.72       5.37\n#> 178 1269 100 6.01       6.72       5.30\n#> 179 1271 100 6.14       6.97       5.32\n#> 180 1275 107 6.47       7.67       5.27\n#> 181 1278 113 6.54       7.67       5.40\n#> 182 1279 101 6.65       7.67       5.62\n#> 183 1280 102 6.69       7.90       5.47\n#> 184 1283 103 6.96       8.41       5.51\n#> 185 1284 102 6.87       8.29       5.46\n#> 186 1285 105 6.56       7.80       5.31\n#> 187 1286 112 6.52       7.72       5.31\n#> 188 1287 110 6.65       7.67       5.63\n#> 189 1288 108 6.80       7.75       5.86\n#> 190 1289 107 6.72       7.83       5.61\n#> 191 1292 107 6.81       7.92       5.71\n#> 192 1294 101 6.82       8.43       5.21\n#> 193 1295 102 6.87       8.66       5.07\n#> 194 1301  95 6.31       7.98       4.65\n#> 195 1302  96 6.11       7.77       4.45\n#> 196 1307 104 5.35       6.89       3.82\n#> 197 1311  94 4.73       6.73       2.74\n#> 198 1312 100 4.88       6.30       3.45\n#> 199 1314 101 5.10       6.26       3.94\n#> 200 1315 111 5.03       6.22       3.84\n#> 201 1316 103 5.11       6.23       3.98\n#> 202 1317 102 5.03       6.09       3.96\n#> 203 1319 116 4.85       5.88       3.82\n#> 204 1321 102 4.99       5.98       4.01\n#> 205 1322 106 4.88       5.91       3.84\n#> 206 1323 124 4.82       5.96       3.68\n#> 207 1324  98 5.09       6.06       4.12\n#> 208 1326 107 4.69       5.56       3.83\n#> 209 1329 112 4.74       5.53       3.95\n#> 210 1330 108 4.89       5.82       3.97\n#> 211 1331 112 4.82       5.82       3.82\n#> 212 1332 115 4.87       5.94       3.79\n#> 213 1342 110 5.72       6.88       4.57\n#> 214 1344 102 5.78       6.85       4.71\n#> 215 1346 107 5.82       6.97       4.68\n#> 216 1347 109 5.81       6.95       4.68\n#> 217 1349 112 5.91       6.97       4.85\n#> 218 1350 101 6.02       7.03       5.00\n#> 219 1351 115 5.89       6.93       4.84\n#> 220 1356 117 6.28       7.09       5.48\n#> 221 1357 107 6.46       7.16       5.76\n#> 222 1358 110 6.54       7.24       5.84\n#> 223 1359 105 6.63       7.31       5.95\n#> 224 1360 101 6.67       7.37       5.97\n#> 225 1361 108 6.66       7.41       5.92\n#> 226 1363 107 6.76       7.53       5.99\n#> 227 1364 110 6.80       7.60       6.01\n#> 228 1366 112 6.83       7.69       5.97\n#> 229 1367  97 6.80       7.73       5.86\n#> 230 1368  88 6.71       7.59       5.83\n#> 231 1369  94 6.66       7.32       5.99\n#> 232 1370 109 6.68       7.36       6.00\n#> 233 1372 102 6.69       7.20       6.18\n#> 234 1373 110 6.64       7.13       6.15\n#> 235 1375 103 6.70       7.15       6.25\n#> 236 1376  99 6.72       7.16       6.29\n#> 237 1377 105 6.73       7.15       6.31\n#> 238 1378 104 6.57       7.09       6.06\n#> 239 1379 103 6.94       7.77       6.11\n#> 240 1380 102 6.94       7.76       6.12\n#> 241 1381 108 6.95       7.82       6.07\n#> 242 1382 105 7.18       8.11       6.26\n#> 243 1383 102 7.40       8.43       6.37\n#> 244 1384  98 7.43       8.53       6.33\n#> 245 1385  99 7.27       8.38       6.17\n#> 246 1386 105 7.19       8.34       6.03\n#> 247 1387 100 7.21       8.37       6.05\n#> 248 1388  99 7.18       8.44       5.91\n#> 249 1398 106 7.10       7.89       6.32\n#> 250 1401 101 7.13       7.86       6.40\n#> 251 1402 103 7.10       7.84       6.36\n#> 252 1403 104 7.16       7.90       6.42\n#> 253 1406 102 7.13       7.88       6.37\n#> 254 1407 101 7.12       7.88       6.36\n#> 255 1408 111 7.14       7.90       6.38\n#> 256 1409  86 7.26       8.03       6.49\n#> 257 1410 102 7.10       7.71       6.49\n#> 258 1412  93 7.05       7.68       6.43\n#> 259 1413  91 6.91       7.50       6.31\n#> 260 1415 105 6.71       7.26       6.17\n#> 261 1416 101 6.67       7.24       6.10\n#> 262 1417 109 6.56       7.18       5.94\n#> 263 1418  98 6.70       7.32       6.08\n#> 264 1419 100 6.75       7.35       6.15\n#> 265 1420 112 6.75       7.35       6.15\n#> 266 1421  99 6.84       7.41       6.26\n#> 267 1422 109 6.92       7.53       6.31\n#> 268 1423  96 6.94       7.54       6.33\n#> 269 1424 103 6.84       7.43       6.25\n#> 270 1425  98 6.91       7.51       6.31\n#> 271 1426  99 6.83       7.43       6.23\n#> 272 1427 102 6.77       7.37       6.17\n#> 273 1429  98 6.57       7.25       5.90\n#> 274 1430 108 6.45       7.14       5.76\n#> 275 1431 102 6.47       7.19       5.75\n#> 276 1432 104 6.43       7.16       5.71\n#> 277 1433  98 6.47       7.18       5.75\n#> 278 1434 113 6.41       7.09       5.72\n#> 279 1435  95 6.56       7.24       5.87\n#> 280 1436 101 6.46       7.09       5.82\n#> 281 1437 103 6.45       7.07       5.83\n#> 282 1438  99 6.47       7.08       5.86\n#> 283 1439 101 6.42       7.04       5.80\n#> 284 1440  98 6.38       7.02       5.74\n#> 285 1441 105 6.33       6.99       5.67\n#> 286 1443 107 6.20       6.91       5.49\n#> 287 1444 109 6.32       7.03       5.60\n#> 288 1446 108 6.40       7.12       5.68\n#> 289 1447 111 6.34       7.09       5.58\n#> 290 1448  96 6.23       7.12       5.35\n#> 291 1449  95 6.10       6.97       5.24\n#> 292 1450 100 6.04       6.81       5.28\n#> 293 1451 103 5.99       6.73       5.26\n#> 294 1452  92 5.95       6.69       5.21\n#> 295 1453 107 5.88       6.48       5.27\n#> 296 1454 105 5.99       6.63       5.36\n#> 297 1455  96 6.13       6.84       5.42\n#> 298 1456 106 6.09       6.85       5.33\n#> 299 1457 105 6.23       7.11       5.35\n#> 300 1458 112 6.24       7.12       5.36\n#> 301 1459 113 6.36       7.19       5.52\n#> 302 1460 111 6.52       7.32       5.72\n#> 303 1461 106 6.57       7.29       5.85\n#> 304 1462 106 6.56       7.25       5.86\n#> 305 1463 101 6.62       7.31       5.93\n#> 306 1464 104 6.56       7.26       5.86\n#> 307 1465  98 6.64       7.41       5.88\n#> 308 1466 105 6.60       7.43       5.77\n#> 309 1467 101 6.59       7.39       5.79\n#> 310 1468 101 6.58       7.44       5.71\n#> 311 1472 114 6.53       7.28       5.77\n#> 312 1474  98 6.52       7.24       5.80\n#> 313 1476 100 6.41       7.10       5.72\n#> 314 1477 110 6.34       7.03       5.65\n#> 315 1478 115 6.33       7.06       5.60\n#> 316 1479 110 6.45       7.19       5.71\n#> 317 1480 104 6.45       7.26       5.63\n#> 318 1481 106 6.38       7.26       5.51\n#> 319 1482 108 6.36       7.29       5.42\n#> 320 1483 101 6.34       7.36       5.32\n#> 321 1484  97 6.30       7.30       5.30\n#> 322 1485  94 6.17       7.17       5.18\n#> 323 1486  99 6.07       6.96       5.19\n#> 324 1487  93 6.01       6.85       5.17\n#> 325 1488 103 5.80       6.60       5.00\n#> 326 1489 100 5.73       6.55       4.91\n#> 327 1490  97 5.68       6.45       4.92\n#> 328 1491 107 5.59       6.28       4.90\n#> 329 1492 108 5.63       6.31       4.95\n#> 330 1493 100 5.62       6.32       4.92\n#> 331 1494 106 5.54       6.21       4.87\n#> 332 1495  94 5.56       6.21       4.91\n#> 333 1496  97 5.38       5.95       4.81\n#> 334 1497 108 5.31       5.80       4.82\n#> 335 1498  97 5.32       5.81       4.83\n#> 336 1499 111 5.23       5.66       4.80\n#> 337 1500  97 5.26       5.69       4.84\n#> 338 1501 103 5.21       5.62       4.80\n#> 339 1502 115 5.16       5.58       4.75\n#> 340 1503 108 5.20       5.56       4.84\n#> 341 1504 114 5.20       5.56       4.84\n#> 342 1505 106 5.37       5.81       4.93\n#> 343 1506 106 5.36       5.80       4.92\n#> 344 1507 108 5.34       5.79       4.90\n#> 345 1508 111 5.31       5.74       4.87\n#> 346 1509 102 5.23       5.70       4.76\n#> 347 1510 110 5.06       5.58       4.53\n#> 348 1511 111 5.07       5.60       4.54\n#> 349 1512 109 5.15       5.69       4.62\n#> 350 1513 110 5.06       5.66       4.47\n#> 351 1514 105 5.09       5.69       4.49\n#> 352 1515 111 5.16       5.77       4.56\n#> 353 1516 104 5.16       5.77       4.54\n#> 354 1517 106 5.16       5.76       4.56\n#> 355 1518 115 5.05       5.69       4.42\n#> 356 1519 111 5.04       5.73       4.35\n#> 357 1520 105 5.04       5.75       4.33\n#> 358 1521 107 5.00       5.70       4.31\n#> 359 1522 103 5.03       5.71       4.35\n#> 360 1523 109 4.89       5.60       4.18\n#> 361 1524 109 4.86       5.61       4.12\n#> 362 1525 104 4.95       5.68       4.22\n#> 363 1526 113 4.80       5.57       4.04\n#> 364 1527 105 5.01       5.81       4.21\n#> 365 1528 106 4.95       5.75       4.16\n#> 366 1529 107 5.06       5.89       4.23\n#> 367 1530 107 5.13       5.99       4.28\n#> 368 1531 103 5.18       6.07       4.30\n#> 369 1532 108 5.23       6.17       4.28\n#> 370 1533 111 5.31       6.31       4.31\n#> 371 1534 108 5.33       6.30       4.35\n#> 372 1535  95 5.26       6.22       4.31\n#> 373 1536 108 5.25       6.31       4.18\n#> 374 1537 108 5.24       6.31       4.18\n#> 375 1538 112 5.26       6.33       4.18\n#> 376 1539 120 5.35       6.45       4.24\n#> 377 1540 122 5.40       6.38       4.42\n#> 378 1541 109 5.51       6.38       4.64\n#> 379 1542 102 5.52       6.38       4.66\n#> 380 1543 119 5.50       6.42       4.59\n#> 381 1544 107 5.70       6.58       4.82\n#> 382 1545  97 5.74       6.66       4.81\n#> 383 1546 112 5.61       6.54       4.68\n#> 384 1547 104 5.73       6.67       4.79\n#> 385 1548 118 5.75       6.76       4.74\n#> 386 1549 117 5.89       6.81       4.96\n#> 387 1550 111 6.08       6.98       5.19\n#> 388 1551 109 6.13       6.98       5.28\n#> 389 1552 104 6.19       7.06       5.32\n#> 390 1553 119 6.13       6.99       5.27\n#> 391 1555 101 6.23       6.97       5.48\n#> 392 1556 120 6.17       6.91       5.44\n#> 393 1557  93 6.31       7.00       5.62\n#> 394 1558 111 6.17       6.77       5.58\n#> 395 1559  96 6.24       6.84       5.64\n#> 396 1560 100 6.18       6.71       5.65\n#> 397 1561 102 6.25       6.84       5.66\n#> 398 1562  99 6.29       6.92       5.65\n#> 399 1563  99 6.13       6.78       5.49\n#> 400 1564 109 6.18       6.96       5.39\n#> 401 1565 114 6.16       6.89       5.43\n#> 402 1566  97 6.28       6.94       5.63\n#> 403 1567 108 6.09       6.76       5.42\n#> 404 1568 106 6.09       6.76       5.43\n#> 405 1570 112 6.16       6.80       5.51\n#> 406 1571 108 6.35       7.04       5.67\n#> 407 1572 107 6.38       7.06       5.71\n#> 408 1574  98 6.54       7.22       5.86\n#> 409 1576 109 6.44       7.09       5.80\n#> 410 1577 100 6.45       7.10       5.81\n#> 411 1579 102 6.37       7.01       5.72\n#> 412 1580  97 6.29       6.95       5.63\n#> 413 1581 105 6.22       6.88       5.55\n#> 414 1583 108 6.41       7.18       5.65\n#> 415 1584 106 6.45       7.20       5.70\n#> 416 1585 105 6.40       7.15       5.65\n#> 417 1586 106 6.23       7.08       5.37\n#> 418 1587 105 6.21       7.08       5.35\n#> 419 1588 106 6.25       7.11       5.39\n#> 420 1589 105 6.39       7.26       5.52\n#> 421 1590 102 6.39       7.31       5.48\n#> 422 1591  94 6.32       7.24       5.39\n#> 423 1592  98 6.20       7.08       5.32\n#> 424 1593 113 6.05       6.94       5.16\n#> 425 1594  93 6.05       6.98       5.13\n#> 426 1595 111 6.05       6.91       5.19\n#> 427 1596 104 6.16       7.02       5.30\n#> 428 1597 114 6.15       7.04       5.26\n#> 429 1598 110 6.25       7.13       5.37\n#> 430 1599 103 6.22       7.15       5.29\n#> 431 1600 101 6.18       7.16       5.21\n#> 432 1601  93 6.12       7.13       5.11\n#> 433 1602 107 6.03       6.94       5.12\n#> 434 1603  99 6.15       7.06       5.24\n#> 435 1604  95 6.21       7.14       5.27\n#> 436 1605 105 6.19       7.18       5.20\n#> 437 1606 106 6.22       7.26       5.18\n#> 438 1607 110 6.21       7.24       5.19\n#> 439 1609 108 6.22       7.14       5.30\n#> 440 1610 110 6.27       7.18       5.36\n#> 441 1612  87 6.07       7.02       5.12\n#> 442 1613 106 5.78       6.60       4.95\n#> 443 1614 103 5.79       6.62       4.96\n#> 444 1615 110 5.83       6.67       4.99\n#> 445 1616 122 6.08       7.06       5.10\n#> 446 1617 107 6.20       7.09       5.31\n#> 447 1618 102 6.23       7.16       5.30\n#> 448 1619  93 6.21       7.13       5.29\n#> 449 1621 109 6.07       6.91       5.22\n#> 450 1622 106 6.09       6.92       5.26\n#> 451 1623 113 6.04       6.89       5.18\n#> 452 1624 112 6.15       7.01       5.29\n#> 453 1625  95 6.26       7.14       5.39\n#> 454 1626 100 6.15       6.95       5.35\n#> 455 1628 104 6.04       6.83       5.26\n#> 456 1629 112 6.03       6.84       5.21\n#> 457 1632 104 6.10       6.92       5.28\n#> 458 1633  98 6.09       6.91       5.27\n#> 459 1634  96 6.02       6.81       5.24\n#> 460 1635  97 5.86       6.63       5.09\n#> 461 1637 105 5.68       6.41       4.96\n#> 462 1638 105 5.63       6.37       4.90\n#> 463 1639 108 5.65       6.38       4.92\n#> 464 1640 103 5.71       6.44       4.97\n#> 465 1641 117 5.58       6.34       4.82\n#> 466 1642 114 5.64       6.42       4.86\n#> 467 1643 114 5.74       6.53       4.95\n#> 468 1644 105 5.75       6.65       4.85\n#> 469 1645  99 5.70       6.63       4.76\n#> 470 1646  87 5.66       6.51       4.81\n#> 471 1647 112 5.51       6.12       4.91\n#> 472 1649 104 5.48       6.11       4.85\n#> 473 1650 102 5.43       6.05       4.81\n#> 474 1651 107 5.35       5.96       4.74\n#> 475 1652 107 5.38       5.98       4.78\n#> 476 1653 112 5.37       5.98       4.77\n#> 477 1654 104 5.34       6.03       4.65\n#> 478 1655 104 5.32       5.98       4.66\n#> 479 1656 103 5.31       5.94       4.67\n#> 480 1658 108 5.22       5.83       4.61\n#> 481 1660 111 5.45       6.14       4.76\n#> 482 1661  93 5.53       6.22       4.84\n#> 483 1662 110 5.38       6.05       4.71\n#> 484 1663 105 5.40       6.06       4.75\n#> 485 1664 105 5.37       6.03       4.71\n#> 486 1665 112 5.24       5.91       4.57\n#> 487 1666 108 5.22       5.88       4.56\n#> 488 1667 112 5.19       5.85       4.53\n#> 489 1668 110 5.29       5.96       4.62\n#> 490 1669 104 5.41       6.12       4.70\n#> 491 1670 102 5.30       6.02       4.59\n#> 492 1671 116 5.21       5.92       4.50\n#> 493 1672 112 5.21       5.92       4.50\n#> 494 1673 105 5.29       6.00       4.58\n#> 495 1674 115 5.22       5.94       4.50\n#> 496 1675 111 5.28       6.00       4.55\n#> 497 1676 103 5.24       6.01       4.47\n#> 498 1677  96 5.17       5.93       4.41\n#> 499 1678 109 5.10       5.78       4.42\n#> 500 1679 114 5.13       5.81       4.45\n#> 501 1680 109 5.17       5.88       4.46\n#> 502 1681 110 5.16       5.90       4.42\n#> 503 1682 104 5.16       5.94       4.37\n#> 504 1683 108 5.19       5.95       4.43\n#> 505 1684 118 5.19       5.95       4.43\n#> 506 1685 106 5.27       6.03       4.52\n#> 507 1686 104 5.24       5.99       4.48\n#> 508 1687 104 5.33       6.07       4.58\n#> 509 1688 114 5.37       6.11       4.62\n#> 510 1689  97 5.49       6.23       4.75\n#> 511 1690  93 5.32       6.04       4.59\n#> 512 1691 103 5.32       6.12       4.52\n#> 513 1692 109 5.35       6.24       4.46\n#> 514 1693 107 5.39       6.29       4.49\n#> 515 1694 109 5.47       6.45       4.49\n#> 516 1695 120 5.49       6.45       4.52\n#> 517 1696 114 5.51       6.32       4.71\n#> 518 1697 111 5.53       6.27       4.79\n#> 519 1698 101 5.60       6.32       4.87\n#> 520 1699  97 5.55       6.30       4.80\n#> 521 1700 116 5.47       6.29       4.65\n#> 522 1701 112 5.58       6.33       4.82\n#> 523 1702 116 5.61       6.32       4.91\n#> 524 1703 103 5.70       6.33       5.08\n#> 525 1704 113 5.70       6.36       5.04\n#> 526 1705 109 5.82       6.47       5.18\n#> 527 1706 115 5.86       6.47       5.24\n#> 528 1707 111 6.08       6.74       5.43\n#> 529 1708 103 6.19       6.82       5.56\n#> 530 1709 106 6.12       6.76       5.49\n#> 531 1713 104 5.95       6.56       5.33\n#> 532 1714 108 6.01       6.62       5.39\n#> 533 1715 107 6.03       6.65       5.42\n#> 534 1716 110 6.01       6.65       5.37\n#> 535 1717  98 6.10       6.73       5.46\n#> 536 1718 102 6.11       6.74       5.49\n#> 537 1719 102 6.13       6.76       5.49\n#> 538 1720 114 6.05       6.69       5.41\n#> 539 1721  95 6.16       6.77       5.55\n#> 540 1722 100 6.01       6.60       5.43\n#> 541 1723 104 6.05       6.68       5.42\n#> 542 1724  99 5.95       6.59       5.30\n#> 543 1725 106 5.76       6.44       5.08\n#> 544 1726 116 5.74       6.42       5.05\n#> 545 1727 112 5.78       6.48       5.09\n#> 546 1728 104 5.89       6.59       5.19\n#> 547 1729 105 5.85       6.55       5.14\n#> 548 1731 104 6.01       6.73       5.28\n#> 549 1732 108 5.99       6.71       5.26\n#> 550 1733 107 5.95       6.69       5.21\n#> 551 1734 102 6.04       6.78       5.29\n#> 552 1735 101 6.00       6.74       5.25\n#> 553 1736 106 5.89       6.64       5.13\n#> 554 1737  93 5.94       6.69       5.18\n#> 555 1738 101 5.71       6.46       4.97\n#> 556 1739 110 5.67       6.41       4.94\n#> 557 1740 115 5.72       6.45       4.99\n#> 558 1741 109 5.82       6.53       5.11\n#> 559 1742 109 5.85       6.56       5.13\n#> 560 1743 108 5.93       6.64       5.21\n#> 561 1744  98 5.91       6.64       5.18\n#> 562 1745 105 5.90       6.61       5.18\n#> 563 1746 110 5.87       6.59       5.16\n#> 564 1747 101 5.93       6.64       5.21\n#> 565 1748  96 5.83       6.55       5.12\n#> 566 1749 100 5.83       6.56       5.10\n#> 567 1750 110 5.75       6.48       5.02\n#> 568 1751 103 5.93       6.75       5.12\n#> 569 1752 110 5.88       6.69       5.07\n#> 570 1753  96 5.94       6.74       5.14\n#> 571 1754 115 5.86       6.68       5.04\n#> 572 1755 120 5.92       6.67       5.17\n#> 573 1756 109 6.04       6.71       5.38\n#> 574 1757 112 6.08       6.74       5.42\n#> 575 1758 101 6.10       6.76       5.45\n#> 576 1759 109 6.05       6.70       5.41\n#> 577 1760 103 5.97       6.70       5.24\n#> 578 1761  89 6.00       6.72       5.29\n#> 579 1762 106 5.89       6.51       5.26\n#> 580 1763 112 5.89       6.51       5.26\n#> 581 1764  98 5.96       6.57       5.35\n#> 582 1765 106 5.82       6.43       5.21\n#> 583 1766 113 5.85       6.46       5.24\n#> 584 1767 101 6.00       6.61       5.38\n#> 585 1768 117 5.91       6.53       5.29\n#> 586 1769 105 6.04       6.62       5.46\n#> 587 1770 105 6.06       6.64       5.48\n#> 588 1771 104 6.06       6.64       5.48\n#> 589 1772 106 6.13       6.74       5.53\n#> 590 1773 100 6.10       6.71       5.49\n#> 591 1774 110 6.06       6.67       5.46\n#> 592 1775  99 6.22       6.87       5.57\n#> 593 1776 108 6.09       6.75       5.43\n#> 594 1777 104 6.02       6.71       5.32\n#> 595 1778 111 6.07       6.77       5.37\n#> 596 1779  96 6.11       6.80       5.41\n#> 597 1780 109 6.07       6.74       5.39\n#> 598 1781  91 6.06       6.74       5.38\n#> 599 1782 109 5.79       6.46       5.13\n#> 600 1783 103 5.81       6.48       5.14\n#> 601 1784 105 5.72       6.41       5.03\n#> 602 1785 108 5.66       6.37       4.96\n#> 603 1786 106 5.71       6.41       5.01\n#> 604 1787 105 5.67       6.39       4.96\n#> 605 1788 109 5.66       6.37       4.95\n#> 606 1789 106 5.69       6.41       4.98\n#> 607 1790 119 5.79       6.52       5.07\n#> 608 1791  99 5.88       6.60       5.16\n#> 609 1792 101 5.85       6.54       5.17\n#> 610 1793 106 5.81       6.48       5.13\n#> 611 1794 104 5.79       6.47       5.11\n#> 612 1795 113 5.70       6.41       4.99\n#> 613 1796 103 5.74       6.47       5.00\n#> 614 1797  97 5.76       6.47       5.05\n#> 615 1798 110 5.62       6.30       4.93\n#> 616 1799 103 5.60       6.32       4.88\n#> 617 1800 103 5.58       6.28       4.88\n#> 618 1801 105 5.48       6.20       4.76\n#> 619 1802  96 5.41       6.15       4.66\n#> 620 1803 110 5.31       5.98       4.64\n#> 621 1804 104 5.30       5.99       4.61\n#> 622 1805  93 5.23       5.92       4.53\n#> 623 1806 114 5.05       5.64       4.46\n#> 624 1807 116 5.07       5.68       4.47\n#> 625 1808  98 5.14       5.76       4.51\n#> 626 1809 107 4.94       5.57       4.32\n#> 627 1810 100 4.96       5.57       4.36\n#> 628 1811 110 4.90       5.45       4.35\n#> 629 1812 120 5.03       5.62       4.44\n#> 630 1813 107 5.15       5.71       4.58\n#> 631 1814 113 5.15       5.71       4.60\n#> 632 1815 111 5.25       5.82       4.69\n#> 633 1816 103 5.37       5.96       4.79\n#> 634 1817 110 5.34       5.92       4.76\n#> 635 1818 106 5.41       6.00       4.82\n#> 636 1819 106 5.46       6.09       4.84\n#> 637 1820  95 5.44       6.06       4.82\n#> 638 1821 109 5.37       6.03       4.72\n#> 639 1822 102 5.34       5.97       4.71\n#> 640 1823 106 5.34       6.02       4.65\n#> 641 1824 108 5.38       6.11       4.64\n#> 642 1825 114 5.44       6.22       4.66\n#> 643 1826 109 5.47       6.17       4.77\n#> 644 1827 100 5.48       6.16       4.79\n#> 645 1828 113 5.44       6.18       4.71\n#> 646 1829 112 5.47       6.14       4.80\n#> 647 1830 105 5.46       6.08       4.84\n#> 648 1831 114 5.50       6.18       4.83\n#> 649 1832 113 5.48       6.10       4.86\n#> 650 1833 107 5.44       6.05       4.82\n#> 651 1834 111 5.45       6.07       4.84\n#> 652 1835 112 5.58       6.23       4.94\n#> 653 1836 112 5.67       6.32       5.03\n#> 654 1837 111 5.77       6.41       5.14\n#> 655 1838 109 5.84       6.47       5.21\n#> 656 1839 120 5.87       6.50       5.25\n#> 657 1840 105 6.06       6.64       5.48\n#> 658 1841 107 6.07       6.65       5.49\n#> 659 1842  96 6.08       6.69       5.48\n#> 660 1843 106 6.01       6.58       5.45\n#> 661 1844 106 6.00       6.57       5.43\n#> 662 1845 102 6.03       6.60       5.46\n#> 663 1846  98 5.94       6.52       5.36\n#> 664 1847 107 5.89       6.45       5.33\n#> 665 1848 102 5.95       6.52       5.38\n#> 666 1849 100 5.96       6.54       5.37\n#> 667 1850 109 5.86       6.44       5.28\n#> 668 1851 102 5.83       6.41       5.26\n#> 669 1852 113 5.88       6.50       5.26\n#> 670 1853 102 5.93       6.51       5.35\n#> 671 1854 102 5.86       6.44       5.28\n#> 672 1855 101 5.76       6.35       5.17\n#> 673 1856 111 5.69       6.28       5.11\n#> 674 1857 108 5.71       6.29       5.14\n#> 675 1858 104 5.72       6.29       5.15\n#> 676 1859 110 5.67       6.25       5.10\n#> 677 1860 113 5.79       6.38       5.20\n#> 678 1861 100 5.92       6.50       5.33\n#> 679 1862 117 5.78       6.39       5.17\n#> 680 1863 118 5.86       6.46       5.25\n#> 681 1864 105 6.04       6.64       5.45\n#> 682 1865  97 6.02       6.65       5.40\n#> 683 1866 102 5.94       6.51       5.37\n#> 684 1867 101 5.90       6.45       5.35\n#> 685 1868 104 5.83       6.37       5.30\n#> 686 1869 105 5.88       6.42       5.35\n#> 687 1870  99 5.91       6.45       5.38\n#> 688 1871 104 5.88       6.42       5.34\n#> 689 1873 104 6.01       6.62       5.41\n#> 690 1874 107 5.97       6.57       5.38\n#> 691 1875 103 5.99       6.58       5.41\n#> 692 1876 111 5.97       6.57       5.38\n#> 693 1877 104 6.05       6.61       5.48\n#> 694 1878 101 6.07       6.65       5.48\n#> 695 1879 102 6.00       6.59       5.41\n#> 696 1880 110 5.98       6.58       5.37\n#> 697 1881 112 6.09       6.71       5.46\n#> 698 1882  97 6.15       6.72       5.58\n#> 699 1883 107 6.09       6.69       5.49\n#> 700 1884 109 6.14       6.74       5.54\n#> 701 1885 113 6.20       6.79       5.62\n#> 702 1886 108 6.22       6.72       5.71\n#> 703 1887 109 6.27       6.76       5.77\n#> 704 1888 107 6.31       6.78       5.84\n#> 705 1889 109 6.35       6.83       5.87\n#> 706 1890  98 6.40       6.86       5.93\n#> 707 1891  99 6.34       6.81       5.87\n#> 708 1892 115 6.33       6.82       5.85\n#> 709 1893 109 6.51       6.94       6.08\n#> 710 1894  98 6.48       6.92       6.04\n#> 711 1896 106 6.33       6.76       5.90\n#> 712 1897 106 6.32       6.77       5.86\n#> 713 1898 108 6.30       6.79       5.82\n#> 714 1899  99 6.35       6.85       5.86\n#> 715 1900 102 6.40       6.86       5.95\n#> 716 1901 102 6.37       6.83       5.92\n#> 717 1902  98 6.34       6.80       5.89\n#> 718 1903  99 6.27       6.70       5.84\n#> 719 1904 108 6.22       6.63       5.81\n#> 720 1905 105 6.28       6.69       5.87\n#> 721 1906 105 6.22       6.67       5.78\n#> 722 1907 103 6.27       6.72       5.82\n#> 723 1908 102 6.32       6.77       5.86\n#> 724 1909 108 6.30       6.75       5.84\n#> 725 1910 104 6.28       6.75       5.81\n#> 726 1911  98 6.32       6.79       5.84\n#> 727 1912 105 6.32       6.83       5.82\n#> 728 1913 103 6.36       6.87       5.85\n#> 729 1914 102 6.38       6.92       5.85\n#> 730 1915 102 6.38       6.94       5.81\n#> 731 1916 112 6.42       7.05       5.79\n#> 732 1917 103 6.47       7.01       5.93\n#> 733 1918 104 6.47       7.03       5.92\n#> 734 1920 104 6.50       7.04       5.96\n#> 735 1922 101 6.55       7.09       6.02\n#> 736 1923  97 6.55       7.11       6.00\n#> 737 1924 111 6.55       7.17       5.93\n#> 738 1925 108 6.68       7.27       6.08\n#> 739 1926 108 6.76       7.34       6.18\n#> 740 1927 107 6.80       7.33       6.27\n#> 741 1928 107 6.88       7.39       6.36\n#> 742 1929 103 7.02       7.60       6.45\n#> 743 1930  95 7.10       7.71       6.49\n#> 744 1931 105 7.06       7.72       6.40\n#> 745 1932 105 7.09       7.71       6.46\n#> 746 1933 106 7.12       7.72       6.53\n#> 747 1934 104 7.19       7.76       6.62\n#> 748 1935 102 7.13       7.67       6.60\n#> 749 1936 111 7.18       7.73       6.63\n#> 750 1937 100 7.31       7.80       6.83\n#> 751 1938  98 7.32       7.82       6.83\n#> 752 1939 104 7.29       7.79       6.79\n#> 753 1940 110 7.27       7.75       6.79\n#> 754 1941 100 7.39       7.82       6.95\n#> 755 1942  96 7.40       7.83       6.96\n#> 756 1943 101 7.38       7.82       6.94\n#> 757 1944 100 7.40       7.84       6.95\n#> 758 1946  97 7.41       7.84       6.98\n#> 759 1947 107 7.45       7.94       6.97\n#> 760 1948 102 7.48       7.92       7.05\n#> 761 1949 107 7.53       7.97       7.09\n#> 762 1950  99 7.58       7.98       7.18\n#> 763 1951  98 7.58       7.98       7.18\n#> 764 1952 105 7.63       8.06       7.20\n#> 765 1953 101 7.68       8.09       7.27\n#> 766 1954  98 7.61       8.06       7.16\n#> 767 1955  97 7.60       8.05       7.15\n#> 768 1956  99 7.55       8.00       7.10\n#> 769 1957 103 7.59       8.05       7.14\n#> 770 1958  99 7.56       8.05       7.08\n#> 771 1959  92 7.63       8.11       7.14\n#> 772 1960  95 7.67       8.22       7.12\n#> 773 1961  99 7.65       8.22       7.07\n#> 774 1962 102 7.70       8.31       7.09\n#> 775 1963 101 7.75       8.35       7.14\n#> 776 1964  99 7.76       8.36       7.17\n#> 777 1965 110 7.76       8.35       7.17\n#> 778 1966  97 7.83       8.32       7.33\n#> 779 1967  97 7.83       8.33       7.32\n#> 780 1968  99 7.89       8.46       7.32\n#> 781 1969 101 7.94       8.54       7.34\n#> 782 1970 107 7.95       8.52       7.37\n#> 783 1971  98 8.05       8.57       7.53\n#> 784 1972  99 8.12       8.69       7.55\n#> 785 1973  97 8.13       8.69       7.57\n#> 786 1974  99 8.18       8.78       7.58\n#> 787 1975 100 8.18       8.76       7.60\n#> 788 1976  99 8.20       8.77       7.63\n#> 789 1977  93 8.22       8.78       7.66\n#> 790 1978 104 8.20       8.78       7.61\n#> 791 1979  97 8.28       8.83       7.73\n#> 792 1980 102 8.30       8.86       7.74\n#> 793 1981  99   NA         NA         NA\n#> 794 1982  93   NA         NA         NA\n#> 795 1983  99   NA         NA         NA\n#> 796 1984 109   NA         NA         NA\n#> 797 1985  99   NA         NA         NA\n#> 798 1986 102   NA         NA         NA\n#> 799 1987  95   NA         NA         NA\n#> 800 1988 106   NA         NA         NA\n#> 801 1989  93   NA         NA         NA\n#> 802 1990  88   NA         NA         NA\n#> 803 1991  97   NA         NA         NA\n#> 804 1992  94   NA         NA         NA\n#> 805 1993  97   NA         NA         NA\n#> 806 1994  99   NA         NA         NA\n#> 807 1995  99   NA         NA         NA\n#> 808 1996 103   NA         NA         NA\n#> 809 1997  97   NA         NA         NA\n#> 810 1998  91   NA         NA         NA\n#> 811 1999  94   NA         NA         NA\n#> 812 2000 100   NA         NA         NA\n#> 813 2001  96   NA         NA         NA\n#> 814 2002  91   NA         NA         NA\n#> 815 2003  98   NA         NA         NA\n#> 816 2004  92   NA         NA         NA\n#> 817 2005  99   NA         NA         NA\n#> 818 2006  98   NA         NA         NA\n#> 819 2007  97   NA         NA         NA\n#> 820 2008  95   NA         NA         NA\n#> 821 2009  95   NA         NA         NA\n#> 822 2010  95   NA         NA         NA\n#> 823 2011  99   NA         NA         NA\n#> 824 2012 101   NA         NA         NA\n#> 825 2013  93   NA         NA         NA\n#> 826 2014  94   NA         NA         NA\n#> 827 2015  93   NA         NA         NA\n```\n:::\n:::\n\n\nBut it worked and resulted in the same 827 records.\n\n##### Choice of knots\n\nNow we start with the spline procedure as mentioned in\n@prp-procedure-for-generating-b-splines with choosing number and\nlocation of knots:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-choose-knots-b lst-cap=\"Choose the knots that serve as pivots for the spline: tiyverse version\"}\nnum_knots15_b <- 15\nknot_list15_b <- quantile(d5_b$year, \n          probs = seq(from = 0, to = 1, length.out = num_knots15_b))\nknot_list15_b\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>        0% 7.142857% 14.28571% 21.42857% 28.57143% 35.71429% 42.85714%       50% \n#>       812      1036      1174      1269      1377      1454      1518      1583 \n#> 57.14286% 64.28571% 71.42857% 78.57143% 85.71429% 92.85714%      100% \n#>      1650      1714      1774      1833      1893      1956      2015\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-chosen-knots-b lst-cap=\"Scatterplot with chosen knots: tiyverse version\"}\nd5_b %>% \n  ggplot(aes(x = year, y = doy)) +\n  geom_vline(xintercept = knot_list15_b, \n             color = \"white\", alpha = 1/2) +\n  geom_point(color = \"#ffb7c5\", alpha = 1/2) +\n  theme_bw() +\n  theme(panel.background = element_rect(fill = \"#4f455c\"),\n        panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![Show scatterplot with chosen knots: tiyverse version](04-geocentric-models_files/figure-html/fig-chosen-knots-b-1.png){#fig-chosen-knots-b width=672}\n:::\n:::\n\n\n##### Choice of polynomial degree\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-compute-b-spline-matrix-b lst-cap=\"Compute the B-spline basis matrix for a cubic spline (degree 3): tidyverse version\"}\nB_b <- splines::bs(d5_b$year,\n        knots = knot_list15_b[-c(1, num_knots15_b)], \n        degree = 3, \n        intercept = TRUE)\n```\n:::\n\n\nLook closely at McElreath's tricky `knot_list[-c(1, num_knots)]` code.\nWhereas `knot_list` contains 15 ordered `year` values, McElreath shaved\noff the first and last `year` values with `knot_list[-c(1, num_knots)]`,\nleaving 13. This is because, by default, the `bs()` function places\nknots at the boundaries. Since the first and 15^th^ values in\n`knot_list15_b` were boundary values for `year`, we removed them to\navoid redundancies. We can confirm this with the code, below.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-show-data-structure-b lst-cap=\"Show data structure to verify that `splines::bs()` adds the boundary knots\"}\nB_b %>% str()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  'bs' num [1:827, 1:17] 1 0.96 0.767 0.563 0.545 ...\n#>  - attr(*, \"dimnames\")=List of 2\n#>   ..$ : NULL\n#>   ..$ : chr [1:17] \"1\" \"2\" \"3\" \"4\" ...\n#>  - attr(*, \"degree\")= int 3\n#>  - attr(*, \"knots\")= Named num [1:13] 1036 1174 1269 1377 1454 ...\n#>   ..- attr(*, \"names\")= chr [1:13] \"7.142857%\" \"14.28571%\" \"21.42857%\" \"28.57143%\" ...\n#>  - attr(*, \"Boundary.knots\")= int [1:2] 812 2015\n#>  - attr(*, \"intercept\")= logi TRUE\n```\n:::\n:::\n\n\nLook at the second to last line,\n`- attr(*, \"Boundary.knots\")= int [1:2] 812 2015`. Those default\n`\"Boundary.knots\"` are the same as `knot_list15_b[c(1, num_knots)]`.\nLet's confirm.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-boundary-knots lst-cap=\"Boundary knots\"}\nknot_list15_b[c(1, num_knots15_b)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>   0% 100% \n#>  812 2015\n```\n:::\n:::\n\n\nBy the `degree = 3` argument, we indicated we wanted a cubic spline.\nMcElreath used `degree = 1` for Figure 4.12. For reasons I'm not\nprepared to get into, here, {**splines**} don't always include intercept\nparameters. Indeed, the `splines::bs()` default is `intercept = FALSE`.\nMcElreath's code indicated he wanted to fit a B-spline that included an\nintercept. Thus: `intercept = TRUE`.\n\nHere's how we might make our version of the top panel of Figure 4.13.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-basis-functions-b lst-cap=\"Basis functions of a cubic spline with 15 knots\"}\n# wrangle a bit\nb_b <-\n  B_b %>% \n  data.frame() %>% \n  set_names(str_c(0, 1:9), 10:17) %>%  \n  bind_cols(select(d5_b, year)) %>% \n  pivot_longer(-year,\n               names_to = \"bias_function\",\n               values_to = \"bias\")\n\n# plot\nb_b %>% \n  ggplot(aes(x = year, y = bias, group = bias_function)) +\n  geom_vline(xintercept = knot_list15_b, color = \"white\", alpha = 1/2) +\n  geom_line(color = \"#ffb7c5\", alpha = 1/2, linewidth = 1.5) +\n  ylab(\"bias value\") +\n  theme_bw() +\n  theme(panel.background = element_rect(fill = \"#4f455c\"),\n        panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![Basis functions of a cubic spline with 15 knots](04-geocentric-models_files/figure-html/fig-basis-functions-b-1.png){#fig-basis-functions-b width=672}\n:::\n:::\n\n\nTo elucidate what's going on in that plot, we might break it up with\n`ggplot2::facet_wrap()`.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-facet-basis-functions-b lst-cap=\"Basis functions of a cubic spline with 15 knots broken up in different facets\"}\nb_b %>% \n  mutate(bias_function = str_c(\"bias function \", \n                               bias_function)) %>% \n  ggplot(aes(x = year, y = bias)) +\n  geom_vline(xintercept = knot_list15_b, \n             color = \"white\", alpha = 1/2) +\n  geom_line(color = \"#ffb7c5\", linewidth = 1.5) +\n  ylab(\"bias value\") +\n  theme_bw() +\n  theme(panel.background = element_rect(fill = \"#4f455c\"),\n        panel.grid = element_blank(),\n        strip.background = element_rect(fill = scales::alpha(\"#ffb7c5\", .25), color = \"transparent\"),\n        strip.text = element_text(size = 8, margin = margin(0.1, 0, 0.1, 0, \"cm\"))) +\n  facet_wrap(~ bias_function, ncol = 1)\n```\n\n::: {.cell-output-display}\n![Basis functions of a cubic spline with 15 knots broken up in different facets](04-geocentric-models_files/figure-html/fig-facet-basis-functions-b-1.png){#fig-facet-basis-functions-b width=672}\n:::\n:::\n\n\n##### Parameter weights for each basis function\n\nTo get the parameter weights for each basis function, we need to\nactually define the model and make it run. The model is just a linear\nregression. The synthetic basis functions do all the work. We'll use\neach column of the matrix `B2_b` as a variable. We'll also have an\nintercept to capture the average blossom day. This will make it easier\nto define priors on the basis weights, because then we can just conceive\nof each as a deviation from the intercept.\n\nOur model will follow the form:\n\n------------------------------------------------------------------------\n\n::: {#def-spines-blossom-model}\n$$ \\begin{align*}\n\\text{day\\_in\\_year}_i \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i  = \\alpha + {\\sum_{k=1}^K w_k B_{k, i}} \\\\\n\\alpha \\sim \\operatorname{Normal}(100, 10) \\\\\nw_j \\sim \\operatorname{Normal}(0, 10) \\\\\n\\sigma \\sim \\operatorname{Exponential}(1)\n\\end{align*}\n$$ {#eq-spines-blossom-model}\n\nwhere $\\alpha$ is the intercept, $B_{k, i}$ is the value of the\n$k^\\text{th}$ bias function on the $i^\\text{th}$ row of the data, and\n$w_k$ is the estimated regression weight for the corresponding\n$k^\\text{th}$ bias function.\n\nAs for the new parameter type for $\\sigma$, the exponential distribution\nis controlled by a single parameter, $\\lambda$, which is also called the\n*rate*. As it turns out, the mean of the exponential distribution is the\ninverse of the rate, $1 / \\lambda$.\n:::\n\n------------------------------------------------------------------------\n\nWe are going to use the `dexp()` function to get a sense of what that\nprior looks like.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-exponential-prior-b lst-cap=\"Using the density of the exponential distribution to show the prior\"}\ntibble(x = seq(from = 0, to = 10, by = 0.1)) %>% \n  mutate(d = dexp(x, rate = 1)) %>% \n  \n  ggplot(aes(x = x, y = d)) +\n  geom_area(fill = \"#ffb7c5\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme_bw() +\n  theme(panel.background = element_rect(fill = \"#4f455c\"),\n        panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![Using the density of the exponential distribution `dexp()` to show the prior](04-geocentric-models_files/figure-html/fig-exponential-prior-b-1.png){#fig-exponential-prior-b width=384}\n:::\n:::\n\n\n> We'll use exponential priors for the rest of the book, in place of\n> uniform priors. It is much more common to have a sense of the average\n> deviation than of the maximum. (McElreath)\n\nBefore fitting this model in {**brms**}, we will take a minor detour on\nthe data structure. In his R code 4.76 (@lst-fit-model-m4.7), McElreath\ndefined his data in a list, `list( D=d5_a$doy , B = B_a)`. Our approach\nwill be a little different. Here, we'll add the `B_b` matrix to our\n`d5_b` data frame and name the results as `d6_b`.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-add-matrix-to-df-b lst-cap=\"Add B-splines matrix for 15 knots to the data frame by creating a new data frame\"}\nd6_b <-\n  d5_b %>% \n  mutate(B = B_b) \n\n# take a look at the structure of the new data frame d6_b\nd6_b %>% glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Rows: 827\n#> Columns: 6\n#> $ year       <int> 812, 815, 831, 851, 853, 864, 866, 869, 889, 891, 892, 894,…\n#> $ doy        <int> 92, 105, 96, 108, 104, 100, 106, 95, 104, 109, 108, 106, 10…\n#> $ temp       <dbl> NA, NA, NA, 7.38, NA, 6.42, 6.44, NA, 6.83, 6.98, 7.11, 6.9…\n#> $ temp_upper <dbl> NA, NA, NA, 12.10, NA, 8.69, 8.11, NA, 8.48, 8.96, 9.11, 8.…\n#> $ temp_lower <dbl> NA, NA, NA, 2.66, NA, 4.14, 4.77, NA, 5.19, 5.00, 5.11, 5.5…\n#> $ B          <bs[,17]> <bs[26 x 17]>\n```\n:::\n:::\n\n\nIn the `d6_b` data, columns `year` through `temp_lower` are all standard\ndata columns. The `B` column is a *matrix column*, which contains the\nsame number of rows as the others, but also smuggled in 17 columns\n*within* that column. Each of those 17 columns corresponds to one of our\nsynthetic $B_{k}$ variables. The advantage of such a data structure is\nwe can simply define our `formula` argument as $doy \\sim 1 + B$, where\n`B` is a stand-in for `B.1 + B.2 + ... + B.17`.\n\nHere's how we fit the model:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fit-model-b4.8 lst-cap=\"Fit model b4.8\"}\nb4.8 <- \n  brms::brm(data = d6_b,\n      family = gaussian,\n      doy ~ 1 + B,\n      prior = c(brms::prior(normal(100, 10), class = Intercept),\n                brms::prior(normal(0, 10), class = b),\n                brms::prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.08\")\n```\n:::\n\n\nHere is the model summary:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-summarize-model-b4.8 lst-cap=\"Summarize model b4.8\"}\nbrms:::print.brmsfit(b4.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: doy ~ 1 + B \n#>    Data: d6_b (Number of observations: 827) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   103.48      2.44    98.62   108.27 1.00      590      932\n#> B1           -3.14      3.89   -10.68     4.52 1.00     1780     2472\n#> B2           -0.92      4.02    -8.63     6.92 1.00     1331     1862\n#> B3           -1.25      3.61    -8.20     6.12 1.00     1233     1842\n#> B4            4.73      3.02    -1.27    10.59 1.00      875     1345\n#> B5           -0.98      2.93    -6.63     4.90 1.00      792     1394\n#> B6            4.18      2.96    -1.66    10.07 1.00      883     1470\n#> B7           -5.44      2.84   -11.00     0.20 1.00      820     1325\n#> B8            7.71      2.88     2.06    13.19 1.00      719     1430\n#> B9           -1.13      2.94    -6.81     4.63 1.00      812     1371\n#> B10           2.87      2.94    -2.95     8.58 1.00      851     1393\n#> B11           4.53      2.96    -1.15    10.53 1.00      820     1265\n#> B12          -0.31      2.95    -6.20     5.45 1.00      805     1411\n#> B13           5.43      2.98    -0.34    11.27 1.00      849     1315\n#> B14           0.55      3.02    -5.38     6.39 1.00      953     1506\n#> B15          -0.90      3.38    -7.52     5.78 1.00     1012     1742\n#> B16          -7.08      3.42   -13.99    -0.42 1.00     1116     1982\n#> B17          -7.79      3.29   -14.37    -1.20 1.00     1074     1763\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     5.95      0.15     5.66     6.25 1.00     5900     2573\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\nIn @lst-summarize-model-b4.8 I have used this time the full call for the\ngeneric function of `print()`. Just using `print()` would have been\nenough, but I wanted to check if I have understood the concept of a\ngeneric S3 method. The help file pf `print()` says:\n\n> It is a generic function which means that new printing methods can be\n> easily added for new classes.\n\n::: callout-note\nTODO: Read [S3 Chapter](https://adv-r.hadley.nz/s3.html) of Advanced R\n:::\n\nLook at that. Each of the 17 columns in our `B` matrix was assigned its\nown parameter. If you fit this model using McElreath's rethinking code,\nyou'll see the results are very similar. Anyway, McElreath's comments\nare in line with the general consensus on spline modes: the parameter\nestimates are very difficult to interpret directly. It's often easier to\njust plot the results.\n\nFirst we'll use `brms::as_draws_df()` to transform `d6_b` to a `draw`\nobject so that it can processed easier by the {**posterior**} package.\n(??)\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-transform-to-draw-objects lst-cap=\"Transform data of model b4.8 to draw objects\"}\npost6_b <- brms::as_draws_df(b4.8)\n\nglimpse(post6_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Rows: 4,000\n#> Columns: 24\n#> $ b_Intercept <dbl> 100.6156, 100.4937, 103.1678, 102.5159, 101.5559, 101.7562…\n#> $ b_B1        <dbl> 2.90164141, 3.32435081, -2.70015340, -0.02253457, -1.97050…\n#> $ b_B2        <dbl> -2.1067230, -0.5311967, 0.6794133, -2.8772090, 3.4878935, …\n#> $ b_B3        <dbl> 2.9992344, 6.5131175, 0.8860770, 1.2321642, -1.3893031, 1.…\n#> $ b_B4        <dbl> 6.1257337, 4.9927835, 5.3287564, 4.6657258, 7.8084901, 4.9…\n#> $ b_B5        <dbl> 1.16796045, 2.69443262, -1.14483788, -0.87995893, 3.187535…\n#> $ b_B6        <dbl> 9.5029937, 5.2401181, 4.1763796, 7.3521813, 3.7853630, 7.5…\n#> $ b_B7        <dbl> -3.8974486, -2.5350548, -3.8448680, -1.7500613, -3.5003478…\n#> $ b_B8        <dbl> 10.234376, 12.795946, 5.555182, 7.698008, 11.170412, 8.328…\n#> $ b_B9        <dbl> -1.516545385, 0.965091340, 3.329924529, 0.450062993, 1.554…\n#> $ b_B10       <dbl> 6.48286406, 6.06286271, 0.64513422, 4.83968514, 3.35499895…\n#> $ b_B11       <dbl> 7.79660448, 7.56857405, 5.66668708, 3.36413271, 9.02955370…\n#> $ b_B12       <dbl> 1.8447173, -0.7395718, 1.8709336, 1.6117838, -0.9294026, 2…\n#> $ b_B13       <dbl> 10.26433473, 10.88433479, 4.55267435, 8.00798110, 6.444273…\n#> $ b_B14       <dbl> -1.8069079, -0.8779897, 5.5164497, -0.4733169, 7.1190998, …\n#> $ b_B15       <dbl> 9.7734694, 8.8519378, -4.2358415, -1.7802159, 0.1933067, 0…\n#> $ b_B16       <dbl> -9.2641966, -14.0515369, -3.1279428, 0.1699908, -7.1740999…\n#> $ b_B17       <dbl> -0.007993781, -1.342456078, -13.852015496, -9.733215118, -…\n#> $ sigma       <dbl> 5.639404, 5.677425, 6.229067, 6.289891, 6.191738, 5.977885…\n#> $ lprior      <dbl> -67.06532, -67.63332, -66.43315, -66.13359, -66.75463, -65…\n#> $ lp__        <dbl> -2723.712, -2724.698, -2719.803, -2721.983, -2720.532, -27…\n#> $ .chain      <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n#> $ .iteration  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n#> $ .draw       <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,…\n```\n:::\n:::\n\n\nWith a little wrangling, we can use summary information from `post6_b`\nto make our version of the middle panel of Figure 4.13.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-basis-fun-weighted-b lst-cap=\"Weight each basis function by its corresponding parameter\"}\npost6_b %>% \n  select(b_B1:b_B17) %>% \n  set_names(c(str_c(0, 1:9), 10:17)) %>% \n  pivot_longer(everything(), names_to = \"bias_function\") %>% \n  group_by(bias_function) %>% \n  summarise(weight = mean(value)) %>% \n  full_join(b_b, by = \"bias_function\") %>% \n  \n  # plot\n  ggplot(aes(x = year, y = bias * weight, group = bias_function)) +\n  geom_vline(xintercept = knot_list15_b, color = \"white\", alpha = 1/2) +\n  geom_line(color = \"#ffb7c5\", alpha = 1/2, linewidth = 1.5) +\n  theme_bw() +\n  theme(panel.background = element_rect(fill = \"#4f455c\"),\n        panel.grid = element_blank()) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n```\n:::\n\n::: {.cell-output-display}\n![Each basis function weighted by its corresponding parameter](04-geocentric-models_files/figure-html/fig-basis-fun-weighted-b-1.png){#fig-basis-fun-weighted-b width=672}\n:::\n:::\n\n\nIn case you missed it, the main action in the {**ggplot2**} code was\n`y = bias * weight`, where we defined the $y$-axis as the product of\n`bias` and `weight`. This is fulfillment of the $w_k B_{k, i}$ parts of\nthe model.\n\nNow here's how we might use `brms:::fitted.brmsfit()` to get the\n*expected values of the posterior predictive distribution* to make the\nlower plot of Figure 4.13.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-post-pred-dist-b lst-cap=\"Expected values of the posterior predictive distribution\"}\nf <- brms:::fitted.brmsfit(b4.8)\n\nf %>% \n  data.frame() %>% \n  bind_cols(d6_b) %>% \n  \n  ggplot(aes(x = year, y = doy, ymin = Q2.5, ymax = Q97.5)) + \n  geom_vline(xintercept = knot_list15_b, color = \"white\", alpha = 1/2) +\n  geom_hline(yintercept = brms::fixef(b4.8)[1, 1], color = \"white\", linetype = 2) +\n  geom_point(color = \"#ffb7c5\", alpha = 1/2) +\n  geom_ribbon(fill = \"white\", alpha = 2/3) +\n  labs(x = \"year\",\n       y = \"day in year\") +\n  theme_bw() +\n  theme(panel.background = element_rect(fill = \"#4f455c\"),\n        panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![Expected values of the posterior predictive distribution](04-geocentric-models_files/figure-html/fig-post-pred-dist-b-1.png){#fig-post-pred-dist-b width=672}\n:::\n:::\n\n\nIf it wasn't clear, the dashed horizontal line intersecting a little\nabove 100 on the $y$-axis is the posterior mean for the intercept.\n\n##### Model with five knots\n\nNow let's use our skills to remake the simpler model expressed in Figure\n4.12. This model, recall, is based on 5 knots.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fit-5-knot-model-b lst-cap=\"Fit model with only 5 knots\"}\n# redo the `B` splines\nnum_knots5_b <- 5\nknot_list5_b <- quantile(d5_b$year, \n             probs = seq(from = 0, to = 1, length.out = num_knots5_b))\n\nB5_b <- splines::bs(d5_b$year,\n        knots = knot_list5_b[-c(1, num_knots5_b)], \n        # this makes the splines liner rater than cubic\n        degree = 1, \n        intercept = TRUE)\n\n# define a new data frame (d7_b)\nd7_b <- \n  d5_b %>% \n  mutate(B = B5_b)\n\nb4.9 <- \n  brms::brm(data = d7_b,\n      family = gaussian,\n      formula = doy ~ 1 + B,\n      prior = c(brms::prior(normal(100, 10), class = Intercept),\n                brms::prior(normal(0, 10), class = b),\n                brms::prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.09\")\n```\n:::\n\n\nReview the new model summary.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-print-5-knot-model-b lst-cap=\"Print summary of 5 knot model\"}\nprint(b4.9)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: doy ~ 1 + B \n#>    Data: d7_b (Number of observations: 827) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   103.65      4.41    94.98   112.25 1.00      924     1112\n#> B1           -0.52      4.46    -9.21     8.23 1.00      946     1346\n#> B2            1.30      4.45    -7.37     9.91 1.01      933     1202\n#> B3            1.36      4.42    -7.23     9.98 1.01      931     1137\n#> B4            3.79      4.45    -4.93    12.40 1.01      938     1205\n#> B5           -5.77      4.46   -14.40     2.95 1.00      940     1272\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     6.10      0.15     5.82     6.40 1.00     2600     2164\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\nHere we do all the work in bulk to make and save the three subplots for\nFigure 4.12.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-prepare-3-subplots-figure-4.12-b lst-cap=\"Prepare three subplots for Figure 4.12\"}\n## top\n## wrangle a bit\n\nb5_b <-\n  invoke(data.frame, d7_b) %>% \n  pivot_longer(starts_with(\"B\"),\n               names_to = \"bias_function\",\n               values_to = \"bias\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning: `invoke()` was deprecated in purrr 1.0.0.\n#> ℹ Please use `exec()` instead.\n```\n:::\n\n```{.r .cell-code #lst-prepare-3-subplots-figure-4.12-b lst-cap=\"Prepare three subplots for Figure 4.12\"}\n# plot\np10 <- \n  b5_b %>% \n  ggplot(aes(x = year, y = bias, group = bias_function)) +\n  geom_vline(xintercept = knot_list5_b, color = \"white\", alpha = 1/2) +\n  geom_line(color = \"#ffb7c5\", alpha = 1/2, linewidth = 1.5) +\n  scale_x_continuous(NULL, breaks = NULL) +\n  ylab(\"bias value\")\n\n## middle\n# wrangle\np11 <-\n  brms::as_draws_df(b4.9) %>% \n  select(b_B1:b_B5) %>% \n  set_names(str_c(\"B.\", 1:5)) %>% \n  pivot_longer(everything(), names_to = \"bias_function\") %>% \n  group_by(bias_function) %>% \n  summarise(weight = mean(value)) %>% \n  full_join(b5_b, by = \"bias_function\") %>% \n  \n  # plot\n  ggplot(aes(x = year, y = bias * weight, group = bias_function)) +\n  geom_vline(xintercept = knot_list5_b, color = \"white\", alpha = 1/2) +\n  geom_line(color = \"#ffb7c5\", alpha = 1/2, linewidth = 1.5) +\n  scale_x_continuous(NULL, breaks = NULL)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n```\n:::\n\n```{.r .cell-code #lst-prepare-3-subplots-figure-4.12-b lst-cap=\"Prepare three subplots for Figure 4.12\"}\n## bottom\n# wrangle\nf2_b <- fitted(b4.9)\n\np12 <-\n  f2_b %>% \n  data.frame() %>% \n  bind_cols(d7_b) %>% \n  \n  # plot\n  ggplot(aes(x = year, y = doy, ymin = Q2.5, ymax = Q97.5)) + \n  geom_vline(xintercept = knot_list5_b, color = \"white\", alpha = 1/2) +\n  geom_hline(yintercept = brms::fixef(b4.9)[1, 1], color = \"white\", linetype = 2) +\n  geom_point(color = \"#ffb7c5\", alpha = 1/2) +\n  geom_ribbon(fill = \"white\", alpha = 2/3) +\n  labs(x = \"year\",\n       y = \"day in year\")\n```\n:::\n\n\nNow combine the subplots with {**patchwork**} syntax and behold their\nglory.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-subplots-figure-4.12-b lst-cap=\"Subplots of Figure 4.12\"}\n# library(patchwork) ## already in setup chunk\n(p10 / p11 / p12) &\n  theme_bw() &\n  theme(panel.background = element_rect(fill = \"#4f455c\"),\n        panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![Subplots of Figure 4.12](04-geocentric-models_files/figure-html/fig-subplots-figure-4.12-b-1.png){#fig-subplots-figure-4.12-b width=672}\n:::\n:::\n\n\n#### Smooth functions for a rough world\n\nTaking the reference to Woods Book about Additive Generalized Models\n(see @prp-resources-for-gams) Kurz adds two bonus section:\n\n-   Smooth functions with `brms::s()` and\n-   Group predictors with matrix columns\n\nI will include these two addition without completely understanding the\ncode at the moment (2023-08-18) hoping that I will come later to these\ntwo section with a richer knowledge.\n\nBut there is already one important learning I have gotten from the bonus\nsection: It is evident that fitting models in different circumstances is\n-- with minor changes -- follow always the same procedures with almost\nthe same R code.\n\n### Bonus sections\n\n#### Smooth functions with `brms::s()`\n\nWe might use the `brms::get_prior()` function to get a sense of how to\nset up the priors when using `brms::s()`\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-get-prior-bonus lst-cap=\"Get priors when using `brms::s()`\"}\nbrms::get_prior(data = d5_b,\n          family = gaussian,\n          doy ~ 1 + s(year))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>                   prior     class    coef group resp dpar nlpar lb ub\n#>                  (flat)         b                                    \n#>                  (flat)         b syear_1                            \n#>  student_t(3, 105, 5.9) Intercept                                    \n#>    student_t(3, 0, 5.9)       sds                                0   \n#>    student_t(3, 0, 5.9)       sds s(year)                        0   \n#>    student_t(3, 0, 5.9)     sigma                                0   \n#>        source\n#>       default\n#>  (vectorized)\n#>       default\n#>       default\n#>  (vectorized)\n#>       default\n```\n:::\n:::\n\n\nWe have an overall intercept (`class = Intercept`), a single $\\beta$\nparameter for year (`class = b`), a $\\sigma$ parameter\n(`class = sigma`), and an unfamiliar parameter of `class = sds`. I'm not\ngoing to go into that last parameter in any detail, here. We'll need to\nwork our way up through @sec-chap13-models-with-memory and the multilevel model\nto get a full picture of what it means. The important thing to note here\nis that the priors for our `s()`-based alternative to the B-spline\nmodels, above, are going to look a little different. Here's how we might\nfit a model `b4.10` as an alternative to `b4.8`.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fit-bosus-model-b4.10 lst-cap=\"Fit bonus model using `brms::s()`\"}\nb4.10 <-\n  brms::brm(data = d5_b,\n      family = gaussian,\n      doy ~ 1 + s(year),\n      prior = c(brms::prior(normal(100, 10), class = Intercept),\n                brms::prior(normal(0, 10), class = b),\n                brms::prior(student_t(3, 0, 5.9), class = sds),\n                brms::prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      control = list(adapt_delta = .99),\n      file = \"fits/b04.10\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code #lst-print-summary-model-b4.10 lst-cap=\"Summarize b4.10 model\"}\nprint(b4.10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: doy ~ 1 + s(year) \n#>    Data: d5_b (Number of observations: 827) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Smooth Terms: \n#>              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sds(syear_1)    22.31      7.39    11.87    41.14 1.00     1089     2012\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   104.54      0.21   104.12   104.96 1.00     3343     2883\n#> syear_1      -9.75      8.88   -27.23     7.53 1.00     3432     2937\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     6.04      0.15     5.75     6.34 1.00     4107     2709\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\nOur intercept and σ summaries are similar to those we got from `b4.8`.\nThe rest looks different and maybe a little disorienting. Here's what\nhappens when we use `brms:::fitted.brmsfit()` to plot the implications\nof the model.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-plot-implications-model-b4.10 lst-cap=\"Implication of model b4.10\"}\nfitted(b4.10) %>% \n  data.frame() %>% \n  bind_cols(select(d5_b, year, doy)) %>% \n  \n  ggplot(aes(x = year, y = doy, ymin = Q2.5, ymax = Q97.5)) +\n  geom_hline(yintercept = brms::fixef(b4.10)[1, 1], color = \"white\", linetype = 2) +\n  geom_point(color = \"#ffb7c5\", alpha = 1/2) +\n  geom_ribbon(fill = \"white\", alpha = 2/3) +\n  labs(subtitle = \"b4.7 using s(year)\",\n       y = \"day in year\") +\n  theme_bw() +\n  theme(panel.background = element_rect(fill = \"#4f455c\"), \n        panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![](04-geocentric-models_files/figure-html/plot-implications-model-b4.10-1.png){width=672}\n:::\n:::\n\n\nThat smooth doesn't look quite the same. Hopefully this isn't terribly\nsurprising. We used a function from a different package and ended up\nwith a different underlying statistical model. In fact, we didn't even\nuse a B-spline. The default for `s()` is to use what's called a *thin\nplate* regression spline. If we'd like to fit a B-spline, we have to set\n`bs = \"bs\"`. Here's an example:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fit-model-b4.11 lst-cap=\"Fit bonus model b4.11\"}\nb4.11 <-\n  brms::brm(data = d5_b,\n      family = gaussian,\n      doy ~ 1 + s(year, bs = \"bs\", k = 19),\n      prior = c(brms::prior(normal(100, 10), class = Intercept),\n                brms::prior(normal(0, 10), class = b),\n                brms::prior(student_t(3, 0, 5.9), class = sds),\n                brms::prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      control = list(adapt_delta = .99),\n      file = \"fits/b04.11\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code #lst-print-summary-model-b4.11 lst-cap=\"Summarize b4.11 model\"}\nprint(b4.11)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: doy ~ 1 + s(year, bs = \"bs\", k = 19) \n#>    Data: d5_b (Number of observations: 827) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Smooth Terms: \n#>              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sds(syear_1)     1.27      0.60     0.51     2.74 1.01      660     1546\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   104.54      0.21   104.15   104.95 1.00     5478     2893\n#> syear_1      -0.10      0.32    -0.74     0.51 1.00     1679     2050\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     5.99      0.15     5.69     6.29 1.00     3853     2857\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\nNow here's the depiction of our `s()`-based B-spline model:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-plot-bonus-model-b4.11 lst-cap=\"B-splines of model b4.11 using `brms::s()`\"}\nfitted(b4.11) %>% \n  data.frame() %>% \n  bind_cols(select(d5_b, year, doy)) %>% \n  \n  ggplot(aes(x = year, y = doy, ymin = Q2.5, ymax = Q97.5)) +\n  geom_hline(yintercept = brms::fixef(b4.11)[1, 1], color = \"white\", linetype = 2) +\n  geom_point(color = \"#ffb7c5\", alpha = 1/2) +\n  geom_ribbon(fill = \"white\", alpha = 2/3) +\n  labs(subtitle = 'b4.7_bs using s(year, bs = \"bs\")',\n       y = \"day in year\") +\n  theme_bw() +\n  theme(panel.background = element_rect(fill = \"#4f455c\"), \n        panel.grid = element_blank())\n```\n\n::: {.cell-output-display}\n![](04-geocentric-models_files/figure-html/plot-bonus-model-b4.11-1.png){width=672}\n:::\n:::\n\n\nThere are still important differences between the underlying statistical\nmodel for `b4.11` and the earlier `b4.8`. Kurz says that he doesn't want\nto go into the details why this is the case. For me this missing\nexplication is both confusing and frustrating! I hope that with more\nknowledge and experience in fitting models I will be able to fill the\ngaps.\n\n***\n:::: {#prp-resources-for-gams2}\nAgain: Resources for GAMs\n\n::: callout-tip\n\nThis is an addition to the in @prp-resources-for-gams already collected\nresources on <a class='glossary' title='A Generalized Additive Model or GAM is a generalized linear model in which the linear response variable depends linearly on unknown smooth functions of some predictor variables, and interest focuses on inference about these smooth functions. They can be interpreted as the discriminative generalization of the naive Bayes generative model. (Wikipedia). | GAMs relax the restriction that the relationship must be a simple weighted sum, and instead assume that the outcome can be modelled by a sum of arbitrary functions of each feature. (Medium member story) (Chap.4)'>GAMS</a>.\n\n-   For more on the B-splines and smooths, more generally, check out the\n    blog post by the great [Gavin Simpson](https://twitter.com/ucfagls),\n    [Extrapolating with B splines and\n    GAMs](https://fromthebottomoftheheap.net/2020/06/03/extrapolating-with-gams/).\n-   For a high-level introduction to the models you can fit with\n    {**mgcv**}, check out the nice talk by [Noam\n    Ross](https://twitter.com/noamross), [Nonlinear models in R: The\n    wonderful world of mgcv](https://youtu.be/q4_t8jXcQgc), or the\n    equally-nice presentation by Simpson, [Introduction to generalized\n    additive models with R and mgcv](https://youtu.be/sgw4cu8hrZM).\n-   Ross offers a free online course covering {**mgcv**}, called [GAMS\n    in R](https://noamross.github.io/gams-in-r-course/), and he\n    maintains a GitHub repo cataloguing other GAM-related resources,\n    called [Resources for learning about and using GAMs in\n    R](https://github.com/noamross/gam-resources).\n-   For specific examples of fitting various GAMS with {**brms**}, check\n    out Simpson's blog post, [Fitting GAMs with brms: part\n    1](https://fromthebottomoftheheap.net/2018/04/21/fitting-gams-with-brms/).\n-   Finally, [Tristan Mahr](https://twitter.com/tjmahr) has a nice blog\n    post called [Random effects and penalized splines are the same\n    thing](https://www.tjmahr.com/random-effects-penalized-splines-same-thing/),\n    where he outlined the connections between penalized smooths, such as\n    you might fit with {**mgcv**}, with the multilevel model, which\n    we'll learn all about starting in @sec-chap13-models-with-memory, which\n    helps explain what's going on with the `s()` function in our last\n    two models, `b4.10` and `b4.11`.\n:::\n\n::::\n***\n\n#### Group predictors with matrix columns\n\nWhen we fit `b4.8`, our direct {**brms**} analogue to McElreath's\n`m4.7`, we used a compact syntax to pass a matrix column of predictors\ninto the `formula.` If memory serves, this is one of the only places in\nthe text where we see this. It would be easy for the casual reader to\nthink this was only appropriate for something like a spline model. But\nthat's not the case. One could use the matrix-column trick as a general\napproach. In this bonus section, we'll explore how.\n\nIn Section 11.2.6 of their book \"Regression and Other Stories\", Gelman,\nHill, and Vehtari worked through an example of a multiple regression\nmodel,\n\n------------------------------------------------------------------------\n\n::: {#def-multiple-regression-model}\nExample of a multiple regression model\n\n$$\n\\begin{align*}\ny_i & \\sim \\operatorname{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = \\alpha + \\theta z_i + \\sum_{k = 1}^K b_k x_{k, i} \\\\\n& \\text{<priors>},\n\\end{align*}\n$$ {#eq-multiple-regression-model}\n:::\n\n------------------------------------------------------------------------\n\nwhere $y_i$ was some continuous variable collected across participants,\n$i$. The $\\alpha$ term was the intercept and the $\\theta$ term was the\nregression slope for a binary variable $z$--we'll practice with binary\npredictors in the section for categorical-variables in @sec-chap05-many-variables-spurious-waffles. More to our interest, the\nlast portion of the equation is a compact way to convey there are $K$\nadditional predictors and their associated regression coefficients,\nwhich we might more explicitly express as\n$\\beta_1 x_{1, i} + \\cdots + \\beta_k x_{k, i}$, where $K \\geq 1$. In\nthis particular example, $K = 10$, meaning there were ten $x_{k, i}$\npredictors, making this an example of a model with 11 total predictor\nvariables.\n\nRiffing off of Gelman and colleagues, here's how you might simulate data\nof this kind.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-simulate-data-multiple-regression-b lst-cap=\"Simulate data for multiple regression analysis\"}\n# how many cases would you like?\nn8_b <- 100\n\n# how many continuous x predictor variables would you like?\nk8_b <- 10\n\n# simulate a dichotomous dummy variable for z\n# simulate an n by k array for X\nset.seed(4)\n\nd8_b <- \n  tibble(z = sample(0:1, size = n8_b, replace = T),\n         X = array(runif(n8_b * k8_b, min = 0, max = 1), dim = c(n8_b, k8_b)))\n\n# set the data-generating parameter values\na8_b     <- 1\ntheta8_b <- 5\nb8_b     <- 1:k8_b\nsigma8_b <- 2\n\n# simulate the criterion\nd9_b <-\n  d8_b %>% \n  mutate(y = as.vector(a8_b + X %*% b8_b + theta8_b * z + rnorm(n8_b, mean = 0, sd = sigma8_b)))\n\n# check the data structure\nd9_b %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> Rows: 100\n#> Columns: 3\n#> $ z <int> 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,…\n#> $ X <dbl[,10]> <matrix[26 x 10]>\n#> $ y <dbl> 19.51231, 26.75217, 28.92495, 20.63144, 29.18219, 43.67198, 36.…\n```\n:::\n:::\n\n\nAlthough our `d9_b` tibble has only three columns, the `X` column is a\nmatrix column into which we've smuggled ten columns more. Here's how we\nmight access them more directly.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-access-nested-df lst-cap=\"Acess nested data frame in tibble\"}\nd9_b %>% \n  pull(X) %>% \n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  num [1:100, 1:10] 0.253 0.63 0.266 0.532 0.468 ...\n```\n:::\n:::\n\n\nSee? There's an $100 \\times 10$ data matrix in there.\n\nHere's how to fit the full model with {**brms**} where we use the\ncompact matrix-column syntax in the `formula` argument.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fit-model-b4.12-b lst-cap=\"Fit model b4.12\"}\nb4.12 <-\n  brms::brm(data = d9_b,\n      family = gaussian,\n      y ~ 1 + z + X,\n      prior = c(brms::prior(normal(0, 2), class = Intercept),\n                brms::prior(normal(0, 10), class = b),\n                brms::prior(exponential(1), class = sigma)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.12\")\n```\n:::\n\n\nCheck the parameter summary.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-print-summary-model-b4.12-b lst-cap=\"Summarize model b4.12\"}\nprint(b4.12)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: y ~ 1 + z + X \n#>    Data: d9_b (Number of observations: 100) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept     0.95      1.21    -1.43     3.31 1.00     7075     3266\n#> z             4.74      0.42     3.93     5.56 1.00     7859     3361\n#> X1            0.57      0.77    -0.96     2.09 1.00     7405     3518\n#> X2            0.88      0.69    -0.46     2.24 1.00     7173     3633\n#> X3            3.40      0.72     2.01     4.83 1.00     7366     3188\n#> X4            2.81      0.75     1.32     4.26 1.00     7412     3099\n#> X5            5.75      0.71     4.37     7.17 1.00     6846     3357\n#> X6            6.40      0.77     4.87     7.88 1.00     7649     3420\n#> X7            8.48      0.76     6.99     9.97 1.00     8442     3385\n#> X8            8.40      0.74     6.90     9.84 1.00     8101     2910\n#> X9            8.83      0.82     7.18    10.45 1.00     7963     3368\n#> X10           9.32      0.73     7.85    10.80 1.00     7369     3055\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     1.99      0.15     1.72     2.33 1.00     5913     3195\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\n**brms** automatically numbered our $K = 10$ `X` variables as `X1`\nthrough `X10`. As far as applications go, I'm not sure where I'd use\nthis way of storing and modeling data in real life. But maybe some of\ny'all work in domains where this is just the right way to approach your\ndata needs. If so, good luck and happy modeling.\n\n## `SYNOPSIS`\n\n### Original\n\nThe chapter introduces linear regression as a Bayesian procedure.\n\n#### Why are normal distributions normal?\n\nIt starts with answering the question: Why are normal distributions\nnormal? Any process that adds together random values from the same\ndistribution converges to a normal. Why? Because the different\nfluctuations around the mean cancel one another out. The result is a\nGaussian distribution.\n\n#### A language for describing models\n\nThen a formal language for describing models is presented. To know this\nlanguage is important because it closes the gap between scientific\nknowledge resp. assumptions and the statistical model. Models are\nmappings of one set of variables through a probability distribution onto\nanother set of variables. In these language for models, defines the\nfirst line the likelihood function used in <a class='glossary' title='This is the theorem that gives Bayesian data analysis its name. But the theorem itself is a trivial implication of probability theory. The mathematical definition of the posterior distribution arises from Bayes’ Theorem. The key lesson is that the posterior is proportional to the product of the prior and the probability of the data. (Chap.2)'>Bayes’ theorem</a>. The other\nlines define priors.\n\nThe procedure: 1. First, we recognize a set of variables to work with.\nSome of these variables are observable. We call these *data.* Others are\nunobservable things like rates and averages. We call these *parameters.*\n2. We define each variable either in terms of the other variables or in\nterms of a probability distribution. 3. The combination of variables and\ntheir probability distributions defines a *joint generative model* that\ncan be used both to simulate hypothetical observations as well as\nanalyze real ones.\n\n#### Gaussian model of heights {#sec-gaussian-model-of-heights-r}\n\nWith the dataset of adults (age \\>=18) of the !Kung San foraging people\nour goal is to model the height values using a Gaussian distribution. To\ndefine the heights as normally distributed with a mean `μ` and standard\ndeviation `σ`, we write $h_i \\sim \\operatorname{Normal}(\\mu, \\sigma)$.\nTo complete the model we need prior for the mean\n$\\mu \\sim \\operatorname{Normal}(178, 20)$ and the standard deviation\n$\\sigma \\sim \\operatorname{Uniform}(0, 50)$.\n\nThe prior for `μ` is a broad Gaussian prior, centered on 178 cm, with\n95% of probability between 178 ± 40 cm. It range from 138 cm to 218 cm\nencompasses a huge range of plausible mean heights for human\npopulations. So domain-specific information has gone into this prior.\nThe `σ` prior is a flat prior, an uniform one, that functions just to\nconstrain `σ` to have positive probability between zero and 50 cm.\n\n\n##### Plotting the priors\n\n***\n:::: {#prp-always-plot-priors}\nAlways plot the priors!\n\n::: callout-important\nIr is important to plot your priors, so you have a sense of the assumption they\nbuild into your model.\n:::\n::::\n***\n\n`graphics::curve()` draws a curve corresponding to a function over the\ninterval `[from, to]`. How to use the interval? Experiment with it to\nget a range that will show all possible values.\n\n***\n:::: {#prp-knowledge-priors}\n\nUsing scientific knowledge to build priors\n\n::: callout-important\nVery useful is the knowledge that if you double $\\sigma$ and subtract resp. add that value to the mean, then you will get an area where 95% of all expected values should be found.\n\nFor instance a standard deviation of 20 cm with a mean of 178 cm would\nimply that 95% of individual heights lie between 138 cm and 218 cm.\nThat's a very large range. The same calculation happens with $\\sigma$\nwhere a standard deviation of 50 means that 95% of all values lie in the\nrange of 100 cm.\n\nUsing scientific knowledge to build priors is not cheating. The\nimportant thing is that your priors are not based on the values in the\ndata, but only on what you know about the data before you see it.\n\n:::\n\n::::\n***\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-ig-priors-r lst-cap=\"Plot priors of height model: rethinking version\"}\n## R code 4.12 & 4.13 ##############\ncurve(dnorm(x, 178, 20), from = 100, to = 250)\ncurve(dunif(x, 0, 50), from = -10, to = 60)\n```\n\n::: {.cell-output-display}\n![Plotting priors of the height model: rethinking version](04-geocentric-models_files/figure-html/fig-priors-r-1.png){#fig-priors-r-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Plotting priors of the height model: rethinking version](04-geocentric-models_files/figure-html/fig-priors-r-2.png){#fig-priors-r-2 width=672}\n:::\n:::\n\n\n\n##### Prior predictive simulation\n\nIt'll help to see what these priors imply about the distribution of\nindividual heights. The <a class='glossary' title='It is an essential part of modeling. Once you’ve chosen priors for all variables these priors imply a joint prior distribution. By simulating from this distribution, you can see what your choices imply about the observable variable. This helps to diagnose bad choices. Prior predictive simulation is therefore very useful for assigning sensible priors. (Chap.4)'>prior predictive simulation</a> is\nan essential part of your modeling. Once you've chosen priors these\nimply a joint prior distribution of individual heights. By simulating\nfrom this distribution, you can see what your choices imply about\nobservable height.\n\nYou can quickly simulate heights by sampling from the prior, like you\nsampled from the posterior back in @sec-sampling-the-imaginary.\nRemember, every posterior is also potentially a prior for a subsequent\nanalysis, so you can process priors just like posteriors.\n\nInstead of using the density family of distributions when plotting the\npriors we are going now to use the random family of distributions (i.e., using `rnorm()` and `runif()` instead of `dnorm()` and `dunif()`.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-prior-predictive-sim-r lst-cap=\"Heights by sampling from the prior: rethinking version\"}\nset.seed(4) # to make example reproducible\n## R code 4.14 #######################################\nsample_mu_r <- rnorm(1e4, 178, 20)\nsample_sigma_r <- runif(1e4, 0, 50)\nprior_h_r <- rnorm(1e4, sample_mu_r, sample_sigma_r)\nrethinking::dens(prior_h_r, norm.comp = TRUE)\n```\n\n::: {.cell-output-display}\n![Simulate heights by sampling from the prior: rethinking version](04-geocentric-models_files/figure-html/fig-prior-predictive-sim-r-1.png){#fig-prior-predictive-sim-r width=672}\n:::\n:::\n\n\n\n#### EMPTY: Grid approximation of the posterior distribution\n\n#### EMPTY: Sampling from the posterior\n\n#### Finding the posterior distribution with quap\n\nI skipped the grid approximation and the sampling from it. It has mostly only educational value as it works only with a few parameters and is computationally expensive. Besides it has complex code that has no additional conceptual value to understand the purpose of the procedure.\n\n***\n:::: {#prp-using-quap}\n\n`rethinking:quap()` has three parts to complete\n\n::: note-important\n\n1.  A formula or `base::alist()` of formulas that define the likelihood\n    and priors.\n2.  A data frame or list containing the data.\n3.  Some options like start values of method for search optimization.\n    Note that the list of start values is a regular `list()`, not an\n    `alist()` like the formula list is.\n:::\n\n::::\n***\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-post-dist-quap-r4-1 lst-cap=\"Finding the posterior distribution with rethinking::quap(): Model r4.1\"}\n## R code 4.26 modified #############\ndata(package = \"rethinking\", list = \"Howell1\")\nd_r <- Howell1\nd2_r <- d_r[d_r$age >= 18, ]\n\n## R code 4.27 & 4.28 modified ##############\nr4.1 <- rethinking::quap(\n    alist(\n        height ~ dnorm(mu, sigma),\n        mu ~ dnorm(178, 20),\n        sigma ~ dunif(0, 50)\n    ),\n    data = d2_r\n)\n\n## R code 4.29 ###########\nrethinking::precis(r4.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>             mean        sd       5.5%      94.5%\n#> mu    154.607024 0.4119947 153.948577 155.265471\n#> sigma   7.731333 0.2913860   7.265642   8.197024\n```\n:::\n:::\n\n\n#### Sampling from the quap\n\nI skip the long and diffusing discussion about the <a class='glossary' title='Also known as covariance matrix, variance-covariance matrix or variance matrix. It tells us how each parameter relates to every other parameter. It is a square matrix giving the covariance between each pair of elements of a given random vector. Any covariance matrix is symmetric and positive semi-definite and its main diagonal contains variances (i.e., the covariance of each element with itself). (Wikipedia(https://en.wikipedia.org/wiki/Covariance_matrix))'>covariance matrix</a>.\n\nInstead of sampling single values from a simple Gaussian distribution, we sample vectors of values from a multi-dimensional Gaussian distribution. The {**rethinking**} package provides a convenience function to do exactly that:\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-extract-samples-r4.1 lst-cap=\"Extract sample vectors of values from the multi-dimensional Gaussian distribution of r4.1: rethinking version\"}\n## R code 4.34 #######################\npost_r <- rethinking::extract.samples(r4.1, n = 1e4)\nrethinking::precis(post_r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>             mean        sd       5.5%      94.5%     histogram\n#> mu    154.612631 0.4106713 153.957094 155.268957      ▁▁▁▅▇▂▁▁\n#> sigma   7.730126 0.2921127   7.266803   8.199107 ▁▁▁▁▂▅▇▇▃▁▁▁▁\n```\n:::\n:::\n\nWe have ended up with a data frame, `post_t`, with 10,000 (1e4) rows and two columns, one column for `μ` and one for `σ`. Each value is a sample from the posterior, so the mean and standard deviation of each column will be very close to the <a class='glossary' title='In Bayesian statistics a Maximum A Posteriori probability or MAP estimate is an estimate of an unknown quantity, that equals the mode of the posterior distribution. The MAP can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data. (Wikipedia) (Chap.4)'>MAP</a> values from @lst-post-dist-quap-r4-1.\n\n#### Linear prediction\n\nHow do we take our Gaussian model from @sec-gaussian-model-of-heights-r that finally resulted in the <a class='glossary' title='It is an essential part of modeling. Once you’ve chosen priors for all variables these priors imply a joint prior distribution. By simulating from this distribution, you can see what your choices imply about the observable variable. This helps to diagnose bad choices. Prior predictive simulation is therefore very useful for assigning sensible priors. (Chap.4)'>prior predictive simulation</a> of @fig-prior-predictive-sim-r and add <a class='glossary' title='Predictor variable – also known sometimes as the independent or explanatory variable – is the counterpart to the response or dependent variable. Predictor variables are used to make predictions for dependent variables. (DeepAI, MiniTab)'>predictor variables</a> to it?\n\nI we plot height against weight of our data in `d2_r` we will intuitively see that there is a relationship. How to make this vague observation into a more precise quantitative <a class='glossary' title='A statistical model is an expression that attempts to explain patterns in the observed values of a response variable by relating the response variable to a set of predictor variables and parameters. (Monash University)Statistical models are mappings of one set of variables through a probability distribution onto another set of variables. Fundamentally, these models define the ways values of some variables can arise, given values of other variables, because it can be quite hard to anticipate how priors influence the observable variables. (Chap.4)'>statistical model</a>?\n\nWe are using the same procedure as in {@sec-gaussian-model-height-r}:\n\n1. Construct the model specification: As always we have to construct sensible priors. It helps to plot a <a class='glossary' title='It is an essential part of modeling. Once you’ve chosen priors for all variables these priors imply a joint prior distribution. By simulating from this distribution, you can see what your choices imply about the observable variable. This helps to diagnose bad choices. Prior predictive simulation is therefore very useful for assigning sensible priors. (Chap.4)'>prior predictive simulation</a>. In comparison to @eq-height-linear-model-m4.1 we have to add a prior for the new\nparameter, `β`. It turned out that this prior has to be in log-scale to prevent height and weight values below zero.\n2. Then we have to incorporate our new model @eq-height-weight-linear-model2-m4.3 for the mean into the model specification inside `rethinking::quap()`\n3. With the resulting model we are able to plot the posterior inference\nagainst the data. \n\n##### Step 1: Construct model with sensible priors\n\n***\n::: {#def-height-weight-linear-model-r4.3}\n\n###### Linear model height against weight (model r4.3)\n\n$$\n\\begin{align*}\nh_{i} \\sim \\operatorname{Normal}(μ_{i}, σ) \\space \\space (1) \\\\ \nμ_{i} = \\alpha + \\beta(x_{i}-\\overline{x}) \\space \\space (2) \\\\\n\\alpha \\sim \\operatorname{Normal}(178, 20) \\space \\space (3)  \\\\ \n\\beta \\sim \\operatorname{Log-Normal}(0,10) \\space \\space (4) \\\\\n\\sigma \\sim \\operatorname{Uniform}(0, 50)  \\space \\space (5) \\\\    \n\\end{align*} \n$$ {#eq-height-weight-linear-model-r4.3}\n\n:::\n***\n\n##### Step 2: Incorporate new model into quap function\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-find-post-dist-r4.3 lst-cap=\"Find the posterior distribution of the linear height-weight model: rethinking version\"}\n## R code 4.42 #############################\n\n# define the average weight, x-bar\nxbar_r <- mean(d2_r$weight)\n\n# fit model\nr4.3 <- rethinking::quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + b * (weight - xbar_r),\n    a ~ dnorm(178, 20),\n    b ~ dlnorm(0, 1),\n    sigma ~ dunif(0, 50)\n  ),\n  data = d2_r\n)\n\n# summary result\n## R code 4.44 ############################\nrethinking::precis(r4.3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>              mean        sd        5.5%       94.5%\n#> a     154.6013731 0.2703075 154.1693695 155.0333766\n#> b       0.9032803 0.0419236   0.8362783   0.9702823\n#> sigma   5.0718774 0.1911545   4.7663757   5.3773791\n```\n:::\n:::\n\n##### Step 3: Plot the posterior inference\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-raw-data-line-r4.3 lst-cap=\"Height against weight with linear regression\"}\n## R code 4.46 ############################################\nplot(height ~ weight, data = d2_r, col = rethinking::rangi2)\npost_r4.3 <- rethinking::extract.samples(r4.3)\na_map <- mean(post_r4.3$a)\nb_map <- mean(post_r4.3$b)\ncurve(a_map + b_map * (x - xbar_r), add = TRUE)\n```\n\n::: {.cell-output-display}\n![Height in centimeters (vertical) plotted against weight in kilograms (horizontal), with the line at the posterior mean plotted in black: rethinking version](04-geocentric-models_files/figure-html/fig-raw-data-line-r4.3-1.png){#fig-raw-data-line-r4.3 width=672}\n:::\n:::\n\n\n\n\n## STOPPED ORIGINAL HERE! (2023-08-20)\n\n\n### Tidyverse\n\n#### EMPTY: Why are normal distribution normal?\n\n#### EMPTY: A language for describing models\n\n#### Gausian model of heights\n\n##### Plotting the priors\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-ig-priors-t lst-cap=\"Plot priors of height model: tidyverse version\"}\ntibble(x = seq(from = 100, to = 250, by = .1)) %>% \nggplot(aes(x = x, y = dnorm(x, mean = 178, sd = 20))) +\ngeom_line() \n\ntibble(x = seq(from = -10, to = 60, by = .1)) %>%\nggplot(aes(x = x, y = dunif(x, min = 0, max = 50))) +\ngeom_line()\n```\n\n::: {.cell-output-display}\n![Plotting priors of the height model: tidyverse version](04-geocentric-models_files/figure-html/fig-priors-t-1.png){#fig-priors-t-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Plotting priors of the height model: tidyverse version](04-geocentric-models_files/figure-html/fig-priors-t-2.png){#fig-priors-t-2 width=672}\n:::\n:::\n\n\n\n##### Prior predictive simulation\n\nWe simulate from both priors at once to get a <a class='glossary' title='It is an essential part of modeling. Once you’ve chosen priors for all variables these priors imply a joint prior distribution. By simulating from this distribution, you can see what your choices imply about the observable variable. This helps to diagnose bad choices. Prior predictive simulation is therefore very useful for assigning sensible priors. (Chap.4)'>prior predictive simulation</a> of `heights.`\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-prior-predictive-sim-t lst-cap=\"Heights by sampling from the prior: tidyverse version\"}\nset.seed(4)\n\n# simulation\n  tibble(sample_mu    = rnorm(1e4, mean = 178, sd  = 20),\n         sample_sigma = runif(1e4, min = 0, max = 50)) %>% \n  mutate(height = rnorm(1e4, mean = sample_mu, sd = sample_sigma)) |> \n  \n# plot\n  ggplot(aes(x = height)) +\n  geom_density(fill = \"deepskyblue\")\n```\n\n::: {.cell-output-display}\n![Simulate heights by sampling from the prior: tidyverse version](04-geocentric-models_files/figure-html/fig-prior-predictive-sim-t-1.png){#fig-prior-predictive-sim-t width=672}\n:::\n:::\n\n\nIn contrast to @fig-prior-predictive-sim2-a where we have seen negative heights and giants of more than 5 meter, the prior predictive simulation here is better but still -- with our world knowledge - not realistic as there never have been people of about three meter. But [Robert Pershing Wadlow](https://en.wikipedia.org/wiki/Robert_Wadlow) as the largest man ever measured with 2.72 meter is pretty near of the maximum height in @fig-prior-predictive-sim-t.\n\n#### Finding the posterior distribution with brm\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-post-dist-brm-t lst-cap=\"Posterior Distribution of height model with `brms::brm()`\"}\ndata(package = \"rethinking\", list = \"Howell1\")\nd_t <- Howell1\nd2_t <- d_t[d_r$age >= 18, ]\n\nt4.1 <- \n  brms::brm(data = d2_t, \n      family = gaussian,\n      height ~ 1,\n      prior = c(brms::prior(normal(178, 20), class = Intercept),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/t04.01\")\n\nprint(t4.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ 1 \n#>    Data: d2_t (Number of observations: 352) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   154.60      0.41   153.81   155.42 1.00     2763     2635\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     7.77      0.29     7.21     8.39 1.00     3400     2561\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\nThe `summary()` functions for `brmsfit` objects gives identical results. A slightly different output results from a Stan-like summary with `t4.1$fit`.\n\nTo get different intervals than the 95% default values you can use the `prob` argument for the `print()` or `summary()` function, for instance `summary(t4.1, prob = .89)`.\n\n\n#### Sampling from a brm() fit\n\nAgain I will skip some details of the complex and confusing <a class='glossary' title='Also known as covariance matrix, variance-covariance matrix or variance matrix. It tells us how each parameter relates to every other parameter. It is a square matrix giving the covariance between each pair of elements of a given random vector. Any covariance matrix is symmetric and positive semi-definite and its main diagonal contains variances (i.e., the covariance of each element with itself). (Wikipedia(https://en.wikipedia.org/wiki/Covariance_matrix))'>dispersion matrix</a> discussion. The important thing in the tidyverse framework is, that we have to put the <a class='glossary' title='Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) method that uses the derivatives of the density function being sampled to generate efficient transitions spanning the posterior. It uses an approximate Hamiltonian dynamics simulation based on numerical integration which is then corrected by performing a Metropolis acceptance step. (Stan Reference Manual)'>HMC</a> chains result of `brms::brm()` in a data frame. We do that with the important `brms::as_draws_df()` function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-sample-post-dist-t4.1 lst-cap=\"Sample posterior distribution from t4.1\"}\npost_t <- brms::as_draws_df(t4.1)\nhead(post_t)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> # A draws_df: 6 iterations, 1 chains, and 4 variables\n#>   b_Intercept sigma lprior  lp__\n#> 1         155   7.5   -8.5 -1227\n#> 2         155   7.0   -8.5 -1230\n#> 3         154   7.6   -8.5 -1226\n#> 4         154   8.0   -8.5 -1226\n#> 5         155   7.6   -8.5 -1226\n#> 6         155   7.4   -8.5 -1227\n#> # ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n```\n:::\n:::\n\n\nFrom here we can compute <a class='glossary' title='Covariance is a measure of how much two random variables vary together. It’s similar to variance, but where variance tells you how a single variable varies, co variance tells you how two variables vary together. (Statistics How To(https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/covariance/))'>covariance</a> and <a class='glossary' title='Correlation coefficients are used to measure how strong a relationship is between two variables. There are several types of correlation coefficient, but the most popular is Pearson’s. Pearson’s correlation (also called Pearson’s R) is a correlation coefficient commonly used in linear regression. (Statistics How To)'>correlation</a>.\n\n\n::: {.cell}\n\n```{.r .cell-code # lst-cov-t4.1 lst-cap=\"Compute covariance matrix for model t.41\"}\npost_t |> \n    select(b_Intercept:sigma) |> \n    cov()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n#>               b_Intercept         sigma\n#> b_Intercept  0.1717797087 -0.0005450089\n#> sigma       -0.0005450089  0.0868161627\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code # lst-cor-t4.1 lst-cap=\"Compute correlation matrix for model t.41\"}\npost_t |> \n    select(b_Intercept:sigma) |> \n    cor()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n#>              b_Intercept        sigma\n#> b_Intercept  1.000000000 -0.004462902\n#> sigma       -0.004462902  1.000000000\n```\n:::\n:::\n\n\n***\n::: callout-note\n###### TODO attr-source\nCheck if all code chunk have an `attr-source:` attribute. The last one I did was in @lst-fig-sim-heights-only-with-priors-b.\n:::\n***\n\n## STOPPED TIDYVERSE HERE! (B: 2023-08-20)\n\n## Practice\n\nProblems are labeled Easy (E), Medium (M), and Hard (H).\n\n### 4E1\n\nIn the model definition below, which line is the likelihood?\n\n$$\ny_{i} \\sim \\operatorname{Normal}(\\mu, \\sigma) \\\\\n\\mu \\sim \\operatorname{Normal}(0, 10) \\\\\n\\sigma \\sim \\operatorname{Exponential}(1)\n$$ {#eq-4e1}\n\n**My answer**: $y_{i} \\sim \\operatorname{Normal}(\\mu, \\sigma)$, the other lines are priors.\n\n### 4E2\n\nIn the model definition just above, how many parameters are in the posterior distribution? \n\n**My answer**: Just one.\n\n::: callout-warning\n#### Wrong Answer\n\nIn the definition @eq-4e1 $y_{i}$  is not to be estimated, but represents the data we have at hand and want to understand through parameters. Erroneously I took this for the parameter. The correct answer is $\\mu$ and §\\sigma$ as both are the parameters which we attempt to estimate. So the correct answer is: Two\n:::\n\n### 4E3\n\nUsing the model definition above, write down the appropriate form of <a class='glossary' title='This is the theorem that gives Bayesian data analysis its name. But the theorem itself is a trivial implication of probability theory. The mathematical definition of the posterior distribution arises from Bayes’ Theorem. The key lesson is that the posterior is proportional to the product of the prior and the probability of the data. (Chap.2)'>Bayes’ theorem</a> that includes the proper likelihood and priors.\n\n\n### 4E4\n\nIn the model definition below, which line is the linear model?\n\n$$\ny_{i} \\sim \\operatorname{Normal}(\\mu, \\sigma) \\\\\n\\mu_{i} = \\alpha + \\beta{x_{i}} \\\\\n\\alpha \\sim \\operatorname{Normal}(0, 10) \\\\\n\\beta \\sim \\operatorname{Normal}(0, 1) \\\\\n\\sigma \\sim \\operatorname{Exponential}(1)\n$$\n**My answer**: $\\mu_{i} = \\alpha + \\beta{x_{i}}$\n\n### 4E5\n\nIn the model definition just above, how many parameters are in the posterior distribution?\n\n**My answer**: There are three parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-session-info lst-cap=\"Session info\"}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> R version 4.3.1 (2023-06-16)\n#> Platform: x86_64-apple-darwin20 (64-bit)\n#> Running under: macOS Ventura 13.5.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n#> \n#> locale:\n#> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n#> \n#> time zone: Europe/Vienna\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] patchwork_1.1.3     lubridate_1.9.2     forcats_1.0.0      \n#>  [4] stringr_1.5.0       dplyr_1.1.3         purrr_1.0.2        \n#>  [7] readr_2.1.4         tidyr_1.3.0         tibble_3.2.1       \n#> [10] ggplot2_3.4.3       tidyverse_2.0.0     glossary_1.0.0.9000\n#> \n#> loaded via a namespace (and not attached):\n#>   [1] tensorA_0.36.2       rstudioapi_0.15.0    jsonlite_1.8.7      \n#>   [4] shape_1.4.6          magrittr_2.0.3       TH.data_1.1-2       \n#>   [7] estimability_1.4.1   farver_2.1.1         rmarkdown_2.24      \n#>  [10] vctrs_0.6.3          base64enc_0.1-3      htmltools_0.5.6     \n#>  [13] distributional_0.3.2 curl_5.0.2           tidybayes_3.0.6     \n#>  [16] StanHeaders_2.26.27  KernSmooth_2.23-22   htmlwidgets_1.6.2   \n#>  [19] plyr_1.8.8           sandwich_3.0-2       emmeans_1.8.8       \n#>  [22] zoo_1.8-12           commonmark_1.9.0     igraph_1.5.1        \n#>  [25] mime_0.12            lifecycle_1.0.3      pkgconfig_2.0.3     \n#>  [28] colourpicker_1.3.0   Matrix_1.6-1         R6_2.5.1            \n#>  [31] fastmap_1.1.1        shiny_1.7.5          digest_0.6.33       \n#>  [34] colorspace_2.1-0     ps_1.7.5             brms_2.20.1         \n#>  [37] crosstalk_1.2.0      labeling_0.4.3       fansi_1.0.4         \n#>  [40] timechange_0.2.0     mgcv_1.9-0           abind_1.4-5         \n#>  [43] compiler_4.3.1       withr_2.5.0          backports_1.4.1     \n#>  [46] inline_0.3.19        shinystan_2.6.0      rethinking_2.31     \n#>  [49] pkgbuild_1.4.2       MASS_7.3-60          gtools_3.9.4        \n#>  [52] loo_2.6.0            tools_4.3.1          httpuv_1.6.11       \n#>  [55] threejs_0.3.3        glue_1.6.2           callr_3.7.3         \n#>  [58] nlme_3.1-163         promises_1.2.1       grid_4.3.1          \n#>  [61] cmdstanr_0.5.3       checkmate_2.2.0      reshape2_1.4.4      \n#>  [64] generics_0.1.3       isoband_0.2.7        gtable_0.3.4        \n#>  [67] tzdb_0.4.0           hms_1.1.3            xml2_1.3.5          \n#>  [70] utf8_1.2.3           pillar_1.9.0         ggdist_3.3.0        \n#>  [73] markdown_1.8         posterior_1.4.1      later_1.3.1         \n#>  [76] splines_4.3.1        lattice_0.21-8       survival_3.5-7      \n#>  [79] tidyselect_1.2.0     miniUI_0.1.1.1       knitr_1.43          \n#>  [82] arrayhelpers_1.1-0   gridExtra_2.3        V8_4.3.3            \n#>  [85] rversions_2.1.2      stats4_4.3.1         xfun_0.40           \n#>  [88] bridgesampling_1.1-2 skimr_2.1.5          matrixStats_1.0.0   \n#>  [91] DT_0.29              rstan_2.26.22        stringi_1.7.12      \n#>  [94] yaml_2.3.7           evaluate_0.21        codetools_0.2-19    \n#>  [97] cli_3.6.1            RcppParallel_5.1.7   shinythemes_1.2.0   \n#> [100] xtable_1.8-4         repr_1.1.6           munsell_0.5.0       \n#> [103] processx_3.8.2       Rcpp_1.0.11          coda_0.19-4         \n#> [106] svUnit_1.0.6         parallel_4.3.1       rstantools_2.3.1.1  \n#> [109] ellipsis_0.3.2       prettyunits_1.1.1    dygraphs_1.1.1.6    \n#> [112] bayesplot_1.10.0     Brobdingnag_1.2-9    viridisLite_0.4.2   \n#> [115] mvtnorm_1.2-3        scales_1.2.1         xts_0.13.1          \n#> [118] crayon_1.5.2         rlang_1.1.1          multcomp_1.4-25     \n#> [121] shinyjs_2.1.0\n```\n:::\n:::\n",
    "supporting": [
      "04-geocentric-models_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
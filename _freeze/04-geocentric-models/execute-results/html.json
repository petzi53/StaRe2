{
  "hash": "5a468a4aae92f9b9a122f95fec354499",
  "result": {
    "markdown": "# Geocentric Models\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: setup\n\nlibrary(tidyverse)\n```\n````\n\n```\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1     \n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n\n## Why normal distributions are normal?\n\nWhy are there so many distribution approximately normal, resulting in a\nGaussian curve? Because there will be more combinations of outcomes that\nsum up to a \"central\" value, rather than to some extreme value.\n\n::: callout-tip\nAny process that adds together random values from the same distribution\nconverges to a normal.\n:::\n\n### Normal by addition\n\nWhatever the average value of the source distribution, each sample from\nit can be thought of as a fluctuation from that average value. When we\nbegin to add these fluctuations together, they also begin to cancel one\nanother out. A large positive fluctuation will cancel a large negative\none. The more terms in the sum, the more chances for each fluctuation to\nbe canceled by another, or by a series of smaller ones in the opposite\ndirection. So eventually the most likely sum, in the sense that there\nare the most ways to realize it, will be a sum in which every\nfluctuation is canceled by another, a sum of zero (relative to the\nmean).\n\nIt doesn't matter what shape the underlying distribution possesses. It\ncould be uniform, like in our example above, or it could be (nearly)\nanything else. Depending upon the underlying distribution, the\nconvergence might be slow, but it will be inevitable.\n\nSee the excellent article [Why is normal distribution so\nubiquitous?](https://ekamperi.github.io/mathematics/2021/01/29/why-is-normal-distribution-so-ubiquitous.html)\nwhich also explains the example of random walks from SR2. See also the\nscientific paper [Why are normal distribution\nnormal?](https://www.journals.uchicago.edu/doi/pdf/10.1093/bjps/axs046)\nof the The British Journal for the Philosophy of Science.\n\n### Normal by multiplication\n\nThis is not only valid for addition but also for multiplication of small\nvalues: Multiplying small numbers is approximately the same as addition.\n\n### Normal by log-multipliation\n\nBut even the multiplication of large values tend to produce Gaussian\ndistributions on the log scale.\n\n### Using Gaussian distribution\n\nThe justifications for using the Gaussian distribution fall into two\nbroad categories:\n\n1.  **Ontological justification**: The world is full of Gaussian\n    distributions, approximately. We're never going to experience a\n    perfect Gaussian distribution. But it is a widespread pattern,\n    appearing again and again at different scales and in different\n    domains. Measurement errors, variations in growth, and the\n    velocities of molecules all tend towards Gaussian distributions.\n\nThere are many other patterns in nature, so make no mistake in assuming\nthat the Gaussian pattern is universal. In later chapters, we'll see how\nother useful and common patterns, like the exponential and gamma and\nPoisson, also arise from natural processes. The Gaussian is a member of\na family of fundamental natural distributions known as the **Exponential\nfamily**. All of the members of this family are important for working\nscience, because they populate our world.\n\n2.  **Epistemological justification**: The Gaussian represents a\n    particular state of ignorance. When all we know or are willing to\n    say about a distribution of measures (measures are continuous values\n    on the real number line) is their mean and variance, then the\n    Gaussian distribution arises as the most consistent with our\n    assumptions. It is the least surprising and least informative\n    assumption to make. --- If you don't think the distribution should\n    be Gaussian, then that implies that you know something else that you\n    should tell your golem about, something that would improve\n    inference.\n\n::: callout-caution\nAlthough the Gaussian distribution is common in nature and has some nice\nproperties, there are some risks in using it as a default data model.\nThe Gaussian distribution has some very thin tails---there is very\nlittle probability in them. Instead most of the mass in the Gaussian\nlies within one standard deviation of the mean. Many natural (and\nunnatural) processes have much heavier tails.\n:::\n\nThe Gaussian is a continuous distribution, unlike the discrete\ndistributions of earlier chapters. Probability distributions with only\ndiscrete outcomes, like the binomial, are called *probability mass*\nfunctions and denoted `Pr`. Continuous ones like the Gaussian are called\n*probability density* functions, denoted with *`p`* or just plain old\n*`f`*, depending upon author and tradition. For mathematical reasons,\nprobability densities can be greater than 1. Try `dnorm(0,0,0.1)`\", for\nexample, which is the way to make R calculate *`p`*`(0|0, 0.1)`. The\nanswer, about 4, is no mistake. Probability *density* is the rate of\nchange in cumulative probability. So where cumulative probability is\nincreasing rapidly, density can easily exceed 1. But if we calculate the\narea under the density function, it will never exceed 1. Such areas are\nalso called *probability mass*.\n\n## A language describing models\n\n1.  First, we recognize a set of variables to work with. Some of these\n    variables are observable. We call these data. Others are\n    unobservable things like rates and averages. We call these\n    parameters.\n2.  We define each variable either in terms of the other variables or in\n    terms of a probability distribution.\n3.  The combination of variables and their probability distributions\n    defines a joint generative model that can be used both to simulate\n    hypothetical observations as well as analyze real ones.\n\nModels are mappings of one set of variables through a probability\ndistribution onto another set of variables. Fundamentally, these models\ndefine the ways values of some variables can arise, given values of\nother variables.\n\n### Re-describing the globe tossing model\n\n::: {#def-glob-tossing-model}\nRecall the proportion of water problem from previous chapters. The model\nin that case was always:\n\n$$\n\\begin{align*}\nW \\sim Binomial(N, p) \\\\\np \\sim Uniform(0, 1)\n\\end{align*}\n$$\n\n-   `W`: observed count of water\n-   `N`: total number of tosses\n-   `p`: proportion of water on the globe\n\nRead the above statement as:\n\n1.  **First line**: The count W is distributed binomially with sample\n    size `N` and probability `p`.\n2.  **Second line**: The prior for `p` is assumed to be uniform between\n    zero and one.\n\nThe first line defines the likelihood function used in Bayes' theorem.\nThe other lines define priors. Both of the lines in this model are\n**stochastic**, as indicated by the `~` symbol. A stochastic\nrelationship is just a mapping of a variable or parameter onto a\ndistribution. It is stochastic because no single instance of the\nvariable on the left is known with certainty. Instead, the mapping is\nprobabilistic: Some values are more plausible than others, but very many\ndifferent values are plausible under any model. Later, we'll have models\nwith deterministic definitions in them.\n:::\n\n## Gaussian model of height\n\nThere are an infinite number of possible Gaussian distributions. Some\nhave small means. Others have large means. Some are wide, with a large\n`σ`. Others are narrow. We want our Bayesian machine to consider every\npossible distribution, each defined by a combination of `μ` and `σ`, and\nrank them by posterior plausibility. Posterior plausibility provides a\nmeasure of the logical compatibility of each possible distribution with\nthe data and model.\n\n### The data\n\n#### Original\n\nThe data contained in `data(Howell1)` are partial census data for the\nDobe area !Kung San, compiled from interviews conducted by Nancy Howell\nin the late 1960s. Much more raw data is available for download from\nhttps://tspace.library.utoronto.ca/handle/1807/10395.\n\nFor the non-anthropologists reading along, the !Kung San are the most\nfamous foraging population of the twentieth century, largely because of\ndetailed quantitative studies by people like Howell.\n\n::: callout-caution\nLoading data from a package with `data()` is only possible if you have\nalready loaded the package. In our example:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: loading-data-from-package1_a\n#| eval: false\n\n\n## R code 4.7 #######################\nlibrary(rethinking)\ndata(Howell1)\nd_a <- Howell1\n```\n````\n:::\n\n\nBecause of many function name conflicts with {**brms**} I do not want to\nload {**rethinking**} and will call the function of these conflicted\npackages with `<package name>::<function name>()` Therefore I have to\nuse another, not so usual loading strategy of the data set:\n\n\n::: {.cell}\n\n````{.cell-code #lst-loading-data-from-package2_a lst-cap=\"Load data `Howell1` from the {**rethinking**} package without loading the package and assign the data to an object. Rethinking\"}\n```{{r}}\n#| label: loading-data-from-package2_a\n#| attr-source: '#lst-loading-data-from-package2_a lst-cap=\"Load data `Howell1` from the {**rethinking**} package without loading the package and assign the data to an object. Rethinking\"'\n\ndata(package = \"rethinking\", list = \"Howell1\")\nd_a <- Howell1\n```\n````\n:::\n\n\nThe advantage of this strategy is that I have not always to detach the\n{**rethinking**} package and to make sure {**rethinking**} is detached\nbefore using {**brms**} as it is necessary in the Kurz's {**tidyverse**}\n/ {**brms**} version.\n:::\n\n##### Show the data\n\n\n::: {.cell}\n\n````{.cell-code #lst-show-howell-data-a lst-cap=\"Show and inspect the data: rethinking\"}\n```{{r}}\n#| label: show-howell-data-a\n#| attr-source: '#lst-show-howell-data-a lst-cap=\"Show and inspect the data: rethinking\"'\n\n## R code 4.8 ####################\nstr(d_a)\n\n## R code 4.9 ###################\nrethinking::precis(d_a)\n```\n````\n\n```\n#> 'data.frame':\t544 obs. of  4 variables:\n#>  $ height: num  152 140 137 157 145 ...\n#>  $ weight: num  47.8 36.5 31.9 53 41.3 ...\n#>  $ age   : num  63 63 65 41 51 35 32 27 19 54 ...\n#>  $ male  : int  1 0 0 1 0 1 0 1 0 1 ...\n#>               mean         sd      5.5%     94.5%     histogram\n#> height 138.2635963 27.6024476 81.108550 165.73500 ▁▁▁▁▁▁▁▂▁▇▇▅▁\n#> weight  35.6106176 14.7191782  9.360721  54.50289 ▁▂▃▂▂▂▂▅▇▇▃▂▁\n#> age     29.3443934 20.7468882  1.000000  66.13500     ▇▅▅▃▅▂▂▁▁\n#> male     0.4724265  0.4996986  0.000000   1.00000    ▇▁▁▁▁▁▁▁▁▇\n```\n:::\n\n\nThis data frame contains four columns. Each column has 544 entries, so\nthere are 544 individuals in these data. Each individual has a recorded\nheight (centimeters), weight (kilograms), age (years), and \"maleness\" (0\nindicating female and 1 indicating male).\n\n##### Select the height data of adults\n\nWe're going to work with just the height column, for the moment. All we\nwant for now are heights of adults in the sample. The reason to filter\nout non-adults for now is that height is strongly correlated with age,\nbefore adulthood.\n\n\n::: {.cell}\n\n````{.cell-code #lst-select-height-adults-a lst-cap=\"Select the height data of adults (individuals older or equal than 18 years): base R version\"}\n```{{r}}\n#| label: select-height-adults-a\n#| attr-source: '#lst-select-height-adults-a lst-cap=\"Select the height data of adults (individuals older or equal than 18 years): base R version\"'\n\n## R code 4.10 ###################\nhead(d_a$height)\n \n## R code 4.11 ###################\nd2_a <- d_a[d_a$age >= 18, ]\n```\n````\n\n```\n#> [1] 151.765 139.700 136.525 156.845 145.415 163.830\n```\n:::\n\n\nWe'll be working with the data frame d2 now. It should have 352 rows\n(individuals) in it. We will check this with `nrow(d2_a)` =\n352.\n\n#### Tidyverse\n\n##### Show the data\n\n\n::: {.cell}\n\n````{.cell-code #lst-loading-data-from-package_b lst-cap=\"Load data `Howell1` from the {**rethinking**} package without loading the package and assign the data to an object. Tidyverse\"}\n```{{r}}\n#| label: loading-data-from-package_b\n#| attr-source: '#lst-loading-data-from-package_b lst-cap=\"Load data `Howell1` from the {**rethinking**} package without loading the package and assign the data to an object. Tidyverse\"'\n\ndata(package = \"rethinking\", list = \"Howell1\")\nd_b <- Howell1\n```\n````\n:::\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: show-howell-data1-b\n\nd_b |>\n    glimpse()\n```\n````\n\n```\n#> Rows: 544\n#> Columns: 4\n#> $ height <dbl> 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149…\n#> $ weight <dbl> 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99259, 38.…\n#> $ age    <dbl> 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 54.0, 47.…\n#> $ male   <int> 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, …\n```\n:::\n\n\n`glimpse()` is the tidyverse analogue for `str()`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: show-howell-data2-b\nd_b |> \n    summary()\n```\n````\n\n```\n#>      height           weight            age             male       \n#>  Min.   : 53.98   Min.   : 4.252   Min.   : 0.00   Min.   :0.0000  \n#>  1st Qu.:125.09   1st Qu.:22.008   1st Qu.:12.00   1st Qu.:0.0000  \n#>  Median :148.59   Median :40.058   Median :27.00   Median :0.0000  \n#>  Mean   :138.26   Mean   :35.611   Mean   :29.34   Mean   :0.4724  \n#>  3rd Qu.:157.48   3rd Qu.:47.209   3rd Qu.:43.00   3rd Qu.:1.0000  \n#>  Max.   :179.07   Max.   :62.993   Max.   :88.00   Max.   :1.0000\n```\n:::\n\n\nKurz tells us that the {**brms**} package does not have a function that\nworks like `rethinking::precis()` for providing numeric and graphical\nsummaries of variables, as in the second part of\n@lst-show-howell-data-a. Kurz suggests `base::summary()` to get some of\nthe information from `rethinking::precis()`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: show-howell-data3-b\nd_b |>            \n    skimr::skim() \n```\n````\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |     |\n|:------------------------|:----|\n|Name                     |d_b  |\n|Number of rows           |544  |\n|Number of columns        |4    |\n|_______________________  |     |\n|Column type frequency:   |     |\n|numeric                  |4    |\n|________________________ |     |\n|Group variables          |None |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|    sd|    p0|    p25|    p50|    p75|   p100|hist  |\n|:-------------|---------:|-------------:|------:|-----:|-----:|------:|------:|------:|------:|:-----|\n|height        |         0|             1| 138.26| 27.60| 53.98| 125.10| 148.59| 157.48| 179.07|▁▂▂▇▇ |\n|weight        |         0|             1|  35.61| 14.72|  4.25|  22.01|  40.06|  47.21|  62.99|▃▂▃▇▂ |\n|age           |         0|             1|  29.34| 20.75|  0.00|  12.00|  27.00|  43.00|  88.00|▇▆▅▂▁ |\n|male          |         0|             1|   0.47|  0.50|  0.00|   0.00|   0.00|   1.00|   1.00|▇▁▁▁▇ |\n:::\n:::\n\n\nI think `skimr::skim()` is a better option as an alternative to\n`rethinking::precis()` as `base::summary()` because it also has a\ngraphical summary of the variables. {**skimr**} has many other useful\nfunctions and is very adaptable. I propose to install and to try it out.\n\n##### Select the height data of adults\n\nWith {**tidyverse**} we can isolate height values with the\n`dplyr::select()` function and we are using the `dplyr::filter()`\nfunction to make an adults-only data frame.\n\n\n::: {.cell}\n\n````{.cell-code #lst-select-height-adults-b lst-cap=\"Select the height data of adults (individuals older or equal than 18 years): tidyverse version\"}\n```{{r}}\n#| label: select-height-adults-b\n#| attr-source: '#lst-select-height-adults-b lst-cap=\"Select the height data of adults (individuals older or equal than 18 years): tidyverse version\"'\n\nd_b %>%\n  select(height) %>% \n  glimpse()\n\nd2_b <- \n  d_b %>%\n  filter(age >= 18) \n \nglimpse(d2_b)\n```\n````\n\n```\n#> Rows: 544\n#> Columns: 1\n#> $ height <dbl> 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149…\n#> Rows: 352\n#> Columns: 4\n#> $ height <dbl> 151.7650, 139.7000, 136.5250, 156.8450, 145.4150, 163.8300, 149…\n#> $ weight <dbl> 47.82561, 36.48581, 31.86484, 53.04191, 41.27687, 62.99259, 38.…\n#> $ age    <dbl> 63.0, 63.0, 65.0, 41.0, 51.0, 35.0, 32.0, 27.0, 19.0, 54.0, 47.…\n#> $ male   <int> 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, …\n```\n:::\n\n\nThe two functions of @lst-select-height-adults-b are much more readable\nand understandable as the weird base R syntax in\n@lst-select-height-adults-a.\n\n### The model\n\n#### Original\n\nOur goal is to model the data in `d2_a` using a Gaussian distribution.\n\nPlot the distribution of heights\n\n\n::: {.cell}\n\n````{.cell-code #lst-fig-dist-heights-a lst-cap=\"Plot the distribution of the heights of adults: rethinking version\"}\n```{{r}}\n#| label: fig-dist-heights-a\n#| fig-cap: \"The distribution of the heights data,overlaid by an ideal Gaussian distribution: rethinking version\"\n#| attr-source: '#lst-fig-dist-heights-a lst-cap=\"Plot the distribution of the heights of adults: rethinking version\"'\n\nrethinking::dens(d2_a$height, norm.comp = TRUE)\n```\n````\n\n::: {.cell-output-display}\n![The distribution of the heights data,overlaid by an ideal Gaussian distribution: rethinking version](04-geocentric-models_files/figure-html/fig-dist-heights-a-1.png){#fig-dist-heights-a width=672}\n:::\n:::\n\n\nWith the option `norm.comp = TRUE` I have overlaid a Gaussian\ndistribution to see the differences to the actual data. There are some\ndifferences locally, especially on the peak of the distribution. But the\ntails looks nice and we can say that the overall impression of the curve\nis Gaussian.\n\n::: callout-caution\n###### Decisions how to model the data\n\nGawking at the raw data, to try to decide how to model them, is usually\nnot a good idea. The data could be, for example, a mixture of different\nGaussian distributions. Furthermore, the empirical distribution need not\nbe actually Gaussian in order to justify using a Gaussian probability\ndistribution.\n:::\n\n::: {#def-heights-normal}\nDefine the heights as normally distributed with a mean `μ` and standard\ndeviation `σ`\n\n$$\nh_{i} \\sim Normal(σ, μ) \n$$\n:::\n\nThe symbol `h` refers to the list of heights, and the subscript `i`\nmeans each individual element of this list. It is conventional to use\n`i` because it stands for index. The index `i` takes on row numbers, and\nso in this example can take any value from 1 to 352 (the number of\nheights in `d2_a$height`). As such, the model above is saying that all\nthe golem knows about each height measurement is defined by the same\nnormal distribution, with mean `μ` and standard deviation `σ`.\n\nThe short model in @def-heights-normal assumes that the values $h_{i}$\nare *independent and identically distributed*, abbreviated `i.i.d.`,\n`iid`, or `IID`.\n\nTo complete the model, we're going to need some priors. The parameters\nto be estimated are both `μ` and `σ`, so we need a prior `Pr(μ, σ)`, the\njoint prior probability for all parameters. In most cases, priors are\nspecified independently for each parameter, which amounts to assuming\n$Pr(μ,σ) = Pr(μ)Pr(σ)$.\n\n::: {#def-height-priors}\nPriors for heights model\n\n$$\n\\begin{align*}\nh_{i} \\sim Normal(μ, σ)  \\\\ \nμ \\sim Normal(178, 20)   \\\\ \nμ \\sim Uniform(0, 50)       \n\\end{align*}\n$$\n\n1.  First line represents the likelihood.\n2.  Second line is the chosen `μ` prior.\n3.  Third line is the chosen `σ` prior.\n:::\n\nLet's think about the chosen value for the priors more in detail:\n\nThe prior for `μ` is a broad Gaussian prior, centered on 178 cm, with\n95% of probability between 178 ± 40 cm.\n\nWhy 178 cm? Your author is 178 cm tall. And the range from 138 cm to 218\ncm encompasses a huge range of plausible mean heights for human\npopulations. So domain-specific information has gone into this prior.\nEveryone knows something about human height and can set a reasonable and\nvague prior of this kind. But in many regression problems, as you'll see\nlater, using prior information is more subtle, because parameters don't\nalways have such clear physical meaning.\n\nWhatever the prior, it's a very good idea to plot your priors, so you\nhave a sense of the assumption they build into the model.\n\n**Plot the mu prior (mean)**\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-mean-prior-a\n#| fig-cap: \"Plot of the chosen mean prior: base R version\"\n\n## R code 4.12 ###############################\ncurve(dnorm(x, 178, 20), from = 100, to = 250)\n```\n````\n\n::: {.cell-output-display}\n![Plot of the chosen mean prior: base R version](04-geocentric-models_files/figure-html/fig-mean-prior-a-1.png){#fig-mean-prior-a width=672}\n:::\n:::\n\n\nYou can see that the golem is assuming that the average height (not each\nindividual height) is almost certainly between 140 cm and 220 cm. So\nthis prior carries a little information, but not a lot.\n\n**Plot the sigma prior (standard deviation)**\n\nA standard deviation like `σ` must be positive, so bounding it at zero\nmakes sense. How should we pick the upper bound? In this case, a\nstandard deviation of 50 cm would imply that 95% of individual heights\nlie within 100 cm of the average height. That's a very large range.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-sd-prior-a\n#| fig-cap: \"Plot the chosen prior for the standard deviation: base R version\"\n\n## R code 4.13 ###########################\ncurve(dunif(x, 0, 50), from = -10, to = 60)\n```\n````\n\n::: {.cell-output-display}\n![Plot the chosen prior for the standard deviation: base R version](04-geocentric-models_files/figure-html/fig-sd-prior-a-1.png){#fig-sd-prior-a width=672}\n:::\n:::\n\n\n**Prior predictive simulation**\n\n> Once you've chosen priors for *h, μ*, and *σ*, these imply a joint\n> prior distribution of individual heights. By simulating from this\n> distribution, you can see what your choices imply about observable\n> height. This helps you diagnose bad choices.\n\nOkay, so how to do this? You can quickly simulate heights by sampling\nfrom the prior, like you sampled from the posterior back in\n@sec-sampling-the-imaginary. Remember, every posterior is also\npotentially a prior for a subsequent analysis, so you can process priors\njust like posteriors.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-prior-predictive-sim-a\n#| fig-cap: \"Simulate heights by sampling from the prior: rethinking version\"\n\nset.seed(4) # to make example reproducible\n## R code 4.14 #######################################\nsample_mu_a <- rnorm(1e4, 178, 20)\nsample_sigma_a <- runif(1e4, 0, 50)\nprior_h_a <- rnorm(1e4, sample_mu_a, sample_sigma_a)\nrethinking::dens(prior_h_a, norm.comp = TRUE)\n```\n````\n\n::: {.cell-output-display}\n![Simulate heights by sampling from the prior: rethinking version](04-geocentric-models_files/figure-html/fig-prior-predictive-sim-a-1.png){#fig-prior-predictive-sim-a width=672}\n:::\n:::\n\n\n> It displays a vaguely bell-shaped density with thick tails. It is the\n> expected distribution of heights, averaged over the prior. Notice that\n> the prior probability distribution of height is not itself Gaussian.\n> This is okay. The distribution you see is not an empirical\n> expectation, but rather the distribution of relative plausibilities of\n> different heights, before seeing the data.\n\nThis comment is strange for me as in my point of view the distribution\n*is* Gaussian. It is true that the tails are (a little bit?) thicker\nthan in the standard Gaussian distribution. But in my view\n@fig-prior-predictive-sim-a is more Gaussian than @fig-dist-heights-a.\nOK, in @fig-dist-heights-a we have just 352 data and in\n@fig-prior-predictive-sim-a we sampled 10,000 times. But this is not a\ncounter argument for @fig-prior-predictive-sim-a not being a a bell\nshaped distribution.\n\n**Simulate heights from priors with large sd**\n\nPrior predictive simulation is very useful for assigning sensible\npriors, because it can be quite hard to anticipate how priors influence\nthe observable variables. As an example, consider a much flatter and\nless informative prior for `μ`, like $μ \\sim Normal(178, 100)$. Priors\nwith such large standard deviations are quite common in Bayesian models,\nbut they are hardly ever sensible.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-prior-predictive-sim2-a\n#| fig-cap: \"Simulate heights from priors with a large standard deviation: rethinking version\"\n\nset.seed(4) # to make example reproducible\n## R code 4.15 ############################\nsample_mu2_a <- rnorm(1e4, 178, 100)\nprior_h2_a <- rnorm(1e4, sample_mu2_a, sample_sigma_a)\nrethinking::dens(prior_h2_a)\n```\n````\n\n::: {.cell-output-display}\n![Simulate heights from priors with a large standard deviation: rethinking version](04-geocentric-models_files/figure-html/fig-prior-predictive-sim2-a-1.png){#fig-prior-predictive-sim2-a width=672}\n:::\n:::\n\n\nThe results of @fig-prior-predictive-sim2-a contradicts our scientific\nknowledge --- but also our common sense --- about possible height values\nof humans. Now the model, before seeing the data, expects people to have\nnegative height. It also expects some giants. One of the tallest people\nin recorded history, [Robert Pershing\nWadlow](https://en.wikipedia.org/wiki/Robert_Wadlow) (1918--1940) stood\n272 cm tall. In our prior predictive simulation many people are taller\nthan this.\n\nDoes this matter? In this case, we have so much data that the silly\nprior is harmless. But that won't always be the case. There are plenty\nof inference problems for which the data alone are not sufficient, no\nmatter how numerous. Bayes lets us proceed in these cases. But only if\nwe use our scientific knowledge to construct sensible priors. Using\nscientific knowledge to build priors is not cheating. The important\nthing is that your prior not be based on the values in the data, but\nonly on what you know about the data before you see it.\n\n#### Tidyverse\n\nThe plot of the heights distribution compared with the standard Gaussian\ndistribution is missing in Kurz's version. I added this plot by using\nthe last example of [How to Plot a Normal Distribution in\nR](https://www.statology.org/plot-normal-distribution-r/).\n\n\n::: {.cell}\n\n````{.cell-code #lst-fig-dist-heights-b lst-cap=\"Plot the distribution of the heights of adults: tidyverse version\"}\n```{{r}}\n#| label: fig-dist-heights-b\n#| fig-cap: \"The distribution of the heights data, overlaid by an ideal Gaussian distribution: tidyverse version\"\n#| attr-source: '#lst-fig-dist-heights-b lst-cap=\"Plot the distribution of the heights of adults: tidyverse version\"'\n\np0 <- \n    d2_b |> \n    ggplot(aes(height)) +\n    geom_density() +\n\n    stat_function(\n        fun = dnorm,\n        args = with(d2_b, c(mean = mean(height), sd = sd(height)))\n        ) +\n    scale_x_continuous(\"Height in cm\")\n\np0\n```\n````\n\n::: {.cell-output-display}\n![The distribution of the heights data, overlaid by an ideal Gaussian distribution: tidyverse version](04-geocentric-models_files/figure-html/fig-dist-heights-b-1.png){#fig-dist-heights-b width=672}\n:::\n:::\n\n\nHere is the shape for the prior $μ \\sim Normal(178, 20)$.\n\n\n::: {.cell}\n\n````{.cell-code #lst-fig-mean-prior-b lst-cap=\"Plot of the chosen mean prior: tidyverse version\"}\n```{{r}}\n#| label: fig-mean-prior-b\n#| fig-cap: \"Plot of the chosen mean prior: tidyverse version\"\n#| attr-source: '#lst-fig-mean-prior-b lst-cap=\"Plot of the chosen mean prior: tidyverse version\"'\n\np1 <-\n  tibble(x = seq(from = 100, to = 250, by = .1)) %>% \n    \n  ggplot(aes(x = x, y = dnorm(x, mean = 178, sd = 20))) +\n  geom_line() +\n  scale_x_continuous(breaks = seq(from = 100, to = 250, by = 75)) +\n  labs(title = \"mu ~ dnorm(178, 20)\",\n       y = \"density\")\n\np1\n```\n````\n\n::: {.cell-output-display}\n![Plot of the chosen mean prior: tidyverse version](04-geocentric-models_files/figure-html/fig-mean-prior-b-1.png){#fig-mean-prior-b width=672}\n:::\n:::\n\n\nAnd here's the ggplot2 code for our prior for `σ`, a uniform\ndistribution with a minimum value of 0 and a maximum value of 50. We\ndon't really need the `y`-axis when looking at the shapes of a density,\nso we'll just remove it with `scale_y_continuous()`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-sd-prior-b\n#| fig-cap: \"Plot the chosen prior for the standard deviation: tidyverse version\"\n\np2 <-\n  tibble(x = seq(from = -10, to = 60, by = .1)) %>%\n  \n  ggplot(aes(x = x, y = dunif(x, min = 0, max = 50))) +\n  geom_line() +\n  scale_x_continuous(breaks = c(0, 50)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  ggtitle(\"sigma ~ dunif(0, 50)\")\n\np2\n```\n````\n\n::: {.cell-output-display}\n![Plot the chosen prior for the standard deviation: tidyverse version](04-geocentric-models_files/figure-html/fig-sd-prior-b-1.png){#fig-sd-prior-b width=672}\n:::\n:::\n\n\nWe can simulate from both priors at once to get a prior probability\ndistribution of `height`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-prior-predictive-sim-b\n#| fig-cap: \"Simulate heights by sampling from the prior: tidyverse version\"\n\nn <- 1e4\nset.seed(4)\n\nsim <-\n  tibble(sample_mu_b    = rnorm(n, mean = 178, sd  = 20),\n         sample_sigma_b = runif(n, min = 0, max = 50)) %>% \n  mutate(height = rnorm(n, mean = sample_mu_b, sd = sample_sigma_b))\n  \np3 <- sim %>% \n  ggplot(aes(x = height)) +\n  geom_density(fill = \"deepskyblue\") +\n  scale_x_continuous(breaks = c(0, 73, 178, 283)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  ggtitle(\"height ~ dnorm(mu, sigma)\") +\n  theme(panel.grid = element_blank())\n\np3\n```\n````\n\n::: {.cell-output-display}\n![Simulate heights by sampling from the prior: tidyverse version](04-geocentric-models_files/figure-html/fig-prior-predictive-sim-b-1.png){#fig-prior-predictive-sim-b width=672}\n:::\n:::\n\n\nIf you look at the `x`-axis breaks on the plot in McElreath's lower left\npanel in Figure 4.3, you'll notice they're intentional. To compute the\nmean and 3 standard deviations above and below, you might do this.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: compute-mean-3sd-b\nsim %>% \n  summarise(ll   = mean(height) - sd(height) * 3,\n            mean = mean(height),\n            ul   = mean(height) + sd(height) * 3) %>% \n  mutate_all(round, digits = 1)\n```\n````\n\n```\n#> # A tibble: 1 × 3\n#>      ll  mean    ul\n#>   <dbl> <dbl> <dbl>\n#> 1  73.9  177.  281.\n```\n:::\n\n\nHere's the work to make the lower right panel of Figure 4.3.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-reproduce-4.3-low-right\n#| fig-cap: \"Reproduce lower right panels of Figure 4.3\"\n\n\n# simulate\nset.seed(4)\n\nsim <-\n  tibble(sample_mu_b    = rnorm(n, mean = 178, sd = 100),\n         sample_sigma_b = runif(n, min = 0, max = 50)) %>% \n  mutate(height = rnorm(n, mean = sample_mu_b, sd = sample_sigma_b))\n\n# compute the values we'll use to break on our x axis\nbreaks <-\n  c(mean(sim$height) - 3 * sd(sim$height), 0, mean(sim$height), mean(sim$height) + 3 * sd(sim$height)) %>% \n  round(digits = 0)\n\n# this is just for aesthetics\ntext <-\n  tibble(height = 272 - 25,\n         y      = .0013,\n         label  = \"tallest man\",\n         angle  = 90)\n\n# plot\np4 <-\n  sim %>% \n  ggplot(aes(x = height)) +\n  geom_density(fill = \"deepskyblue\", color = \"black\") +\n  geom_vline(xintercept = 0, color = \"black\") +\n  geom_vline(xintercept = 272, color = \"black\", linetype = 3) +\n  geom_text(data = text,\n            aes(y = y, label = label, angle = angle),\n            color = \"black\") +\n  scale_x_continuous(breaks = breaks) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  ggtitle(\"height ~ dnorm(mu, sigma)\\nmu ~ dnorm(178, 100)\") +\n  theme(panel.grid = element_blank())\n\np4\n```\n````\n\n::: {.cell-output-display}\n![Reproduce lower right panels of Figure 4.3](04-geocentric-models_files/figure-html/fig-reproduce-4.3-low-right-1.png){#fig-reproduce-4.3-low-right width=672}\n:::\n:::\n\n\nLet's combine the four to make our version of McElreath's Figure 4.3.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-reproduce-3.4\n#| fig-cap: \"Reproduction of Figure 3.4\"\n\nlibrary(patchwork)\n(p1 + xlab(\"mu\") | p2 + xlab(\"sigma\")) / (p3 | p4)\n```\n````\n\n::: {.cell-output-display}\n![Reproduction of Figure 3.4](04-geocentric-models_files/figure-html/fig-reproduce-3.4-1.png){#fig-reproduce-3.4 width=672}\n:::\n:::\n\n\nOn page 84, McElreath said his prior simulation indicated 4% of the\nheights would be below zero. He also drew the break down compared to the\ntallest man on record, [Robert Pershing\nWadlow](https://en.wikipedia.org/wiki/Robert_Wadlow) (1918--1940).\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: calc-breaks-b\n\nsim %>% \n  count(height < 0) %>% \n  mutate(percent = 100 * n / sum(n))\n\nsim %>% \n  count(height < 272) %>% \n  mutate(percent = 100 * n / sum(n))\n```\n````\n\n```\n#> # A tibble: 2 × 3\n#>   `height < 0`     n percent\n#>   <lgl>        <int>   <dbl>\n#> 1 FALSE         9571   95.7 \n#> 2 TRUE           429    4.29\n#> # A tibble: 2 × 3\n#>   `height < 272`     n percent\n#>   <lgl>          <int>   <dbl>\n#> 1 FALSE           1761    17.6\n#> 2 TRUE            8239    82.4\n```\n:::\n\n\n### Grid approximation of the posterior distribution\n\n#### Original\n\nWe are going to map out the posterior distribution through brute force\ncalculations.\n\nThis is not recommended because it is\n\n-   laborious and computationally expensive\n-   usually so impractical as to be essentially impossible.\n\nTherefor the grid approximation technique has limited relevance. Later\non we will use the quadratic approximation with `rethinking::quap()`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: grid-approx-posterior-a\n\n## R code 4.16 ##################################\n\n# establish range of μ and σ values, respectively, to calculate over \n# as well as how many points to calculate in-between. \nmu.list_a <- seq(from = 150, to = 160, length.out = 100)\nsigma.list_a <- seq(from = 7, to = 9, length.out = 100)\n\n# expands μ & σ values into a matrix of all of the combinations\npost_a <- expand.grid(mu_a = mu.list_a, sigma_a = sigma.list_a)\n\n# compute the log-likelihood at each combination of μ and σ\npost_a$LL <- sapply(1:nrow(post_a), function(i) {\n  sum(\n    dnorm(d2_a$height, post_a$mu[i], post_a$sigma[i], log = TRUE)\n  )\n})\n\n# multiply the prior by the likelihood\n# as the priors are on the log scale adding = multiplying\npost_a$prod <- post_a$LL + dnorm(post_a$mu_a, 178, 20, TRUE) +\n  dunif(post_a$sigma_a, 0, 50, TRUE)\n\n# getting back on the probability scale without rounding error \npost_a$prob <- exp(post_a$prod - max(post_a$prod))\n```\n````\n:::\n\n\n> **Comment to the last line**: the obstacle for getting back on the\n> probability scale is that rounding error is always a threat when\n> moving from log-probability to probability. If you use the obvious\n> approach, like `exp( post$prod )`, you'll get a vector full of zeros,\n> which isn't very helpful. This is a result of R's rounding very small\n> probabilities to zero.\n\n**Plot contour lines**\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-contour-plot-a\n#| fig-cap: \"Draw a contour plot: rethinking version\"\n\n## R code 4.17 ##################################\nrethinking::contour_xyz(post_a$mu_a, post_a$sigma_a, post_a$prob)\n```\n````\n\n::: {.cell-output-display}\n![Draw a contour plot: rethinking version](04-geocentric-models_files/figure-html/fig-contour-plot-a-1.png){#fig-contour-plot-a width=672}\n:::\n:::\n\n\nYou can inspect this posterior distribution, now residing in\n`post_a$prob`, using a variety of plotting commands.\n\n**Plot heat map**\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-heat-map-a\n#| fig-cap: \"Draw a heat map: rethinking version\"\n\n## R code 4.18 ##################################\nrethinking::image_xyz(post_a$mu_a, post_a$sigma_a, post_a$prob)\n```\n````\n\n::: {.cell-output-display}\n![Draw a heat map: rethinking version](04-geocentric-models_files/figure-html/fig-heat-map-a-1.png){#fig-heat-map-a width=672}\n:::\n:::\n\n\n#### Tidyverse\n\nWith grid approximation we are going to use the brute force method for\nthe calculation of the posterior distribution. This technique has\nlimited relevance. Later on we will use the quadratic approximation with\n`brms::brm()`.\n\nIt is the same technique we have use in\n@sec-sampling-from-a-grid-approximate-posterior respectively in the\ntidyverse version in @sec-grid-approximation-b. As there is no\nconceptually new information to learn, I am not going into the details\nof the following code. (It combines several code chunk from Kurz's\nversion.) But I am going to foreshadow the most important differences in\nthe tidyverse approach of the grid approximation technique:\n\nInstead of `base::grid_expand()` we will use `tidyr::crossing()` Instead\nof `base::sapply()` we will use `purr::map2()`\n\nThe produced tibble contains data frames in its cells, so that we have\nto use the `tidyr::unnest()` function to expand the list-column\ncontaining data frames into rows and columns.\n\nReferring to the plots:\n\n-   Instead of `rethinking::contour_xyz()` we will use\n    `ggplot2::geom_contour()`\n-   Instead of `rethinking::image_xyz()` we will use\n    `ggplot2::geom_raster()`\n\n\n::: {.cell}\n\n````{.cell-code #lst-grid-approx-posterior-b lst-cap=\"Grid Approximation of the posterior distribution: tidyverse version\"}\n```{{r}}\n#| label: grid-approx-posterior-b\n#| attr-source: '#lst-grid-approx-posterior-b lst-cap=\"Grid Approximation of the posterior distribution: tidyverse version\"'\n\nn <- 200\n\nd_grid_b <-\n  # we'll accomplish with `tidyr::crossing()` what McElreath did with base R `expand.grid()`\n  crossing(mu_b    = seq(from = 140, to = 160, length.out = n),\n           sigma_b = seq(from = 4, to = 9, length.out = n))\n\nglimpse(d_grid_b)\n\ngrid_function <- function(mu, sigma) {\n  \n  dnorm(d2_b$height, mean = mu, sd = sigma, log = TRUE) %>% \n    sum()\n  \n}\n\nd_grid2_b <-\n  d_grid_b %>% \n  mutate(log_likelihood_b = map2(mu_b, sigma_b, grid_function)) %>%\n  unnest(log_likelihood_b) %>% \n  mutate(prior_mu_b    = dnorm(mu_b, mean = 178, sd = 20, log = T),\n         prior_sigma_b = dunif(sigma_b, min = 0, max = 50, log = T)) %>% \n  mutate(product_b = log_likelihood_b + prior_mu_b + prior_sigma_b) %>% \n  mutate(probability_b = exp(product_b - max(product_b)))\n  \nhead(d_grid2_b)\n```\n````\n\n```\n#> Rows: 40,000\n#> Columns: 2\n#> $ mu_b    <dbl> 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 140, 14…\n#> $ sigma_b <dbl> 4.000000, 4.025126, 4.050251, 4.075377, 4.100503, 4.125628, 4.…\n#> # A tibble: 6 × 7\n#>    mu_b sigma_b log_likelihood_b prior_mu_b prior_sigma_b product_b\n#>   <dbl>   <dbl>            <dbl>      <dbl>         <dbl>     <dbl>\n#> 1   140    4              -3813.      -5.72         -3.91    -3822.\n#> 2   140    4.03           -3778.      -5.72         -3.91    -3787.\n#> 3   140    4.05           -3743.      -5.72         -3.91    -3753.\n#> 4   140    4.08           -3709.      -5.72         -3.91    -3719.\n#> 5   140    4.10           -3676.      -5.72         -3.91    -3686.\n#> 6   140    4.13           -3644.      -5.72         -3.91    -3653.\n#> # ℹ 1 more variable: probability_b <dbl>\n```\n:::\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-contour-b\n#| fig-cap: \"Draw 2D contours of a 3D surface\"\n\nd_grid2_b %>% \n  ggplot(aes(x = mu_b, y = sigma_b, z = probability_b)) + \n  geom_contour() +\n  labs(x = expression(mu),\n       y = expression(sigma)) +\n  coord_cartesian(xlim = range(d_grid2_b$mu_b),\n                  ylim = range(d_grid2_b$sigma_b)) +\n  theme(panel.grid = element_blank())\n```\n````\n\n::: {.cell-output-display}\n![Draw 2D contours of a 3D surface](04-geocentric-models_files/figure-html/fig-contour-b-1.png){#fig-contour-b width=672}\n:::\n:::\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-heatmap-b\n#| fig-cap: \"Draw heat map\"\n\nd_grid2_b %>% \n  ggplot(aes(x = mu_b, y = sigma_b, fill = probability_b)) + \n  geom_raster(interpolate = TRUE) +\n  scale_fill_viridis_c(option = \"B\") +\n  labs(x = expression(mu),\n       y = expression(sigma)) +\n  theme(panel.grid = element_blank())\n```\n````\n\n::: {.cell-output-display}\n![Draw heat map](04-geocentric-models_files/figure-html/fig-heatmap-b-1.png){#fig-heatmap-b width=672}\n:::\n:::\n\n\n### Sampling from the posterior\n\n#### Original\n\nTo study this posterior distribution in more detail, again I'll push the\nflexible approach of sampling parameter values from it. This works just\nlike it did in @sec-sampling-to-summarize, when you sampled values of\n`p` from the posterior distribution for the globe tossing example. The\nonly new trick is that since there are two parameters, and we want to\nsample combinations of them, we first randomly sample row numbers in\npost in proportion to the values in \\`post_a\\$prob´. Then we pull out\nthe parameter values on those randomly sampled rows.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-posterior-sample-a\n#| fig-cap: \"Samples from the posterior distribution for the heights data. The density of points is highest in the center, reflecting the most plausible combinations of μ and σ. There are many more ways for these parameter values to produce the data, conditional on the model. (rethinking version)\"\n\n## R code 4.19 ###########################\n\n# randomly sample row numbers in post_a \n# in proportion to the values in post_a$prob. \nsample.rows <- sample(1:nrow(post_a),\n  size = 1e4, replace = TRUE,\n  prob = post_a$prob\n)\n\n# pull out the parameter values\nsample.mu_a <- post_a$mu[sample.rows]\nsample.sigma_a <- post_a$sigma[sample.rows]\n\n## R code 4.20 ###########################\nplot(sample.mu_a, sample.sigma_a, cex = 0.8, pch = 21, col = rethinking::col.alpha(rethinking:::rangi2, 0.1))\n```\n````\n\n::: {.cell-output-display}\n![Samples from the posterior distribution for the heights data. The density of points is highest in the center, reflecting the most plausible combinations of μ and σ. There are many more ways for these parameter values to produce the data, conditional on the model. (rethinking version)](04-geocentric-models_files/figure-html/fig-posterior-sample-a-1.png){#fig-posterior-sample-a width=672}\n:::\n:::\n\n\nThe function `col.alpha()` is part of the {**rethinking**} R package.\nAll it does is make colors transparent, which helps the plot in FIGURE\n4.4 (here: @fig-posterior-sample-a) more easily show density, where\nsamples overlap. Adjust the plot to your tastes by playing around with\n`cex` (character expansion, the size of the points), `pch` (plot\ncharacter), and the 0.1 transparency value.\n\n**Marginal Posterior Density**\n\nNow that you have these samples, you can describe the distribution of\nconfidence in each combination of `μ` and `σ` by summarizing the\nsamples. Think of them like data and describe them, just like in\n@sec-sampling-to-summarize. For example, to characterize the shapes of\nthe marginal posterior densities of `μ` and `σ`, all we need to do is:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-marg-post-density-a\n#| fig-cap: \"Shapes of the marginal posterior densities of μ and σ: rethinking version\"\n\n## R code 4.21 #########################\nrethinking::dens(sample.mu_a)\nrethinking::dens(sample.sigma_a)\n```\n````\n\n::: {.cell-output-display}\n![Shapes of the marginal posterior densities of μ and σ: rethinking version](04-geocentric-models_files/figure-html/fig-marg-post-density-a-1.png){#fig-marg-post-density-a-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Shapes of the marginal posterior densities of μ and σ: rethinking version](04-geocentric-models_files/figure-html/fig-marg-post-density-a-2.png){#fig-marg-post-density-a-2 width=672}\n:::\n:::\n\n\nThe jargon \"marginal\" here means \"averaging over the other parameters.\"\nExecute the above code and inspect the plots. These densities are very\nclose to being normal distributions. And this is quite typical. As\nsample size increases, posterior densities approach the normal\ndistribution. If you look closely, though, you'll notice that the\ndensity for σ has a longer right-hand tail. I'll exaggerate this\ntendency a bit later, to show you that this condition is very common for\nstandard deviation parameters.\n\n**Posterior Compatibility Intervals (PIs)**\n\nTo summarize the widths of these densities with posterior compatibility\nintervals we use:\n\n\n::: {.cell}\n\n````{.cell-code #lst-post-comp-intervals-a lst-cap=\"Posterior Compatibility Intervals (PIs): rethinking version\"}\n```{{r}}\n#| label: post-comp-intervals-a\n#| attr-source: '#lst-post-comp-intervals-a lst-cap=\"Posterior Compatibility Intervals (PIs): rethinking version\"'\n\n## R code 4.22 ####################\nrethinking::PI(sample.mu_a)\nrethinking::PI(sample.sigma_a)\n```\n````\n\n```\n#>       5%      94% \n#> 153.9394 155.2525 \n#>       5%      94% \n#> 7.323232 8.252525\n```\n:::\n\n\nSince these samples are just vectors of numbers, you can compute any\nstatistic from them that you could from ordinary data: `mean`, `median`,\nor `quantile`, for example.\n\n**Sample size and the normality of sigmas posterior**\n\nBefore moving on to using quadratic approximation `rethinking::quap()`\nas shortcut to all of this inference, it is worth repeating the analysis\nof the height data above, but now with only a fraction of the original\ndata. The reason to do this is to demonstrate that, in principle, the\nposterior is not always so Gaussian in shape. There's no trouble with\nthe mean, `μ`. For a Gaussian likelihood and a Gaussian prior on `μ`,\nthe posterior distribution is always Gaussian as well, regardless of\nsample size. It is the standard deviation `σ` that causes problems. So\nif you care about `σ`---often people do not---you do need to be careful\nof abusing the quadratic approximation.\n\nThe deep reasons for the posterior of `σ` tending to have a long\nright-hand tail are complex. But a useful way to conceive of the problem\nis that variances must be positive. As a result, there must be more\nuncertainty about how big the variance (or standard deviation) is than\nabout how small it is. For example, if the variance is estimated to be\nnear zero, then you know for sure that it can't be much smaller. But it\ncould be a lot bigger.\n\nLet's quickly analyze only 20 of the heights from the height data to\nreveal this issue. To sample 20 random heights from the original list:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-sample-only-20-a\n#| fig-cap: \"Sample 20 heights: rethinking version\"\n\n## R code 4.23 ######################################\nd3_a <- sample(d2_a$height, size = 20)\n\n## R code 4.24 ######################################\nmu2_a.list <- seq(from = 150, to = 170, length.out = 200)\nsigma2_a.list <- seq(from = 4, to = 20, length.out = 200)\npost2_a <- expand.grid(mu = mu2_a.list, sigma = sigma2_a.list)\npost2_a$LL <- sapply(1:nrow(post2_a), function(i) {\n  sum(dnorm(d3_a,\n    mean = post2_a$mu[i], sd = post2_a$sigma[i],\n    log = TRUE\n  ))\n})\npost2_a$prod <- post2_a$LL + dnorm(post2_a$mu, 178, 20, TRUE) +\n  dunif(post2_a$sigma, 0, 50, TRUE)\npost2_a$prob <- exp(post2_a$prod - max(post2_a$prod))\nsample2_a.rows <- sample(1:nrow(post2_a),\n  size = 1e4, replace = TRUE,\n  prob = post2_a$prob\n)\nsample2_a.mu <- post2_a$mu[sample2_a.rows]\nsample2_a.sigma <- post2_a$sigma[sample2_a.rows]\nplot(sample2_a.mu, sample2_a.sigma,\n  cex = 0.5,\n  col = rethinking::col.alpha(rethinking:::rangi2, 0.1),\n  xlab = \"mu\", ylab = \"sigma\", pch = 16\n)\n```\n````\n\n::: {.cell-output-display}\n![Sample 20 heights: rethinking version](04-geocentric-models_files/figure-html/fig-sample-only-20-a-1.png){#fig-sample-only-20-a width=672}\n:::\n:::\n\n\nyou'll see another scatter plot of the samples from the posterior\ndensity, but this time you'll notice a distinctly longer tail at the top\nof the cloud of points.\n\n**Marginal Posterior Density with only 20 rows**\n\nYou should also inspect the marginal posterior density for σ, averaging\nover μ, produced with:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-marg-post-density-a2\n#| fig-cap: \"Marginal posterior density for σ, averaging over μ: rethinking version\"\n\n## R code 4.25\nrethinking::dens(sample2_a.sigma, norm.comp = TRUE)\n```\n````\n\n::: {.cell-output-display}\n![Marginal posterior density for σ, averaging over μ: rethinking version](04-geocentric-models_files/figure-html/fig-marg-post-density-a2-1.png){#fig-marg-post-density-a2 width=672}\n:::\n:::\n\n\n#### Tidyverse\n\nWe can use `dplyr::sample_n()` to sample rows, with replacement, from\n`d_grid2_b`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-posterior-sample-b\n#| fig-cap: \"Samples from the posterior distribution for the heights data. The density of points is highest in the center, reflecting the most plausible combinations of μ and σ. There are many more ways for these parameter values to produce the data, conditional on the model. (tidyverse version)\"\n\n\nset.seed(4)\n\nd_grid_samples_b <- \n  d_grid2_b %>% \n  sample_n(size = 1e4, replace = T, weight = probability_b)\n\nd_grid_samples_b %>% \n  ggplot(aes(x = mu_b, y = sigma_b)) + \n  geom_point(size = .9, alpha = 1/15) +\n  scale_fill_viridis_c() +\n  labs(x = expression(mu[samples]),\n       y = expression(sigma[samples])) +\n  theme(panel.grid = element_blank())\n```\n````\n\n::: {.cell-output-display}\n![Samples from the posterior distribution for the heights data. The density of points is highest in the center, reflecting the most plausible combinations of μ and σ. There are many more ways for these parameter values to produce the data, conditional on the model. (tidyverse version)](04-geocentric-models_files/figure-html/fig-posterior-sample-b-1.png){#fig-posterior-sample-b width=672}\n:::\n:::\n\n\nWe can use `tidyr::pivot_longer()` and then `ggplot2::facet_wrap()` to\nplot the densities for both `mu` and `sigma` at once.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-densities-mu-sigma\n#| fig-cap: \"Shapes of the marginal posterior densities of μ and σ: tidyverse version\"\n\nd_grid_samples_b %>% \n  pivot_longer(mu_b:sigma_b) %>% \n\n  ggplot(aes(x = value)) + \n  geom_density(fill = \"deepskyblue\", color = \"black\") +\n  scale_y_continuous(NULL, breaks = NULL) +\n  xlab(NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free\", labeller = label_parsed)\n```\n````\n\n::: {.cell-output-display}\n![Shapes of the marginal posterior densities of μ and σ: tidyverse version](04-geocentric-models_files/figure-html/fig-densities-mu-sigma-1.png){#fig-densities-mu-sigma width=672}\n:::\n:::\n\n\nWe'll use the {**tidybayes**} package to compute their posterior modes\nand 95% HDIs.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: post-mode-hdi95-b\n\nd_grid_samples_b %>% \n  pivot_longer(mu_b:sigma_b) %>% \n  group_by(name) %>% \n  tidybayes::mode_hdi(value) \n```\n````\n\n```\n#> # A tibble: 2 × 7\n#>   name     value .lower .upper .width .point .interval\n#>   <chr>    <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n#> 1 mu_b    155.   154.   155.     0.95 mode   hdi      \n#> 2 sigma_b   7.82   7.19   8.35   0.95 mode   hdi\n```\n:::\n\n\nLet's say you wanted their posterior medians and 50% quantile-based\nintervals, instead. Just switch out the last line for\n`tidybayes::median_qi(value, .width = .5)`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: post-median-qi90-b\n\nd_grid_samples_b %>% \n  pivot_longer(mu_b:sigma_b) %>% \n  group_by(name) %>% \n  tidybayes::median_qi(value, .width = .5)\n```\n````\n\n```\n#> # A tibble: 2 × 7\n#>   name     value .lower .upper .width .point .interval\n#>   <chr>    <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n#> 1 mu_b    155.   154.   155.      0.5 median qi       \n#> 2 sigma_b   7.77   7.57   7.97    0.5 median qi\n```\n:::\n\n\n**Sample size and the normality of σ's posterior**\n\nI will skip this part as there is nothing conceptually new in this\nsection.\n\n### Finding the posterior distribution with `quap()` resp. `brms()`\n\n#### Original\n\n> To build the **quadratic approximation**, we'll use quap, a command in\n> the `rethinking` package. The `quap` function works by using the model\n> definition you were introduced to earlier in this chapter. Each line\n> in the definition has a corresponding definition in the form of R\n> code. The engine inside quap then uses these definitions to define the\n> posterior probability at each combination of parameter values. Then it\n> can climb the posterior distribution and find the peak, its MAP\n> (**Maximum A Posteriori** estimate). Finally, it estimates the\n> quadratic curvature at the MAP to produce an approximation of the\n> posterior distribution. (parenthesis and emphasis are mine)\n\n::: callout-note\nThe procedure used by `rethinking:quap()` is very similar to what many\nnon-Bayesian procedures do, just without any priors.\n:::\n\n1.  We start with the Howell1 data frame for adults `d2_a` (age \\>= 18).\n    We will place the R code equivalents into an `alist()` We are going\n    to use the @def-height-priors. (Code 4.27).\n2.  Then we fit the model with `rethinking::quap()` to the data in the\n    data frame `d2_a` (Code 4.28) to `m4.1`.\n3.  Now we can have a look with `rethinking::precis()` at the posterior\n    distribution (Code 4.29).\n\n\n::: {.cell}\n\n````{.cell-code #lst-post-dist-quap-m4-1-a lst-cap=\"Finding the posterior distribution with rethinking::quap()\"}\n```{{r}}\n#| label: post-dist-quap-m4-1-a\n#| attr-source: '#lst-post-dist-quap-m4-1-a lst-cap=\"Finding the posterior distribution with rethinking::quap()\"'\n\n## R code 4.27 ######################\nflist <- alist(\n  height ~ dnorm(mu, sigma),\n  mu ~ dnorm(178, 20),\n  sigma ~ dunif(0, 50)\n)\n\n## R code 4.28 ######################\nm4.1 <- rethinking::quap(flist, data = d2_a)\n\n## R code 4.29 ######################\nrethinking::precis(m4.1)\n```\n````\n\n```\n#>             mean        sd       5.5%     94.5%\n#> mu    154.607022 0.4119945 153.948575 155.26547\n#> sigma   7.731329 0.2913857   7.265638   8.19702\n```\n:::\n\n\n> These numbers provide Gaussian approximations for each parameter's\n> *marginal* distribution. This means the plausibility of each value of\n> `_μ_`, after averaging over the plausibilities of each value of `_σ_`,\n> is given by a Gaussian distribution with mean 154.6 and standard\n> deviation 0.4.\n>\n> The 5.5% and 94.5% quantiles are percentile interval boundaries,\n> corresponding to an 89% compatibility interval. Why 89%? It's just the\n> default. It displays a quite wide interval, so it shows a\n> high-probability range of parameter values. If you want another\n> interval, such as the conventional and mindless 95%, you can use\n> `precis(m4.1, prob=0.95)`. But I don't recommend 95% intervals,\n> because readers will have a hard time not viewing them as significance\n> tests. 89 is also a prime number, so if someone asks you to justify\n> it, you can stare at them meaningfully and incant, \"Because it is\n> prime.\" That's no worse justification than the conventional\n> justification for 95%.\n\nI encourage you to compare these 89% boundaries to the compatibility\nintervals from the grid approximation in @lst-post-comp-intervals-a\nearlier. You'll find that they are almost identical. When the posterior\nis approximately Gaussian, then this is what you should expect.\n\n##### Start values for `rethinking::quap()` {#sec-start-values-rethinking}\n\nMean and standard deviation are good values to start values for hill\nclimbing. If you don't specify `rethinking::quap()` will use a random\nvalue.\n\n\n::: {.cell}\n\n````{.cell-code #start-values-quap lst-cap=\"Define start values for rethinking::quap()\"}\n```{{r}}\n#| label: start-values-quap\n#| attr-source: '#start-values-quap lst-cap=\"Define start values for rethinking::quap()\"'\n\n## R code 4.30 ######################\nstart <- list(\n  mu = mean(d2_a$height),\n  sigma = sd(d2_a$height)\n)\nm4.1_2 <- rethinking::quap(flist, data = d2_a, start = start)\nrethinking::precis(m4.1_2)\n```\n````\n\n```\n#>             mean        sd       5.5%      94.5%\n#> mu    154.607024 0.4119947 153.948576 155.265471\n#> sigma   7.731333 0.2913860   7.265642   8.197024\n```\n:::\n\n\n::: callout-note\n###### list() and alist()\n\nNote that the list of start values is a regular `list`, not an `alist`\nlike the formula list is. The two functions `alist` and `list` do the\nsame basic thing: allow you to make a collection of arbitrary R objects.\nThey differ in one important respect: `list` evaluates the code you\nembed inside it, while `alist` does not. So when you define a list of\nformulas, you should use `alist`, so the code isn't executed. But when\nyou define a list of start values for parameters, you should use `list`,\nso that code like `mean(d2_a$height)` will be evaluated to a numeric\nvalue.\n:::\n\n**Slicing in more information**\n\n> The priors we used before are very weak, both because they are nearly\n> flat and because there is so much data. So I'll splice in a more\n> informative prior for `*μ*`, so you can see the effect. All I'm going\n> to do is change the standard deviation of the prior to 0.1, so it's a\n> very narrow prior. I'll also build the formula right into the call to\n> `quap` this time.\n\n\n::: {.cell}\n\n````{.cell-code #lst-post-dist-quap-m4.2 lst-cap=\"Finding the posterior distribution with a narrower prior rethinking::quap()\"}\n```{{r}}\n#| label: post-dist-quap-m4.2\n#| attr-source: '#lst-post-dist-quap-m4.2 lst-cap=\"Finding the posterior distribution with a narrower prior rethinking::quap()\"'\n\n## R code 4.31 ###########################\nm4.2 <- rethinking::quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu ~ dnorm(178, 0.1),\n    sigma ~ dunif(0, 50)\n  ),\n  data = d2_a\n)\nrethinking::precis(m4.2)\n```\n````\n\n```\n#>            mean        sd      5.5%     94.5%\n#> mu    177.86375 0.1002354 177.70356 178.02395\n#> sigma  24.51757 0.9289236  23.03297  26.00216\n```\n:::\n\n\n> Notice that the estimate for `*μ*` has hardly moved off the prior. The\n> prior was very concentrated around 178. So this is not surprising. But\n> also notice that the estimate for `*σ*` has changed quite a lot, even\n> though we didn't change its prior at all. Once the golem is certain\n> that the mean is near 178---as the prior insists---then the golem has\n> to estimate `*σ*` conditional on that fact. This results in a\n> different posterior for `*σ*`, even though all we changed is prior\n> information about the other parameter.\n\n::: callout-caution\n###### `μ` has hardly moved off the prior\n\nAt first I did not understand \"that the estimate for `*μ*` has hardly\nmoved off the prior\". I thought this assertion refers to the value of\n`*μ*` in both calculation. *μ* has changed considerably from 154.61 to\n177.86 and under that assumption the above quote does not make sense.\n\nBut in contrast to my wrong assumption the assertion refers to the\ndifference between the chosen prior (178) and the resulting value of\n`*μ*` (177.86).\n:::\n\n#### Tidyverse\n\n> In the text, McElreath indexed his models with names like `m4.1`. I\n> will largely follow that convention, but will replace the *m* with a\n> *b* to stand for the **`brms`** package.\n\nHere's how to fit the first model for this chapter.\n\n\n::: {.cell att-source='#lst-post-dist-brms-b4.1 lst-cap=\"Finding the posterior distribution with brms::brm()\"'}\n\n````{.cell-code}\n```{{r}}\n#| label: post-dist-brms-b4.1\n#| att-source: '#lst-post-dist-brms-b4.1 lst-cap=\"Finding the posterior distribution with brms::brm()\"'\n\nb4.1 <- \n  brms::brm(data = d2_b, \n      family = gaussian,\n      height ~ 1,\n      prior = c(brms::prior(normal(178, 20), class = Intercept),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.01\")\n\nbrms:::plot.brmsfit(b4.1)\n```\n````\n\n::: {.cell-output-display}\n![](04-geocentric-models_files/figure-html/post-dist-brms-b4.1-1.png){width=672}\n:::\n:::\n\n\nIf you want detailed diagnostics for the HMC chains, call\n`brms::launch_shinystan(b4.1)`. That'll keep you busy for a while. See\nthe [ShinyStan website](https://mc-stan.org/users/interfaces/shinystan)\nfor more information.\n\n::: callout-caution\n###### Launch of shinystan turned off\n\nI turned off the evaluation of the following chunk. It took some time\nand the it referred to a local page `http://127.0.0.1:6367/`\\`where I\ncould inspect many details of the model. But this is at the moment too\ncomplex to me: I do not understand all the parameters and the many\nconfigurable options programmed with a {**shiny**) interface.\n:::\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: detailled-diganostic-chains-brms-b4.1\n#| eval: false\n\nbrms::launch_shinystan(b4.1)\n\n```\n````\n:::\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: print-summary-brms-b4.1\n\nbrms:::print.brmsfit(b4.1)\n```\n````\n\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ 1 \n#>    Data: d2_b (Number of observations: 352) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   154.60      0.41   153.81   155.42 1.00     2763     2635\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     7.77      0.29     7.21     8.39 1.00     3400     2561\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: print-stan-like-summary-brms-b4.1\n\nb4.1$fit\n```\n````\n\n```\n#> Inference for Stan model: anon_model.\n#> 4 chains, each with iter=2000; warmup=1000; thin=1; \n#> post-warmup draws per chain=1000, total post-warmup draws=4000.\n#> \n#>                 mean se_mean   sd     2.5%      25%      50%      75%    97.5%\n#> b_Intercept   154.60    0.01 0.41   153.81   154.31   154.59   154.88   155.42\n#> sigma           7.77    0.01 0.29     7.21     7.57     7.76     7.97     8.39\n#> lprior         -8.51    0.00 0.02    -8.56    -8.53    -8.51    -8.50    -8.46\n#> lp__        -1227.04    0.02 0.97 -1229.63 -1227.44 -1226.75 -1226.33 -1226.06\n#>             n_eff Rhat\n#> b_Intercept  2778    1\n#> sigma        3404    1\n#> lprior       2773    1\n#> lp__         1798    1\n#> \n#> Samples were drawn using NUTS(diag_e) at Sat Aug  5 13:01:11 2023.\n#> For each parameter, n_eff is a crude measure of effective sample size,\n#> and Rhat is the potential scale reduction factor on split chains (at \n#> convergence, Rhat=1).\n```\n:::\n\n\nWhereas rethinking defaults to 89% intervals, using `print()` or\n`summary()` with {**brms**} models defaults to 95% intervals.\n\n::: callout-note\nAs I have learned shortly: `print()` or `summary()` are generic\nfunctions where one can add new printing methods with new classes. In\nthis case `class(b4.1)` = brmsfit. This means I do not need to\nadd `brms::` to secure that I will get the {**brms**} printing or\nsummary method as I didn't load the {**brms**} package. Quite the\ncontrary: Adding `brms::` would result into the message: \"Error:\n'summary' is not an exported object from 'namespace:brms'\".\n\nAs I really want to specify explicitly the method these generic\nfunctions should use, I need to use the syntax `brms:::print.brmsfit()`\nor `brms:::summary.brmsfit()` respectively.\n\nIn this respect I have to learn more about S3 classes. There are many\nimportant web resources about this subject that I have found with the\nsearch string \"r what is s3 class\". Maybe I should start with the [S3\nchapter in Advanced R](https://adv-r.hadley.nz/s3.html).\n:::\n\nUnless otherwise specified, Kurz will stick with 95% intervals\nthroughout. To get those 89% intervals or McElreath approach, one could\nuse the `prob` argument within `summary()` or `print()`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: summary-interval-.89-brms-b4.1\n\nbrms:::summary.brmsfit(b4.1, prob = .89)\n```\n````\n\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ 1 \n#>    Data: d2_b (Number of observations: 352) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   154.60      0.41   153.94   155.25 1.00     2763     2635\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     7.77      0.29     7.33     8.26 1.00     3400     2561\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n\nHere's the `brms::brm()` code for the model with the very narrow `_μ_`\nprior corresponding to the `rethinking::quap()` code in\n@lst-post-dist-quap-m4.2.\n\n\n::: {.cell att-source='#lst-post-dist-brms-b4.2 lst-cap=\"Finding the posterior distribution with a narrower prior using brms::brm()\"'}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-post-dist-brms-b4.2\n#| fig-cap: \"Finding the posterior distribution with a narrower prior using brms::brm()\"\n#| att-source: '#lst-post-dist-brms-b4.2 lst-cap=\"Finding the posterior distribution with a narrower prior using brms::brm()\"'\n\nb4.2 <- \n  brms::brm(data = d2_b, \n      family = gaussian,\n      height ~ 1,\n      prior = c(brms::prior(normal(178, 0.1), class = Intercept),\n                brms::prior(uniform(0, 50), class = sigma, ub = 50)),\n      iter = 2000, warmup = 1000, chains = 4, cores = 4,\n      seed = 4,\n      file = \"fits/b04.02\")\n\nbrms:::plot.brmsfit(b4.2, widths = c(1, 2))\n```\n````\n\n::: {.cell-output-display}\n![Finding the posterior distribution with a narrower prior using brms::brm()](04-geocentric-models_files/figure-html/fig-post-dist-brms-b4.2-1.png){#fig-post-dist-brms-b4.2 width=672}\n:::\n:::\n\n\nAnd here's the model `summary()`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: summary-narrow-prior\n\nbrms:::summary.brmsfit(b4.2)\n```\n````\n\n```\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ 1 \n#>    Data: d2_b (Number of observations: 352) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   177.86      0.11   177.66   178.07 1.00     2721     2586\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma    24.60      0.91    22.87    26.47 1.00     3400     2729\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n\n\nSubsetting the `summary()` output with `$fixed` provides a convenient\nway to compare the Intercept summaries between `b4.1` and `b4.2`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: compare-summaries-b4.1-b4.2\n\nrbind(brms:::summary.brmsfit(b4.1)$fixed,\n      brms:::summary.brmsfit(b4.2)$fixed)\n```\n````\n\n```\n#>            Estimate Est.Error l-95% CI u-95% CI     Rhat Bulk_ESS Tail_ESS\n#> Intercept  154.5959 0.4144632 153.8123 155.4221 1.000708 2762.982 2635.159\n#> Intercept1 177.8647 0.1052528 177.6577 178.0705 1.001224 2720.796 2586.491\n```\n:::\n\n\n### Sampling from a quap\n\n#### Original\n\nThe above explains how to get a quadratic approximation of the\nposterior, using `rethinking::quap()`. But how do we then get samples\nfrom the quadratic approximate posterior distribution? --- When R\nconstructs a quadratic approximation, it calculates not only standard\ndeviations for all parameters, but also the covariances among all pairs\nof parameters. Just like a mean and standard deviation (or its square, a\nvariance) are sufficient to describe a one-dimensional Gaussian\ndistribution, a list of means and a matrix of variances and covariances\nare sufficient to describe a multi-dimensional Gaussian distribution.\n\n\n::: {.cell}\n\n````{.cell-code #lst-calc-var-cov-m4.1-a lst-cap=\"Calculation of the variance-covariance matrix: rethinking version\"}\n```{{r}}\n#| label: calc-var-cov-m4.1-a\n#| attr-source: '#lst-calc-var-cov-m4.1-a lst-cap=\"Calculation of the variance-covariance matrix: rethinking version\"'\n\n## R code 4.32 ###################\nrethinking::vcov(m4.1)\n```\n````\n\n```\n#>                 mu        sigma\n#> mu    0.1697394338 0.0002179886\n#> sigma 0.0002179886 0.0849055981\n```\n:::\n\n\n`vcov()` returns the variance-covariance matrix of the main parameters\nof a fitted model object. In the above {**rethinking**} version is uses\nthe class `map2stan` for a fitted Stan model as `m4.1` is of class\n`map`.\n\n::: callout-caution\n###### rethinking::vcov\n\nIn @lst-calc-var-cov-m4.1-a I am explicitly using the package\n{**rethinking**} for the `vcov()` function. The same function is also\navailable as a base R function with `stats::vcov()`. But this generates\nan error because there is no method known for an object of class `map`\nfrom the rethinking package. The help file for `stats::vcov()` only says\nthat the `vcov` object is an S3 method for classes `lm`, `glm`, `mlm`\nand `aov` but not for `map`.\n\n> Error in UseMethod(\"vcov\") : no applicable method for 'vcov' applied\n> to an object of class \"map\"\n\nI could have used only `vcov()`. But this only works when the\n{**rethinking**} package is already loaded. In that case R knows because\nof the class of the object which `vcov()` version to use. In this case:\nclass of object = `class(m4.1)` map.\n:::\n\n@lst-calc-var-cov-m4.1-a results in a variance-covariance matrix. It is\nthe multi-dimensional glue of a quadratic approximation, because it\ntells us how each parameter relates to every other parameter in the\nposterior distribution. A variance-covariance matrix can be factored\ninto two elements: (1) a vector of variances for the parameters and (2)\na correlation matrix that tells us how changes in any parameter lead to\ncorrelated changes in the others.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: vcov-decomp-m4.1-a\n\n## R code 4.33 #######################\nbase::diag(rethinking::vcov(m4.1))      # <1>\nstats::cov2cor(rethinking::vcov(m4.1))  # <2>\n```\n````\n\n```\n#>        mu     sigma \n#> 0.1697394 0.0849056 \n#>                mu       sigma\n#> mu    1.000000000 0.001815826\n#> sigma 0.001815826 1.000000000\n```\n:::\n\n\n1.  `base::diag()` extracts the diagonal of the (variance-covariance)\n    matrix. The two-element vector in the output is the list of\n    variances. If you take the square root of this vector, you get the\n    standard deviations that are shown in `rethinking::precis()` output.\n2.  `stats::cov2cor()` scales a covariance matrix into the corresponding\n    correlation matrix. The two-by-two matrix in the output is this\n    correlation matrix. Each entry shows the correlation, bounded\n    between −1 and +1, for each pair of parameters. The 1's indicate a\n    parameter's correlation with itself. If these values were anything\n    except 1, we would be worried. The other entries are typically\n    closer to zero, and they are very close to zero in this example.\n    This indicates that learning μ tells us nothing about σ and likewise\n    that learning σ tells us nothing about μ. This is typical of simple\n    Gaussian models of this kind. But it is quite rare more generally,\n    as you'll see in later chapters.\n\nOkay, so how do we get samples from this multi-dimensional posterior?\nNow instead of sampling single values from a simple Gaussian\ndistribution, we sample vectors of values from a multi-dimensional\nGaussian distribution.\n\n\n::: {.cell}\n\n````{.cell-code #lst-extract-samples-m4.1-a lst-cap=\"Extract sample vectors of values from the multi-dimensional Gaussian distribution of m4.1: rethinking version\"}\n```{{r}}\n#| label: extract-samples-m4.1-a\n#| attr-source: '#lst-extract-samples-m4.1-a lst-cap=\"Extract sample vectors of values from the multi-dimensional Gaussian distribution of m4.1: rethinking version\"'\n\n\n## R code 4.34 #######################\npost3_a <- rethinking::extract.samples(m4.1, n = 1e4)\nhead(post3_a)\n```\n````\n\n```\n#>         mu    sigma\n#> 1 154.1191 7.722540\n#> 2 154.9340 7.560591\n#> 3 154.8558 8.023537\n#> 4 153.5471 7.733505\n#> 5 154.6297 7.327114\n#> 6 154.6631 8.316943\n```\n:::\n\n\nYou end up with a data frame, post, with 10,000 (1e4) rows and two\ncolumns, one column for `_μ_` and one for `_σ_`. Each value is a sample\nfrom the posterior, so the mean and standard deviation of each column\nwill be very close to the MAP values from before. You can confirm this\nby summarizing the samples:\n\n\n::: {.cell}\n\n````{.cell-code #lst-summary-samples-m4.1-a lst-cap=\"Summary the extracted samples: rethinking version\"}\n```{{r}}\n#| label: summary-samples-m4.1-a\n#| attr-source: '#lst-summary-samples-m4.1-a lst-cap=\"Summary the extracted samples: rethinking version\"'\n\n## R code 4.35 ##################\nrethinking::precis(post3_a)\n```\n````\n\n```\n#>             mean        sd      5.5%     94.5%   histogram\n#> mu    154.606881 0.4077628 153.96082 155.26492     ▁▁▅▇▂▁▁\n#> sigma   7.731493 0.2884440   7.26373   8.19481 ▁▁▁▂▅▇▇▃▁▁▁\n```\n:::\n\n\nCompare these values to the output from @lst-post-dist-quap-m4-1-a. And\nyou can use `plot(post)` to see how much they resemble the samples from\nthe grid approximation in FIGURE 4.4 (here @fig-posterior-sample-a).\nThese samples also preserve the covariance between `_μ_` and `_σ_`. This\nhardly matters right now, because `_μ_` and `_σ_` don't co-vary at all\nin this model. But once you add a predictor variable to your model,\ncovariance will matter a lot.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-posterior-sample-vectors-a\n#| fig-cap: \"Samples from the vectors of values from a multi-dimensional Gaussian distribution. The density of points is highest in the center, reflecting the most plausible combinations of μ and σ. It is very similar to @fig-posterior-sample-a (rethinking version)\"\n\nbase::plot(post3_a)\n```\n````\n\n::: {.cell-output-display}\n![Samples from the vectors of values from a multi-dimensional Gaussian distribution. The density of points is highest in the center, reflecting the most plausible combinations of μ and σ. It is very similar to @fig-posterior-sample-a (rethinking version)](04-geocentric-models_files/figure-html/fig-posterior-sample-vectors-a-1.png){#fig-posterior-sample-vectors-a width=672}\n:::\n:::\n\n\n##### Under the hood with multivariate sampling {#sec-under-the-hood-multivariate-sampling}\n\nThe function `rethinking::extract.samples()` is for convenience. It is\njust running a simple simulation of the sort you conducted near the end\nof @sec-sampling-the-imaginary with @lst-sim-pred-samples-a. Here's a\npeak at the motor. The work is done by a multi-dimensional version of\n`stats::rnorm()`, `MASS::mvrnorm()`. The function `stats::rnorm()`\nsimulates random Gaussian values, while `MASS::mvrnorm()` simulates\nrandom vectors of multivariate Gaussian values. Here's how to use it the\n{**MASS**} function to do what `rethinking::extract.samples()` does:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-posterior-sample-vectors-MASS-a\n#| fig-cap: \"Extract samples from the vectors of values from a multi-dimensional Gaussian distribution using the {**MASS**} package. It is same calculation as in @fig-posterior-sample-a (rethinking version)\"\n\n\n## R code 4.36 ######################\npost4_a <- MASS::mvrnorm(n = 1e4, mu = rethinking::coef(m4.1), \n                      Sigma = rethinking::vcov(m4.1))\nplot(post4_a)\n```\n````\n\n::: {.cell-output-display}\n![Extract samples from the vectors of values from a multi-dimensional Gaussian distribution using the {**MASS**} package. It is same calculation as in @fig-posterior-sample-a (rethinking version)](04-geocentric-models_files/figure-html/fig-posterior-sample-vectors-MASS-a-1.png){#fig-posterior-sample-vectors-MASS-a width=672}\n:::\n:::\n\n\n#### Tidyverse\n\n{**brms**} doesn't seem to have a convenience function that works the\nway `vcov()` does for {**rethinking**}.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#|label: calc-var-cov-m4.1-b\n#|attr-source: '#lst-calc-var-cov-m4.1-b lst-cap=\"Calculation of vcov(): tidyverse version.\"'\n\nbrms:::vcov.brmsfit(b4.1)\n```\n````\n\n```\n#>           Intercept\n#> Intercept 0.1717797\n```\n:::\n\n\nThis only returns the first element in the matrix it did for\n{**rethinking**}. That is, it appears `brms::vcov()` only returns the\nvariance/covariance matrix for the single-level `_β_` parameters.\n\n::: callout-caution\n###### brms::vcov()\n\nReferring to a similar situation with `rethinking::vcov()` in\n@lst-calc-var-cov-m4.1-a I cannot write `brms::vcov()`, but have to use\neither `brms:::vcov.brmsfit(b4.1)` or just `vcov(b4.1)`. The weird thing\nis that the first time it also works with `brms::vcov()` but only the\nfirst time!\n:::\n\nHowever, if you really wanted this information, you could get it after\nputting the Hamilton Monte Carlo (HMC) chains in a data frame. We do\nthat with the `brms::as_draws_df()` function:\n\n\n::: {.cell}\n\n````{.cell-code #lst-put-hmc-into-df-b lst-cap=\"Extract the iteration of the Hamiliton Monte Carlo (HMC) chains into a data frame\"}\n```{{r}}\n#| label: put-hmc-into-df-b\n#| attr-source: '#lst-put-hmc-into-df-b lst-cap=\"Extract the iteration of the Hamiliton Monte Carlo (HMC) chains into a data frame\"'\n\npost_b <- brms::as_draws_df(b4.1)\n\nhead(post_b)\n```\n````\n\n```\n#> # A draws_df: 6 iterations, 1 chains, and 4 variables\n#>   b_Intercept sigma lprior  lp__\n#> 1         155   7.5   -8.5 -1227\n#> 2         155   7.0   -8.5 -1230\n#> 3         154   7.6   -8.5 -1226\n#> 4         154   8.0   -8.5 -1226\n#> 5         155   7.6   -8.5 -1226\n#> 6         155   7.4   -8.5 -1227\n#> # ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n```\n:::\n\n\n::: callout-tip\n###### draws object\n\nThe functions of the family `as_draws()` transform `brmsfit` objects to\n`draws` objects, a format supported by the {**posterior**} package.\n{brms} currently imports the family of `as_draws()`functions from the\n{**posterior**} package, a tool for working with posterior\ndistributions.\n\n@lst-put-hmc-into-df-b produced the {**brms**} version of what McElreath\nachieved with `extract.samples()` in @lst-extract-samples-m4.1-a.\nHowever, what happened under the hood was different. Whereas rethinking\nused the `mvnorm()` function from the {**MASS**} package, with\n{**brms**} we just extracted the iterations of the HMC chains and put\nthem in a data frame.\n:::\n\nNow `select()` the columns containing the draws from the desired\nparameters `b_Intercept` and `sigma` and feed them into `cov()`.\n\n\n::: {.cell}\n\n````{.cell-code #lst-calc-cov-post-b lst-cap=\"Calculate the vector of variances and correlation matrix for b_Intercept and sigma\"}\n```{{r}}\n#| label: calc-cov-post-b\n#| attr-source: '#lst-calc-cov-post-b lst-cap=\"Calculate the vector of variances and correlation matrix for b_Intercept and sigma\"'\n\nselect(post_b, b_Intercept:sigma) %>% \n  stats::cov()\n```\n````\n\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n#>               b_Intercept         sigma\n#> b_Intercept  0.1717797087 -0.0005450089\n#> sigma       -0.0005450089  0.0868161627\n```\n:::\n\n\n@lst-calc-cov-post-b displays \"(1) a vector of variances for the\nparameters and (2) a correlation matrix\" for them (p. 90). Here are just\nthe variances (i.e., the diagonal elements) and the correlation matrix.\n\n\n::: {.cell}\n\n````{.cell-code #lst-calc-var-post-b lst-cap=\"Calculate only variances (the diagonal values)\"}\n```{{r}}\n#| label: calc-var-post-b\n#| attr-source: '#lst-calc-var-post-b lst-cap=\"Calculate only variances (the diagonal values)\"'\n\nselect(post_b, b_Intercept:sigma) %>%\n  stats::cov() %>%\n  base::diag()\n```\n````\n\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n#> b_Intercept       sigma \n#>  0.17177971  0.08681616\n```\n:::\n\n::: {.cell}\n\n````{.cell-code #lst-calc-corr-matrix-post-b lst-cap=\"Calculate only crrelation\"}\n```{{r}}\n#| label: calc-corr-matrix-post-b\n#| attr-source: '#lst-calc-corr-matrix-post-b lst-cap=\"Calculate only crrelation\"'\n\n# correlation\npost_b %>%\n  select(b_Intercept, sigma) %>%\n  stats::cor()\n```\n````\n\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n#>              b_Intercept        sigma\n#> b_Intercept  1.000000000 -0.004462902\n#> sigma       -0.004462902  1.000000000\n```\n:::\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: show-data-structure\n\nstr(post_b)\n```\n````\n\n```\n#> draws_df [4,000 × 7] (S3: draws_df/draws/tbl_df/tbl/data.frame)\n#>  $ b_Intercept: num [1:4000] 155 155 154 154 155 ...\n#>  $ sigma      : num [1:4000] 7.55 7.02 7.58 7.95 7.63 ...\n#>  $ lprior     : num [1:4000] -8.49 -8.49 -8.52 -8.52 -8.52 ...\n#>  $ lp__       : num [1:4000] -1227 -1230 -1226 -1226 -1226 ...\n#>  $ .chain     : int [1:4000] 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ .iteration : int [1:4000] 1 2 3 4 5 6 7 8 9 10 ...\n#>  $ .draw      : int [1:4000] 1 2 3 4 5 6 7 8 9 10 ...\n```\n:::\n\n\nThe `post_b` object is not just a data frame, but also of class\n`draws_df`, which means it contains three metadata variables ----\n`.chain`, `.iteration`, and `.draw` --- which are often hidden from\nview, but are there in the background when needed. As you'll see, we'll\nmake good use of the `.draw` variable in the future. Notice how our post\ndata frame also includes a vector named `lp__`. That's the log\nposterior.\n\nFor details, see: - The [Log-Posterior (function and\ngradient)](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html#the-log-posterior-function-and-gradient)\nsection of the Stan Development Team's (2023) vignette [RStan: the R\ninterface to\nStan](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html)\nand - Stephen Martin's [explanation of the log\nposterior](https://discourse.mc-stan.org/t/basic-question-what-is-lp-in-posterior-samples-of-a-brms-regression/17567/2)\non the Stan Forums.\n\n::: callout-caution\n###### Summaries of {brms} and {posterior} packages\n\nKurz claims that `summary()` function doesn't work for {**brms**}\nposterior data frames quite the way `rethinking::precis()` does for\nposterior data frames from the {\\*\\*rethinking\\*} package. But I this\nobservation is somewhat misleading.\n\nThe posterior data frame `post_b` is not of class `brms`. Let's check\nthis:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: class-post_b-b\n\nclass(post_b)\n```\n````\n\n```\n#> [1] \"draws_df\"   \"draws\"      \"tbl_df\"     \"tbl\"        \"data.frame\"\n```\n:::\n\n\nThe class `draws_df`and `draws` refers to the {**posterior**} and not to\nthe {**brms**} package. Remember: In @lst-put-hmc-into-df-b we\ntransformed with the function `as_draws_df` the `brms` object into a\n`draws_df` and `draws` object.\n\nTherefore Kurz's claim should be read: The `summary()` function doesn't\nwork for {**posterior**} posterior data frames quite the way\n`rethinking::precis()` does for posterior data frames from the\n{**rethinking**} package. Instead of calling `brms:::summary.brmsfit()`\nI will use `posterior:::summary.draws()`.\n\nI wouldn't have noticed this difference if I hadn't mentioned explicitly\nthe name of the packages in front of the function, because in that case\nR would have used `base::summary()` as in Kurz's text.\n:::\n\n\n::: {.cell}\n\n````{.cell-code #lst-base-summary-samples-b4.1-b lst-cap=\"Summary the extracted iterations of the HMC chains: base version\"}\n```{{r}}\n#| label: base-summary-samples-b4.1-b\n#| attr-source: '#lst-base-summary-samples-b4.1-b lst-cap=\"Summary the extracted iterations of the HMC chains: base version\"'\n\nbase::summary(post_b[, 1:2])\n```\n````\n\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n#>   b_Intercept        sigma      \n#>  Min.   :153.3   Min.   :6.841  \n#>  1st Qu.:154.3   1st Qu.:7.567  \n#>  Median :154.6   Median :7.757  \n#>  Mean   :154.6   Mean   :7.771  \n#>  3rd Qu.:154.9   3rd Qu.:7.971  \n#>  Max.   :156.2   Max.   :8.864\n```\n:::\n\n::: {.cell}\n\n````{.cell-code #lst-posterior-summary-samples-b4.1-b lst-cap=\"Summary the extracted iterations of the HMC chains: posterior version\"}\n```{{r}}\n#| label: posterior-summary-samples-b4.1-b\n#| attr-source: '#lst-posterior-summary-samples-b4.1-b lst-cap=\"Summary the extracted iterations of the HMC chains: posterior version\"'\n\nposterior:::summary.draws(post_b[, 1:2])\n```\n````\n\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n#> # A tibble: 2 × 10\n#>   variable      mean median    sd   mad     q5    q95  rhat ess_bulk ess_tail\n#>   <chr>        <num>  <num> <num> <num>  <num>  <num> <num>    <num>    <num>\n#> 1 b_Intercept 155.   155.   0.414 0.421 154.   155.    1.00    2722.    2624.\n#> 2 sigma         7.77   7.76 0.295 0.301   7.31   8.27  1.00    3369.    2537.\n```\n:::\n\n\nTo get a similar summary with tiny histograms Kurz offers different\nsolutions:\n\n-   A base R approach by using the transpose of a `stats::quantile()`\n    call nested within `base::apply()`\n-   A {**tidyverse**} approach\n-   A {**brms**} approach by just putting the `brm()` fit object into\n    `posterior_summary()`\n-   A {**tidybayes**} approach using `tidybayes::mean_hdi()` if you're\n    willing to drop the posterior `sd` and\n-   Using additionally the [function\n    `histospark()`](https://github.com/hadley/precis/blob/master/R/histospark.R)\n    (from the unfinished {**precis**} package by Hadley Wickham supposed\n    to replace `base::summary()`) to get the tiny histograms and to add\n    them into the tidyverse approach.\n\nAdditionally I will propose using the {**skimr**} packages:\n\n\n::: {.cell}\n\n````{.cell-code #lst-skim-summary-samples-b4.1-b lst-cap=\"Summary the extracted iterations of the HMC chains: skimr version\"}\n```{{r}}\n#| label: skim-summary-samples-b4.1-b\n#| attr-source: '#lst-skim-summary-samples-b4.1-b lst-cap=\"Summary the extracted iterations of the HMC chains: skimr version\"'\n\nskimr::skim(post_b[, 1:2])\n```\n````\n\n```\n#> Warning: Dropping 'draws_df' class as required metadata was removed.\n```\n\n::: {.cell-output-display}\nTable: Data summary\n\n|                         |              |\n|:------------------------|:-------------|\n|Name                     |post_b[, 1:2] |\n|Number of rows           |4000          |\n|Number of columns        |2             |\n|_______________________  |              |\n|Column type frequency:   |              |\n|numeric                  |2             |\n|________________________ |              |\n|Group variables          |None          |\n\n\n**Variable type: numeric**\n\n|skim_variable | n_missing| complete_rate|   mean|   sd|     p0|    p25|    p50|    p75|   p100|hist  |\n|:-------------|---------:|-------------:|------:|----:|------:|------:|------:|------:|------:|:-----|\n|b_Intercept   |         0|             1| 154.60| 0.41| 153.29| 154.31| 154.59| 154.88| 156.22|▁▆▇▂▁ |\n|sigma         |         0|             1|   7.77| 0.29|   6.84|   7.57|   7.76|   7.97|   8.86|▁▆▇▂▁ |\n:::\n:::\n\n\nKurz refers only shortly to both `overthinking` blocks of this section:\n\n- Start values for `rethinking::quap()` resp. `brms::brm()` (See\n@sec-start-values-rethinking): Within the `brm()` function, you use the `init` argument fpr the start values.\n- Under the hood with multivariate sampling (See @sec-under-the-hood-multivariate-sampling): Again Kurz remarked that `brms::as_draws_df()` is not the same as `rethinking::extract.samples()`. What this exactly means will (hopefully) explained later in @sec-markov-chain-monte-carlo.\n",
    "supporting": [
      "04-geocentric-models_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "fcc5ad4c7b99cc4fcc68c2eaccce4236",
  "result": {
    "markdown": "# 4a: Geocentric Models\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: setup\n\nlibrary(conflicted)\nlibrary(rethinking)\n```\n````\n\n```\n#> Loading required package: rstan\n#> Loading required package: StanHeaders\n#> \n#> rstan version 2.26.22 (Stan version 2.26.1)\n#> For execution on a local, multicore CPU with excess RAM we recommend calling\n#> options(mc.cores = parallel::detectCores()).\n#> To avoid recompilation of unchanged Stan programs, we recommend calling\n#> rstan_options(auto_write = TRUE)\n#> For within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\n#> change `threads_per_chain` option:\n#> rstan_options(threads_per_chain = 1)\n#> Loading required package: cmdstanr\n#> This is cmdstanr version 0.5.3\n#> - CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n#> - CmdStan path: /Users/petzi/.cmdstan/cmdstan-2.32.2\n#> - CmdStan version: 2.32.2\n#> Loading required package: parallel\n#> rethinking (Version 2.31)\n```\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: setup\n\nlibrary(tidyverse)\n```\n````\n\n```\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1\n```\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: setup\n\nlibrary(brms)\n```\n````\n\n```\n#> Loading required package: Rcpp\n#> Loading 'brms' package (version 2.19.0). Useful instructions\n#> can be found by typing help('brms'). A more detailed introduction\n#> to the package is available through vignette('brms_overview').\n```\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: setup\n\nlibrary(skimr)\n\nconflicts_prefer(dplyr::filter)\n```\n````\n\n```\n#> [conflicted] Will prefer dplyr::filter over any other package.\n```\n:::\n\n\n## 4.1a Normal Distributions\n\n### 4.1.1a Normal by addition\n\n> Suppose you and a thousand of your closest friends line up on the\n> halfway line of a soccer field (football pitch). Each of you has a\n> coin in your hand. At the sound of the whistle, you begin flipping the\n> coins. Each time a coin comes up heads, that person moves one step\n> towards the left-hand goal. Each time a coin comes up tails, that\n> person moves one step towards the right-hand goal. Each person flips\n> the coin 16 times, follows the implied moves, and then stands still.\n> Now we measure the distance of each person from the halfway line. Can\n> you predict what proportion of the thousand people who are standing on\n> the halfway line? How about the proportion 5 yards left of the line?\n\nShowing that there's nothing special about the underlying coin flip:\n\n> Assume ... that each step is different from all the others, a random\n> distance between zero and one yard. Thus a coin is flipped, a distance\n> between zero and one yard is taken in the indicated direction, and the\n> process repeats. To simulate this, we generate for each person a list\n> of 16 random numbers between −1 and 1. These are the individual steps.\n> Then we add these steps together to get the position after 16 steps.\n> Then we need to replicate this procedure 1000 times.\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: sim-step-experiment-a\n\n# to replicate with the b-model\nset.seed(4)\n\n## R code 4.1\npos_a1 <- replicate(1000, sum(runif(16, -1, 1)))\npos_a2 <- replicate(1000, sum(runif(32, -1, 1)))\n\npar(mfrow = c(2, 2)) # 2-by-2 grid of plots\nhist(pos_a1)\nhist(pos_a2)\n\nplot(density(pos_a1))\nplot(density(pos_a2))\n```\n````\n\n::: {.cell-output-display}\n![](04a-geocentric-models_files/figure-html/sim-step-experiment-a-1.png){width=672}\n:::\n:::\n\n\n> Any process that adds together random values from the same\n> distribution converges to a normal. ... It doesn't matter what shape\n> the underlying distribution possesses. It could be uniform, like in\n> our example above, or it could be (nearly) anything else. Depending\n> upon the underlying distribution, the convergence might be slow, but\n> it will be inevitable.\n\n### 4.1.2a Normal by multiplication\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: random-sample-growth\n\n## R code 4.2\nprod(1 + runif(12, 0, 0.1))\n```\n````\n\n```\n#> [1] 1.819134\n```\n:::\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: normal-by-multiplaction\n\n## R code 4.3\ngrowth <- replicate(10000, prod(1 + runif(12, 0, 0.1)))\ndens(growth, norm.comp = TRUE)\n```\n````\n\n::: {.cell-output-display}\n![](04a-geocentric-models_files/figure-html/normal-by-multiplaction-1.png){width=672}\n:::\n:::\n\n\n> ... small effects that multiply together are approximately additive,\n> and so they also tend to stabilize on Gaussian distributions.\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: compare-small-big-growth\n\n## R code 4.4\nbig <- replicate(10000, prod(1 + runif(12, 0, 0.5)))\nsmall <- replicate(10000, prod(1 + runif(12, 0, 0.01)))\n\npar(mfrow = c(2, 2)) # 2-by-2 grid of plots\ndens(big, norm.comp = TRUE)\ndens(small, norm.comp = TRUE)\n```\n````\n\n::: {.cell-output-display}\n![](04a-geocentric-models_files/figure-html/compare-small-big-growth-1.png){width=672}\n:::\n:::\n\n\n> The interacting growth deviations, as long as they are sufficiently\n> small, converge to a Gaussian distribution. In this way, the range of\n> causal forces that tend towards Gaussian distributions extends well\n> beyond purely additive interactions.\n\n### 4.1.3a Normal by log-multiplication\n\n> Large deviates that are multiplied together do not produce Gaussian\n> distributions, but they do tend to produce Gaussian distributions on\n> the log scale.\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: normal-by-log-multi\n\n## R code 4.5\nlog.big <- replicate(10000, log(prod(1 + runif(12, 0, 0.5))))\n\npar(mfrow = c(2, 2)) # 2-by-2 grid of plots\ndens(big, norm.comp = TRUE)\ndens(log.big, norm.comp = TRUE)\n```\n````\n\n::: {.cell-output-display}\n![](04a-geocentric-models_files/figure-html/normal-by-log-multi-1.png){width=672}\n:::\n:::\n\n\n> We get the Gaussian distribution back, because adding logs is\n> equivalent to multiplying the original numbers. So even multiplicative\n> interactions of large deviations can produce Gaussian distributions,\n> once we measure the outcomes on the log scale. Since measurement\n> scales are arbitrary, there's nothing suspicious about this\n> transformation.\n\n### 4.1.4a Using Gaussian Distribution\n\n**Ontological Reasons**: the world is full of Gaussian distributions,\napproximately. We're never going to experience a perfect Gaussian\ndistribution. But it is a widespread pattern, appearing again and again\nat different scales and in different domains. ... The Gaussian is a\nmember of a family of fundamental natural distributions known as the\n**EXPONENTIAL FAMILY**. All of the members of this family are important\nfor working science, because they populate our world.\n\n**Epistemological Reasons**: the Gaussian represents a particular state\nof ignorance. When all we know or are willing to say about a\ndistribution of measures (measures are continuous values on the real\nnumber line) is their mean and variance, then the Gaussian distribution\narises as the most consistent with our assumptions.\n\n::: callout-tip\n## Gaussian distribution\n\nThe Gaussian is a continuous distribution, unlike the discrete\ndistributions of earlier chapters. Probability distributions with only\ndiscrete outcomes, like the binomial, are called *probability mass*\nfunctions and denoted `Pr`. Continuous ones like the Gaussian are called\n*probability density* functions, denoted with *`p`* or just plain old\n*`f`*, depending upon author and tradition. For mathematical reasons,\nprobability densities can be greater than 1. Try dnorm(0,0,0.1), for\nexample, which is the way to make R calculate *`p`*`(0|0, 0.1)`. The\nanswer, about 4, is no mistake. Probability *density* is the rate of\nchange in cumulative probability. So where cumulative probability is\nincreasing rapidly, density can easily exceed 1. But if we calculate the\narea under the density function, it will never exceed 1. Such areas are\nalso called *probability mass*.\n:::\n\n## 4.2a Language describing models\n\n> (1)  First, we recognize a set of variables to work with. Some of\n> these variables are observable. We call these *data*. Others are\n> unobservable things like rates and averages. We call these\n> *parameters*.\n>\n> (2)  We define each variable either in terms of the other variables or\n> in terms of a probability distribution.\n>\n> (3)  The combination of variables and their probability distributions\n> defines a *joint generative model* that can be used both to simulate\n> hypothetical observations as well as analyze real ones.\n\n## 4.3a Gaussian model of height\n\n### 4.3.1a The data\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: load-howell-data-a\n\n## R code 4.7\ndata(Howell1)\nd_a <- Howell1\n```\n````\n:::\n\n\n#### 4.3.1.1a Show the data\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: show-howell-data-a\n\n## R code 4.8\nstr(d_a)\n\n## R code 4.9\nprecis(d_a)\n```\n````\n\n```\n#> 'data.frame':\t544 obs. of  4 variables:\n#>  $ height: num  152 140 137 157 145 ...\n#>  $ weight: num  47.8 36.5 31.9 53 41.3 ...\n#>  $ age   : num  63 63 65 41 51 35 32 27 19 54 ...\n#>  $ male  : int  1 0 0 1 0 1 0 1 0 1 ...\n#>               mean         sd      5.5%     94.5%     histogram\n#> height 138.2635963 27.6024476 81.108550 165.73500 ▁▁▁▁▁▁▁▂▁▇▇▅▁\n#> weight  35.6106176 14.7191782  9.360721  54.50289 ▁▂▃▂▂▂▂▅▇▇▃▂▁\n#> age     29.3443934 20.7468882  1.000000  66.13500     ▇▅▅▃▅▂▂▁▁\n#> male     0.4724265  0.4996986  0.000000   1.00000    ▇▁▁▁▁▁▁▁▁▇\n```\n:::\n\n\n#### 4.3.1.2a Select the height data of adults\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: select-height-adults-a\n\n## R code 4.10\nhead(d_a$height)\n \n## R code 4.11\nd2_a <- d_a[d_a$age >= 18, ]\n```\n````\n\n```\n#> [1] 151.765 139.700 136.525 156.845 145.415 163.830\n```\n:::\n\n\n### 4.3.2a The model\n\n> Our goal is to model these values using a Gaussian distribution.\n\n#### 4.3.2.1a Plot the priors\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: print-height-dist\n\ndens(d2_a$height, norm.comp = TRUE)\n```\n````\n\n::: {.cell-output-display}\n![](04a-geocentric-models_files/figure-html/print-height-dist-1.png){width=672}\n:::\n:::\n\n\n**Plot the mu prior (mean)**\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: plot-mean-prior-a\n\n## R code 4.12\ncurve(dnorm(x, 178, 20), from = 100, to = 250)\n```\n````\n\n::: {.cell-output-display}\n![](04a-geocentric-models_files/figure-html/plot-mean-prior-a-1.png){width=672}\n:::\n:::\n\n\n**Plot the sigma prior (standard deviation)**\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: plot-sd-prior-a\n\n## R code 4.13\ncurve(dunif(x, 0, 50), from = -10, to = 60)\n```\n````\n\n::: {.cell-output-display}\n![](04a-geocentric-models_files/figure-html/plot-sd-prior-a-1.png){width=672}\n:::\n:::\n\n\n#### 4.3.2.2a Prior predictive simulation\n\n> Once you've chosen priors for *h, μ*, and *σ*, these imply a joint\n> prior distribution of individual heights. By simulating from this\n> distribution, you can see what your choices imply about observable\n> height. This helps you diagnose bad choices.\n\n**Simulate heights by sampling from the prior**\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: prior-predictive-sim-a\n\n## R code 4.14\nsample_mu_a <- rnorm(1e4, 178, 20)\nsample_sigma_a <- runif(1e4, 0, 50)\nprior_h_a <- rnorm(1e4, sample_mu_a, sample_sigma_a)\ndens(prior_h_a)\n```\n````\n\n::: {.cell-output-display}\n![](04a-geocentric-models_files/figure-html/prior-predictive-sim-a-1.png){width=672}\n:::\n:::\n\n\nPlaying around with different priors coming from scientific background\nknowledge about heights of adults:\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: prior2-predictive-sim-a\n\n## R code 4.14\nsample_mu2_a <- rnorm(1e4, 170, 20)\nsample_sigma2_a <- runif(1e4, 0, 35)\nprior2_h_a <- rnorm(1e4, sample_mu2_a, sample_sigma2_a)\ndens(prior2_h_a)\n```\n````\n\n::: {.cell-output-display}\n![](04a-geocentric-models_files/figure-html/prior2-predictive-sim-a-1.png){width=672}\n:::\n:::\n\n\n**Simulate heights from priors with large sd**\n\n> Priors with ... large standard deviations are quite common in Bayesian\n> models, but they are hardly ever sensible.\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: plot-predictive-sim2-a\n\nsample_mu3_a <- rnorm(1e4, 178, 100)\nprior_h3_a <- rnorm(1e4, sample_mu3_a, sample_sigma_a)\ndens(prior_h3_a)\n```\n````\n\n::: {.cell-output-display}\n![](04a-geocentric-models_files/figure-html/plot-predictive-sim2-a-1.png){width=672}\n:::\n:::\n\n\n#### 4.3.2.3a Personal comment\n\nBoth simulation show unsensitive data with negative height to the left\nand gigantic humans in comparison to the tallest person --- [Robert\nPershing Wadlow](https://en.wikipedia.org/wiki/Robert_Wadlow)\n(1918--1940) --- ever measured (272cm).\n\nSo what? Are the priors chosen wrongly? Or does these impossibilities\nnot matter?\n\n> Does this matter? In this case, we have so much data that the silly\n> prior is harmless. But that won't always be the case. There are plenty\n> of inference problems for which the data alone are not sufficient, no\n> matter how numerous. Bayes lets us proceed in these cases. But only if\n> **we use our scientific knowledge to construct sensible priors**.\n> Using scientific knowledge to build priors is not cheating. The\n> important thing is that your prior not be based on the values in the\n> data, but only on what you know about the data before you see it.\n> (emphasis is mine)\n\n### 4.3.3a Grid approximation of the posterior distribution\n\n> mapping out the posterior distribution through brute force\n> calculations.\n\nThis is not recommended because it is\n\n-   laborious and computationally expensive\n-   usually so impractical as to be essentially impossible.\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: grid-approx-posterior-a\n\n## R code 4.16\n\n# establish range of μ and σ values, respectively, to calculate over # as well as how many points to calculate in-between. \nmu.list <- seq(from = 150, to = 160, length.out = 100)\nsigma.list <- seq(from = 7, to = 9, length.out = 100)\n\n# expands μ & σ values into a matrix of all of the combinations\npost <- expand.grid(mu = mu.list, sigma = sigma.list)\n\n# compute the log-likelihood at each combination of μ and σ\npost$LL <- sapply(1:nrow(post), function(i) {\n  sum(\n    dnorm(d2_a$height, post$mu[i], post$sigma[i], log = TRUE)\n  )\n})\n\n# multiply the prior by the likelihood\n# as the priors are on the log scale adding = multiplying\npost$prod <- post$LL + dnorm(post$mu, 178, 20, TRUE) +\n  dunif(post$sigma, 0, 50, TRUE)\n\n# getting back on the probability scale without rounding error \npost$prob <- exp(post$prod - max(post$prod))\n```\n````\n:::\n\n\n> **Comment to the last line**: the obstacle for getting back on the\n> probability scale is that rounding error is always a threat when\n> moving from log-probability to probability. If you use the obvious\n> approach, like `exp( post$prod )`, you'll get a vector full of zeros,\n> which isn't very helpful. This is a result of R's rounding very small\n> probabilities to zero.\n\n**Plot contour lines**\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: contour-plot-a\n\n## R code 4.17\nrethinking::contour_xyz(post$mu, post$sigma, post$prob)\n```\n````\n\n::: {.cell-output-display}\n![](04a-geocentric-models_files/figure-html/contour-plot-a-1.png){width=672}\n:::\n:::\n\n\n**Plot heat map**\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: plot-heat-map-a\n\n## R code 4.18\nrethinking::image_xyz(post$mu, post$sigma, post$prob)\n```\n````\n\n::: {.cell-output-display}\n![](04a-geocentric-models_files/figure-html/plot-heat-map-a-1.png){width=672}\n:::\n:::\n\n\n### 4.3.4a Sampling from the posterior\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: posterior-sample-a\n\n## R code 4.19\n\n# randomly sample row numbers in post \n# in proportion to the values in post$prob. \nsample.rows <- sample(1:nrow(post),\n  size = 1e4, replace = TRUE,\n  prob = post$prob\n)\n\n# pull out the parameter values\nsample.mu4_a <- post$mu[sample.rows]\nsample.sigma4_a <- post$sigma[sample.rows]\n\n## R code 4.20\nplot(sample.mu4_a, sample.sigma4_a, cex = 0.8, pch = 21, col = col.alpha(rangi2, 0.1))\n```\n````\n\n::: {.cell-output-display}\n![](04a-geocentric-models_files/figure-html/posterior-sample-a-1.png){width=672}\n:::\n:::\n\n\n**Marginal Posterior Density**\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: marg-post-density-a\n\n## R code 4.21\ndens(sample.mu4_a)\ndens(sample.sigma4_a)\n```\n````\n\n::: {.cell-output-display}\n![](04a-geocentric-models_files/figure-html/marg-post-density-a-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](04a-geocentric-models_files/figure-html/marg-post-density-a-2.png){width=672}\n:::\n:::\n\n\n**Posterior Compatibility Intervals (PIs)**\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: post-comp-intervals-a\n\n## R code 4.22\nPI(sample.mu4_a)\nPI(sample.sigma4_a)\n```\n````\n\n```\n#>       5%      94% \n#> 153.9394 155.2525 \n#>       5%      94% \n#> 7.323232 8.232323\n```\n:::\n\n\n#### 4.3.4.1a Sample Size & Sigma Posterior\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: sample-only-20\n\n## R code 4.23\nd3_a <- sample(d2_a$height, size = 20)\n\n## R code 4.24\nmu.list <- seq(from = 150, to = 170, length.out = 200)\nsigma.list <- seq(from = 4, to = 20, length.out = 200)\npost2 <- expand.grid(mu = mu.list, sigma = sigma.list)\npost2$LL <- sapply(1:nrow(post2), function(i) {\n  sum(dnorm(d3_a,\n    mean = post2$mu[i], sd = post2$sigma[i],\n    log = TRUE\n  ))\n})\npost2$prod <- post2$LL + dnorm(post2$mu, 178, 20, TRUE) +\n  dunif(post2$sigma, 0, 50, TRUE)\npost2$prob <- exp(post2$prod - max(post2$prod))\nsample2.rows <- sample(1:nrow(post2),\n  size = 1e4, replace = TRUE,\n  prob = post2$prob\n)\nsample2.mu <- post2$mu[sample2.rows]\nsample2.sigma <- post2$sigma[sample2.rows]\nplot(sample2.mu, sample2.sigma,\n  cex = 0.5,\n  col = col.alpha(rangi2, 0.1),\n  xlab = \"mu\", ylab = \"sigma\", pch = 16\n)\n```\n````\n\n::: {.cell-output-display}\n![](04a-geocentric-models_files/figure-html/sample-only-20-1.png){width=672}\n:::\n:::\n\n\n**Marginal Posterior Density with only 20 rows**\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: marg-post-density-a2\n\n## R code 4.25\ndens(sample2.sigma, norm.comp = TRUE)\n```\n````\n\n::: {.cell-output-display}\n![](04a-geocentric-models_files/figure-html/marg-post-density-a2-1.png){width=672}\n:::\n:::\n\n\n### 4.3.5a Using `quap()`\n\n> To build the **quadratic approximation**, we'll use quap, a command in\n> the `rethinking` package. The `quap` function works by using the model\n> definition you were introduced to earlier in this chapter. Each line\n> in the definition has a corresponding definition in the form of R\n> code. The engine inside quap then uses these definitions to define the\n> posterior probability at each combination of parameter values. Then it\n> can climb the posterior distribution and find the peak, its MAP\n> (**Maximum A Posteriori** estimate). Finally, it estimates the\n> quadratic curvature at the MAP to produce an approximation of the\n> posterior distribution. (parenthesis and emphasis are mine)\n\n1.  We start with the Howell data frame for adults `d2_a` (age \\>= 18).\n    We will place the R code equivalents into an `alist` (4.27).\n2.  Then we fit the model to the data in the data frame `d2_a` (4.28) to\n    `m4.1`.\n3.  Now we can have a look at the posterior distribution (4.29).\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: using-quap\n\n## R code 4.27\nflist <- alist(\n  height ~ dnorm(mu, sigma),\n  mu ~ dnorm(178, 20),\n  sigma ~ dunif(0, 50)\n)\n\n## R code 4.28\nm4.1 <- quap(flist, data = d2_a)\n\n## R code 4.29\nprecis(m4.1)\n```\n````\n\n```\n#>             mean        sd       5.5%      94.5%\n#> mu    154.606914 0.4120144 153.948435 155.265393\n#> sigma   7.731704 0.2914210   7.265957   8.197451\n```\n:::\n\n\n> These numbers provide Gaussian approximations for each parameter's\n> *marginal* distribution. This means the plausibility of each value of\n> μ, after averaging over the plausibilities of each value of *σ*, is\n> given by a Gaussian distribution with mean 154.6 and standard\n> deviation 0.4.\n>\n> The 5.5% and 94.5% quantiles are percentile interval boundaries,\n> corresponding to an 89% compatibility interval. Why 89%? It's just the\n> default. It displays a quite wide interval, so it shows a\n> high-probability range of parameter values. If you want another\n> interval, such as the conventional and mindless 95%, you can use\n> `precis(m4.1,prob=0.95)`. But I don't recommend 95% intervals, because\n> readers will have a hard time not viewing them as significance tests.\n> 89 is also a prime number, so if someone asks you to justify it, you\n> can stare at them meaningfully and incant, \"Because it is prime.\"\n> That's no worse justification than the conventional justification for\n> 95%.\n\nMean and standard deviation are good values to start values for hill\nclimbing. If you don't specify `quap()` will use a random value.\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: start-values-for-quap\n\n## R code 4.30\nstart <- list(\n  mu = mean(d2_a$height),\n  sigma = sd(d2_a$height)\n)\nm4.1_2 <- quap(flist, data = d2_a, start = start)\nprecis(m4.1_2)\n```\n````\n\n```\n#>             mean        sd       5.5%      94.5%\n#> mu    154.607024 0.4119947 153.948576 155.265471\n#> sigma   7.731333 0.2913860   7.265642   8.197024\n```\n:::\n\n\n::: callout-note\n## list() and alist()\n\nNote that the list of start values is a regular `list`, not an `alist`\nlike the formula list is. The two functions `alist` and `list` do the\nsame basic thing: allow you to make a collection of arbitrary R objects.\nThey differ in one important respect: `list` evaluates the code you\nembed inside it, while `alist` does not. So when you define a list of\nformulas, you should use `alist`, so the code isn't executed. But when\nyou define a list of start values for parameters, you should use `list`,\nso that code like `mean(d2$height)` will be evaluated to a numeric\nvalue.\n:::\n\n**Slicing in more information**\n\n> The priors we used before are very weak, both because they are nearly\n> flat and because there is so much data. So I'll splice in a more\n> informative prior for *μ*, so you can see the effect. All I'm going to\n> do is change the standard deviation of the prior to 0.1, so it's a\n> very narrow prior. I'll also build the formula right into the call to\n> `quap` this time.\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: smaller-prior\n\n## R code 4.31\nm4.2 <- quap(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu ~ dnorm(178, 0.1),\n    sigma ~ dunif(0, 50)\n  ),\n  data = d2_a\n)\nprecis(m4.2)\n```\n````\n\n```\n#>            mean        sd      5.5%     94.5%\n#> mu    177.86258 0.1002353 177.70238 178.02277\n#> sigma  24.49249 0.9266199  23.01158  25.97341\n```\n:::\n\n\n> Notice that the estimate for *μ* has hardly moved off the prior. The\n> prior was very concentrated around 178. So this is not surprising. But\n> also notice that the estimate for *σ* has changed quite a lot, even\n> though we didn't change its prior at all. Once the golem is certain\n> that the mean is near 178---as the prior insists---then the golem has\n> to estimate *σ* conditional on that fact. This results in a different\n> posterior for σ, even though all we changed is prior information about\n> the other parameter.\n\n::: callout-caution\n## Change of μ?\n\nI do not understand the hint, that \"*μ* has hardly moved off the prior\",\nbecause for me *μ* has changed considerably from 154 to 177.\n:::\n",
    "supporting": [
      "04a-geocentric-models_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
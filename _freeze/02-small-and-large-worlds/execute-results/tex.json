{
  "hash": "70d69bc53334ff2af55b659346c1f44b",
  "result": {
    "markdown": "# Small and Large Worlds\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: setup\n\nlibrary(conflicted)\nlibrary(rethinking)\n```\n````\n\n```\n#> Loading required package: rstan\n#> Loading required package: StanHeaders\n#> \n#> rstan version 2.26.22 (Stan version 2.26.1)\n#> For execution on a local, multicore CPU with excess RAM we recommend calling\n#> options(mc.cores = parallel::detectCores()).\n#> To avoid recompilation of unchanged Stan programs, we recommend calling\n#> rstan_options(auto_write = TRUE)\n#> For within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\n#> change `threads_per_chain` option:\n#> rstan_options(threads_per_chain = 1)\n#> Loading required package: cmdstanr\n#> This is cmdstanr version 0.5.3\n#> - CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n#> - CmdStan path: /Users/petzi/.cmdstan/cmdstan-2.32.2\n#> - CmdStan version: 2.32.2\n#> Loading required package: parallel\n#> rethinking (Version 2.31)\n```\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: setup\n\nlibrary(tidyverse)\n```\n````\n\n```\n#> -- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\n#> v dplyr     1.1.2     v readr     2.1.4\n#> v forcats   1.0.0     v stringr   1.5.0\n#> v ggplot2   3.4.2     v tibble    3.2.1\n#> v lubridate 1.9.2     v tidyr     1.3.0\n#> v purrr     1.0.1\n```\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: setup\n\nlibrary(patchwork)\n\nconflicts_prefer(dplyr::lag)\n```\n````\n\n```\n#> [conflicted] Will prefer dplyr::lag over any other package.\n```\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: setup\n\nconflicts_prefer(dplyr::filter)\n```\n````\n\n```\n#> [conflicted] Will prefer dplyr::filter over any other package.\n```\n:::\n\n\n\n::: callout-note\n###### Setup chunk results in several output chunks\n\nThe single setup chunk produces in the output several separate chunks\nthat are tied with the specifics of the loaded packages. This split-up\nhelps to see the association of messages with their corresponding\npackages.\n:::\n\n## Original {.unnumbered}\n\n-   The **small world** is the self-contained logical world of the\n    model.\n-   The **large world** is the broader context in which one deploys a\n    model.\n\n**Meta Remark (Study Guide)**\n\n> This chapter focuses on the small world. It explains probability\n> theory in its essential form: counting the ways things can happen.\n> Bayesian inference arises automatically from this perspective. Then\n> the chapter presents the stylized components of a Bayesian statistical\n> model, a model for learning from data. Then it shows you how to\n> animate the model, to produce estimates.\n>\n> All this work provides a foundation for the next chapter, in which\n> you'll learn to summarize Bayesian estimates, as well as begin to\n> consider large world obligations.\n\n## Tidyverse {.unnumbered}\n\n**Meta Remark (My Replication Strategy)**\n\nThe work by Solomon Kurz has many references to R specifics, so that\npeople new to R can follow the course. Most of these references are not\nnew to me, so I will not include them in my personal notes. There are\nalso very important references to other relevant articles I do not know.\nBut I will put these kind of references for now aside and will me mostly\nconcentrate on the replication and understanding of the code examples.\n\nOne challenge for the author (Kurz) was to replicate all the graphics of\nthe original version, even if they were produced just for understanding\nwithout underlying code. I will use only code lines that are essential\nto display Bayesian results. Therefore I will not replicate the very\nextensive explication how to produce with tidyverse means the graphic of\nthe garden of forking data.\n\n## The Garden of Forking Data\n\n### Original\n\n::: callout-tip\n###### Bayesian inference is counting of possibilities\n\nBayesian inference is really just counting and comparing of\npossibilities. ... In order to make good inference about what actually\nhappened, it helps to consider everything that could have happened. A\nBayesian analysis is a garden of forking data, in which alternative\nsequences of events are cultivated. As we learn about what did happen,\nsome of these alternative sequences are pruned. In the end, what remains\nis only what is logically consistent with our knowledge.\n:::\n\n#### Counting possibilities\n\n> Suppose there's a bag, and it contains four marbles. These marbles\n> come in two colors: blue and white. We know there are four marbles in\n> the bag, but we don't know how many are of each color. We do know that\n> there are five possibilities: (1) \\[⚪⚪⚪⚪\\], (2) \\[⚫⚪⚪⚪\\], (3)\n> \\[⚫⚫⚪⚪\\], (4) \\[⚫⚫⚫⚪\\], (5) \\[⚫⚫⚫⚫\\]. These are the only\n> possibilities consistent with what we know about the contents of the\n> bag. **Call these five possibilities the *conjectures*.**\n>\n> Our goal is to figure out which of these conjectures is most\n> plausible, given some evidence about the contents of the bag. We do\n> have some evidence: A sequence of three marbles is pulled from the\n> bag, one at a time, replacing the marble each time and shaking the bag\n> before drawing another marble. **The sequence that emerges is: ⚫⚪⚫,\n> in that order. These are the data.** (bold emphasis pb)\n>\n> So now let's plant the garden and see how to use the data to infer\n> what's in the bag. Let's begin by considering just the single\n> conjecture, \\[⚫⚪⚪⚪\\], that the bag contains one blue and three\n> white marbles. ...\n>\n> ...\n>\n> Notice that even though the three white marbles look the same from a\n> data perspective---we just record the color of the marbles, after\n> all---they are really different events. This is important, because it\n> means that there are three more ways to see ⚪ than to see ⚫.\n\n|              |                        |\n|--------------|------------------------|\n| Conjecture   | Ways to produce ⚫⚪⚫ |\n| \\[⚪⚪⚪⚪\\] | 0 × 4 × 0 = 0          |\n| \\[⚫⚪⚪⚪\\] | 1 × 3 × 1 = 3          |\n| \\[⚫⚫⚪⚪\\] | 2 × 2 × 2 = 8          |\n| \\[⚫⚫⚫⚪\\] | 3 × 1 × 3 = 9          |\n| \\[⚫⚫⚫⚫\\] | 4 × 0 × 4 = 0          |\n\nI have bypassed the counting procedure related with the step-by-step\nvisualization of the garden of forking data. The multiplication in the\nabove table is still a summarized counting:\n\n::: callout-important\n> Multiplication is just a shortcut to enumerating and counting up all\n> of the paths through the garden that could produce all the\n> observations.\n:::\n\n> Notice that the number of ways to produce the data, for each\n> conjecture, can be computed by first counting the number of paths in\n> each \"ring\" of the garden and then by multiplying these counts\n> together. ... The fact that numbers are multiplied during calculation\n> doesn't change the fact that this is still just counting of logically\n> possible paths. This point will come up again, when you meet a formal\n> representation of Bayesian inference.\n\nThe multiplication in the above table has to be interpreted the\nfollowing way:\n\n1.  The possibility of the conjecture that the bag contains four white\n    marbles is zero because the result shows also black marbles. This is\n    the other way around for the last conjecture of four blue/black\n    marbles.\n2.  The possibility of the conjecture that the bag contains one black\n    and three white marbles is calculated the following way: The first\n    marble of the result is black and --- according to our conjecture\n    --- there is only one way (=1) to produce this black marble. The\n    next marble we have drawn is white. This is consistent with three\n    (=3) different ways( marbles) of our conjecture. The last drawn\n    marble is again black which corresponds again with just one way\n    (possibility) following our conjecture. So we get as result of the\n    garden of forking data: `1 x 3 x 1`.\n3.  The calculation of the other conjectures follows the same pattern.\n\n#### Combining Other Information\n\n> We may have additional information about the relative plausibility of\n> each conjecture. This information could arise from knowledge of how\n> the contents of the bag were generated. It could also arise from\n> previous data. Whatever the source, it would help to have a way to\n> combine different sources of information to update the plausibilities.\n> Luckily there is a natural solution: Just multiply the counts.\n>\n> To grasp this solution, suppose we're willing to say each conjecture\n> is equally plausible at the start. So we just compare the counts of\n> ways in which each conjecture is compatible with the observed data.\n> This comparison suggests that \\[⚫⚫⚫⚪\\] is slightly more plausible\n> than \\[⚫⚫⚪⚪\\], and both are about three times more plausible than\n> \\[⚫⚪⚪⚪\\]. **Since these are our initial counts, and we are going\n> to update them next, let's label them *prior*.** (bold emphasis pb)\n\n::: callout-tip\n###### Principle of Indifference\n\nAs we will see later: the choice of the prior is not relevant for the\nfinal result. This is called the *Principle of Indifference*.\n\nThe only difference between a good or bad choice is the time (the number\nof steps) the updating process needs to produce the final result.\n\nBefore seeing any data the most common solution is to assign an equal\nnumber of ways that each conjecture could be correct.\n:::\n\n> Which assumption should we use, when there is no previous information\n> about the conjectures? The most common solution is to assign an equal\n> number of ways that each conjecture could be correct, before seeing\n> any data. This is sometimes known as the **PRINCIPLE OF\n> INDIFFERENCE**: When there is no reason to say that one conjecture is\n> more plausible than another, weigh all of the conjectures equally. ...\n>\n> For the sort of problems we examine in this book, the principle of\n> indifference results in inferences very comparable to mainstream\n> non-Bayesian approaches, most of which contain implicit equal\n> weighting of possibilities. For example a typical non-Bayesian\n> confidence interval weighs equally all of the possible values a\n> parameter could take, regardless of how implausible some of them are.\n> In addition, many non-Bayesian procedures have moved away from equal\n> weighting, through the use of penalized likelihood and other methods.\n\n**Bayesian Updating Process**\n\nHere's how we do the updating:\n\n1.  First we count the numbers of ways each conjecture could produce the\n    new observation, ⚫.\n2.  Then we multiply each of these new counts by the prior numbers of\n    ways for each conjecture.\n\n|     |              |     |                    |     |              |     |            |     |\n|--------|--------|--------|--------|--------|--------|--------|--------|--------|\n|     |              |     |                    |     |              |     |            |     |\n|     | Conjecture   |     | Ways to produce ⚫ |     | Prior counts |     | New count  |     |\n|     | \\[⚪⚪⚪⚪\\] |     | 0                  |     | 0            |     | 0 × 0 = 0  |     |\n|     | \\[⚫⚪⚪⚪\\] |     | 1                  |     | 3            |     | 3 × 1 = 3  |     |\n|     | \\[⚫⚫⚪⚪\\] |     | 2                  |     | 8            |     | 8 × 2 = 16 |     |\n|     | \\[⚫⚫⚫⚪\\] |     | 3                  |     | 9            |     | 9 × 3 = 27 |     |\n|     | \\[⚫⚫⚫⚫\\] |     | 4                  |     | 0            |     | 0 × 4 = 0  |     |\n\n::: {.callout-note style=\"color: blue;\"}\n###### Typo\n\nIn the book the table header \"Ways to produce\" includes ⚪ instead of\n--- as I think is correct --- ⚫.\n:::\n\n#### From Counts to Probability\n\n::: {.callout-tip style=\"color: green\"}\n###### Principle of honest ignorance\n\n*When we don't know what caused the data, potential causes that may\nproduce the data in more ways are more plausible*.\n:::\n\nTwo reasons for using probabilities instead of counts:\n\n1.  Only relative value matters.\n2.  Counts will very fast grow very large and difficult to manipulate.\n\n![](img/plausibility-formula-1-min.png){fig-align=\"center\"}\n\n\n\n```{=html}\n<center>\n  That little <span style=\"font-size: 40px;\">∝</span> means <em>proportional to</em>.\n</center>\n```\n\n\n**Standardizing the plausibility**\n\n![](img/plausibility-formula-2-min.png){fig-align=\"center\"}\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: compute-plausibilities\n\n## R code 2.1\nways <- c(0, 3, 8, 9, 0)\nways / sum(ways)\n```\n````\n\n```\n#> [1] 0.00 0.15 0.40 0.45 0.00\n```\n:::\n\n\n\nI understand that in the above code we assume as the very first prior\nplausibility the ways the results can be produced by the assumed\nconjecture proportion of blue marbles. It corresponds to the parameter\nvalue. This conclusion is somewhat hidden as our very first calculation\nalready takes three drawn marbles (⚫⚪⚫) into account. From another\nperspective this could also be seen as the first draw (⚫) followed by\ntwo Bayesian updates (⚪⚫). This corresponds to the following counting:\n(In the second and third draw the first factor of the multiplication is\nalways the prior.)\n\n1.  First draw ⚫: 0,1,2,3,4 ways corresponding to the 4 chosen\n    conjectures.\n2.  Second draw ⚪: 0 x 4 = 0, 1 x 3 = 3, 2 x 2 = 4, 3 x 1 = 3, 4 x 0 =\n    0\n3.  Third draw ⚫: 0 x 0 = 0, 3 x 1 = 3, 4 x 2 = 8, 3 x 3 = 9, 0 x 4 = 0\n\nThis demonstration shows that the book example of ⚫⚪⚫ already\ncontains three priors: The first is identical with the conjectured\nproportion of blue marbles (0,1,2,3,4) and is equivalent to the\nparameter value for each conjecture. The second and third marbles\nalready uses Bayesian updating.\n\n**Names of the different parts of the formula**\n\nData = ⚫⚪⚫.\n\n-   A conjectured proportion of blue marbles, *p*, is usually called a\n    **PARAMETER** value. It's just a way of indexing possible\n    explanations of the data. For instance one conjectured proportion of\n    one blue marble could be: ⚫⚪⚪⚪ (`p = 1`). The others are:\n    ⚪⚪⚪⚪ (`p = 0`), ⚫⚫⚪⚪ (`p = 2` , ⚫⚫⚫⚪ (`p = 3`), and\n    ⚫⚫⚫⚫ (`p = 4` ways).\n-   The relative number of ways that a value *p* can produce the data is\n    usually called a **LIKELIHOOD**. It is derived by enumerating all\n    the possible data sequences that could have happened and then\n    eliminating those sequences inconsistent with the data. For\n    instance: `0.00, 0.15, 0.40, 0.45, 0.00`\n-   The prior plausibility of any specific *p* is usually called the\n    **PRIOR PROBABILITY**. For instance: `0, 3, 8, 9, 0`\n-   The new, updated plausibility of any specific *p* is usually called\n    the **POSTERIOR PROBABILITY**. For instance: `0, 3, 16, 27, 0`\n\n### Tidyverse\n\n#### Counting possibilities\n\n> If we're willing to code the marbles as 0 = \"white\" 1 = \"blue\", we can\n> arrange the possibility data in a tibble as follows.\n\n::: {.callout-note style=\"color: blue;\"}\n###### Changed code slightly\n\nI changed `rep()` to `rep.int()` and added `L` to the value of p1 resp.\np5 to get integers (instead of doubles).\n:::\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: create-marble-data\n\nd <-\n  tibble(p1 = 0L,\n         p2 = rep.int(1:0, times = c(1, 3)),\n         p3 = rep.int(1:0, times = c(2, 2)),\n         p4 = rep.int(1:0, times = c(3, 1)),\n         p5 = 1L)\n\nhead(d)\n```\n````\n\n```\n#> # A tibble: 4 x 5\n#>      p1    p2    p3    p4    p5\n#>   <int> <int> <int> <int> <int>\n#> 1     0     1     1     1     1\n#> 2     0     0     1     1     1\n#> 3     0     0     0     1     1\n#> 4     0     0     0     0     1\n```\n:::\n\n\n\n> You might depict the possibility data in a plot.\n\n\n\n::: {.cell}\n\n````{.cell-code #lst-marble-data lst-cap=\"Code listing to show the marble data as graph\" code-line-numbers=\"false\"}\n```{{r}}\n#| label: fig-show-marble-data\n#| fig-cap: \"Marble Data\"\n#| attr-source: '#lst-marble-data lst-cap=\"Code listing to show the marble data as graph\"'\n\nd %>% \n  set_names(1:5) %>%    # <1>\n  mutate(x = 1:4) %>%  # <2> \n  pivot_longer(-x, names_to = \"possibility\") %>%  # <3>\n  mutate(value = value %>% as.character()) %>%    # <4>\n  \n  ggplot(aes(x = x, y = possibility, fill = value)) + # <5>\n  geom_point(shape = 21, size = 5) + # <5>\n  scale_fill_manual(values = c(\"white\", \"navy\")) + # <5>\n  scale_x_discrete(NULL, breaks = NULL) + # <5>\n  theme(legend.position = \"none\") # <5>\n \n```\n````\n\n::: {.cell-output-display}\n![Marble Data](02-small-and-large-worlds_files/figure-pdf/fig-show-marble-data-1.pdf){#fig-show-marble-data fig-pos='H'}\n:::\n:::\n\n\n\n1.  Change the column names from p\\<number\\> to \\<number\\>.\n2.  Add a new column with the values 1 to 4 for each row.\n3.  Convert data frame from wide to long, excluding the `x` column.\n4.  Change the variable type of `value` column from double to character.\n5.  Plot the results.\n\nAt first I could not understand the code lines. I had to execute line by\nline to see what happens:\n\n##### Annotation (1)\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: using-set-names\n\nset_names(d, 1:5)\n```\n````\n\n```\n#> # A tibble: 4 x 5\n#>     `1`   `2`   `3`   `4`   `5`\n#>   <int> <int> <int> <int> <int>\n#> 1     0     1     1     1     1\n#> 2     0     0     1     1     1\n#> 3     0     0     0     1     1\n#> 4     0     0     0     0     1\n```\n:::\n\n\n\n`set_names()` comes from {**rlang**} and is exported to {**purrr**}. It\nis equivalent to `stats::setNames()` but has more features and stricter\nargument checking. I does nothing more as to change the column names\nfrom p\\<number\\> to \\<number\\>. If one had used just numbers for the\nprobability columns this line wouldn't have been necessary as it is\nshown in the next chunk. (I omitted the `head()` argument of the last\nline as it is not necessary.)\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: create-marble-data-2\n\ndf <-\n  tibble(`1` = 0,\n         `2` = rep(1:0, times = c(1, 3)),\n         `3` = rep(1:0, times = c(2, 2)),\n         `4` = rep(1:0, times = c(3, 1)),\n         `5` = 1)\n\ndf\n```\n````\n\n```\n#> # A tibble: 4 x 5\n#>     `1`   `2`   `3`   `4`   `5`\n#>   <dbl> <int> <int> <int> <dbl>\n#> 1     0     1     1     1     1\n#> 2     0     0     1     1     1\n#> 3     0     0     0     1     1\n#> 4     0     0     0     0     1\n```\n:::\n\n\n\nIt is interesting to see that the first and last column are doubles and\nnot integers. I believe that the reason is that these two columns do not\nhave variations, e.g. do not contain both variable values, so that R\nassumes the more general class of `numeric` and type of `double`.\n\n-   `class(5L)` and `typeof(5L`) both results to *integer* .\n-   Whereas `class(5)` is *integer* but `typeof(5)` is *double*.\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: number-types\n\nclass(5L)\ntypeof(5L)\nclass(5)\ntypeof(5)\n```\n````\n\n```\n#> [1] \"integer\"\n#> [1] \"integer\"\n#> [1] \"numeric\"\n#> [1] \"double\"\n```\n:::\n\n\n\n##### Annotation (2)\n\nAfter understanding what set_names() does the next line with `mutate()`\nis easy. It adds a new column `x` with the values 1 to 4 for each row.\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: add-x-column\n\ndf <- df |> \n    set_names(1:5) |> \n    mutate(x = 1:4)\ndf\n```\n````\n\n```\n#> # A tibble: 4 x 6\n#>     `1`   `2`   `3`   `4`   `5`     x\n#>   <dbl> <int> <int> <int> <dbl> <int>\n#> 1     0     1     1     1     1     1\n#> 2     0     0     1     1     1     2\n#> 3     0     0     0     1     1     3\n#> 4     0     0     0     0     1     4\n```\n:::\n\n\n\n##### Annotation (3)\n\nI understood that the data frame is converted from a wide to a long\nstructure. But together with the pipe and not naming the first parameter\n`-x` I did not catch the essence of the command.\n\nA first understanding comes from the fact, that it is wrong to covert\nall columns to the long format:\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: wrong-long-conversion\n\npivot_longer(data = df, cols = everything(), names_to = \"possibility\") |> \n    print(n = 24)\n```\n````\n\n```\n#> # A tibble: 24 x 2\n#>    possibility value\n#>    <chr>       <dbl>\n#>  1 1               0\n#>  2 2               1\n#>  3 3               1\n#>  4 4               1\n#>  5 5               1\n#>  6 x               1\n#>  7 1               0\n#>  8 2               0\n#>  9 3               1\n#> 10 4               1\n#> 11 5               1\n#> 12 x               2\n#> 13 1               0\n#> 14 2               0\n#> 15 3               0\n#> 16 4               1\n#> 17 5               1\n#> 18 x               3\n#> 19 1               0\n#> 20 2               0\n#> 21 3               0\n#> 22 4               0\n#> 23 5               1\n#> 24 x               4\n```\n:::\n\n\n\nThis (wrong) example shows that it is mandatory to exclude `x` from the\nconversion. Otherwise it would be included and integrated into the\n`possibility` column.\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: correct-long-conversion\n\n(d <- \n    pivot_longer(data = df, cols = -x, names_to = \"possibility\"))\n```\n````\n\n```\n#> # A tibble: 20 x 3\n#>        x possibility value\n#>    <int> <chr>       <dbl>\n#>  1     1 1               0\n#>  2     1 2               1\n#>  3     1 3               1\n#>  4     1 4               1\n#>  5     1 5               1\n#>  6     2 1               0\n#>  7     2 2               0\n#>  8     2 3               1\n#>  9     2 4               1\n#> 10     2 5               1\n#> 11     3 1               0\n#> 12     3 2               0\n#> 13     3 3               0\n#> 14     3 4               1\n#> 15     3 5               1\n#> 16     4 1               0\n#> 17     4 2               0\n#> 18     4 3               0\n#> 19     4 4               0\n#> 20     4 5               1\n```\n:::\n\n\n\nThe above code line for the conversion from wide to long is equivalent\nwith naming explicitly all column names to convert:\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: test-if-identical\n\nd1 <- pivot_longer(data = df, cols = -x, names_to = \"possibility\")\nd2 <- pivot_longer(data = df, \n                   cols = c(`1`, `2`, `3`, `4`, `5`), \n                   names_to = \"possibility\")\nidentical(d1, d2)\n```\n````\n\n```\n#> [1] TRUE\n```\n:::\n\n\n\nThe identity demonstrates: The `-x` parameter excludes the `x` column\nfrom the wide to long transformation. It is a shorthand for naming all 5\ncolumns that should be transformed.\n\nInstead of the `identical()` function you could also use `all.equal()`.\nComparing data frames is a rather complicated action summarized by a\nblog post from Sharla Gelfand. But keep in mind that the publication\ndate is 2020-02-17 and that therefore some commands are outdated. Most\nimportant: use `base::all.equal()` instead of `dplyr::all_equal()`.\n\n::: {.callout-warning style=\"color: orange;\"}\n> `all_equal()` allows you to compare data frames, optionally ignoring\n> row and column names. It is deprecated as of dplyr 1.1.0, because it\n> makes it too easy to ignore important differences.\n\nThe current version of the {dplyr) packages is 1.1.2.\n:::\n\n##### Annotation (4)\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: change-column-type\n(d <- d |> \n    mutate(value = value %>% as.character()))\n```\n````\n\n```\n#> # A tibble: 20 x 3\n#>        x possibility value\n#>    <int> <chr>       <chr>\n#>  1     1 1           0    \n#>  2     1 2           1    \n#>  3     1 3           1    \n#>  4     1 4           1    \n#>  5     1 5           1    \n#>  6     2 1           0    \n#>  7     2 2           0    \n#>  8     2 3           1    \n#>  9     2 4           1    \n#> 10     2 5           1    \n#> 11     3 1           0    \n#> 12     3 2           0    \n#> 13     3 3           0    \n#> 14     3 4           1    \n#> 15     3 5           1    \n#> 16     4 1           0    \n#> 17     4 2           0    \n#> 18     4 3           0    \n#> 19     4 4           0    \n#> 20     4 5           1\n```\n:::\n\n\n\nThe `value` column is of type `double` as can be seen in the result of\nAnnotation (3). For the plot it has to be changed to the type of\n`character`. Otherwise it could not be used with the `fill` option.\n\n##### Annotation (5)\n\nThe plot uses code from the {**ggplot2**} package, which I do understand\nand will therefore not explain here.\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: plot-possibilities-conjectures\n\nd |> \n  ggplot(aes(x = x, y = possibility, fill = value)) + \n  geom_point(shape = 21, size = 5) + \n  scale_fill_manual(values = c(\"white\", \"navy\")) +\n  scale_x_discrete(NULL, breaks = NULL) + \n  theme(legend.position = \"none\")\n```\n````\n\n::: {.cell-output-display}\n![](02-small-and-large-worlds_files/figure-pdf/plot-possibilities-conjectures-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n##### Summarize the Possiblity Structure\n\n> Here's the basic structure of the possibilities per marble draw.\n\n\n\n::: {#tbl-basic-prob-struct .cell tbl-cap='possibility-structure'}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: tbl-basic-prob-struct\n#| tbl-cap: possibility-structure\n\ntibble(draw    = 1:3,\n       marbles = 4) %>% \n  mutate(possibilities = marbles ^ draw) %>% \n  kableExtra::kbl() |> \n  kableExtra::kable_classic(full_width = F)\n```\n````\n\n::: {.cell-output-display}\n\\begin{table}\n\\centering\n\\begin{tabular}[t]{r|r|r}\n\\hline\ndraw & marbles & possibilities\\\\\n\\hline\n1 & 4 & 4\\\\\n\\hline\n2 & 4 & 16\\\\\n\\hline\n3 & 4 & 64\\\\\n\\hline\n\\end{tabular}\n\\end{table}\n:::\n:::\n\n\n\n::: {.callout-note style=\"color: blue\"}\n###### Table Packages Used\n\nKurz employed the {**flextable**} package to print tables. As I have no\nexperience with this package, I will apply {**kableExtra**} in this\ndocument.\\\n\\\nUntil now I had used most of the time kableExtra and sometimes DT. For a\nshort compilation of available table packages see the section on [Other\npackages for creating\ntables](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html)\nin the R Markdown Cookbook. The following excursion on tables follows\nthe blog article [Top 7 Packages for Making Beautiful Tables in\nR](https://towardsdatascience.com/top-7-packages-for-making-beautiful-tables-in-r-7683d054e541)\nby Devashree Madhugiri.\n:::\n\n#### Excursion: Tables\n\n-   [{**gt**}](https://gt.rstudio.com/): The gt package offers a\n    different and easy-to-use set of functions that helps us build\n    display tables from tabular data. The gt philosophy states that a\n    comprehensive collection of table parts can be used to create a\n    broad range of functional tables. These are the table body, the\n    table footer, the spanner column labels, the column labels, and the\n    table header. (I should look into the {**gt**} package in more\n    detail as it is developed by the RStudio/Posit team, that stands not\n    only for high quality but also for tidyverse compatibility.)\n\n    ![](https://gt.rstudio.com/reference/figures/gt_parts_of_a_table.svg)\n\n-   [{**formattable**}](https://renkun-ken.github.io/formattable/):\n    Formattable data frames are data frames that will be displayed in\n    HTML tables using formatter functions. This package includes\n    techniques to produce data structures with predefined formatting\n    rules, such that the objects maintain the original data but are\n    formatted. The package consists of several standard formattable\n    objects, including percent, comma, currency, accounting, and\n    scientific.\n\n-   [{**kableExtra**}](https://haozhu233.github.io/kableExtra/): It\n    extends the basic functionality of `knitr::kable()` tables. Although\n    `knitr::kable()` is simple by design, it has many features missing\n    which are usually available in other packages. {**kableExtra**} has\n    filled the gap nicely. One of the best thing about {**kableExtra**}\n    is that most of its table capabilities work for both HTML and PDF\n    formats.\n\n-   [{**DT**}](https://rstudio.github.io/DT/): dt is an abbreviation of\n    'DataTables.' Data objects in R can be rendered as HTML tables using\n    the JavaScript library 'DataTables' (typically via R Markdown or\n    Shiny).\n\n-   [{**flextable**}](https://davidgohel.github.io/flextable/): This\n    package helps you to create reporting table from a data frame\n    easily. You can merge cells, add headers, add footers, change\n    formatting, and set how data in cells is displayed. Table content\n    can also contain mixed types of text and image content. Tables can\n    be embedded from R Markdown documents into HTML, PDF, Word, and\n    PowerPoint documents and can be embedded using Package Officer for\n    Microsoft Word or PowerPoint documents. Tables can also be exported\n    as R plots or graphic files, e.g., png, pdf, and jpeg.\n\n-   [{**reactable**}](https://glin.github.io/reactable/): `reactable()`\n    creates a data table from tabular data with sorting and pagination\n    by default. The data table is an HTML widget that can be used in R\n    Markdown documents and Shiny applications or viewed from an R\n    console. It is based on the React Table library and made with\n    reactR. Features are:\n\n    -   It creates a data table with sorting, filtering, and pagination\n    -   It has built-in column formatting\n    -   It supports custom rendering via R or JavaScript\n    -   It works seamlessly within R Markdown documents and the Shiny\n        app\n\n-   [{**ractablefmtr**}](https://kcuilla.github.io/reactablefmtr/index.html):\n    The package improves the appearance and formatting of tables created\n    using the reactable R library. It includes many conditional\n    formatters that are highly customizable and easy to use.\n\nI sure there are other packages as well. But the above seven packages\nare a first starting point to learn creating and displaying\nsophisticated data tables in R.\n\n> The authors of R Markdown Cookbook (Yihui Xie, Christophe Dervieux,\n> Emily Riederer) mention also several other table packages in the\n> section [Other packages for creating\n> tables](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html):\n>\n> -   **rhandsontable** ([Owen\n>     2021](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html#ref-R-rhandsontable)):\n>     Also similar to **DT**, and has an Excel feel (e.g., you can edit\n>     data directly in the table). Visit\n>     <https://jrowen.github.io/rhandsontable/> to learn more about it.\n>\n> -   **pixiedust** ([Nutter\n>     2021](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html#ref-R-pixiedust)):\n>     Features creating tables for models (such as linear models)\n>     converted through the **broom** package ([Robinson, Hayes, and\n>     Couch\n>     2023](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html#ref-R-broom)).\n>     It supports Markdown, HTML, and LaTeX output formats. Its\n>     repository is at <https://github.com/nutterb/pixiedust>.\n>\n> -   **stargazer** ([Hlavac\n>     2022](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html#ref-R-stargazer)):\n>     Features formatting regression models and summary statistics\n>     tables. The package is available on CRAN at\n>     <https://cran.r-project.org/package=stargazer>.\n>\n> -   **xtable** ([Dahl et al.\n>     2019](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html#ref-R-xtable)):\n>     Perhaps the oldest package for creating tables---the first release\n>     was made in 2000. It supports both LaTeX and HTML formats. The\n>     package is available on CRAN at\n>     <https://cran.r-project.org/package=xtable>.\n>\n> I'm not going to introduce the rest of packages, but will just list\n> them here: **tables** ([Murdoch\n> 2023](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html#ref-R-tables)),\n> **pander** ([Daróczi and Tsegelskyi\n> 2022](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html#ref-R-pander)),\n> **tangram** ([S. Garbett\n> 2023](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html#ref-R-tangram)),\n> **ztable** ([Moon\n> 2021](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html#ref-R-ztable)),\n> and **condformat** ([Oller Moreno\n> 2022](https://bookdown.org/yihui/rmarkdown-cookbook/table-other.html#ref-R-condformat)).\n\n## Building a Model\n\n### Original\n\n**Globe-tossing**\n\nWe are going to use a toy example, but it has the same structure as a\ntypical statistical analyses. The first nine samples produce the\nfollowing data:\n\n`W L W W W L W L W` (W indicates water and L indicates land.)\n\nDesigning a simple Bayesian model benefits from a design loop with three\nsteps.\n\n1.  **Data story**: Motivate the model by narrating how the data might\n    arise.\n2.  **Update**: Educate your model by feeding it the data.\n3.  **Evaluate**: All statistical models require supervision, leading to\n    model revision.\n\n#### A Data Story\n\n> Bayesian data analysis usually means producing a story for how the\n> data came to be. This story may be *descriptive*, specifying\n> associations that can be used to predict outcomes, given observations.\n> Or it may be *causal*, a theory of how some events produce other\n> events.\n\nAll data stories have to be complete in the sense that they are\nsufficient for specifying an algorithm for simulating new data.\n\n> You can motivate your data story by trying to explain how each piece\n> of data is born. This usually means describing aspects of the\n> underlying reality as well as the sampling process. The data story in\n> ... \\[our toy case of globe-tossing\\] is simply a restatement of the\n> sampling process:\n>\n> 1.  The true proportion of water covering the globe is *`p`*.\n> 2.  A single toss of the globe has a probability *`p`* of producing a\n>     water (W) observation. It has a probability `1 – p` of producing a\n>     land (L) observation.\n> 3.  Each toss of the globe is independent of the others.\n\n#### Bayesian Updating\n\n> A Bayesian model begins with one set of plausibilities assigned to\n> each of these possibilities. These are the prior plausibilities. Then\n> it updates them in light of the data, to produce the posterior\n> plausibilities. This updating process is a kind of learning, called\n> **BAYESIAN UPDATING**.\n>\n> ...\n>\n> For the sake of the example only, let's program our Bayesian machine\n> to initially assign the same plausibility to every proportion of\n> water, every value of *p*. We'll do better than this later.\n>\n> ...\n>\n> Every time a \"W\" is seen, the peak of the plausibility curve moves to\n> the right, towards larger values of *p*. Every time an \"L\" is seen, it\n> moves the other direction. The maximum height of the curve increases\n> with each sample, meaning that fewer values of *p* amass more\n> plausibility as the amount of evidence increases. As each new\n> observation is added, the curve is updated consistent with all\n> previous observations.\n\nTo see the results of the different (updating) steps I am going to\nreproduce Figure 2.5 (How a Bayesian model learns) of the book. In the\ntidyverse-version we will learn how to write R code to reproduce the\ngraphic.\n\n![Copy of Figure 2.5: **How a Bayesian model learns**. In each plot,\nprevious plausibilities (dashed curve) are updated in light of the\nlatest observation to produce a new set of plausibilities (solid\ncurve).](img/bayesian_model_learns_step_by_step-min.png){#fig-2-5-book-copy\nfig-alt=\"Nine small diagrams to show the relationship between plausibility against proportion of water after each sample.\"}\n\n#### Evaluate\n\nKeep in mind two cautious principles:\n\n1.  <div>\n\n    > -   First, **the model's certainty is no guarantee that the model\n    >     is a good one**. ... \\[M\\]odels of all sorts---Bayesian or\n    >     not---can be very confident about an inference, even when the\n    >     model is seriously misleading. This is because the inferences\n    >     are conditional on the model. What your model is telling you\n    >     is that, given a commitment to this particular model, it can\n    >     be very sure that the plausible values are in a narrow range.\n    >     Under a different model, things might look differently.\n    >\n    > -   Second, **it is important to supervise and critique your\n    >     model's work**. ... When something is irrelevant to the\n    >     machine, it won't affect the inference directly. But it may\n    >     affect it indirectly ... So it is important to check the\n    >     model's inferences in light of aspects of the data it does not\n    >     know about. Such checks are an inherently creative enterprise,\n    >     left to the analyst and the scientific community. Golems are\n    >     very bad at it. (emphasis pb)\n\n    </div>\n\nFurther keep in mind that\n\n> the goal is not to test the truth value of the model's assumptions. We\n> know the model's assumptions are never exactly right, in the sense of\n> matching the true data generating process. ... Moreover, models do not\n> need to be exactly true in order to produce highly precise and useful\n> inferences.\n>\n> Instead, the objective is to check the model's adequacy for some\n> purpose. This usually means asking and answering additional questions,\n> beyond those that originally constructed the model. Both the questions\n> and answers will depend upon the scientific context. So it's hard to\n> provide general advice.\n\n### Tidyverse\n\nLet's save our globe-tossing data `W L W W W L W L W` in a tibble:\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: globe-tossing-data\n\n(d <- tibble(toss = c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\")))\n```\n````\n\n```\n#> # A tibble: 9 x 1\n#>   toss \n#>   <chr>\n#> 1 w    \n#> 2 l    \n#> 3 w    \n#> 4 w    \n#> 5 w    \n#> 6 l    \n#> 7 w    \n#> 8 l    \n#> 9 w\n```\n:::\n\n\n\n#### A Data Story\n\n#### Bayesian Updating {#sec-expand_grid}\n\nFor the updating process we need to add to the data the cumulative\nnumber of trials, `n_trials`, and the cumulative number of successes,\n`n_successes` (i.e., `toss == \"w\"`).\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: bayesian-updating-start\n\n(\n  d <-\n  d %>% \n  mutate(n_trials  = 1:9,\n         n_success = cumsum(toss == \"w\"))\n  )\n```\n````\n\n```\n#> # A tibble: 9 x 3\n#>   toss  n_trials n_success\n#>   <chr>    <int>     <int>\n#> 1 w            1         1\n#> 2 l            2         1\n#> 3 w            3         2\n#> 4 w            4         3\n#> 5 w            5         4\n#> 6 l            6         4\n#> 7 w            7         5\n#> 8 l            8         5\n#> 9 w            9         6\n```\n:::\n\n\n\nThe program code for reproducing the Figure 2.5 of the book (here in in\nthis document it is @fig-2-5-book-copy) is pretty complex. Again I have\nto inspect the results line by line as I had done for the\n@lst-marble-data. At first I will give a short introduction what each\nline does. In the next steps I will explain each step more in detail and\nshow the result of the corresponding lines of code.\n\n::: {.callout-warning style=\"color: orange;\"}\n###### Preliminary Explanation of the Next Graph\n\nThe grid approximation used for the Bayesian updating in the next graph\nare explained later in this chapter.#\n\nIn the meanwhile I called the code from the following code chunk line by\nline and inspected the result to understand what it does.\n:::\n\n::: {.callout-note style=\"color: blue;\"}\n###### Changed parameter name\n\nIn the following listing I had to change in the `lag()` function the\nparameter `k` of the Kurz'sche version to `default` as it is described\nin the corresponding [help\nfile](https://dplyr.tidyverse.org/reference/lead-lag.html). I don't\nunderstand why `k`\\`was used. Maybe `k` was the name of the parameter of\na previous {**dplyr**} version?\n:::\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: fig-bayesian-model-learning\n#| fig-cap: \"How a Bayesian model learns\"\n\n# starting data\nd <- tibble(toss = c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\")) |> \n    mutate(n_trials  = 1:9, n_success = cumsum(toss == \"w\"))\n\nsequence_length <- 50 # <1>\n\nd %>% \n  expand_grid(p_water = seq(from = 0, to = 1, \n                            length.out = sequence_length)) %>% # <1>\n  group_by(p_water) %>% # <2>\n  mutate(lagged_n_trials  = lag(n_trials, default = 1),\n         lagged_n_success = lag(n_success, default = 1)) %>% # <3>\n  ungroup() %>% # <4>\n  mutate(prior      = ifelse(n_trials == 1, .5,\n                             dbinom(x    = lagged_n_success, \n                                    size = lagged_n_trials, \n                                    prob = p_water)),\n         likelihood = dbinom(x    = n_success, \n                             size = n_trials, \n                             prob = p_water),\n         strip      = str_c(\"n = \", n_trials)) %>% # <5>\n  \n  # normalize prior and likelihood \n  group_by(n_trials) %>% # <6>\n  mutate(prior      = prior / sum(prior),\n         likelihood = likelihood / sum(likelihood)) %>% # <6>\n  \n  # plot the result\n  ggplot(aes(x = p_water)) + # <7>\n  geom_line(aes(y = prior), \n            linetype = 2) + # <7>\n  geom_line(aes(y = likelihood)) + # <7>\n  scale_x_continuous(\"proportion water\", breaks = c(0, .5, 1)) + # <7>\n  scale_y_continuous(\"plausibility\", breaks = NULL) + # <7>\n  theme(panel.grid = element_blank()) + # <7>\n  facet_wrap(~ strip, scales = \"free_y\") # <7>\n```\n````\n\n::: {.cell-output-display}\n![How a Bayesian model learns](02-small-and-large-worlds_files/figure-pdf/fig-bayesian-model-learning-1.pdf){#fig-bayesian-model-learning fig-pos='H'}\n:::\n:::\n\n\n\n1.  Creates a tibble from all combinations of inputs.\n2.  Group data by the parameter `p-water`.\n3.  Create two columns filled with the value of the previous row using\n    the `lag()` function.\n4.  Restore the original ungrouped data structure.\n    @sec-annotation-4-ungroup.\n5.  Calculate Prior and Likelihood and create a column for each\n    parameter. @sec-annotation-5-dbinom.\n6.  Normalize Prior and Likelihood to put both of them in a probability\n    metric. @sec-annotation-6-normalize.\n7.  Plot the result. @sec-annotation-7-ggplot.\n\n##### Annotation (1): `tidyr::expand_grid()` {#sec-annotation-1-expand-grid}\n\n`tidyr::expand_grid()` creates a tibble from all combinations of inputs.\nInput are generalized vectors in contrast to `expand()` that generates\nall combination of variables as well but needs as input a dataset. The\nrange between 0 and 1 is divided into 50 part and then it generates all\ncombinations by varying all columns from left to right. The first column\nis the slowest, the second is faster and so on.). It generates 450 data\npoints (50 \\* 9 trials).\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: bayesian-model-learning-anno1\n\n# starting data\ntbl <- tibble(toss = c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\")) |> \n    mutate(n_trials  = 1:9, n_success = cumsum(toss == \"w\"))\n\n# start of bayesian modeling\n\n### add code lines of annotation <1> #########################################\nsequence_length <- 50\n\ntbl %>% \n  expand_grid(p_water = seq(from = 0, to = 1, \n                            length.out = sequence_length))\n```\n````\n\n```\n#> # A tibble: 450 x 4\n#>    toss  n_trials n_success p_water\n#>    <chr>    <int>     <int>   <dbl>\n#>  1 w            1         1  0     \n#>  2 w            1         1  0.0204\n#>  3 w            1         1  0.0408\n#>  4 w            1         1  0.0612\n#>  5 w            1         1  0.0816\n#>  6 w            1         1  0.102 \n#>  7 w            1         1  0.122 \n#>  8 w            1         1  0.143 \n#>  9 w            1         1  0.163 \n#> 10 w            1         1  0.184 \n#> # i 440 more rows\n```\n:::\n\n\n\n##### Annotation (2): `dplyr::group_by()` {#sec-annotation-2-group_by}\n\nAt first I did not understand the line `group_by(p_water)`. Why has the\ndata to be grouped when every row has a different value --- as I have\nthought from a cursory inspection of the result? But it turned out that\nafter 100 records the parameter `p_water` is repeating its value.\n\n::: {.callout-tip style=\"color: green;\"}\n## Special table format\n\nFor a better comparison (and for a later cross reference I will append\nto the next code chunk a new column ID with line number and a table\nformat where one can scroll the long table with it 450 rows.\n:::\n\n\n\n::: {#tbl-bayesian-model-learning-anno2-1 .cell tbl-cap='Bayesian Updating: Lagged with groupping'}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: tbl-bayesian-model-learning-anno2-1\n#| tbl-cap: \"Bayesian Updating: Lagged with groupping\"\n\n# starting data\ntbl1 <- tibble(toss = c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\")) |>\n    mutate(n_trials  = 1:9, n_success = cumsum(toss == \"w\"))\n\n# start of bayesian modeling with grouping (as in the original)\nsequence_length <- 50\n\ntbl1 <- tbl %>% \n  expand_grid(p_water = seq(from = 0, to = 1, \n                            length.out = sequence_length)) %>% \n    \n  #### add code lines to show effect of annoation <2> ######################\n  group_by(p_water) %>% \n  mutate(lagged_n_trials  = lag(n_trials, default = 1),\n         lagged_n_success = lag(n_success, default = 1)) %>% \n  ungroup() |> \n    \n\n  # add new column ID with row numbers \n  # and relocate it to be the first column\n  mutate(ID = row_number()) |> \n  relocate(ID, .before = toss) |> \n\n # provide a different format and a scroll box for the table\n  kableExtra::kbl() %>%\n  kableExtra::kable_classic() %>%\n  kableExtra::scroll_box(height = \"600px\")\ntbl1\n```\n````\n\n::: {.cell-output-display}\n\\begin{table}\n\\centering\n\\begin{tabular}[t]{r|l|r|r|r|r|r}\n\\hline\nID & toss & n\\_trials & n\\_success & p\\_water & lagged\\_n\\_trials & lagged\\_n\\_success\\\\\n\\hline\n1 & w & 1 & 1 & 0.0000000 & 1 & 1\\\\\n\\hline\n2 & w & 1 & 1 & 0.0204082 & 1 & 1\\\\\n\\hline\n3 & w & 1 & 1 & 0.0408163 & 1 & 1\\\\\n\\hline\n4 & w & 1 & 1 & 0.0612245 & 1 & 1\\\\\n\\hline\n5 & w & 1 & 1 & 0.0816327 & 1 & 1\\\\\n\\hline\n6 & w & 1 & 1 & 0.1020408 & 1 & 1\\\\\n\\hline\n7 & w & 1 & 1 & 0.1224490 & 1 & 1\\\\\n\\hline\n8 & w & 1 & 1 & 0.1428571 & 1 & 1\\\\\n\\hline\n9 & w & 1 & 1 & 0.1632653 & 1 & 1\\\\\n\\hline\n10 & w & 1 & 1 & 0.1836735 & 1 & 1\\\\\n\\hline\n11 & w & 1 & 1 & 0.2040816 & 1 & 1\\\\\n\\hline\n12 & w & 1 & 1 & 0.2244898 & 1 & 1\\\\\n\\hline\n13 & w & 1 & 1 & 0.2448980 & 1 & 1\\\\\n\\hline\n14 & w & 1 & 1 & 0.2653061 & 1 & 1\\\\\n\\hline\n15 & w & 1 & 1 & 0.2857143 & 1 & 1\\\\\n\\hline\n16 & w & 1 & 1 & 0.3061224 & 1 & 1\\\\\n\\hline\n17 & w & 1 & 1 & 0.3265306 & 1 & 1\\\\\n\\hline\n18 & w & 1 & 1 & 0.3469388 & 1 & 1\\\\\n\\hline\n19 & w & 1 & 1 & 0.3673469 & 1 & 1\\\\\n\\hline\n20 & w & 1 & 1 & 0.3877551 & 1 & 1\\\\\n\\hline\n21 & w & 1 & 1 & 0.4081633 & 1 & 1\\\\\n\\hline\n22 & w & 1 & 1 & 0.4285714 & 1 & 1\\\\\n\\hline\n23 & w & 1 & 1 & 0.4489796 & 1 & 1\\\\\n\\hline\n24 & w & 1 & 1 & 0.4693878 & 1 & 1\\\\\n\\hline\n25 & w & 1 & 1 & 0.4897959 & 1 & 1\\\\\n\\hline\n26 & w & 1 & 1 & 0.5102041 & 1 & 1\\\\\n\\hline\n27 & w & 1 & 1 & 0.5306122 & 1 & 1\\\\\n\\hline\n28 & w & 1 & 1 & 0.5510204 & 1 & 1\\\\\n\\hline\n29 & w & 1 & 1 & 0.5714286 & 1 & 1\\\\\n\\hline\n30 & w & 1 & 1 & 0.5918367 & 1 & 1\\\\\n\\hline\n31 & w & 1 & 1 & 0.6122449 & 1 & 1\\\\\n\\hline\n32 & w & 1 & 1 & 0.6326531 & 1 & 1\\\\\n\\hline\n33 & w & 1 & 1 & 0.6530612 & 1 & 1\\\\\n\\hline\n34 & w & 1 & 1 & 0.6734694 & 1 & 1\\\\\n\\hline\n35 & w & 1 & 1 & 0.6938776 & 1 & 1\\\\\n\\hline\n36 & w & 1 & 1 & 0.7142857 & 1 & 1\\\\\n\\hline\n37 & w & 1 & 1 & 0.7346939 & 1 & 1\\\\\n\\hline\n38 & w & 1 & 1 & 0.7551020 & 1 & 1\\\\\n\\hline\n39 & w & 1 & 1 & 0.7755102 & 1 & 1\\\\\n\\hline\n40 & w & 1 & 1 & 0.7959184 & 1 & 1\\\\\n\\hline\n41 & w & 1 & 1 & 0.8163265 & 1 & 1\\\\\n\\hline\n42 & w & 1 & 1 & 0.8367347 & 1 & 1\\\\\n\\hline\n43 & w & 1 & 1 & 0.8571429 & 1 & 1\\\\\n\\hline\n44 & w & 1 & 1 & 0.8775510 & 1 & 1\\\\\n\\hline\n45 & w & 1 & 1 & 0.8979592 & 1 & 1\\\\\n\\hline\n46 & w & 1 & 1 & 0.9183673 & 1 & 1\\\\\n\\hline\n47 & w & 1 & 1 & 0.9387755 & 1 & 1\\\\\n\\hline\n48 & w & 1 & 1 & 0.9591837 & 1 & 1\\\\\n\\hline\n49 & w & 1 & 1 & 0.9795918 & 1 & 1\\\\\n\\hline\n50 & w & 1 & 1 & 1.0000000 & 1 & 1\\\\\n\\hline\n51 & l & 2 & 1 & 0.0000000 & 1 & 1\\\\\n\\hline\n52 & l & 2 & 1 & 0.0204082 & 1 & 1\\\\\n\\hline\n53 & l & 2 & 1 & 0.0408163 & 1 & 1\\\\\n\\hline\n54 & l & 2 & 1 & 0.0612245 & 1 & 1\\\\\n\\hline\n55 & l & 2 & 1 & 0.0816327 & 1 & 1\\\\\n\\hline\n56 & l & 2 & 1 & 0.1020408 & 1 & 1\\\\\n\\hline\n57 & l & 2 & 1 & 0.1224490 & 1 & 1\\\\\n\\hline\n58 & l & 2 & 1 & 0.1428571 & 1 & 1\\\\\n\\hline\n59 & l & 2 & 1 & 0.1632653 & 1 & 1\\\\\n\\hline\n60 & l & 2 & 1 & 0.1836735 & 1 & 1\\\\\n\\hline\n61 & l & 2 & 1 & 0.2040816 & 1 & 1\\\\\n\\hline\n62 & l & 2 & 1 & 0.2244898 & 1 & 1\\\\\n\\hline\n63 & l & 2 & 1 & 0.2448980 & 1 & 1\\\\\n\\hline\n64 & l & 2 & 1 & 0.2653061 & 1 & 1\\\\\n\\hline\n65 & l & 2 & 1 & 0.2857143 & 1 & 1\\\\\n\\hline\n66 & l & 2 & 1 & 0.3061224 & 1 & 1\\\\\n\\hline\n67 & l & 2 & 1 & 0.3265306 & 1 & 1\\\\\n\\hline\n68 & l & 2 & 1 & 0.3469388 & 1 & 1\\\\\n\\hline\n69 & l & 2 & 1 & 0.3673469 & 1 & 1\\\\\n\\hline\n70 & l & 2 & 1 & 0.3877551 & 1 & 1\\\\\n\\hline\n71 & l & 2 & 1 & 0.4081633 & 1 & 1\\\\\n\\hline\n72 & l & 2 & 1 & 0.4285714 & 1 & 1\\\\\n\\hline\n73 & l & 2 & 1 & 0.4489796 & 1 & 1\\\\\n\\hline\n74 & l & 2 & 1 & 0.4693878 & 1 & 1\\\\\n\\hline\n75 & l & 2 & 1 & 0.4897959 & 1 & 1\\\\\n\\hline\n76 & l & 2 & 1 & 0.5102041 & 1 & 1\\\\\n\\hline\n77 & l & 2 & 1 & 0.5306122 & 1 & 1\\\\\n\\hline\n78 & l & 2 & 1 & 0.5510204 & 1 & 1\\\\\n\\hline\n79 & l & 2 & 1 & 0.5714286 & 1 & 1\\\\\n\\hline\n80 & l & 2 & 1 & 0.5918367 & 1 & 1\\\\\n\\hline\n81 & l & 2 & 1 & 0.6122449 & 1 & 1\\\\\n\\hline\n82 & l & 2 & 1 & 0.6326531 & 1 & 1\\\\\n\\hline\n83 & l & 2 & 1 & 0.6530612 & 1 & 1\\\\\n\\hline\n84 & l & 2 & 1 & 0.6734694 & 1 & 1\\\\\n\\hline\n85 & l & 2 & 1 & 0.6938776 & 1 & 1\\\\\n\\hline\n86 & l & 2 & 1 & 0.7142857 & 1 & 1\\\\\n\\hline\n87 & l & 2 & 1 & 0.7346939 & 1 & 1\\\\\n\\hline\n88 & l & 2 & 1 & 0.7551020 & 1 & 1\\\\\n\\hline\n89 & l & 2 & 1 & 0.7755102 & 1 & 1\\\\\n\\hline\n90 & l & 2 & 1 & 0.7959184 & 1 & 1\\\\\n\\hline\n91 & l & 2 & 1 & 0.8163265 & 1 & 1\\\\\n\\hline\n92 & l & 2 & 1 & 0.8367347 & 1 & 1\\\\\n\\hline\n93 & l & 2 & 1 & 0.8571429 & 1 & 1\\\\\n\\hline\n94 & l & 2 & 1 & 0.8775510 & 1 & 1\\\\\n\\hline\n95 & l & 2 & 1 & 0.8979592 & 1 & 1\\\\\n\\hline\n96 & l & 2 & 1 & 0.9183673 & 1 & 1\\\\\n\\hline\n97 & l & 2 & 1 & 0.9387755 & 1 & 1\\\\\n\\hline\n98 & l & 2 & 1 & 0.9591837 & 1 & 1\\\\\n\\hline\n99 & l & 2 & 1 & 0.9795918 & 1 & 1\\\\\n\\hline\n100 & l & 2 & 1 & 1.0000000 & 1 & 1\\\\\n\\hline\n101 & w & 3 & 2 & 0.0000000 & 2 & 1\\\\\n\\hline\n102 & w & 3 & 2 & 0.0204082 & 2 & 1\\\\\n\\hline\n103 & w & 3 & 2 & 0.0408163 & 2 & 1\\\\\n\\hline\n104 & w & 3 & 2 & 0.0612245 & 2 & 1\\\\\n\\hline\n105 & w & 3 & 2 & 0.0816327 & 2 & 1\\\\\n\\hline\n106 & w & 3 & 2 & 0.1020408 & 2 & 1\\\\\n\\hline\n107 & w & 3 & 2 & 0.1224490 & 2 & 1\\\\\n\\hline\n108 & w & 3 & 2 & 0.1428571 & 2 & 1\\\\\n\\hline\n109 & w & 3 & 2 & 0.1632653 & 2 & 1\\\\\n\\hline\n110 & w & 3 & 2 & 0.1836735 & 2 & 1\\\\\n\\hline\n111 & w & 3 & 2 & 0.2040816 & 2 & 1\\\\\n\\hline\n112 & w & 3 & 2 & 0.2244898 & 2 & 1\\\\\n\\hline\n113 & w & 3 & 2 & 0.2448980 & 2 & 1\\\\\n\\hline\n114 & w & 3 & 2 & 0.2653061 & 2 & 1\\\\\n\\hline\n115 & w & 3 & 2 & 0.2857143 & 2 & 1\\\\\n\\hline\n116 & w & 3 & 2 & 0.3061224 & 2 & 1\\\\\n\\hline\n117 & w & 3 & 2 & 0.3265306 & 2 & 1\\\\\n\\hline\n118 & w & 3 & 2 & 0.3469388 & 2 & 1\\\\\n\\hline\n119 & w & 3 & 2 & 0.3673469 & 2 & 1\\\\\n\\hline\n120 & w & 3 & 2 & 0.3877551 & 2 & 1\\\\\n\\hline\n121 & w & 3 & 2 & 0.4081633 & 2 & 1\\\\\n\\hline\n122 & w & 3 & 2 & 0.4285714 & 2 & 1\\\\\n\\hline\n123 & w & 3 & 2 & 0.4489796 & 2 & 1\\\\\n\\hline\n124 & w & 3 & 2 & 0.4693878 & 2 & 1\\\\\n\\hline\n125 & w & 3 & 2 & 0.4897959 & 2 & 1\\\\\n\\hline\n126 & w & 3 & 2 & 0.5102041 & 2 & 1\\\\\n\\hline\n127 & w & 3 & 2 & 0.5306122 & 2 & 1\\\\\n\\hline\n128 & w & 3 & 2 & 0.5510204 & 2 & 1\\\\\n\\hline\n129 & w & 3 & 2 & 0.5714286 & 2 & 1\\\\\n\\hline\n130 & w & 3 & 2 & 0.5918367 & 2 & 1\\\\\n\\hline\n131 & w & 3 & 2 & 0.6122449 & 2 & 1\\\\\n\\hline\n132 & w & 3 & 2 & 0.6326531 & 2 & 1\\\\\n\\hline\n133 & w & 3 & 2 & 0.6530612 & 2 & 1\\\\\n\\hline\n134 & w & 3 & 2 & 0.6734694 & 2 & 1\\\\\n\\hline\n135 & w & 3 & 2 & 0.6938776 & 2 & 1\\\\\n\\hline\n136 & w & 3 & 2 & 0.7142857 & 2 & 1\\\\\n\\hline\n137 & w & 3 & 2 & 0.7346939 & 2 & 1\\\\\n\\hline\n138 & w & 3 & 2 & 0.7551020 & 2 & 1\\\\\n\\hline\n139 & w & 3 & 2 & 0.7755102 & 2 & 1\\\\\n\\hline\n140 & w & 3 & 2 & 0.7959184 & 2 & 1\\\\\n\\hline\n141 & w & 3 & 2 & 0.8163265 & 2 & 1\\\\\n\\hline\n142 & w & 3 & 2 & 0.8367347 & 2 & 1\\\\\n\\hline\n143 & w & 3 & 2 & 0.8571429 & 2 & 1\\\\\n\\hline\n144 & w & 3 & 2 & 0.8775510 & 2 & 1\\\\\n\\hline\n145 & w & 3 & 2 & 0.8979592 & 2 & 1\\\\\n\\hline\n146 & w & 3 & 2 & 0.9183673 & 2 & 1\\\\\n\\hline\n147 & w & 3 & 2 & 0.9387755 & 2 & 1\\\\\n\\hline\n148 & w & 3 & 2 & 0.9591837 & 2 & 1\\\\\n\\hline\n149 & w & 3 & 2 & 0.9795918 & 2 & 1\\\\\n\\hline\n150 & w & 3 & 2 & 1.0000000 & 2 & 1\\\\\n\\hline\n151 & w & 4 & 3 & 0.0000000 & 3 & 2\\\\\n\\hline\n152 & w & 4 & 3 & 0.0204082 & 3 & 2\\\\\n\\hline\n153 & w & 4 & 3 & 0.0408163 & 3 & 2\\\\\n\\hline\n154 & w & 4 & 3 & 0.0612245 & 3 & 2\\\\\n\\hline\n155 & w & 4 & 3 & 0.0816327 & 3 & 2\\\\\n\\hline\n156 & w & 4 & 3 & 0.1020408 & 3 & 2\\\\\n\\hline\n157 & w & 4 & 3 & 0.1224490 & 3 & 2\\\\\n\\hline\n158 & w & 4 & 3 & 0.1428571 & 3 & 2\\\\\n\\hline\n159 & w & 4 & 3 & 0.1632653 & 3 & 2\\\\\n\\hline\n160 & w & 4 & 3 & 0.1836735 & 3 & 2\\\\\n\\hline\n161 & w & 4 & 3 & 0.2040816 & 3 & 2\\\\\n\\hline\n162 & w & 4 & 3 & 0.2244898 & 3 & 2\\\\\n\\hline\n163 & w & 4 & 3 & 0.2448980 & 3 & 2\\\\\n\\hline\n164 & w & 4 & 3 & 0.2653061 & 3 & 2\\\\\n\\hline\n165 & w & 4 & 3 & 0.2857143 & 3 & 2\\\\\n\\hline\n166 & w & 4 & 3 & 0.3061224 & 3 & 2\\\\\n\\hline\n167 & w & 4 & 3 & 0.3265306 & 3 & 2\\\\\n\\hline\n168 & w & 4 & 3 & 0.3469388 & 3 & 2\\\\\n\\hline\n169 & w & 4 & 3 & 0.3673469 & 3 & 2\\\\\n\\hline\n170 & w & 4 & 3 & 0.3877551 & 3 & 2\\\\\n\\hline\n171 & w & 4 & 3 & 0.4081633 & 3 & 2\\\\\n\\hline\n172 & w & 4 & 3 & 0.4285714 & 3 & 2\\\\\n\\hline\n173 & w & 4 & 3 & 0.4489796 & 3 & 2\\\\\n\\hline\n174 & w & 4 & 3 & 0.4693878 & 3 & 2\\\\\n\\hline\n175 & w & 4 & 3 & 0.4897959 & 3 & 2\\\\\n\\hline\n176 & w & 4 & 3 & 0.5102041 & 3 & 2\\\\\n\\hline\n177 & w & 4 & 3 & 0.5306122 & 3 & 2\\\\\n\\hline\n178 & w & 4 & 3 & 0.5510204 & 3 & 2\\\\\n\\hline\n179 & w & 4 & 3 & 0.5714286 & 3 & 2\\\\\n\\hline\n180 & w & 4 & 3 & 0.5918367 & 3 & 2\\\\\n\\hline\n181 & w & 4 & 3 & 0.6122449 & 3 & 2\\\\\n\\hline\n182 & w & 4 & 3 & 0.6326531 & 3 & 2\\\\\n\\hline\n183 & w & 4 & 3 & 0.6530612 & 3 & 2\\\\\n\\hline\n184 & w & 4 & 3 & 0.6734694 & 3 & 2\\\\\n\\hline\n185 & w & 4 & 3 & 0.6938776 & 3 & 2\\\\\n\\hline\n186 & w & 4 & 3 & 0.7142857 & 3 & 2\\\\\n\\hline\n187 & w & 4 & 3 & 0.7346939 & 3 & 2\\\\\n\\hline\n188 & w & 4 & 3 & 0.7551020 & 3 & 2\\\\\n\\hline\n189 & w & 4 & 3 & 0.7755102 & 3 & 2\\\\\n\\hline\n190 & w & 4 & 3 & 0.7959184 & 3 & 2\\\\\n\\hline\n191 & w & 4 & 3 & 0.8163265 & 3 & 2\\\\\n\\hline\n192 & w & 4 & 3 & 0.8367347 & 3 & 2\\\\\n\\hline\n193 & w & 4 & 3 & 0.8571429 & 3 & 2\\\\\n\\hline\n194 & w & 4 & 3 & 0.8775510 & 3 & 2\\\\\n\\hline\n195 & w & 4 & 3 & 0.8979592 & 3 & 2\\\\\n\\hline\n196 & w & 4 & 3 & 0.9183673 & 3 & 2\\\\\n\\hline\n197 & w & 4 & 3 & 0.9387755 & 3 & 2\\\\\n\\hline\n198 & w & 4 & 3 & 0.9591837 & 3 & 2\\\\\n\\hline\n199 & w & 4 & 3 & 0.9795918 & 3 & 2\\\\\n\\hline\n200 & w & 4 & 3 & 1.0000000 & 3 & 2\\\\\n\\hline\n201 & w & 5 & 4 & 0.0000000 & 4 & 3\\\\\n\\hline\n202 & w & 5 & 4 & 0.0204082 & 4 & 3\\\\\n\\hline\n203 & w & 5 & 4 & 0.0408163 & 4 & 3\\\\\n\\hline\n204 & w & 5 & 4 & 0.0612245 & 4 & 3\\\\\n\\hline\n205 & w & 5 & 4 & 0.0816327 & 4 & 3\\\\\n\\hline\n206 & w & 5 & 4 & 0.1020408 & 4 & 3\\\\\n\\hline\n207 & w & 5 & 4 & 0.1224490 & 4 & 3\\\\\n\\hline\n208 & w & 5 & 4 & 0.1428571 & 4 & 3\\\\\n\\hline\n209 & w & 5 & 4 & 0.1632653 & 4 & 3\\\\\n\\hline\n210 & w & 5 & 4 & 0.1836735 & 4 & 3\\\\\n\\hline\n211 & w & 5 & 4 & 0.2040816 & 4 & 3\\\\\n\\hline\n212 & w & 5 & 4 & 0.2244898 & 4 & 3\\\\\n\\hline\n213 & w & 5 & 4 & 0.2448980 & 4 & 3\\\\\n\\hline\n214 & w & 5 & 4 & 0.2653061 & 4 & 3\\\\\n\\hline\n215 & w & 5 & 4 & 0.2857143 & 4 & 3\\\\\n\\hline\n216 & w & 5 & 4 & 0.3061224 & 4 & 3\\\\\n\\hline\n217 & w & 5 & 4 & 0.3265306 & 4 & 3\\\\\n\\hline\n218 & w & 5 & 4 & 0.3469388 & 4 & 3\\\\\n\\hline\n219 & w & 5 & 4 & 0.3673469 & 4 & 3\\\\\n\\hline\n220 & w & 5 & 4 & 0.3877551 & 4 & 3\\\\\n\\hline\n221 & w & 5 & 4 & 0.4081633 & 4 & 3\\\\\n\\hline\n222 & w & 5 & 4 & 0.4285714 & 4 & 3\\\\\n\\hline\n223 & w & 5 & 4 & 0.4489796 & 4 & 3\\\\\n\\hline\n224 & w & 5 & 4 & 0.4693878 & 4 & 3\\\\\n\\hline\n225 & w & 5 & 4 & 0.4897959 & 4 & 3\\\\\n\\hline\n226 & w & 5 & 4 & 0.5102041 & 4 & 3\\\\\n\\hline\n227 & w & 5 & 4 & 0.5306122 & 4 & 3\\\\\n\\hline\n228 & w & 5 & 4 & 0.5510204 & 4 & 3\\\\\n\\hline\n229 & w & 5 & 4 & 0.5714286 & 4 & 3\\\\\n\\hline\n230 & w & 5 & 4 & 0.5918367 & 4 & 3\\\\\n\\hline\n231 & w & 5 & 4 & 0.6122449 & 4 & 3\\\\\n\\hline\n232 & w & 5 & 4 & 0.6326531 & 4 & 3\\\\\n\\hline\n233 & w & 5 & 4 & 0.6530612 & 4 & 3\\\\\n\\hline\n234 & w & 5 & 4 & 0.6734694 & 4 & 3\\\\\n\\hline\n235 & w & 5 & 4 & 0.6938776 & 4 & 3\\\\\n\\hline\n236 & w & 5 & 4 & 0.7142857 & 4 & 3\\\\\n\\hline\n237 & w & 5 & 4 & 0.7346939 & 4 & 3\\\\\n\\hline\n238 & w & 5 & 4 & 0.7551020 & 4 & 3\\\\\n\\hline\n239 & w & 5 & 4 & 0.7755102 & 4 & 3\\\\\n\\hline\n240 & w & 5 & 4 & 0.7959184 & 4 & 3\\\\\n\\hline\n241 & w & 5 & 4 & 0.8163265 & 4 & 3\\\\\n\\hline\n242 & w & 5 & 4 & 0.8367347 & 4 & 3\\\\\n\\hline\n243 & w & 5 & 4 & 0.8571429 & 4 & 3\\\\\n\\hline\n244 & w & 5 & 4 & 0.8775510 & 4 & 3\\\\\n\\hline\n245 & w & 5 & 4 & 0.8979592 & 4 & 3\\\\\n\\hline\n246 & w & 5 & 4 & 0.9183673 & 4 & 3\\\\\n\\hline\n247 & w & 5 & 4 & 0.9387755 & 4 & 3\\\\\n\\hline\n248 & w & 5 & 4 & 0.9591837 & 4 & 3\\\\\n\\hline\n249 & w & 5 & 4 & 0.9795918 & 4 & 3\\\\\n\\hline\n250 & w & 5 & 4 & 1.0000000 & 4 & 3\\\\\n\\hline\n251 & l & 6 & 4 & 0.0000000 & 5 & 4\\\\\n\\hline\n252 & l & 6 & 4 & 0.0204082 & 5 & 4\\\\\n\\hline\n253 & l & 6 & 4 & 0.0408163 & 5 & 4\\\\\n\\hline\n254 & l & 6 & 4 & 0.0612245 & 5 & 4\\\\\n\\hline\n255 & l & 6 & 4 & 0.0816327 & 5 & 4\\\\\n\\hline\n256 & l & 6 & 4 & 0.1020408 & 5 & 4\\\\\n\\hline\n257 & l & 6 & 4 & 0.1224490 & 5 & 4\\\\\n\\hline\n258 & l & 6 & 4 & 0.1428571 & 5 & 4\\\\\n\\hline\n259 & l & 6 & 4 & 0.1632653 & 5 & 4\\\\\n\\hline\n260 & l & 6 & 4 & 0.1836735 & 5 & 4\\\\\n\\hline\n261 & l & 6 & 4 & 0.2040816 & 5 & 4\\\\\n\\hline\n262 & l & 6 & 4 & 0.2244898 & 5 & 4\\\\\n\\hline\n263 & l & 6 & 4 & 0.2448980 & 5 & 4\\\\\n\\hline\n264 & l & 6 & 4 & 0.2653061 & 5 & 4\\\\\n\\hline\n265 & l & 6 & 4 & 0.2857143 & 5 & 4\\\\\n\\hline\n266 & l & 6 & 4 & 0.3061224 & 5 & 4\\\\\n\\hline\n267 & l & 6 & 4 & 0.3265306 & 5 & 4\\\\\n\\hline\n268 & l & 6 & 4 & 0.3469388 & 5 & 4\\\\\n\\hline\n269 & l & 6 & 4 & 0.3673469 & 5 & 4\\\\\n\\hline\n270 & l & 6 & 4 & 0.3877551 & 5 & 4\\\\\n\\hline\n271 & l & 6 & 4 & 0.4081633 & 5 & 4\\\\\n\\hline\n272 & l & 6 & 4 & 0.4285714 & 5 & 4\\\\\n\\hline\n273 & l & 6 & 4 & 0.4489796 & 5 & 4\\\\\n\\hline\n274 & l & 6 & 4 & 0.4693878 & 5 & 4\\\\\n\\hline\n275 & l & 6 & 4 & 0.4897959 & 5 & 4\\\\\n\\hline\n276 & l & 6 & 4 & 0.5102041 & 5 & 4\\\\\n\\hline\n277 & l & 6 & 4 & 0.5306122 & 5 & 4\\\\\n\\hline\n278 & l & 6 & 4 & 0.5510204 & 5 & 4\\\\\n\\hline\n279 & l & 6 & 4 & 0.5714286 & 5 & 4\\\\\n\\hline\n280 & l & 6 & 4 & 0.5918367 & 5 & 4\\\\\n\\hline\n281 & l & 6 & 4 & 0.6122449 & 5 & 4\\\\\n\\hline\n282 & l & 6 & 4 & 0.6326531 & 5 & 4\\\\\n\\hline\n283 & l & 6 & 4 & 0.6530612 & 5 & 4\\\\\n\\hline\n284 & l & 6 & 4 & 0.6734694 & 5 & 4\\\\\n\\hline\n285 & l & 6 & 4 & 0.6938776 & 5 & 4\\\\\n\\hline\n286 & l & 6 & 4 & 0.7142857 & 5 & 4\\\\\n\\hline\n287 & l & 6 & 4 & 0.7346939 & 5 & 4\\\\\n\\hline\n288 & l & 6 & 4 & 0.7551020 & 5 & 4\\\\\n\\hline\n289 & l & 6 & 4 & 0.7755102 & 5 & 4\\\\\n\\hline\n290 & l & 6 & 4 & 0.7959184 & 5 & 4\\\\\n\\hline\n291 & l & 6 & 4 & 0.8163265 & 5 & 4\\\\\n\\hline\n292 & l & 6 & 4 & 0.8367347 & 5 & 4\\\\\n\\hline\n293 & l & 6 & 4 & 0.8571429 & 5 & 4\\\\\n\\hline\n294 & l & 6 & 4 & 0.8775510 & 5 & 4\\\\\n\\hline\n295 & l & 6 & 4 & 0.8979592 & 5 & 4\\\\\n\\hline\n296 & l & 6 & 4 & 0.9183673 & 5 & 4\\\\\n\\hline\n297 & l & 6 & 4 & 0.9387755 & 5 & 4\\\\\n\\hline\n298 & l & 6 & 4 & 0.9591837 & 5 & 4\\\\\n\\hline\n299 & l & 6 & 4 & 0.9795918 & 5 & 4\\\\\n\\hline\n300 & l & 6 & 4 & 1.0000000 & 5 & 4\\\\\n\\hline\n301 & w & 7 & 5 & 0.0000000 & 6 & 4\\\\\n\\hline\n302 & w & 7 & 5 & 0.0204082 & 6 & 4\\\\\n\\hline\n303 & w & 7 & 5 & 0.0408163 & 6 & 4\\\\\n\\hline\n304 & w & 7 & 5 & 0.0612245 & 6 & 4\\\\\n\\hline\n305 & w & 7 & 5 & 0.0816327 & 6 & 4\\\\\n\\hline\n306 & w & 7 & 5 & 0.1020408 & 6 & 4\\\\\n\\hline\n307 & w & 7 & 5 & 0.1224490 & 6 & 4\\\\\n\\hline\n308 & w & 7 & 5 & 0.1428571 & 6 & 4\\\\\n\\hline\n309 & w & 7 & 5 & 0.1632653 & 6 & 4\\\\\n\\hline\n310 & w & 7 & 5 & 0.1836735 & 6 & 4\\\\\n\\hline\n311 & w & 7 & 5 & 0.2040816 & 6 & 4\\\\\n\\hline\n312 & w & 7 & 5 & 0.2244898 & 6 & 4\\\\\n\\hline\n313 & w & 7 & 5 & 0.2448980 & 6 & 4\\\\\n\\hline\n314 & w & 7 & 5 & 0.2653061 & 6 & 4\\\\\n\\hline\n315 & w & 7 & 5 & 0.2857143 & 6 & 4\\\\\n\\hline\n316 & w & 7 & 5 & 0.3061224 & 6 & 4\\\\\n\\hline\n317 & w & 7 & 5 & 0.3265306 & 6 & 4\\\\\n\\hline\n318 & w & 7 & 5 & 0.3469388 & 6 & 4\\\\\n\\hline\n319 & w & 7 & 5 & 0.3673469 & 6 & 4\\\\\n\\hline\n320 & w & 7 & 5 & 0.3877551 & 6 & 4\\\\\n\\hline\n321 & w & 7 & 5 & 0.4081633 & 6 & 4\\\\\n\\hline\n322 & w & 7 & 5 & 0.4285714 & 6 & 4\\\\\n\\hline\n323 & w & 7 & 5 & 0.4489796 & 6 & 4\\\\\n\\hline\n324 & w & 7 & 5 & 0.4693878 & 6 & 4\\\\\n\\hline\n325 & w & 7 & 5 & 0.4897959 & 6 & 4\\\\\n\\hline\n326 & w & 7 & 5 & 0.5102041 & 6 & 4\\\\\n\\hline\n327 & w & 7 & 5 & 0.5306122 & 6 & 4\\\\\n\\hline\n328 & w & 7 & 5 & 0.5510204 & 6 & 4\\\\\n\\hline\n329 & w & 7 & 5 & 0.5714286 & 6 & 4\\\\\n\\hline\n330 & w & 7 & 5 & 0.5918367 & 6 & 4\\\\\n\\hline\n331 & w & 7 & 5 & 0.6122449 & 6 & 4\\\\\n\\hline\n332 & w & 7 & 5 & 0.6326531 & 6 & 4\\\\\n\\hline\n333 & w & 7 & 5 & 0.6530612 & 6 & 4\\\\\n\\hline\n334 & w & 7 & 5 & 0.6734694 & 6 & 4\\\\\n\\hline\n335 & w & 7 & 5 & 0.6938776 & 6 & 4\\\\\n\\hline\n336 & w & 7 & 5 & 0.7142857 & 6 & 4\\\\\n\\hline\n337 & w & 7 & 5 & 0.7346939 & 6 & 4\\\\\n\\hline\n338 & w & 7 & 5 & 0.7551020 & 6 & 4\\\\\n\\hline\n339 & w & 7 & 5 & 0.7755102 & 6 & 4\\\\\n\\hline\n340 & w & 7 & 5 & 0.7959184 & 6 & 4\\\\\n\\hline\n341 & w & 7 & 5 & 0.8163265 & 6 & 4\\\\\n\\hline\n342 & w & 7 & 5 & 0.8367347 & 6 & 4\\\\\n\\hline\n343 & w & 7 & 5 & 0.8571429 & 6 & 4\\\\\n\\hline\n344 & w & 7 & 5 & 0.8775510 & 6 & 4\\\\\n\\hline\n345 & w & 7 & 5 & 0.8979592 & 6 & 4\\\\\n\\hline\n346 & w & 7 & 5 & 0.9183673 & 6 & 4\\\\\n\\hline\n347 & w & 7 & 5 & 0.9387755 & 6 & 4\\\\\n\\hline\n348 & w & 7 & 5 & 0.9591837 & 6 & 4\\\\\n\\hline\n349 & w & 7 & 5 & 0.9795918 & 6 & 4\\\\\n\\hline\n350 & w & 7 & 5 & 1.0000000 & 6 & 4\\\\\n\\hline\n351 & l & 8 & 5 & 0.0000000 & 7 & 5\\\\\n\\hline\n352 & l & 8 & 5 & 0.0204082 & 7 & 5\\\\\n\\hline\n353 & l & 8 & 5 & 0.0408163 & 7 & 5\\\\\n\\hline\n354 & l & 8 & 5 & 0.0612245 & 7 & 5\\\\\n\\hline\n355 & l & 8 & 5 & 0.0816327 & 7 & 5\\\\\n\\hline\n356 & l & 8 & 5 & 0.1020408 & 7 & 5\\\\\n\\hline\n357 & l & 8 & 5 & 0.1224490 & 7 & 5\\\\\n\\hline\n358 & l & 8 & 5 & 0.1428571 & 7 & 5\\\\\n\\hline\n359 & l & 8 & 5 & 0.1632653 & 7 & 5\\\\\n\\hline\n360 & l & 8 & 5 & 0.1836735 & 7 & 5\\\\\n\\hline\n361 & l & 8 & 5 & 0.2040816 & 7 & 5\\\\\n\\hline\n362 & l & 8 & 5 & 0.2244898 & 7 & 5\\\\\n\\hline\n363 & l & 8 & 5 & 0.2448980 & 7 & 5\\\\\n\\hline\n364 & l & 8 & 5 & 0.2653061 & 7 & 5\\\\\n\\hline\n365 & l & 8 & 5 & 0.2857143 & 7 & 5\\\\\n\\hline\n366 & l & 8 & 5 & 0.3061224 & 7 & 5\\\\\n\\hline\n367 & l & 8 & 5 & 0.3265306 & 7 & 5\\\\\n\\hline\n368 & l & 8 & 5 & 0.3469388 & 7 & 5\\\\\n\\hline\n369 & l & 8 & 5 & 0.3673469 & 7 & 5\\\\\n\\hline\n370 & l & 8 & 5 & 0.3877551 & 7 & 5\\\\\n\\hline\n371 & l & 8 & 5 & 0.4081633 & 7 & 5\\\\\n\\hline\n372 & l & 8 & 5 & 0.4285714 & 7 & 5\\\\\n\\hline\n373 & l & 8 & 5 & 0.4489796 & 7 & 5\\\\\n\\hline\n374 & l & 8 & 5 & 0.4693878 & 7 & 5\\\\\n\\hline\n375 & l & 8 & 5 & 0.4897959 & 7 & 5\\\\\n\\hline\n376 & l & 8 & 5 & 0.5102041 & 7 & 5\\\\\n\\hline\n377 & l & 8 & 5 & 0.5306122 & 7 & 5\\\\\n\\hline\n378 & l & 8 & 5 & 0.5510204 & 7 & 5\\\\\n\\hline\n379 & l & 8 & 5 & 0.5714286 & 7 & 5\\\\\n\\hline\n380 & l & 8 & 5 & 0.5918367 & 7 & 5\\\\\n\\hline\n381 & l & 8 & 5 & 0.6122449 & 7 & 5\\\\\n\\hline\n382 & l & 8 & 5 & 0.6326531 & 7 & 5\\\\\n\\hline\n383 & l & 8 & 5 & 0.6530612 & 7 & 5\\\\\n\\hline\n384 & l & 8 & 5 & 0.6734694 & 7 & 5\\\\\n\\hline\n385 & l & 8 & 5 & 0.6938776 & 7 & 5\\\\\n\\hline\n386 & l & 8 & 5 & 0.7142857 & 7 & 5\\\\\n\\hline\n387 & l & 8 & 5 & 0.7346939 & 7 & 5\\\\\n\\hline\n388 & l & 8 & 5 & 0.7551020 & 7 & 5\\\\\n\\hline\n389 & l & 8 & 5 & 0.7755102 & 7 & 5\\\\\n\\hline\n390 & l & 8 & 5 & 0.7959184 & 7 & 5\\\\\n\\hline\n391 & l & 8 & 5 & 0.8163265 & 7 & 5\\\\\n\\hline\n392 & l & 8 & 5 & 0.8367347 & 7 & 5\\\\\n\\hline\n393 & l & 8 & 5 & 0.8571429 & 7 & 5\\\\\n\\hline\n394 & l & 8 & 5 & 0.8775510 & 7 & 5\\\\\n\\hline\n395 & l & 8 & 5 & 0.8979592 & 7 & 5\\\\\n\\hline\n396 & l & 8 & 5 & 0.9183673 & 7 & 5\\\\\n\\hline\n397 & l & 8 & 5 & 0.9387755 & 7 & 5\\\\\n\\hline\n398 & l & 8 & 5 & 0.9591837 & 7 & 5\\\\\n\\hline\n399 & l & 8 & 5 & 0.9795918 & 7 & 5\\\\\n\\hline\n400 & l & 8 & 5 & 1.0000000 & 7 & 5\\\\\n\\hline\n401 & w & 9 & 6 & 0.0000000 & 8 & 5\\\\\n\\hline\n402 & w & 9 & 6 & 0.0204082 & 8 & 5\\\\\n\\hline\n403 & w & 9 & 6 & 0.0408163 & 8 & 5\\\\\n\\hline\n404 & w & 9 & 6 & 0.0612245 & 8 & 5\\\\\n\\hline\n405 & w & 9 & 6 & 0.0816327 & 8 & 5\\\\\n\\hline\n406 & w & 9 & 6 & 0.1020408 & 8 & 5\\\\\n\\hline\n407 & w & 9 & 6 & 0.1224490 & 8 & 5\\\\\n\\hline\n408 & w & 9 & 6 & 0.1428571 & 8 & 5\\\\\n\\hline\n409 & w & 9 & 6 & 0.1632653 & 8 & 5\\\\\n\\hline\n410 & w & 9 & 6 & 0.1836735 & 8 & 5\\\\\n\\hline\n411 & w & 9 & 6 & 0.2040816 & 8 & 5\\\\\n\\hline\n412 & w & 9 & 6 & 0.2244898 & 8 & 5\\\\\n\\hline\n413 & w & 9 & 6 & 0.2448980 & 8 & 5\\\\\n\\hline\n414 & w & 9 & 6 & 0.2653061 & 8 & 5\\\\\n\\hline\n415 & w & 9 & 6 & 0.2857143 & 8 & 5\\\\\n\\hline\n416 & w & 9 & 6 & 0.3061224 & 8 & 5\\\\\n\\hline\n417 & w & 9 & 6 & 0.3265306 & 8 & 5\\\\\n\\hline\n418 & w & 9 & 6 & 0.3469388 & 8 & 5\\\\\n\\hline\n419 & w & 9 & 6 & 0.3673469 & 8 & 5\\\\\n\\hline\n420 & w & 9 & 6 & 0.3877551 & 8 & 5\\\\\n\\hline\n421 & w & 9 & 6 & 0.4081633 & 8 & 5\\\\\n\\hline\n422 & w & 9 & 6 & 0.4285714 & 8 & 5\\\\\n\\hline\n423 & w & 9 & 6 & 0.4489796 & 8 & 5\\\\\n\\hline\n424 & w & 9 & 6 & 0.4693878 & 8 & 5\\\\\n\\hline\n425 & w & 9 & 6 & 0.4897959 & 8 & 5\\\\\n\\hline\n426 & w & 9 & 6 & 0.5102041 & 8 & 5\\\\\n\\hline\n427 & w & 9 & 6 & 0.5306122 & 8 & 5\\\\\n\\hline\n428 & w & 9 & 6 & 0.5510204 & 8 & 5\\\\\n\\hline\n429 & w & 9 & 6 & 0.5714286 & 8 & 5\\\\\n\\hline\n430 & w & 9 & 6 & 0.5918367 & 8 & 5\\\\\n\\hline\n431 & w & 9 & 6 & 0.6122449 & 8 & 5\\\\\n\\hline\n432 & w & 9 & 6 & 0.6326531 & 8 & 5\\\\\n\\hline\n433 & w & 9 & 6 & 0.6530612 & 8 & 5\\\\\n\\hline\n434 & w & 9 & 6 & 0.6734694 & 8 & 5\\\\\n\\hline\n435 & w & 9 & 6 & 0.6938776 & 8 & 5\\\\\n\\hline\n436 & w & 9 & 6 & 0.7142857 & 8 & 5\\\\\n\\hline\n437 & w & 9 & 6 & 0.7346939 & 8 & 5\\\\\n\\hline\n438 & w & 9 & 6 & 0.7551020 & 8 & 5\\\\\n\\hline\n439 & w & 9 & 6 & 0.7755102 & 8 & 5\\\\\n\\hline\n440 & w & 9 & 6 & 0.7959184 & 8 & 5\\\\\n\\hline\n441 & w & 9 & 6 & 0.8163265 & 8 & 5\\\\\n\\hline\n442 & w & 9 & 6 & 0.8367347 & 8 & 5\\\\\n\\hline\n443 & w & 9 & 6 & 0.8571429 & 8 & 5\\\\\n\\hline\n444 & w & 9 & 6 & 0.8775510 & 8 & 5\\\\\n\\hline\n445 & w & 9 & 6 & 0.8979592 & 8 & 5\\\\\n\\hline\n446 & w & 9 & 6 & 0.9183673 & 8 & 5\\\\\n\\hline\n447 & w & 9 & 6 & 0.9387755 & 8 & 5\\\\\n\\hline\n448 & w & 9 & 6 & 0.9591837 & 8 & 5\\\\\n\\hline\n449 & w & 9 & 6 & 0.9795918 & 8 & 5\\\\\n\\hline\n450 & w & 9 & 6 & 1.0000000 & 8 & 5\\\\\n\\hline\n\\end{tabular}\n\\end{table}\n:::\n:::\n\n\n\nI want to see the differences in detail. So I will provide also the\nungrouped version.\n\n\n\n::: {#tbl-bayesian-model-learning-anno2-2 .cell tbl-cap='Bayesian Updating: Lagged without groupping (FALSE)'}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: tbl-bayesian-model-learning-anno2-2\n#| tbl-cap: \"Bayesian Updating: Lagged without groupping (FALSE)\"\n\n# starting data\ntbl <- tibble(toss = c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\")) |> \n    mutate(n_trials  = 1:9, n_success = cumsum(toss == \"w\"))\n\n# start of bayesian modeling without grouping\nsequence_length <- 50\n\ntbl2 <- tbl %>% \n  expand_grid(p_water = seq(from = 0, to = 1, \n                            length.out = sequence_length)) |> \n    \n  ### add code lines to show effect without annotation <2> ##################\n  mutate(lagged_n_trials  = lag(n_trials, default = 1),\n         lagged_n_success = lag(n_success, default = 1)) |> \n  \n  # add new column ID with row numbers and relocate it as first column\n  mutate(ID = row_number()) |> \n  relocate(ID, .before = toss) |> \n\n  # provide a different format and a scroll box for the table\n  kableExtra::kbl() %>%\n  kableExtra::kable_classic() %>%\n  kableExtra::scroll_box(height = \"600px\")\ntbl2\n```\n````\n\n::: {.cell-output-display}\n\\begin{table}\n\\centering\n\\begin{tabular}[t]{r|l|r|r|r|r|r}\n\\hline\nID & toss & n\\_trials & n\\_success & p\\_water & lagged\\_n\\_trials & lagged\\_n\\_success\\\\\n\\hline\n1 & w & 1 & 1 & 0.0000000 & 1 & 1\\\\\n\\hline\n2 & w & 1 & 1 & 0.0204082 & 1 & 1\\\\\n\\hline\n3 & w & 1 & 1 & 0.0408163 & 1 & 1\\\\\n\\hline\n4 & w & 1 & 1 & 0.0612245 & 1 & 1\\\\\n\\hline\n5 & w & 1 & 1 & 0.0816327 & 1 & 1\\\\\n\\hline\n6 & w & 1 & 1 & 0.1020408 & 1 & 1\\\\\n\\hline\n7 & w & 1 & 1 & 0.1224490 & 1 & 1\\\\\n\\hline\n8 & w & 1 & 1 & 0.1428571 & 1 & 1\\\\\n\\hline\n9 & w & 1 & 1 & 0.1632653 & 1 & 1\\\\\n\\hline\n10 & w & 1 & 1 & 0.1836735 & 1 & 1\\\\\n\\hline\n11 & w & 1 & 1 & 0.2040816 & 1 & 1\\\\\n\\hline\n12 & w & 1 & 1 & 0.2244898 & 1 & 1\\\\\n\\hline\n13 & w & 1 & 1 & 0.2448980 & 1 & 1\\\\\n\\hline\n14 & w & 1 & 1 & 0.2653061 & 1 & 1\\\\\n\\hline\n15 & w & 1 & 1 & 0.2857143 & 1 & 1\\\\\n\\hline\n16 & w & 1 & 1 & 0.3061224 & 1 & 1\\\\\n\\hline\n17 & w & 1 & 1 & 0.3265306 & 1 & 1\\\\\n\\hline\n18 & w & 1 & 1 & 0.3469388 & 1 & 1\\\\\n\\hline\n19 & w & 1 & 1 & 0.3673469 & 1 & 1\\\\\n\\hline\n20 & w & 1 & 1 & 0.3877551 & 1 & 1\\\\\n\\hline\n21 & w & 1 & 1 & 0.4081633 & 1 & 1\\\\\n\\hline\n22 & w & 1 & 1 & 0.4285714 & 1 & 1\\\\\n\\hline\n23 & w & 1 & 1 & 0.4489796 & 1 & 1\\\\\n\\hline\n24 & w & 1 & 1 & 0.4693878 & 1 & 1\\\\\n\\hline\n25 & w & 1 & 1 & 0.4897959 & 1 & 1\\\\\n\\hline\n26 & w & 1 & 1 & 0.5102041 & 1 & 1\\\\\n\\hline\n27 & w & 1 & 1 & 0.5306122 & 1 & 1\\\\\n\\hline\n28 & w & 1 & 1 & 0.5510204 & 1 & 1\\\\\n\\hline\n29 & w & 1 & 1 & 0.5714286 & 1 & 1\\\\\n\\hline\n30 & w & 1 & 1 & 0.5918367 & 1 & 1\\\\\n\\hline\n31 & w & 1 & 1 & 0.6122449 & 1 & 1\\\\\n\\hline\n32 & w & 1 & 1 & 0.6326531 & 1 & 1\\\\\n\\hline\n33 & w & 1 & 1 & 0.6530612 & 1 & 1\\\\\n\\hline\n34 & w & 1 & 1 & 0.6734694 & 1 & 1\\\\\n\\hline\n35 & w & 1 & 1 & 0.6938776 & 1 & 1\\\\\n\\hline\n36 & w & 1 & 1 & 0.7142857 & 1 & 1\\\\\n\\hline\n37 & w & 1 & 1 & 0.7346939 & 1 & 1\\\\\n\\hline\n38 & w & 1 & 1 & 0.7551020 & 1 & 1\\\\\n\\hline\n39 & w & 1 & 1 & 0.7755102 & 1 & 1\\\\\n\\hline\n40 & w & 1 & 1 & 0.7959184 & 1 & 1\\\\\n\\hline\n41 & w & 1 & 1 & 0.8163265 & 1 & 1\\\\\n\\hline\n42 & w & 1 & 1 & 0.8367347 & 1 & 1\\\\\n\\hline\n43 & w & 1 & 1 & 0.8571429 & 1 & 1\\\\\n\\hline\n44 & w & 1 & 1 & 0.8775510 & 1 & 1\\\\\n\\hline\n45 & w & 1 & 1 & 0.8979592 & 1 & 1\\\\\n\\hline\n46 & w & 1 & 1 & 0.9183673 & 1 & 1\\\\\n\\hline\n47 & w & 1 & 1 & 0.9387755 & 1 & 1\\\\\n\\hline\n48 & w & 1 & 1 & 0.9591837 & 1 & 1\\\\\n\\hline\n49 & w & 1 & 1 & 0.9795918 & 1 & 1\\\\\n\\hline\n50 & w & 1 & 1 & 1.0000000 & 1 & 1\\\\\n\\hline\n51 & l & 2 & 1 & 0.0000000 & 1 & 1\\\\\n\\hline\n52 & l & 2 & 1 & 0.0204082 & 2 & 1\\\\\n\\hline\n53 & l & 2 & 1 & 0.0408163 & 2 & 1\\\\\n\\hline\n54 & l & 2 & 1 & 0.0612245 & 2 & 1\\\\\n\\hline\n55 & l & 2 & 1 & 0.0816327 & 2 & 1\\\\\n\\hline\n56 & l & 2 & 1 & 0.1020408 & 2 & 1\\\\\n\\hline\n57 & l & 2 & 1 & 0.1224490 & 2 & 1\\\\\n\\hline\n58 & l & 2 & 1 & 0.1428571 & 2 & 1\\\\\n\\hline\n59 & l & 2 & 1 & 0.1632653 & 2 & 1\\\\\n\\hline\n60 & l & 2 & 1 & 0.1836735 & 2 & 1\\\\\n\\hline\n61 & l & 2 & 1 & 0.2040816 & 2 & 1\\\\\n\\hline\n62 & l & 2 & 1 & 0.2244898 & 2 & 1\\\\\n\\hline\n63 & l & 2 & 1 & 0.2448980 & 2 & 1\\\\\n\\hline\n64 & l & 2 & 1 & 0.2653061 & 2 & 1\\\\\n\\hline\n65 & l & 2 & 1 & 0.2857143 & 2 & 1\\\\\n\\hline\n66 & l & 2 & 1 & 0.3061224 & 2 & 1\\\\\n\\hline\n67 & l & 2 & 1 & 0.3265306 & 2 & 1\\\\\n\\hline\n68 & l & 2 & 1 & 0.3469388 & 2 & 1\\\\\n\\hline\n69 & l & 2 & 1 & 0.3673469 & 2 & 1\\\\\n\\hline\n70 & l & 2 & 1 & 0.3877551 & 2 & 1\\\\\n\\hline\n71 & l & 2 & 1 & 0.4081633 & 2 & 1\\\\\n\\hline\n72 & l & 2 & 1 & 0.4285714 & 2 & 1\\\\\n\\hline\n73 & l & 2 & 1 & 0.4489796 & 2 & 1\\\\\n\\hline\n74 & l & 2 & 1 & 0.4693878 & 2 & 1\\\\\n\\hline\n75 & l & 2 & 1 & 0.4897959 & 2 & 1\\\\\n\\hline\n76 & l & 2 & 1 & 0.5102041 & 2 & 1\\\\\n\\hline\n77 & l & 2 & 1 & 0.5306122 & 2 & 1\\\\\n\\hline\n78 & l & 2 & 1 & 0.5510204 & 2 & 1\\\\\n\\hline\n79 & l & 2 & 1 & 0.5714286 & 2 & 1\\\\\n\\hline\n80 & l & 2 & 1 & 0.5918367 & 2 & 1\\\\\n\\hline\n81 & l & 2 & 1 & 0.6122449 & 2 & 1\\\\\n\\hline\n82 & l & 2 & 1 & 0.6326531 & 2 & 1\\\\\n\\hline\n83 & l & 2 & 1 & 0.6530612 & 2 & 1\\\\\n\\hline\n84 & l & 2 & 1 & 0.6734694 & 2 & 1\\\\\n\\hline\n85 & l & 2 & 1 & 0.6938776 & 2 & 1\\\\\n\\hline\n86 & l & 2 & 1 & 0.7142857 & 2 & 1\\\\\n\\hline\n87 & l & 2 & 1 & 0.7346939 & 2 & 1\\\\\n\\hline\n88 & l & 2 & 1 & 0.7551020 & 2 & 1\\\\\n\\hline\n89 & l & 2 & 1 & 0.7755102 & 2 & 1\\\\\n\\hline\n90 & l & 2 & 1 & 0.7959184 & 2 & 1\\\\\n\\hline\n91 & l & 2 & 1 & 0.8163265 & 2 & 1\\\\\n\\hline\n92 & l & 2 & 1 & 0.8367347 & 2 & 1\\\\\n\\hline\n93 & l & 2 & 1 & 0.8571429 & 2 & 1\\\\\n\\hline\n94 & l & 2 & 1 & 0.8775510 & 2 & 1\\\\\n\\hline\n95 & l & 2 & 1 & 0.8979592 & 2 & 1\\\\\n\\hline\n96 & l & 2 & 1 & 0.9183673 & 2 & 1\\\\\n\\hline\n97 & l & 2 & 1 & 0.9387755 & 2 & 1\\\\\n\\hline\n98 & l & 2 & 1 & 0.9591837 & 2 & 1\\\\\n\\hline\n99 & l & 2 & 1 & 0.9795918 & 2 & 1\\\\\n\\hline\n100 & l & 2 & 1 & 1.0000000 & 2 & 1\\\\\n\\hline\n101 & w & 3 & 2 & 0.0000000 & 2 & 1\\\\\n\\hline\n102 & w & 3 & 2 & 0.0204082 & 3 & 2\\\\\n\\hline\n103 & w & 3 & 2 & 0.0408163 & 3 & 2\\\\\n\\hline\n104 & w & 3 & 2 & 0.0612245 & 3 & 2\\\\\n\\hline\n105 & w & 3 & 2 & 0.0816327 & 3 & 2\\\\\n\\hline\n106 & w & 3 & 2 & 0.1020408 & 3 & 2\\\\\n\\hline\n107 & w & 3 & 2 & 0.1224490 & 3 & 2\\\\\n\\hline\n108 & w & 3 & 2 & 0.1428571 & 3 & 2\\\\\n\\hline\n109 & w & 3 & 2 & 0.1632653 & 3 & 2\\\\\n\\hline\n110 & w & 3 & 2 & 0.1836735 & 3 & 2\\\\\n\\hline\n111 & w & 3 & 2 & 0.2040816 & 3 & 2\\\\\n\\hline\n112 & w & 3 & 2 & 0.2244898 & 3 & 2\\\\\n\\hline\n113 & w & 3 & 2 & 0.2448980 & 3 & 2\\\\\n\\hline\n114 & w & 3 & 2 & 0.2653061 & 3 & 2\\\\\n\\hline\n115 & w & 3 & 2 & 0.2857143 & 3 & 2\\\\\n\\hline\n116 & w & 3 & 2 & 0.3061224 & 3 & 2\\\\\n\\hline\n117 & w & 3 & 2 & 0.3265306 & 3 & 2\\\\\n\\hline\n118 & w & 3 & 2 & 0.3469388 & 3 & 2\\\\\n\\hline\n119 & w & 3 & 2 & 0.3673469 & 3 & 2\\\\\n\\hline\n120 & w & 3 & 2 & 0.3877551 & 3 & 2\\\\\n\\hline\n121 & w & 3 & 2 & 0.4081633 & 3 & 2\\\\\n\\hline\n122 & w & 3 & 2 & 0.4285714 & 3 & 2\\\\\n\\hline\n123 & w & 3 & 2 & 0.4489796 & 3 & 2\\\\\n\\hline\n124 & w & 3 & 2 & 0.4693878 & 3 & 2\\\\\n\\hline\n125 & w & 3 & 2 & 0.4897959 & 3 & 2\\\\\n\\hline\n126 & w & 3 & 2 & 0.5102041 & 3 & 2\\\\\n\\hline\n127 & w & 3 & 2 & 0.5306122 & 3 & 2\\\\\n\\hline\n128 & w & 3 & 2 & 0.5510204 & 3 & 2\\\\\n\\hline\n129 & w & 3 & 2 & 0.5714286 & 3 & 2\\\\\n\\hline\n130 & w & 3 & 2 & 0.5918367 & 3 & 2\\\\\n\\hline\n131 & w & 3 & 2 & 0.6122449 & 3 & 2\\\\\n\\hline\n132 & w & 3 & 2 & 0.6326531 & 3 & 2\\\\\n\\hline\n133 & w & 3 & 2 & 0.6530612 & 3 & 2\\\\\n\\hline\n134 & w & 3 & 2 & 0.6734694 & 3 & 2\\\\\n\\hline\n135 & w & 3 & 2 & 0.6938776 & 3 & 2\\\\\n\\hline\n136 & w & 3 & 2 & 0.7142857 & 3 & 2\\\\\n\\hline\n137 & w & 3 & 2 & 0.7346939 & 3 & 2\\\\\n\\hline\n138 & w & 3 & 2 & 0.7551020 & 3 & 2\\\\\n\\hline\n139 & w & 3 & 2 & 0.7755102 & 3 & 2\\\\\n\\hline\n140 & w & 3 & 2 & 0.7959184 & 3 & 2\\\\\n\\hline\n141 & w & 3 & 2 & 0.8163265 & 3 & 2\\\\\n\\hline\n142 & w & 3 & 2 & 0.8367347 & 3 & 2\\\\\n\\hline\n143 & w & 3 & 2 & 0.8571429 & 3 & 2\\\\\n\\hline\n144 & w & 3 & 2 & 0.8775510 & 3 & 2\\\\\n\\hline\n145 & w & 3 & 2 & 0.8979592 & 3 & 2\\\\\n\\hline\n146 & w & 3 & 2 & 0.9183673 & 3 & 2\\\\\n\\hline\n147 & w & 3 & 2 & 0.9387755 & 3 & 2\\\\\n\\hline\n148 & w & 3 & 2 & 0.9591837 & 3 & 2\\\\\n\\hline\n149 & w & 3 & 2 & 0.9795918 & 3 & 2\\\\\n\\hline\n150 & w & 3 & 2 & 1.0000000 & 3 & 2\\\\\n\\hline\n151 & w & 4 & 3 & 0.0000000 & 3 & 2\\\\\n\\hline\n152 & w & 4 & 3 & 0.0204082 & 4 & 3\\\\\n\\hline\n153 & w & 4 & 3 & 0.0408163 & 4 & 3\\\\\n\\hline\n154 & w & 4 & 3 & 0.0612245 & 4 & 3\\\\\n\\hline\n155 & w & 4 & 3 & 0.0816327 & 4 & 3\\\\\n\\hline\n156 & w & 4 & 3 & 0.1020408 & 4 & 3\\\\\n\\hline\n157 & w & 4 & 3 & 0.1224490 & 4 & 3\\\\\n\\hline\n158 & w & 4 & 3 & 0.1428571 & 4 & 3\\\\\n\\hline\n159 & w & 4 & 3 & 0.1632653 & 4 & 3\\\\\n\\hline\n160 & w & 4 & 3 & 0.1836735 & 4 & 3\\\\\n\\hline\n161 & w & 4 & 3 & 0.2040816 & 4 & 3\\\\\n\\hline\n162 & w & 4 & 3 & 0.2244898 & 4 & 3\\\\\n\\hline\n163 & w & 4 & 3 & 0.2448980 & 4 & 3\\\\\n\\hline\n164 & w & 4 & 3 & 0.2653061 & 4 & 3\\\\\n\\hline\n165 & w & 4 & 3 & 0.2857143 & 4 & 3\\\\\n\\hline\n166 & w & 4 & 3 & 0.3061224 & 4 & 3\\\\\n\\hline\n167 & w & 4 & 3 & 0.3265306 & 4 & 3\\\\\n\\hline\n168 & w & 4 & 3 & 0.3469388 & 4 & 3\\\\\n\\hline\n169 & w & 4 & 3 & 0.3673469 & 4 & 3\\\\\n\\hline\n170 & w & 4 & 3 & 0.3877551 & 4 & 3\\\\\n\\hline\n171 & w & 4 & 3 & 0.4081633 & 4 & 3\\\\\n\\hline\n172 & w & 4 & 3 & 0.4285714 & 4 & 3\\\\\n\\hline\n173 & w & 4 & 3 & 0.4489796 & 4 & 3\\\\\n\\hline\n174 & w & 4 & 3 & 0.4693878 & 4 & 3\\\\\n\\hline\n175 & w & 4 & 3 & 0.4897959 & 4 & 3\\\\\n\\hline\n176 & w & 4 & 3 & 0.5102041 & 4 & 3\\\\\n\\hline\n177 & w & 4 & 3 & 0.5306122 & 4 & 3\\\\\n\\hline\n178 & w & 4 & 3 & 0.5510204 & 4 & 3\\\\\n\\hline\n179 & w & 4 & 3 & 0.5714286 & 4 & 3\\\\\n\\hline\n180 & w & 4 & 3 & 0.5918367 & 4 & 3\\\\\n\\hline\n181 & w & 4 & 3 & 0.6122449 & 4 & 3\\\\\n\\hline\n182 & w & 4 & 3 & 0.6326531 & 4 & 3\\\\\n\\hline\n183 & w & 4 & 3 & 0.6530612 & 4 & 3\\\\\n\\hline\n184 & w & 4 & 3 & 0.6734694 & 4 & 3\\\\\n\\hline\n185 & w & 4 & 3 & 0.6938776 & 4 & 3\\\\\n\\hline\n186 & w & 4 & 3 & 0.7142857 & 4 & 3\\\\\n\\hline\n187 & w & 4 & 3 & 0.7346939 & 4 & 3\\\\\n\\hline\n188 & w & 4 & 3 & 0.7551020 & 4 & 3\\\\\n\\hline\n189 & w & 4 & 3 & 0.7755102 & 4 & 3\\\\\n\\hline\n190 & w & 4 & 3 & 0.7959184 & 4 & 3\\\\\n\\hline\n191 & w & 4 & 3 & 0.8163265 & 4 & 3\\\\\n\\hline\n192 & w & 4 & 3 & 0.8367347 & 4 & 3\\\\\n\\hline\n193 & w & 4 & 3 & 0.8571429 & 4 & 3\\\\\n\\hline\n194 & w & 4 & 3 & 0.8775510 & 4 & 3\\\\\n\\hline\n195 & w & 4 & 3 & 0.8979592 & 4 & 3\\\\\n\\hline\n196 & w & 4 & 3 & 0.9183673 & 4 & 3\\\\\n\\hline\n197 & w & 4 & 3 & 0.9387755 & 4 & 3\\\\\n\\hline\n198 & w & 4 & 3 & 0.9591837 & 4 & 3\\\\\n\\hline\n199 & w & 4 & 3 & 0.9795918 & 4 & 3\\\\\n\\hline\n200 & w & 4 & 3 & 1.0000000 & 4 & 3\\\\\n\\hline\n201 & w & 5 & 4 & 0.0000000 & 4 & 3\\\\\n\\hline\n202 & w & 5 & 4 & 0.0204082 & 5 & 4\\\\\n\\hline\n203 & w & 5 & 4 & 0.0408163 & 5 & 4\\\\\n\\hline\n204 & w & 5 & 4 & 0.0612245 & 5 & 4\\\\\n\\hline\n205 & w & 5 & 4 & 0.0816327 & 5 & 4\\\\\n\\hline\n206 & w & 5 & 4 & 0.1020408 & 5 & 4\\\\\n\\hline\n207 & w & 5 & 4 & 0.1224490 & 5 & 4\\\\\n\\hline\n208 & w & 5 & 4 & 0.1428571 & 5 & 4\\\\\n\\hline\n209 & w & 5 & 4 & 0.1632653 & 5 & 4\\\\\n\\hline\n210 & w & 5 & 4 & 0.1836735 & 5 & 4\\\\\n\\hline\n211 & w & 5 & 4 & 0.2040816 & 5 & 4\\\\\n\\hline\n212 & w & 5 & 4 & 0.2244898 & 5 & 4\\\\\n\\hline\n213 & w & 5 & 4 & 0.2448980 & 5 & 4\\\\\n\\hline\n214 & w & 5 & 4 & 0.2653061 & 5 & 4\\\\\n\\hline\n215 & w & 5 & 4 & 0.2857143 & 5 & 4\\\\\n\\hline\n216 & w & 5 & 4 & 0.3061224 & 5 & 4\\\\\n\\hline\n217 & w & 5 & 4 & 0.3265306 & 5 & 4\\\\\n\\hline\n218 & w & 5 & 4 & 0.3469388 & 5 & 4\\\\\n\\hline\n219 & w & 5 & 4 & 0.3673469 & 5 & 4\\\\\n\\hline\n220 & w & 5 & 4 & 0.3877551 & 5 & 4\\\\\n\\hline\n221 & w & 5 & 4 & 0.4081633 & 5 & 4\\\\\n\\hline\n222 & w & 5 & 4 & 0.4285714 & 5 & 4\\\\\n\\hline\n223 & w & 5 & 4 & 0.4489796 & 5 & 4\\\\\n\\hline\n224 & w & 5 & 4 & 0.4693878 & 5 & 4\\\\\n\\hline\n225 & w & 5 & 4 & 0.4897959 & 5 & 4\\\\\n\\hline\n226 & w & 5 & 4 & 0.5102041 & 5 & 4\\\\\n\\hline\n227 & w & 5 & 4 & 0.5306122 & 5 & 4\\\\\n\\hline\n228 & w & 5 & 4 & 0.5510204 & 5 & 4\\\\\n\\hline\n229 & w & 5 & 4 & 0.5714286 & 5 & 4\\\\\n\\hline\n230 & w & 5 & 4 & 0.5918367 & 5 & 4\\\\\n\\hline\n231 & w & 5 & 4 & 0.6122449 & 5 & 4\\\\\n\\hline\n232 & w & 5 & 4 & 0.6326531 & 5 & 4\\\\\n\\hline\n233 & w & 5 & 4 & 0.6530612 & 5 & 4\\\\\n\\hline\n234 & w & 5 & 4 & 0.6734694 & 5 & 4\\\\\n\\hline\n235 & w & 5 & 4 & 0.6938776 & 5 & 4\\\\\n\\hline\n236 & w & 5 & 4 & 0.7142857 & 5 & 4\\\\\n\\hline\n237 & w & 5 & 4 & 0.7346939 & 5 & 4\\\\\n\\hline\n238 & w & 5 & 4 & 0.7551020 & 5 & 4\\\\\n\\hline\n239 & w & 5 & 4 & 0.7755102 & 5 & 4\\\\\n\\hline\n240 & w & 5 & 4 & 0.7959184 & 5 & 4\\\\\n\\hline\n241 & w & 5 & 4 & 0.8163265 & 5 & 4\\\\\n\\hline\n242 & w & 5 & 4 & 0.8367347 & 5 & 4\\\\\n\\hline\n243 & w & 5 & 4 & 0.8571429 & 5 & 4\\\\\n\\hline\n244 & w & 5 & 4 & 0.8775510 & 5 & 4\\\\\n\\hline\n245 & w & 5 & 4 & 0.8979592 & 5 & 4\\\\\n\\hline\n246 & w & 5 & 4 & 0.9183673 & 5 & 4\\\\\n\\hline\n247 & w & 5 & 4 & 0.9387755 & 5 & 4\\\\\n\\hline\n248 & w & 5 & 4 & 0.9591837 & 5 & 4\\\\\n\\hline\n249 & w & 5 & 4 & 0.9795918 & 5 & 4\\\\\n\\hline\n250 & w & 5 & 4 & 1.0000000 & 5 & 4\\\\\n\\hline\n251 & l & 6 & 4 & 0.0000000 & 5 & 4\\\\\n\\hline\n252 & l & 6 & 4 & 0.0204082 & 6 & 4\\\\\n\\hline\n253 & l & 6 & 4 & 0.0408163 & 6 & 4\\\\\n\\hline\n254 & l & 6 & 4 & 0.0612245 & 6 & 4\\\\\n\\hline\n255 & l & 6 & 4 & 0.0816327 & 6 & 4\\\\\n\\hline\n256 & l & 6 & 4 & 0.1020408 & 6 & 4\\\\\n\\hline\n257 & l & 6 & 4 & 0.1224490 & 6 & 4\\\\\n\\hline\n258 & l & 6 & 4 & 0.1428571 & 6 & 4\\\\\n\\hline\n259 & l & 6 & 4 & 0.1632653 & 6 & 4\\\\\n\\hline\n260 & l & 6 & 4 & 0.1836735 & 6 & 4\\\\\n\\hline\n261 & l & 6 & 4 & 0.2040816 & 6 & 4\\\\\n\\hline\n262 & l & 6 & 4 & 0.2244898 & 6 & 4\\\\\n\\hline\n263 & l & 6 & 4 & 0.2448980 & 6 & 4\\\\\n\\hline\n264 & l & 6 & 4 & 0.2653061 & 6 & 4\\\\\n\\hline\n265 & l & 6 & 4 & 0.2857143 & 6 & 4\\\\\n\\hline\n266 & l & 6 & 4 & 0.3061224 & 6 & 4\\\\\n\\hline\n267 & l & 6 & 4 & 0.3265306 & 6 & 4\\\\\n\\hline\n268 & l & 6 & 4 & 0.3469388 & 6 & 4\\\\\n\\hline\n269 & l & 6 & 4 & 0.3673469 & 6 & 4\\\\\n\\hline\n270 & l & 6 & 4 & 0.3877551 & 6 & 4\\\\\n\\hline\n271 & l & 6 & 4 & 0.4081633 & 6 & 4\\\\\n\\hline\n272 & l & 6 & 4 & 0.4285714 & 6 & 4\\\\\n\\hline\n273 & l & 6 & 4 & 0.4489796 & 6 & 4\\\\\n\\hline\n274 & l & 6 & 4 & 0.4693878 & 6 & 4\\\\\n\\hline\n275 & l & 6 & 4 & 0.4897959 & 6 & 4\\\\\n\\hline\n276 & l & 6 & 4 & 0.5102041 & 6 & 4\\\\\n\\hline\n277 & l & 6 & 4 & 0.5306122 & 6 & 4\\\\\n\\hline\n278 & l & 6 & 4 & 0.5510204 & 6 & 4\\\\\n\\hline\n279 & l & 6 & 4 & 0.5714286 & 6 & 4\\\\\n\\hline\n280 & l & 6 & 4 & 0.5918367 & 6 & 4\\\\\n\\hline\n281 & l & 6 & 4 & 0.6122449 & 6 & 4\\\\\n\\hline\n282 & l & 6 & 4 & 0.6326531 & 6 & 4\\\\\n\\hline\n283 & l & 6 & 4 & 0.6530612 & 6 & 4\\\\\n\\hline\n284 & l & 6 & 4 & 0.6734694 & 6 & 4\\\\\n\\hline\n285 & l & 6 & 4 & 0.6938776 & 6 & 4\\\\\n\\hline\n286 & l & 6 & 4 & 0.7142857 & 6 & 4\\\\\n\\hline\n287 & l & 6 & 4 & 0.7346939 & 6 & 4\\\\\n\\hline\n288 & l & 6 & 4 & 0.7551020 & 6 & 4\\\\\n\\hline\n289 & l & 6 & 4 & 0.7755102 & 6 & 4\\\\\n\\hline\n290 & l & 6 & 4 & 0.7959184 & 6 & 4\\\\\n\\hline\n291 & l & 6 & 4 & 0.8163265 & 6 & 4\\\\\n\\hline\n292 & l & 6 & 4 & 0.8367347 & 6 & 4\\\\\n\\hline\n293 & l & 6 & 4 & 0.8571429 & 6 & 4\\\\\n\\hline\n294 & l & 6 & 4 & 0.8775510 & 6 & 4\\\\\n\\hline\n295 & l & 6 & 4 & 0.8979592 & 6 & 4\\\\\n\\hline\n296 & l & 6 & 4 & 0.9183673 & 6 & 4\\\\\n\\hline\n297 & l & 6 & 4 & 0.9387755 & 6 & 4\\\\\n\\hline\n298 & l & 6 & 4 & 0.9591837 & 6 & 4\\\\\n\\hline\n299 & l & 6 & 4 & 0.9795918 & 6 & 4\\\\\n\\hline\n300 & l & 6 & 4 & 1.0000000 & 6 & 4\\\\\n\\hline\n301 & w & 7 & 5 & 0.0000000 & 6 & 4\\\\\n\\hline\n302 & w & 7 & 5 & 0.0204082 & 7 & 5\\\\\n\\hline\n303 & w & 7 & 5 & 0.0408163 & 7 & 5\\\\\n\\hline\n304 & w & 7 & 5 & 0.0612245 & 7 & 5\\\\\n\\hline\n305 & w & 7 & 5 & 0.0816327 & 7 & 5\\\\\n\\hline\n306 & w & 7 & 5 & 0.1020408 & 7 & 5\\\\\n\\hline\n307 & w & 7 & 5 & 0.1224490 & 7 & 5\\\\\n\\hline\n308 & w & 7 & 5 & 0.1428571 & 7 & 5\\\\\n\\hline\n309 & w & 7 & 5 & 0.1632653 & 7 & 5\\\\\n\\hline\n310 & w & 7 & 5 & 0.1836735 & 7 & 5\\\\\n\\hline\n311 & w & 7 & 5 & 0.2040816 & 7 & 5\\\\\n\\hline\n312 & w & 7 & 5 & 0.2244898 & 7 & 5\\\\\n\\hline\n313 & w & 7 & 5 & 0.2448980 & 7 & 5\\\\\n\\hline\n314 & w & 7 & 5 & 0.2653061 & 7 & 5\\\\\n\\hline\n315 & w & 7 & 5 & 0.2857143 & 7 & 5\\\\\n\\hline\n316 & w & 7 & 5 & 0.3061224 & 7 & 5\\\\\n\\hline\n317 & w & 7 & 5 & 0.3265306 & 7 & 5\\\\\n\\hline\n318 & w & 7 & 5 & 0.3469388 & 7 & 5\\\\\n\\hline\n319 & w & 7 & 5 & 0.3673469 & 7 & 5\\\\\n\\hline\n320 & w & 7 & 5 & 0.3877551 & 7 & 5\\\\\n\\hline\n321 & w & 7 & 5 & 0.4081633 & 7 & 5\\\\\n\\hline\n322 & w & 7 & 5 & 0.4285714 & 7 & 5\\\\\n\\hline\n323 & w & 7 & 5 & 0.4489796 & 7 & 5\\\\\n\\hline\n324 & w & 7 & 5 & 0.4693878 & 7 & 5\\\\\n\\hline\n325 & w & 7 & 5 & 0.4897959 & 7 & 5\\\\\n\\hline\n326 & w & 7 & 5 & 0.5102041 & 7 & 5\\\\\n\\hline\n327 & w & 7 & 5 & 0.5306122 & 7 & 5\\\\\n\\hline\n328 & w & 7 & 5 & 0.5510204 & 7 & 5\\\\\n\\hline\n329 & w & 7 & 5 & 0.5714286 & 7 & 5\\\\\n\\hline\n330 & w & 7 & 5 & 0.5918367 & 7 & 5\\\\\n\\hline\n331 & w & 7 & 5 & 0.6122449 & 7 & 5\\\\\n\\hline\n332 & w & 7 & 5 & 0.6326531 & 7 & 5\\\\\n\\hline\n333 & w & 7 & 5 & 0.6530612 & 7 & 5\\\\\n\\hline\n334 & w & 7 & 5 & 0.6734694 & 7 & 5\\\\\n\\hline\n335 & w & 7 & 5 & 0.6938776 & 7 & 5\\\\\n\\hline\n336 & w & 7 & 5 & 0.7142857 & 7 & 5\\\\\n\\hline\n337 & w & 7 & 5 & 0.7346939 & 7 & 5\\\\\n\\hline\n338 & w & 7 & 5 & 0.7551020 & 7 & 5\\\\\n\\hline\n339 & w & 7 & 5 & 0.7755102 & 7 & 5\\\\\n\\hline\n340 & w & 7 & 5 & 0.7959184 & 7 & 5\\\\\n\\hline\n341 & w & 7 & 5 & 0.8163265 & 7 & 5\\\\\n\\hline\n342 & w & 7 & 5 & 0.8367347 & 7 & 5\\\\\n\\hline\n343 & w & 7 & 5 & 0.8571429 & 7 & 5\\\\\n\\hline\n344 & w & 7 & 5 & 0.8775510 & 7 & 5\\\\\n\\hline\n345 & w & 7 & 5 & 0.8979592 & 7 & 5\\\\\n\\hline\n346 & w & 7 & 5 & 0.9183673 & 7 & 5\\\\\n\\hline\n347 & w & 7 & 5 & 0.9387755 & 7 & 5\\\\\n\\hline\n348 & w & 7 & 5 & 0.9591837 & 7 & 5\\\\\n\\hline\n349 & w & 7 & 5 & 0.9795918 & 7 & 5\\\\\n\\hline\n350 & w & 7 & 5 & 1.0000000 & 7 & 5\\\\\n\\hline\n351 & l & 8 & 5 & 0.0000000 & 7 & 5\\\\\n\\hline\n352 & l & 8 & 5 & 0.0204082 & 8 & 5\\\\\n\\hline\n353 & l & 8 & 5 & 0.0408163 & 8 & 5\\\\\n\\hline\n354 & l & 8 & 5 & 0.0612245 & 8 & 5\\\\\n\\hline\n355 & l & 8 & 5 & 0.0816327 & 8 & 5\\\\\n\\hline\n356 & l & 8 & 5 & 0.1020408 & 8 & 5\\\\\n\\hline\n357 & l & 8 & 5 & 0.1224490 & 8 & 5\\\\\n\\hline\n358 & l & 8 & 5 & 0.1428571 & 8 & 5\\\\\n\\hline\n359 & l & 8 & 5 & 0.1632653 & 8 & 5\\\\\n\\hline\n360 & l & 8 & 5 & 0.1836735 & 8 & 5\\\\\n\\hline\n361 & l & 8 & 5 & 0.2040816 & 8 & 5\\\\\n\\hline\n362 & l & 8 & 5 & 0.2244898 & 8 & 5\\\\\n\\hline\n363 & l & 8 & 5 & 0.2448980 & 8 & 5\\\\\n\\hline\n364 & l & 8 & 5 & 0.2653061 & 8 & 5\\\\\n\\hline\n365 & l & 8 & 5 & 0.2857143 & 8 & 5\\\\\n\\hline\n366 & l & 8 & 5 & 0.3061224 & 8 & 5\\\\\n\\hline\n367 & l & 8 & 5 & 0.3265306 & 8 & 5\\\\\n\\hline\n368 & l & 8 & 5 & 0.3469388 & 8 & 5\\\\\n\\hline\n369 & l & 8 & 5 & 0.3673469 & 8 & 5\\\\\n\\hline\n370 & l & 8 & 5 & 0.3877551 & 8 & 5\\\\\n\\hline\n371 & l & 8 & 5 & 0.4081633 & 8 & 5\\\\\n\\hline\n372 & l & 8 & 5 & 0.4285714 & 8 & 5\\\\\n\\hline\n373 & l & 8 & 5 & 0.4489796 & 8 & 5\\\\\n\\hline\n374 & l & 8 & 5 & 0.4693878 & 8 & 5\\\\\n\\hline\n375 & l & 8 & 5 & 0.4897959 & 8 & 5\\\\\n\\hline\n376 & l & 8 & 5 & 0.5102041 & 8 & 5\\\\\n\\hline\n377 & l & 8 & 5 & 0.5306122 & 8 & 5\\\\\n\\hline\n378 & l & 8 & 5 & 0.5510204 & 8 & 5\\\\\n\\hline\n379 & l & 8 & 5 & 0.5714286 & 8 & 5\\\\\n\\hline\n380 & l & 8 & 5 & 0.5918367 & 8 & 5\\\\\n\\hline\n381 & l & 8 & 5 & 0.6122449 & 8 & 5\\\\\n\\hline\n382 & l & 8 & 5 & 0.6326531 & 8 & 5\\\\\n\\hline\n383 & l & 8 & 5 & 0.6530612 & 8 & 5\\\\\n\\hline\n384 & l & 8 & 5 & 0.6734694 & 8 & 5\\\\\n\\hline\n385 & l & 8 & 5 & 0.6938776 & 8 & 5\\\\\n\\hline\n386 & l & 8 & 5 & 0.7142857 & 8 & 5\\\\\n\\hline\n387 & l & 8 & 5 & 0.7346939 & 8 & 5\\\\\n\\hline\n388 & l & 8 & 5 & 0.7551020 & 8 & 5\\\\\n\\hline\n389 & l & 8 & 5 & 0.7755102 & 8 & 5\\\\\n\\hline\n390 & l & 8 & 5 & 0.7959184 & 8 & 5\\\\\n\\hline\n391 & l & 8 & 5 & 0.8163265 & 8 & 5\\\\\n\\hline\n392 & l & 8 & 5 & 0.8367347 & 8 & 5\\\\\n\\hline\n393 & l & 8 & 5 & 0.8571429 & 8 & 5\\\\\n\\hline\n394 & l & 8 & 5 & 0.8775510 & 8 & 5\\\\\n\\hline\n395 & l & 8 & 5 & 0.8979592 & 8 & 5\\\\\n\\hline\n396 & l & 8 & 5 & 0.9183673 & 8 & 5\\\\\n\\hline\n397 & l & 8 & 5 & 0.9387755 & 8 & 5\\\\\n\\hline\n398 & l & 8 & 5 & 0.9591837 & 8 & 5\\\\\n\\hline\n399 & l & 8 & 5 & 0.9795918 & 8 & 5\\\\\n\\hline\n400 & l & 8 & 5 & 1.0000000 & 8 & 5\\\\\n\\hline\n401 & w & 9 & 6 & 0.0000000 & 8 & 5\\\\\n\\hline\n402 & w & 9 & 6 & 0.0204082 & 9 & 6\\\\\n\\hline\n403 & w & 9 & 6 & 0.0408163 & 9 & 6\\\\\n\\hline\n404 & w & 9 & 6 & 0.0612245 & 9 & 6\\\\\n\\hline\n405 & w & 9 & 6 & 0.0816327 & 9 & 6\\\\\n\\hline\n406 & w & 9 & 6 & 0.1020408 & 9 & 6\\\\\n\\hline\n407 & w & 9 & 6 & 0.1224490 & 9 & 6\\\\\n\\hline\n408 & w & 9 & 6 & 0.1428571 & 9 & 6\\\\\n\\hline\n409 & w & 9 & 6 & 0.1632653 & 9 & 6\\\\\n\\hline\n410 & w & 9 & 6 & 0.1836735 & 9 & 6\\\\\n\\hline\n411 & w & 9 & 6 & 0.2040816 & 9 & 6\\\\\n\\hline\n412 & w & 9 & 6 & 0.2244898 & 9 & 6\\\\\n\\hline\n413 & w & 9 & 6 & 0.2448980 & 9 & 6\\\\\n\\hline\n414 & w & 9 & 6 & 0.2653061 & 9 & 6\\\\\n\\hline\n415 & w & 9 & 6 & 0.2857143 & 9 & 6\\\\\n\\hline\n416 & w & 9 & 6 & 0.3061224 & 9 & 6\\\\\n\\hline\n417 & w & 9 & 6 & 0.3265306 & 9 & 6\\\\\n\\hline\n418 & w & 9 & 6 & 0.3469388 & 9 & 6\\\\\n\\hline\n419 & w & 9 & 6 & 0.3673469 & 9 & 6\\\\\n\\hline\n420 & w & 9 & 6 & 0.3877551 & 9 & 6\\\\\n\\hline\n421 & w & 9 & 6 & 0.4081633 & 9 & 6\\\\\n\\hline\n422 & w & 9 & 6 & 0.4285714 & 9 & 6\\\\\n\\hline\n423 & w & 9 & 6 & 0.4489796 & 9 & 6\\\\\n\\hline\n424 & w & 9 & 6 & 0.4693878 & 9 & 6\\\\\n\\hline\n425 & w & 9 & 6 & 0.4897959 & 9 & 6\\\\\n\\hline\n426 & w & 9 & 6 & 0.5102041 & 9 & 6\\\\\n\\hline\n427 & w & 9 & 6 & 0.5306122 & 9 & 6\\\\\n\\hline\n428 & w & 9 & 6 & 0.5510204 & 9 & 6\\\\\n\\hline\n429 & w & 9 & 6 & 0.5714286 & 9 & 6\\\\\n\\hline\n430 & w & 9 & 6 & 0.5918367 & 9 & 6\\\\\n\\hline\n431 & w & 9 & 6 & 0.6122449 & 9 & 6\\\\\n\\hline\n432 & w & 9 & 6 & 0.6326531 & 9 & 6\\\\\n\\hline\n433 & w & 9 & 6 & 0.6530612 & 9 & 6\\\\\n\\hline\n434 & w & 9 & 6 & 0.6734694 & 9 & 6\\\\\n\\hline\n435 & w & 9 & 6 & 0.6938776 & 9 & 6\\\\\n\\hline\n436 & w & 9 & 6 & 0.7142857 & 9 & 6\\\\\n\\hline\n437 & w & 9 & 6 & 0.7346939 & 9 & 6\\\\\n\\hline\n438 & w & 9 & 6 & 0.7551020 & 9 & 6\\\\\n\\hline\n439 & w & 9 & 6 & 0.7755102 & 9 & 6\\\\\n\\hline\n440 & w & 9 & 6 & 0.7959184 & 9 & 6\\\\\n\\hline\n441 & w & 9 & 6 & 0.8163265 & 9 & 6\\\\\n\\hline\n442 & w & 9 & 6 & 0.8367347 & 9 & 6\\\\\n\\hline\n443 & w & 9 & 6 & 0.8571429 & 9 & 6\\\\\n\\hline\n444 & w & 9 & 6 & 0.8775510 & 9 & 6\\\\\n\\hline\n445 & w & 9 & 6 & 0.8979592 & 9 & 6\\\\\n\\hline\n446 & w & 9 & 6 & 0.9183673 & 9 & 6\\\\\n\\hline\n447 & w & 9 & 6 & 0.9387755 & 9 & 6\\\\\n\\hline\n448 & w & 9 & 6 & 0.9591837 & 9 & 6\\\\\n\\hline\n449 & w & 9 & 6 & 0.9795918 & 9 & 6\\\\\n\\hline\n450 & w & 9 & 6 & 1.0000000 & 9 & 6\\\\\n\\hline\n\\end{tabular}\n\\end{table}\n:::\n:::\n\n\n\nIt turned out that the two version differ after 51 records. (Not after\n50 as one would have assumed. Apparently this has to do with the `lag()`\ncommand because the first 51 records are identical. Beginning with row\nnumber 52 there are differences in the column `lagged_n_trials`:\n\n-   In the original version `lagged_n_trials` remains `1` until 100\n    (included), then it changes to `2`.\n-   In the version without grouping however `lagged_n_trials` changes to\n    `2` with record 52. (not 51)\n-   This pattern is repeated: Original version always changes after 100\n    records. The version without grouping changes after 50 rows but\n    starting with row 51.\n-   The same differences appear with `lagged_n_success` but 50 records\n    later: The first difference appears after row 101.\n\n##### Annotation (3): `dplyr::lag()` {#sec-annotation-3-lag}\n\nThe next line uses the `dplyr::lag()` command: The function (`lag()`)\nfinds the \"previous\" values in a vector (time series). This is useful\nfor comparing values behind of the current values. See [Compute lagged\nor leading values](https://dplyr.tidyverse.org/reference/lead-lag.html).\n\nWe need to get the immediately previous values for drawing the prior\nprobabilities in the current graph (= dashed line or `linetype = 2` in\nggplot parlance). In the relation with the posterior probabilities the\ndifference form the prior possibility is always `1`\\` (this is the\noption `default = 1` in the `lag()` function. This is now the correct\nexplanation for the differences starting with rows 52 resp. 102 (and not\n51 resp. 101).\n\nThe result is already shown under @sec-annotation-2-group_by in the\nresult of the code listing @tbl-bayesian-model-learning-anno2-1.\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: bayesian-model-learning-anno3\n\n# starting data\ntbl <- tibble(toss = c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\")) |> \n    mutate(n_trials  = 1:9, n_success = cumsum(toss == \"w\"))\n\n# start of bayesian modeling with grouping (as in the original)\nsequence_length <- 50\n\ntbl3 <- tbl %>% \n  expand_grid(p_water = seq(from = 0, to = 1, \n                            length.out = sequence_length)) %>% \n  group_by(p_water) %>% \n    \n  ### add code lines of annotation <3> ####################################\n  mutate(lagged_n_trials  = lag(n_trials, default = 1),\n         lagged_n_success = lag(n_success, default = 1)) %>% \n  ungroup()\ntbl3\n```\n````\n\n```\n#> # A tibble: 450 x 6\n#>    toss  n_trials n_success p_water lagged_n_trials lagged_n_success\n#>    <chr>    <int>     <int>   <dbl>           <int>            <int>\n#>  1 w            1         1  0                    1                1\n#>  2 w            1         1  0.0204               1                1\n#>  3 w            1         1  0.0408               1                1\n#>  4 w            1         1  0.0612               1                1\n#>  5 w            1         1  0.0816               1                1\n#>  6 w            1         1  0.102                1                1\n#>  7 w            1         1  0.122                1                1\n#>  8 w            1         1  0.143                1                1\n#>  9 w            1         1  0.163                1                1\n#> 10 w            1         1  0.184                1                1\n#> # i 440 more rows\n```\n:::\n\n\n\n##### Annotation (4): `dplyr::ungroup()` {#sec-annotation-4-ungroup}\n\nThis is just the reversion of the grouping command `group_by(p_water)`\nmentioned in @sec-annotation-2-group_by.\n\n##### Annotation (5): dbinom() {#sec-annotation-5-dbinom}\n\nThis is the core of the prior and likelihood calculation. It uses\n`base::dbinom()`, to calculate two alternative events. `dbinom()` is the\nR function for the binomial distribution, a distribution provided by the\nprobability theory for \"coin tossing\" problems.\n\nThe \"`d`\" in `dbinom()` stands for *density*. Functions named in this\nway almost always have corresponding partners that begin with \"`r`\" for\nrandom samples and that begin with \"`p`\" for cumulative probabilities.\nSee for example the [help\nfile](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Binomial.html).\nSee also the blog entries [An Introduction to the Binomial\nDistribution](https://www.statology.org/binomial-distribution/) and [A\nGuide to dbinom, pbinom, qbinom, and rbinom in\nR](https://www.statology.org/dbinom-pbinom-qbinom-rbinom-in-r/) of the\nStatalogy website.\n\nThe results of each of the different calculation (prior and pikelihood)\nare collected in with `mutate()` into two new generated columns.\n\nThere is no prior for the first trial, so it is assumed that it is 0.5.\nThe formula for the binomial distribution uses for the prior the\nlagged-version whereas the likelihood uses the current version. These\ntwo lines provide the essential calculations: They match the 50 grid\npoints as assumed water probabilities of every trial to their trial\noutcome (`W` or `L`) probabilities.\n\nThe third `mutate()` generates the `strip` variable consisting of the\nprefix \"N =\" followed by the counts of the number of trials. This will\nlater provide the title for the the different facets of the plot.\n\n\n\n::: {#tbl-bayesian-model-learning-anno5 .cell tbl-cap='Bayesian Updating: Calculating prior and likelihood'}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: tbl-bayesian-model-learning-anno5\n#| tbl-cap: \"Bayesian Updating: Calculating prior and likelihood\"\n\n# starting data\ntbl <- tibble(toss = c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\")) |> \n    mutate(n_trials  = 1:9, n_success = cumsum(toss == \"w\"))\n\n# start of bayesian modeling with grouping (as in the original)\nsequence_length <- 50\n\ntbl5 <- tbl %>% \n  expand_grid(p_water = seq(from = 0, to = 1, \n                            length.out = sequence_length)) %>% \n  group_by(p_water) %>% \n    \n  mutate(lagged_n_trials  = lag(n_trials, default = 1),\n         lagged_n_success = lag(n_success, default = 1)) %>% \n  ungroup() %>% \n    \n  ### add code lines of annotation <5> ########################\n  mutate(prior      = ifelse(n_trials == 1, .5,\n                             dbinom(x    = lagged_n_success, \n                                    size = lagged_n_trials, \n                                    prob = p_water)),\n         likelihood = dbinom(x    = n_success, \n                             size = n_trials, \n                             prob = p_water),\n         strip      = str_c(\"n = \", n_trials)) |> \n \n  # provide table scroll box\n  mutate(ID = row_number()) |> \n  relocate(ID, .before = toss) |> \n  kableExtra::kbl() %>%\n  kableExtra::kable_classic() %>%\n  kableExtra::scroll_box(height = \"600px\")\ntbl5\n```\n````\n\n::: {.cell-output-display}\n\\begin{table}\n\\centering\n\\begin{tabular}[t]{r|l|r|r|r|r|r|r|r|l}\n\\hline\nID & toss & n\\_trials & n\\_success & p\\_water & lagged\\_n\\_trials & lagged\\_n\\_success & prior & likelihood & strip\\\\\n\\hline\n1 & w & 1 & 1 & 0.0000000 & 1 & 1 & 0.5000000 & 0.0000000 & n = 1\\\\\n\\hline\n2 & w & 1 & 1 & 0.0204082 & 1 & 1 & 0.5000000 & 0.0204082 & n = 1\\\\\n\\hline\n3 & w & 1 & 1 & 0.0408163 & 1 & 1 & 0.5000000 & 0.0408163 & n = 1\\\\\n\\hline\n4 & w & 1 & 1 & 0.0612245 & 1 & 1 & 0.5000000 & 0.0612245 & n = 1\\\\\n\\hline\n5 & w & 1 & 1 & 0.0816327 & 1 & 1 & 0.5000000 & 0.0816327 & n = 1\\\\\n\\hline\n6 & w & 1 & 1 & 0.1020408 & 1 & 1 & 0.5000000 & 0.1020408 & n = 1\\\\\n\\hline\n7 & w & 1 & 1 & 0.1224490 & 1 & 1 & 0.5000000 & 0.1224490 & n = 1\\\\\n\\hline\n8 & w & 1 & 1 & 0.1428571 & 1 & 1 & 0.5000000 & 0.1428571 & n = 1\\\\\n\\hline\n9 & w & 1 & 1 & 0.1632653 & 1 & 1 & 0.5000000 & 0.1632653 & n = 1\\\\\n\\hline\n10 & w & 1 & 1 & 0.1836735 & 1 & 1 & 0.5000000 & 0.1836735 & n = 1\\\\\n\\hline\n11 & w & 1 & 1 & 0.2040816 & 1 & 1 & 0.5000000 & 0.2040816 & n = 1\\\\\n\\hline\n12 & w & 1 & 1 & 0.2244898 & 1 & 1 & 0.5000000 & 0.2244898 & n = 1\\\\\n\\hline\n13 & w & 1 & 1 & 0.2448980 & 1 & 1 & 0.5000000 & 0.2448980 & n = 1\\\\\n\\hline\n14 & w & 1 & 1 & 0.2653061 & 1 & 1 & 0.5000000 & 0.2653061 & n = 1\\\\\n\\hline\n15 & w & 1 & 1 & 0.2857143 & 1 & 1 & 0.5000000 & 0.2857143 & n = 1\\\\\n\\hline\n16 & w & 1 & 1 & 0.3061224 & 1 & 1 & 0.5000000 & 0.3061224 & n = 1\\\\\n\\hline\n17 & w & 1 & 1 & 0.3265306 & 1 & 1 & 0.5000000 & 0.3265306 & n = 1\\\\\n\\hline\n18 & w & 1 & 1 & 0.3469388 & 1 & 1 & 0.5000000 & 0.3469388 & n = 1\\\\\n\\hline\n19 & w & 1 & 1 & 0.3673469 & 1 & 1 & 0.5000000 & 0.3673469 & n = 1\\\\\n\\hline\n20 & w & 1 & 1 & 0.3877551 & 1 & 1 & 0.5000000 & 0.3877551 & n = 1\\\\\n\\hline\n21 & w & 1 & 1 & 0.4081633 & 1 & 1 & 0.5000000 & 0.4081633 & n = 1\\\\\n\\hline\n22 & w & 1 & 1 & 0.4285714 & 1 & 1 & 0.5000000 & 0.4285714 & n = 1\\\\\n\\hline\n23 & w & 1 & 1 & 0.4489796 & 1 & 1 & 0.5000000 & 0.4489796 & n = 1\\\\\n\\hline\n24 & w & 1 & 1 & 0.4693878 & 1 & 1 & 0.5000000 & 0.4693878 & n = 1\\\\\n\\hline\n25 & w & 1 & 1 & 0.4897959 & 1 & 1 & 0.5000000 & 0.4897959 & n = 1\\\\\n\\hline\n26 & w & 1 & 1 & 0.5102041 & 1 & 1 & 0.5000000 & 0.5102041 & n = 1\\\\\n\\hline\n27 & w & 1 & 1 & 0.5306122 & 1 & 1 & 0.5000000 & 0.5306122 & n = 1\\\\\n\\hline\n28 & w & 1 & 1 & 0.5510204 & 1 & 1 & 0.5000000 & 0.5510204 & n = 1\\\\\n\\hline\n29 & w & 1 & 1 & 0.5714286 & 1 & 1 & 0.5000000 & 0.5714286 & n = 1\\\\\n\\hline\n30 & w & 1 & 1 & 0.5918367 & 1 & 1 & 0.5000000 & 0.5918367 & n = 1\\\\\n\\hline\n31 & w & 1 & 1 & 0.6122449 & 1 & 1 & 0.5000000 & 0.6122449 & n = 1\\\\\n\\hline\n32 & w & 1 & 1 & 0.6326531 & 1 & 1 & 0.5000000 & 0.6326531 & n = 1\\\\\n\\hline\n33 & w & 1 & 1 & 0.6530612 & 1 & 1 & 0.5000000 & 0.6530612 & n = 1\\\\\n\\hline\n34 & w & 1 & 1 & 0.6734694 & 1 & 1 & 0.5000000 & 0.6734694 & n = 1\\\\\n\\hline\n35 & w & 1 & 1 & 0.6938776 & 1 & 1 & 0.5000000 & 0.6938776 & n = 1\\\\\n\\hline\n36 & w & 1 & 1 & 0.7142857 & 1 & 1 & 0.5000000 & 0.7142857 & n = 1\\\\\n\\hline\n37 & w & 1 & 1 & 0.7346939 & 1 & 1 & 0.5000000 & 0.7346939 & n = 1\\\\\n\\hline\n38 & w & 1 & 1 & 0.7551020 & 1 & 1 & 0.5000000 & 0.7551020 & n = 1\\\\\n\\hline\n39 & w & 1 & 1 & 0.7755102 & 1 & 1 & 0.5000000 & 0.7755102 & n = 1\\\\\n\\hline\n40 & w & 1 & 1 & 0.7959184 & 1 & 1 & 0.5000000 & 0.7959184 & n = 1\\\\\n\\hline\n41 & w & 1 & 1 & 0.8163265 & 1 & 1 & 0.5000000 & 0.8163265 & n = 1\\\\\n\\hline\n42 & w & 1 & 1 & 0.8367347 & 1 & 1 & 0.5000000 & 0.8367347 & n = 1\\\\\n\\hline\n43 & w & 1 & 1 & 0.8571429 & 1 & 1 & 0.5000000 & 0.8571429 & n = 1\\\\\n\\hline\n44 & w & 1 & 1 & 0.8775510 & 1 & 1 & 0.5000000 & 0.8775510 & n = 1\\\\\n\\hline\n45 & w & 1 & 1 & 0.8979592 & 1 & 1 & 0.5000000 & 0.8979592 & n = 1\\\\\n\\hline\n46 & w & 1 & 1 & 0.9183673 & 1 & 1 & 0.5000000 & 0.9183673 & n = 1\\\\\n\\hline\n47 & w & 1 & 1 & 0.9387755 & 1 & 1 & 0.5000000 & 0.9387755 & n = 1\\\\\n\\hline\n48 & w & 1 & 1 & 0.9591837 & 1 & 1 & 0.5000000 & 0.9591837 & n = 1\\\\\n\\hline\n49 & w & 1 & 1 & 0.9795918 & 1 & 1 & 0.5000000 & 0.9795918 & n = 1\\\\\n\\hline\n50 & w & 1 & 1 & 1.0000000 & 1 & 1 & 0.5000000 & 1.0000000 & n = 1\\\\\n\\hline\n51 & l & 2 & 1 & 0.0000000 & 1 & 1 & 0.0000000 & 0.0000000 & n = 2\\\\\n\\hline\n52 & l & 2 & 1 & 0.0204082 & 1 & 1 & 0.0204082 & 0.0399833 & n = 2\\\\\n\\hline\n53 & l & 2 & 1 & 0.0408163 & 1 & 1 & 0.0408163 & 0.0783007 & n = 2\\\\\n\\hline\n54 & l & 2 & 1 & 0.0612245 & 1 & 1 & 0.0612245 & 0.1149521 & n = 2\\\\\n\\hline\n55 & l & 2 & 1 & 0.0816327 & 1 & 1 & 0.0816327 & 0.1499375 & n = 2\\\\\n\\hline\n56 & l & 2 & 1 & 0.1020408 & 1 & 1 & 0.1020408 & 0.1832570 & n = 2\\\\\n\\hline\n57 & l & 2 & 1 & 0.1224490 & 1 & 1 & 0.1224490 & 0.2149105 & n = 2\\\\\n\\hline\n58 & l & 2 & 1 & 0.1428571 & 1 & 1 & 0.1428571 & 0.2448980 & n = 2\\\\\n\\hline\n59 & l & 2 & 1 & 0.1632653 & 1 & 1 & 0.1632653 & 0.2732195 & n = 2\\\\\n\\hline\n60 & l & 2 & 1 & 0.1836735 & 1 & 1 & 0.1836735 & 0.2998751 & n = 2\\\\\n\\hline\n61 & l & 2 & 1 & 0.2040816 & 1 & 1 & 0.2040816 & 0.3248646 & n = 2\\\\\n\\hline\n62 & l & 2 & 1 & 0.2244898 & 1 & 1 & 0.2244898 & 0.3481883 & n = 2\\\\\n\\hline\n63 & l & 2 & 1 & 0.2448980 & 1 & 1 & 0.2448980 & 0.3698459 & n = 2\\\\\n\\hline\n64 & l & 2 & 1 & 0.2653061 & 1 & 1 & 0.2653061 & 0.3898376 & n = 2\\\\\n\\hline\n65 & l & 2 & 1 & 0.2857143 & 1 & 1 & 0.2857143 & 0.4081633 & n = 2\\\\\n\\hline\n66 & l & 2 & 1 & 0.3061224 & 1 & 1 & 0.3061224 & 0.4248230 & n = 2\\\\\n\\hline\n67 & l & 2 & 1 & 0.3265306 & 1 & 1 & 0.3265306 & 0.4398167 & n = 2\\\\\n\\hline\n68 & l & 2 & 1 & 0.3469388 & 1 & 1 & 0.3469388 & 0.4531445 & n = 2\\\\\n\\hline\n69 & l & 2 & 1 & 0.3673469 & 1 & 1 & 0.3673469 & 0.4648063 & n = 2\\\\\n\\hline\n70 & l & 2 & 1 & 0.3877551 & 1 & 1 & 0.3877551 & 0.4748022 & n = 2\\\\\n\\hline\n71 & l & 2 & 1 & 0.4081633 & 1 & 1 & 0.4081633 & 0.4831320 & n = 2\\\\\n\\hline\n72 & l & 2 & 1 & 0.4285714 & 1 & 1 & 0.4285714 & 0.4897959 & n = 2\\\\\n\\hline\n73 & l & 2 & 1 & 0.4489796 & 1 & 1 & 0.4489796 & 0.4947938 & n = 2\\\\\n\\hline\n74 & l & 2 & 1 & 0.4693878 & 1 & 1 & 0.4693878 & 0.4981258 & n = 2\\\\\n\\hline\n75 & l & 2 & 1 & 0.4897959 & 1 & 1 & 0.4897959 & 0.4997918 & n = 2\\\\\n\\hline\n76 & l & 2 & 1 & 0.5102041 & 1 & 1 & 0.5102041 & 0.4997918 & n = 2\\\\\n\\hline\n77 & l & 2 & 1 & 0.5306122 & 1 & 1 & 0.5306122 & 0.4981258 & n = 2\\\\\n\\hline\n78 & l & 2 & 1 & 0.5510204 & 1 & 1 & 0.5510204 & 0.4947938 & n = 2\\\\\n\\hline\n79 & l & 2 & 1 & 0.5714286 & 1 & 1 & 0.5714286 & 0.4897959 & n = 2\\\\\n\\hline\n80 & l & 2 & 1 & 0.5918367 & 1 & 1 & 0.5918367 & 0.4831320 & n = 2\\\\\n\\hline\n81 & l & 2 & 1 & 0.6122449 & 1 & 1 & 0.6122449 & 0.4748022 & n = 2\\\\\n\\hline\n82 & l & 2 & 1 & 0.6326531 & 1 & 1 & 0.6326531 & 0.4648063 & n = 2\\\\\n\\hline\n83 & l & 2 & 1 & 0.6530612 & 1 & 1 & 0.6530612 & 0.4531445 & n = 2\\\\\n\\hline\n84 & l & 2 & 1 & 0.6734694 & 1 & 1 & 0.6734694 & 0.4398167 & n = 2\\\\\n\\hline\n85 & l & 2 & 1 & 0.6938776 & 1 & 1 & 0.6938776 & 0.4248230 & n = 2\\\\\n\\hline\n86 & l & 2 & 1 & 0.7142857 & 1 & 1 & 0.7142857 & 0.4081633 & n = 2\\\\\n\\hline\n87 & l & 2 & 1 & 0.7346939 & 1 & 1 & 0.7346939 & 0.3898376 & n = 2\\\\\n\\hline\n88 & l & 2 & 1 & 0.7551020 & 1 & 1 & 0.7551020 & 0.3698459 & n = 2\\\\\n\\hline\n89 & l & 2 & 1 & 0.7755102 & 1 & 1 & 0.7755102 & 0.3481883 & n = 2\\\\\n\\hline\n90 & l & 2 & 1 & 0.7959184 & 1 & 1 & 0.7959184 & 0.3248646 & n = 2\\\\\n\\hline\n91 & l & 2 & 1 & 0.8163265 & 1 & 1 & 0.8163265 & 0.2998751 & n = 2\\\\\n\\hline\n92 & l & 2 & 1 & 0.8367347 & 1 & 1 & 0.8367347 & 0.2732195 & n = 2\\\\\n\\hline\n93 & l & 2 & 1 & 0.8571429 & 1 & 1 & 0.8571429 & 0.2448980 & n = 2\\\\\n\\hline\n94 & l & 2 & 1 & 0.8775510 & 1 & 1 & 0.8775510 & 0.2149105 & n = 2\\\\\n\\hline\n95 & l & 2 & 1 & 0.8979592 & 1 & 1 & 0.8979592 & 0.1832570 & n = 2\\\\\n\\hline\n96 & l & 2 & 1 & 0.9183673 & 1 & 1 & 0.9183673 & 0.1499375 & n = 2\\\\\n\\hline\n97 & l & 2 & 1 & 0.9387755 & 1 & 1 & 0.9387755 & 0.1149521 & n = 2\\\\\n\\hline\n98 & l & 2 & 1 & 0.9591837 & 1 & 1 & 0.9591837 & 0.0783007 & n = 2\\\\\n\\hline\n99 & l & 2 & 1 & 0.9795918 & 1 & 1 & 0.9795918 & 0.0399833 & n = 2\\\\\n\\hline\n100 & l & 2 & 1 & 1.0000000 & 1 & 1 & 1.0000000 & 0.0000000 & n = 2\\\\\n\\hline\n101 & w & 3 & 2 & 0.0000000 & 2 & 1 & 0.0000000 & 0.0000000 & n = 3\\\\\n\\hline\n102 & w & 3 & 2 & 0.0204082 & 2 & 1 & 0.0399833 & 0.0012240 & n = 3\\\\\n\\hline\n103 & w & 3 & 2 & 0.0408163 & 2 & 1 & 0.0783007 & 0.0047939 & n = 3\\\\\n\\hline\n104 & w & 3 & 2 & 0.0612245 & 2 & 1 & 0.1149521 & 0.0105568 & n = 3\\\\\n\\hline\n105 & w & 3 & 2 & 0.0816327 & 2 & 1 & 0.1499375 & 0.0183597 & n = 3\\\\\n\\hline\n106 & w & 3 & 2 & 0.1020408 & 2 & 1 & 0.1832570 & 0.0280495 & n = 3\\\\\n\\hline\n107 & w & 3 & 2 & 0.1224490 & 2 & 1 & 0.2149105 & 0.0394733 & n = 3\\\\\n\\hline\n108 & w & 3 & 2 & 0.1428571 & 2 & 1 & 0.2448980 & 0.0524781 & n = 3\\\\\n\\hline\n109 & w & 3 & 2 & 0.1632653 & 2 & 1 & 0.2732195 & 0.0669109 & n = 3\\\\\n\\hline\n110 & w & 3 & 2 & 0.1836735 & 2 & 1 & 0.2998751 & 0.0826186 & n = 3\\\\\n\\hline\n111 & w & 3 & 2 & 0.2040816 & 2 & 1 & 0.3248646 & 0.0994484 & n = 3\\\\\n\\hline\n112 & w & 3 & 2 & 0.2244898 & 2 & 1 & 0.3481883 & 0.1172471 & n = 3\\\\\n\\hline\n113 & w & 3 & 2 & 0.2448980 & 2 & 1 & 0.3698459 & 0.1358618 & n = 3\\\\\n\\hline\n114 & w & 3 & 2 & 0.2653061 & 2 & 1 & 0.3898376 & 0.1551394 & n = 3\\\\\n\\hline\n115 & w & 3 & 2 & 0.2857143 & 2 & 1 & 0.4081633 & 0.1749271 & n = 3\\\\\n\\hline\n116 & w & 3 & 2 & 0.3061224 & 2 & 1 & 0.4248230 & 0.1950718 & n = 3\\\\\n\\hline\n117 & w & 3 & 2 & 0.3265306 & 2 & 1 & 0.4398167 & 0.2154204 & n = 3\\\\\n\\hline\n118 & w & 3 & 2 & 0.3469388 & 2 & 1 & 0.4531445 & 0.2358201 & n = 3\\\\\n\\hline\n119 & w & 3 & 2 & 0.3673469 & 2 & 1 & 0.4648063 & 0.2561178 & n = 3\\\\\n\\hline\n120 & w & 3 & 2 & 0.3877551 & 2 & 1 & 0.4748022 & 0.2761604 & n = 3\\\\\n\\hline\n121 & w & 3 & 2 & 0.4081633 & 2 & 1 & 0.4831320 & 0.2957951 & n = 3\\\\\n\\hline\n122 & w & 3 & 2 & 0.4285714 & 2 & 1 & 0.4897959 & 0.3148688 & n = 3\\\\\n\\hline\n123 & w & 3 & 2 & 0.4489796 & 2 & 1 & 0.4947938 & 0.3332285 & n = 3\\\\\n\\hline\n124 & w & 3 & 2 & 0.4693878 & 2 & 1 & 0.4981258 & 0.3507212 & n = 3\\\\\n\\hline\n125 & w & 3 & 2 & 0.4897959 & 2 & 1 & 0.4997918 & 0.3671939 & n = 3\\\\\n\\hline\n126 & w & 3 & 2 & 0.5102041 & 2 & 1 & 0.4997918 & 0.3824937 & n = 3\\\\\n\\hline\n127 & w & 3 & 2 & 0.5306122 & 2 & 1 & 0.4981258 & 0.3964675 & n = 3\\\\\n\\hline\n128 & w & 3 & 2 & 0.5510204 & 2 & 1 & 0.4947938 & 0.4089623 & n = 3\\\\\n\\hline\n129 & w & 3 & 2 & 0.5714286 & 2 & 1 & 0.4897959 & 0.4198251 & n = 3\\\\\n\\hline\n130 & w & 3 & 2 & 0.5918367 & 2 & 1 & 0.4831320 & 0.4289029 & n = 3\\\\\n\\hline\n131 & w & 3 & 2 & 0.6122449 & 2 & 1 & 0.4748022 & 0.4360428 & n = 3\\\\\n\\hline\n132 & w & 3 & 2 & 0.6326531 & 2 & 1 & 0.4648063 & 0.4410917 & n = 3\\\\\n\\hline\n133 & w & 3 & 2 & 0.6530612 & 2 & 1 & 0.4531445 & 0.4438967 & n = 3\\\\\n\\hline\n134 & w & 3 & 2 & 0.6734694 & 2 & 1 & 0.4398167 & 0.4443047 & n = 3\\\\\n\\hline\n135 & w & 3 & 2 & 0.6938776 & 2 & 1 & 0.4248230 & 0.4421627 & n = 3\\\\\n\\hline\n136 & w & 3 & 2 & 0.7142857 & 2 & 1 & 0.4081633 & 0.4373178 & n = 3\\\\\n\\hline\n137 & w & 3 & 2 & 0.7346939 & 2 & 1 & 0.3898376 & 0.4296169 & n = 3\\\\\n\\hline\n138 & w & 3 & 2 & 0.7551020 & 2 & 1 & 0.3698459 & 0.4189071 & n = 3\\\\\n\\hline\n139 & w & 3 & 2 & 0.7755102 & 2 & 1 & 0.3481883 & 0.4050353 & n = 3\\\\\n\\hline\n140 & w & 3 & 2 & 0.7959184 & 2 & 1 & 0.3248646 & 0.3878486 & n = 3\\\\\n\\hline\n141 & w & 3 & 2 & 0.8163265 & 2 & 1 & 0.2998751 & 0.3671939 & n = 3\\\\\n\\hline\n142 & w & 3 & 2 & 0.8367347 & 2 & 1 & 0.2732195 & 0.3429183 & n = 3\\\\\n\\hline\n143 & w & 3 & 2 & 0.8571429 & 2 & 1 & 0.2448980 & 0.3148688 & n = 3\\\\\n\\hline\n144 & w & 3 & 2 & 0.8775510 & 2 & 1 & 0.2149105 & 0.2828923 & n = 3\\\\\n\\hline\n145 & w & 3 & 2 & 0.8979592 & 2 & 1 & 0.1832570 & 0.2468359 & n = 3\\\\\n\\hline\n146 & w & 3 & 2 & 0.9183673 & 2 & 1 & 0.1499375 & 0.2065466 & n = 3\\\\\n\\hline\n147 & w & 3 & 2 & 0.9387755 & 2 & 1 & 0.1149521 & 0.1618713 & n = 3\\\\\n\\hline\n148 & w & 3 & 2 & 0.9591837 & 2 & 1 & 0.0783007 & 0.1126571 & n = 3\\\\\n\\hline\n149 & w & 3 & 2 & 0.9795918 & 2 & 1 & 0.0399833 & 0.0587510 & n = 3\\\\\n\\hline\n150 & w & 3 & 2 & 1.0000000 & 2 & 1 & 0.0000000 & 0.0000000 & n = 3\\\\\n\\hline\n151 & w & 4 & 3 & 0.0000000 & 3 & 2 & 0.0000000 & 0.0000000 & n = 4\\\\\n\\hline\n152 & w & 4 & 3 & 0.0204082 & 3 & 2 & 0.0012240 & 0.0000333 & n = 4\\\\\n\\hline\n153 & w & 4 & 3 & 0.0408163 & 3 & 2 & 0.0047939 & 0.0002609 & n = 4\\\\\n\\hline\n154 & w & 4 & 3 & 0.0612245 & 3 & 2 & 0.0105568 & 0.0008618 & n = 4\\\\\n\\hline\n155 & w & 4 & 3 & 0.0816327 & 3 & 2 & 0.0183597 & 0.0019983 & n = 4\\\\\n\\hline\n156 & w & 4 & 3 & 0.1020408 & 3 & 2 & 0.0280495 & 0.0038163 & n = 4\\\\\n\\hline\n157 & w & 4 & 3 & 0.1224490 & 3 & 2 & 0.0394733 & 0.0064446 & n = 4\\\\\n\\hline\n158 & w & 4 & 3 & 0.1428571 & 3 & 2 & 0.0524781 & 0.0099958 & n = 4\\\\\n\\hline\n159 & w & 4 & 3 & 0.1632653 & 3 & 2 & 0.0669109 & 0.0145656 & n = 4\\\\\n\\hline\n160 & w & 4 & 3 & 0.1836735 & 3 & 2 & 0.0826186 & 0.0202331 & n = 4\\\\\n\\hline\n161 & w & 4 & 3 & 0.2040816 & 3 & 2 & 0.0994484 & 0.0270608 & n = 4\\\\\n\\hline\n162 & w & 4 & 3 & 0.2244898 & 3 & 2 & 0.1172471 & 0.0350944 & n = 4\\\\\n\\hline\n163 & w & 4 & 3 & 0.2448980 & 3 & 2 & 0.1358618 & 0.0443630 & n = 4\\\\\n\\hline\n164 & w & 4 & 3 & 0.2653061 & 3 & 2 & 0.1551394 & 0.0548793 & n = 4\\\\\n\\hline\n165 & w & 4 & 3 & 0.2857143 & 3 & 2 & 0.1749271 & 0.0666389 & n = 4\\\\\n\\hline\n166 & w & 4 & 3 & 0.3061224 & 3 & 2 & 0.1950718 & 0.0796211 & n = 4\\\\\n\\hline\n167 & w & 4 & 3 & 0.3265306 & 3 & 2 & 0.2154204 & 0.0937885 & n = 4\\\\\n\\hline\n168 & w & 4 & 3 & 0.3469388 & 3 & 2 & 0.2358201 & 0.1090869 & n = 4\\\\\n\\hline\n169 & w & 4 & 3 & 0.3673469 & 3 & 2 & 0.2561178 & 0.1254454 & n = 4\\\\\n\\hline\n170 & w & 4 & 3 & 0.3877551 & 3 & 2 & 0.2761604 & 0.1427768 & n = 4\\\\\n\\hline\n171 & w & 4 & 3 & 0.4081633 & 3 & 2 & 0.2957951 & 0.1609769 & n = 4\\\\\n\\hline\n172 & w & 4 & 3 & 0.4285714 & 3 & 2 & 0.3148688 & 0.1799250 & n = 4\\\\\n\\hline\n173 & w & 4 & 3 & 0.4489796 & 3 & 2 & 0.3332285 & 0.1994837 & n = 4\\\\\n\\hline\n174 & w & 4 & 3 & 0.4693878 & 3 & 2 & 0.3507212 & 0.2194990 & n = 4\\\\\n\\hline\n175 & w & 4 & 3 & 0.4897959 & 3 & 2 & 0.3671939 & 0.2398001 & n = 4\\\\\n\\hline\n176 & w & 4 & 3 & 0.5102041 & 3 & 2 & 0.3824937 & 0.2601998 & n = 4\\\\\n\\hline\n177 & w & 4 & 3 & 0.5306122 & 3 & 2 & 0.3964675 & 0.2804940 & n = 4\\\\\n\\hline\n178 & w & 4 & 3 & 0.5510204 & 3 & 2 & 0.4089623 & 0.3004621 & n = 4\\\\\n\\hline\n179 & w & 4 & 3 & 0.5714286 & 3 & 2 & 0.4198251 & 0.3198667 & n = 4\\\\\n\\hline\n180 & w & 4 & 3 & 0.5918367 & 3 & 2 & 0.4289029 & 0.3384540 & n = 4\\\\\n\\hline\n181 & w & 4 & 3 & 0.6122449 & 3 & 2 & 0.4360428 & 0.3559533 & n = 4\\\\\n\\hline\n182 & w & 4 & 3 & 0.6326531 & 3 & 2 & 0.4410917 & 0.3720774 & n = 4\\\\\n\\hline\n183 & w & 4 & 3 & 0.6530612 & 3 & 2 & 0.4438967 & 0.3865223 & n = 4\\\\\n\\hline\n184 & w & 4 & 3 & 0.6734694 & 3 & 2 & 0.4443047 & 0.3989675 & n = 4\\\\\n\\hline\n185 & w & 4 & 3 & 0.6938776 & 3 & 2 & 0.4421627 & 0.4090757 & n = 4\\\\\n\\hline\n186 & w & 4 & 3 & 0.7142857 & 3 & 2 & 0.4373178 & 0.4164931 & n = 4\\\\\n\\hline\n187 & w & 4 & 3 & 0.7346939 & 3 & 2 & 0.4296169 & 0.4208492 & n = 4\\\\\n\\hline\n188 & w & 4 & 3 & 0.7551020 & 3 & 2 & 0.4189071 & 0.4217568 & n = 4\\\\\n\\hline\n189 & w & 4 & 3 & 0.7755102 & 3 & 2 & 0.4050353 & 0.4188120 & n = 4\\\\\n\\hline\n190 & w & 4 & 3 & 0.7959184 & 3 & 2 & 0.3878486 & 0.4115944 & n = 4\\\\\n\\hline\n191 & w & 4 & 3 & 0.8163265 & 3 & 2 & 0.3671939 & 0.3996669 & n = 4\\\\\n\\hline\n192 & w & 4 & 3 & 0.8367347 & 3 & 2 & 0.3429183 & 0.3825756 & n = 4\\\\\n\\hline\n193 & w & 4 & 3 & 0.8571429 & 3 & 2 & 0.3148688 & 0.3598501 & n = 4\\\\\n\\hline\n194 & w & 4 & 3 & 0.8775510 & 3 & 2 & 0.2828923 & 0.3310033 & n = 4\\\\\n\\hline\n195 & w & 4 & 3 & 0.8979592 & 3 & 2 & 0.2468359 & 0.2955315 & n = 4\\\\\n\\hline\n196 & w & 4 & 3 & 0.9183673 & 3 & 2 & 0.2065466 & 0.2529142 & n = 4\\\\\n\\hline\n197 & w & 4 & 3 & 0.9387755 & 3 & 2 & 0.1618713 & 0.2026145 & n = 4\\\\\n\\hline\n198 & w & 4 & 3 & 0.9591837 & 3 & 2 & 0.1126571 & 0.1440785 & n = 4\\\\\n\\hline\n199 & w & 4 & 3 & 0.9795918 & 3 & 2 & 0.0587510 & 0.0767360 & n = 4\\\\\n\\hline\n200 & w & 4 & 3 & 1.0000000 & 3 & 2 & 0.0000000 & 0.0000000 & n = 4\\\\\n\\hline\n201 & w & 5 & 4 & 0.0000000 & 4 & 3 & 0.0000000 & 0.0000000 & n = 5\\\\\n\\hline\n202 & w & 5 & 4 & 0.0204082 & 4 & 3 & 0.0000333 & 0.0000008 & n = 5\\\\\n\\hline\n203 & w & 5 & 4 & 0.0408163 & 4 & 3 & 0.0002609 & 0.0000133 & n = 5\\\\\n\\hline\n204 & w & 5 & 4 & 0.0612245 & 4 & 3 & 0.0008618 & 0.0000660 & n = 5\\\\\n\\hline\n205 & w & 5 & 4 & 0.0816327 & 4 & 3 & 0.0019983 & 0.0002039 & n = 5\\\\\n\\hline\n206 & w & 5 & 4 & 0.1020408 & 4 & 3 & 0.0038163 & 0.0004868 & n = 5\\\\\n\\hline\n207 & w & 5 & 4 & 0.1224490 & 4 & 3 & 0.0064446 & 0.0009864 & n = 5\\\\\n\\hline\n208 & w & 5 & 4 & 0.1428571 & 4 & 3 & 0.0099958 & 0.0017850 & n = 5\\\\\n\\hline\n209 & w & 5 & 4 & 0.1632653 & 4 & 3 & 0.0145656 & 0.0029726 & n = 5\\\\\n\\hline\n210 & w & 5 & 4 & 0.1836735 & 4 & 3 & 0.0202331 & 0.0046454 & n = 5\\\\\n\\hline\n211 & w & 5 & 4 & 0.2040816 & 4 & 3 & 0.0270608 & 0.0069033 & n = 5\\\\\n\\hline\n212 & w & 5 & 4 & 0.2244898 & 4 & 3 & 0.0350944 & 0.0098479 & n = 5\\\\\n\\hline\n213 & w & 5 & 4 & 0.2448980 & 4 & 3 & 0.0443630 & 0.0135805 & n = 5\\\\\n\\hline\n214 & w & 5 & 4 & 0.2653061 & 4 & 3 & 0.0548793 & 0.0181998 & n = 5\\\\\n\\hline\n215 & w & 5 & 4 & 0.2857143 & 4 & 3 & 0.0666389 & 0.0237996 & n = 5\\\\\n\\hline\n216 & w & 5 & 4 & 0.3061224 & 4 & 3 & 0.0796211 & 0.0304673 & n = 5\\\\\n\\hline\n217 & w & 5 & 4 & 0.3265306 & 4 & 3 & 0.0937885 & 0.0382810 & n = 5\\\\\n\\hline\n218 & w & 5 & 4 & 0.3469388 & 4 & 3 & 0.1090869 & 0.0473081 & n = 5\\\\\n\\hline\n219 & w & 5 & 4 & 0.3673469 & 4 & 3 & 0.1254454 & 0.0576025 & n = 5\\\\\n\\hline\n220 & w & 5 & 4 & 0.3877551 & 4 & 3 & 0.1427768 & 0.0692031 & n = 5\\\\\n\\hline\n221 & w & 5 & 4 & 0.4081633 & 4 & 3 & 0.1609769 & 0.0821311 & n = 5\\\\\n\\hline\n222 & w & 5 & 4 & 0.4285714 & 4 & 3 & 0.1799250 & 0.0963884 & n = 5\\\\\n\\hline\n223 & w & 5 & 4 & 0.4489796 & 4 & 3 & 0.1994837 & 0.1119552 & n = 5\\\\\n\\hline\n224 & w & 5 & 4 & 0.4693878 & 4 & 3 & 0.2194990 & 0.1287877 & n = 5\\\\\n\\hline\n225 & w & 5 & 4 & 0.4897959 & 4 & 3 & 0.2398001 & 0.1468164 & n = 5\\\\\n\\hline\n226 & w & 5 & 4 & 0.5102041 & 4 & 3 & 0.2601998 & 0.1659437 & n = 5\\\\\n\\hline\n227 & w & 5 & 4 & 0.5306122 & 4 & 3 & 0.2804940 & 0.1860419 & n = 5\\\\\n\\hline\n228 & w & 5 & 4 & 0.5510204 & 4 & 3 & 0.3004621 & 0.2069509 & n = 5\\\\\n\\hline\n229 & w & 5 & 4 & 0.5714286 & 4 & 3 & 0.3198667 & 0.2284762 & n = 5\\\\\n\\hline\n230 & w & 5 & 4 & 0.5918367 & 4 & 3 & 0.3384540 & 0.2503869 & n = 5\\\\\n\\hline\n231 & w & 5 & 4 & 0.6122449 & 4 & 3 & 0.3559533 & 0.2724132 & n = 5\\\\\n\\hline\n232 & w & 5 & 4 & 0.6326531 & 4 & 3 & 0.3720774 & 0.2942449 & n = 5\\\\\n\\hline\n233 & w & 5 & 4 & 0.6530612 & 4 & 3 & 0.3865223 & 0.3155284 & n = 5\\\\\n\\hline\n234 & w & 5 & 4 & 0.6734694 & 4 & 3 & 0.3989675 & 0.3358655 & n = 5\\\\\n\\hline\n235 & w & 5 & 4 & 0.6938776 & 4 & 3 & 0.4090757 & 0.3548106 & n = 5\\\\\n\\hline\n236 & w & 5 & 4 & 0.7142857 & 4 & 3 & 0.4164931 & 0.3718689 & n = 5\\\\\n\\hline\n237 & w & 5 & 4 & 0.7346939 & 4 & 3 & 0.4208492 & 0.3864942 & n = 5\\\\\n\\hline\n238 & w & 5 & 4 & 0.7551020 & 4 & 3 & 0.4217568 & 0.3980868 & n = 5\\\\\n\\hline\n239 & w & 5 & 4 & 0.7755102 & 4 & 3 & 0.4188120 & 0.4059913 & n = 5\\\\\n\\hline\n240 & w & 5 & 4 & 0.7959184 & 4 & 3 & 0.4115944 & 0.4094945 & n = 5\\\\\n\\hline\n241 & w & 5 & 4 & 0.8163265 & 4 & 3 & 0.3996669 & 0.4078233 & n = 5\\\\\n\\hline\n242 & w & 5 & 4 & 0.8367347 & 4 & 3 & 0.3825756 & 0.4001428 & n = 5\\\\\n\\hline\n243 & w & 5 & 4 & 0.8571429 & 4 & 3 & 0.3598501 & 0.3855536 & n = 5\\\\\n\\hline\n244 & w & 5 & 4 & 0.8775510 & 4 & 3 & 0.3310033 & 0.3630903 & n = 5\\\\\n\\hline\n245 & w & 5 & 4 & 0.8979592 & 4 & 3 & 0.2955315 & 0.3317190 & n = 5\\\\\n\\hline\n246 & w & 5 & 4 & 0.9183673 & 4 & 3 & 0.2529142 & 0.2903352 & n = 5\\\\\n\\hline\n247 & w & 5 & 4 & 0.9387755 & 4 & 3 & 0.2026145 & 0.2377619 & n = 5\\\\\n\\hline\n248 & w & 5 & 4 & 0.9591837 & 4 & 3 & 0.1440785 & 0.1727472 & n = 5\\\\\n\\hline\n249 & w & 5 & 4 & 0.9795918 & 4 & 3 & 0.0767360 & 0.0939625 & n = 5\\\\\n\\hline\n250 & w & 5 & 4 & 1.0000000 & 4 & 3 & 0.0000000 & 0.0000000 & n = 5\\\\\n\\hline\n251 & l & 6 & 4 & 0.0000000 & 5 & 4 & 0.0000000 & 0.0000000 & n = 6\\\\\n\\hline\n252 & l & 6 & 4 & 0.0204082 & 5 & 4 & 0.0000008 & 0.0000025 & n = 6\\\\\n\\hline\n253 & l & 6 & 4 & 0.0408163 & 5 & 4 & 0.0000133 & 0.0000383 & n = 6\\\\\n\\hline\n254 & l & 6 & 4 & 0.0612245 & 5 & 4 & 0.0000660 & 0.0001857 & n = 6\\\\\n\\hline\n255 & l & 6 & 4 & 0.0816327 & 5 & 4 & 0.0002039 & 0.0005618 & n = 6\\\\\n\\hline\n256 & l & 6 & 4 & 0.1020408 & 5 & 4 & 0.0004868 & 0.0013113 & n = 6\\\\\n\\hline\n257 & l & 6 & 4 & 0.1224490 & 5 & 4 & 0.0009864 & 0.0025969 & n = 6\\\\\n\\hline\n258 & l & 6 & 4 & 0.1428571 & 5 & 4 & 0.0017850 & 0.0045899 & n = 6\\\\\n\\hline\n259 & l & 6 & 4 & 0.1632653 & 5 & 4 & 0.0029726 & 0.0074618 & n = 6\\\\\n\\hline\n260 & l & 6 & 4 & 0.1836735 & 5 & 4 & 0.0046454 & 0.0113764 & n = 6\\\\\n\\hline\n261 & l & 6 & 4 & 0.2040816 & 5 & 4 & 0.0069033 & 0.0164833 & n = 6\\\\\n\\hline\n262 & l & 6 & 4 & 0.2244898 & 5 & 4 & 0.0098479 & 0.0229115 & n = 6\\\\\n\\hline\n263 & l & 6 & 4 & 0.2448980 & 5 & 4 & 0.0135805 & 0.0307640 & n = 6\\\\\n\\hline\n264 & l & 6 & 4 & 0.2653061 & 5 & 4 & 0.0181998 & 0.0401137 & n = 6\\\\\n\\hline\n265 & l & 6 & 4 & 0.2857143 & 5 & 4 & 0.0237996 & 0.0509992 & n = 6\\\\\n\\hline\n266 & l & 6 & 4 & 0.3061224 & 5 & 4 & 0.0304673 & 0.0634217 & n = 6\\\\\n\\hline\n267 & l & 6 & 4 & 0.3265306 & 5 & 4 & 0.0382810 & 0.0773433 & n = 6\\\\\n\\hline\n268 & l & 6 & 4 & 0.3469388 & 5 & 4 & 0.0473081 & 0.0926852 & n = 6\\\\\n\\hline\n269 & l & 6 & 4 & 0.3673469 & 5 & 4 & 0.0576025 & 0.1093272 & n = 6\\\\\n\\hline\n270 & l & 6 & 4 & 0.3877551 & 5 & 4 & 0.0692031 & 0.1271077 & n = 6\\\\\n\\hline\n271 & l & 6 & 4 & 0.4081633 & 5 & 4 & 0.0821311 & 0.1458246 & n = 6\\\\\n\\hline\n272 & l & 6 & 4 & 0.4285714 & 5 & 4 & 0.0963884 & 0.1652373 & n = 6\\\\\n\\hline\n273 & l & 6 & 4 & 0.4489796 & 5 & 4 & 0.1119552 & 0.1850687 & n = 6\\\\\n\\hline\n274 & l & 6 & 4 & 0.4693878 & 5 & 4 & 0.1287877 & 0.2050089 & n = 6\\\\\n\\hline\n275 & l & 6 & 4 & 0.4897959 & 5 & 4 & 0.1468164 & 0.2247190 & n = 6\\\\\n\\hline\n276 & l & 6 & 4 & 0.5102041 & 5 & 4 & 0.1659437 & 0.2438357 & n = 6\\\\\n\\hline\n277 & l & 6 & 4 & 0.5306122 & 5 & 4 & 0.1860419 & 0.2619774 & n = 6\\\\\n\\hline\n278 & l & 6 & 4 & 0.5510204 & 5 & 4 & 0.2069509 & 0.2787502 & n = 6\\\\\n\\hline\n279 & l & 6 & 4 & 0.5714286 & 5 & 4 & 0.2284762 & 0.2937552 & n = 6\\\\\n\\hline\n280 & l & 6 & 4 & 0.5918367 & 5 & 4 & 0.2503869 & 0.3065962 & n = 6\\\\\n\\hline\n281 & l & 6 & 4 & 0.6122449 & 5 & 4 & 0.2724132 & 0.3168889 & n = 6\\\\\n\\hline\n282 & l & 6 & 4 & 0.6326531 & 5 & 4 & 0.2942449 & 0.3242698 & n = 6\\\\\n\\hline\n283 & l & 6 & 4 & 0.6530612 & 5 & 4 & 0.3155284 & 0.3284071 & n = 6\\\\\n\\hline\n284 & l & 6 & 4 & 0.6734694 & 5 & 4 & 0.3358655 & 0.3290111 & n = 6\\\\\n\\hline\n285 & l & 6 & 4 & 0.6938776 & 5 & 4 & 0.3548106 & 0.3258464 & n = 6\\\\\n\\hline\n286 & l & 6 & 4 & 0.7142857 & 5 & 4 & 0.3718689 & 0.3187447 & n = 6\\\\\n\\hline\n287 & l & 6 & 4 & 0.7346939 & 5 & 4 & 0.3864942 & 0.3076178 & n = 6\\\\\n\\hline\n288 & l & 6 & 4 & 0.7551020 & 5 & 4 & 0.3980868 & 0.2924719 & n = 6\\\\\n\\hline\n289 & l & 6 & 4 & 0.7755102 & 5 & 4 & 0.4059913 & 0.2734227 & n = 6\\\\\n\\hline\n290 & l & 6 & 4 & 0.7959184 & 5 & 4 & 0.4094945 & 0.2507109 & n = 6\\\\\n\\hline\n291 & l & 6 & 4 & 0.8163265 & 5 & 4 & 0.4078233 & 0.2247190 & n = 6\\\\\n\\hline\n292 & l & 6 & 4 & 0.8367347 & 5 & 4 & 0.4001428 & 0.1959883 & n = 6\\\\\n\\hline\n293 & l & 6 & 4 & 0.8571429 & 5 & 4 & 0.3855536 & 0.1652373 & n = 6\\\\\n\\hline\n294 & l & 6 & 4 & 0.8775510 & 5 & 4 & 0.3630903 & 0.1333801 & n = 6\\\\\n\\hline\n295 & l & 6 & 4 & 0.8979592 & 5 & 4 & 0.3317190 & 0.1015466 & n = 6\\\\\n\\hline\n296 & l & 6 & 4 & 0.9183673 & 5 & 4 & 0.2903352 & 0.0711025 & n = 6\\\\\n\\hline\n297 & l & 6 & 4 & 0.9387755 & 5 & 4 & 0.2377619 & 0.0436705 & n = 6\\\\\n\\hline\n298 & l & 6 & 4 & 0.9591837 & 5 & 4 & 0.1727472 & 0.0211527 & n = 6\\\\\n\\hline\n299 & l & 6 & 4 & 0.9795918 & 5 & 4 & 0.0939625 & 0.0057528 & n = 6\\\\\n\\hline\n300 & l & 6 & 4 & 1.0000000 & 5 & 4 & 0.0000000 & 0.0000000 & n = 6\\\\\n\\hline\n301 & w & 7 & 5 & 0.0000000 & 6 & 4 & 0.0000000 & 0.0000000 & n = 7\\\\\n\\hline\n302 & w & 7 & 5 & 0.0204082 & 6 & 4 & 0.0000025 & 0.0000001 & n = 7\\\\\n\\hline\n303 & w & 7 & 5 & 0.0408163 & 6 & 4 & 0.0000383 & 0.0000022 & n = 7\\\\\n\\hline\n304 & w & 7 & 5 & 0.0612245 & 6 & 4 & 0.0001857 & 0.0000159 & n = 7\\\\\n\\hline\n305 & w & 7 & 5 & 0.0816327 & 6 & 4 & 0.0005618 & 0.0000642 & n = 7\\\\\n\\hline\n306 & w & 7 & 5 & 0.1020408 & 6 & 4 & 0.0013113 & 0.0001873 & n = 7\\\\\n\\hline\n307 & w & 7 & 5 & 0.1224490 & 6 & 4 & 0.0025969 & 0.0004452 & n = 7\\\\\n\\hline\n308 & w & 7 & 5 & 0.1428571 & 6 & 4 & 0.0045899 & 0.0009180 & n = 7\\\\\n\\hline\n309 & w & 7 & 5 & 0.1632653 & 6 & 4 & 0.0074618 & 0.0017055 & n = 7\\\\\n\\hline\n310 & w & 7 & 5 & 0.1836735 & 6 & 4 & 0.0113764 & 0.0029254 & n = 7\\\\\n\\hline\n311 & w & 7 & 5 & 0.2040816 & 6 & 4 & 0.0164833 & 0.0047095 & n = 7\\\\\n\\hline\n312 & w & 7 & 5 & 0.2244898 & 6 & 4 & 0.0229115 & 0.0072007 & n = 7\\\\\n\\hline\n313 & w & 7 & 5 & 0.2448980 & 6 & 4 & 0.0307640 & 0.0105477 & n = 7\\\\\n\\hline\n314 & w & 7 & 5 & 0.2653061 & 6 & 4 & 0.0401137 & 0.0148994 & n = 7\\\\\n\\hline\n315 & w & 7 & 5 & 0.2857143 & 6 & 4 & 0.0509992 & 0.0203997 & n = 7\\\\\n\\hline\n316 & w & 7 & 5 & 0.3061224 & 6 & 4 & 0.0634217 & 0.0271807 & n = 7\\\\\n\\hline\n317 & w & 7 & 5 & 0.3265306 & 6 & 4 & 0.0773433 & 0.0353569 & n = 7\\\\\n\\hline\n318 & w & 7 & 5 & 0.3469388 & 6 & 4 & 0.0926852 & 0.0450185 & n = 7\\\\\n\\hline\n319 & w & 7 & 5 & 0.3673469 & 6 & 4 & 0.1093272 & 0.0562254 & n = 7\\\\\n\\hline\n320 & w & 7 & 5 & 0.3877551 & 6 & 4 & 0.1271077 & 0.0690013 & n = 7\\\\\n\\hline\n321 & w & 7 & 5 & 0.4081633 & 6 & 4 & 0.1458246 & 0.0833283 & n = 7\\\\\n\\hline\n322 & w & 7 & 5 & 0.4285714 & 6 & 4 & 0.1652373 & 0.0991424 & n = 7\\\\\n\\hline\n323 & w & 7 & 5 & 0.4489796 & 6 & 4 & 0.1850687 & 0.1163289 & n = 7\\\\\n\\hline\n324 & w & 7 & 5 & 0.4693878 & 6 & 4 & 0.2050089 & 0.1347202 & n = 7\\\\\n\\hline\n325 & w & 7 & 5 & 0.4897959 & 6 & 4 & 0.2247190 & 0.1540930 & n = 7\\\\\n\\hline\n326 & w & 7 & 5 & 0.5102041 & 6 & 4 & 0.2438357 & 0.1741684 & n = 7\\\\\n\\hline\n327 & w & 7 & 5 & 0.5306122 & 6 & 4 & 0.2619774 & 0.1946118 & n = 7\\\\\n\\hline\n328 & w & 7 & 5 & 0.5510204 & 6 & 4 & 0.2787502 & 0.2150359 & n = 7\\\\\n\\hline\n329 & w & 7 & 5 & 0.5714286 & 6 & 4 & 0.2937552 & 0.2350041 & n = 7\\\\\n\\hline\n330 & w & 7 & 5 & 0.5918367 & 6 & 4 & 0.3065962 & 0.2540368 & n = 7\\\\\n\\hline\n331 & w & 7 & 5 & 0.6122449 & 6 & 4 & 0.3168889 & 0.2716190 & n = 7\\\\\n\\hline\n332 & w & 7 & 5 & 0.6326531 & 6 & 4 & 0.3242698 & 0.2872104 & n = 7\\\\\n\\hline\n333 & w & 7 & 5 & 0.6530612 & 6 & 4 & 0.3284071 & 0.3002579 & n = 7\\\\\n\\hline\n334 & w & 7 & 5 & 0.6734694 & 6 & 4 & 0.3290111 & 0.3102104 & n = 7\\\\\n\\hline\n335 & w & 7 & 5 & 0.6938776 & 6 & 4 & 0.3258464 & 0.3165365 & n = 7\\\\\n\\hline\n336 & w & 7 & 5 & 0.7142857 & 6 & 4 & 0.3187447 & 0.3187447 & n = 7\\\\\n\\hline\n337 & w & 7 & 5 & 0.7346939 & 6 & 4 & 0.3076178 & 0.3164069 & n = 7\\\\\n\\hline\n338 & w & 7 & 5 & 0.7551020 & 6 & 4 & 0.2924719 & 0.3091846 & n = 7\\\\\n\\hline\n339 & w & 7 & 5 & 0.7755102 & 6 & 4 & 0.2734227 & 0.2968589 & n = 7\\\\\n\\hline\n340 & w & 7 & 5 & 0.7959184 & 6 & 4 & 0.2507109 & 0.2793636 & n = 7\\\\\n\\hline\n341 & w & 7 & 5 & 0.8163265 & 6 & 4 & 0.2247190 & 0.2568217 & n = 7\\\\\n\\hline\n342 & w & 7 & 5 & 0.8367347 & 6 & 4 & 0.1959883 & 0.2295863 & n = 7\\\\\n\\hline\n343 & w & 7 & 5 & 0.8571429 & 6 & 4 & 0.1652373 & 0.1982847 & n = 7\\\\\n\\hline\n344 & w & 7 & 5 & 0.8775510 & 6 & 4 & 0.1333801 & 0.1638670 & n = 7\\\\\n\\hline\n345 & w & 7 & 5 & 0.8979592 & 6 & 4 & 0.1015466 & 0.1276586 & n = 7\\\\\n\\hline\n346 & w & 7 & 5 & 0.9183673 & 6 & 4 & 0.0711025 & 0.0914175 & n = 7\\\\\n\\hline\n347 & w & 7 & 5 & 0.9387755 & 6 & 4 & 0.0436705 & 0.0573956 & n = 7\\\\\n\\hline\n348 & w & 7 & 5 & 0.9591837 & 6 & 4 & 0.0211527 & 0.0284051 & n = 7\\\\\n\\hline\n349 & w & 7 & 5 & 0.9795918 & 6 & 4 & 0.0057528 & 0.0078896 & n = 7\\\\\n\\hline\n350 & w & 7 & 5 & 1.0000000 & 6 & 4 & 0.0000000 & 0.0000000 & n = 7\\\\\n\\hline\n351 & l & 8 & 5 & 0.0000000 & 7 & 5 & 0.0000000 & 0.0000000 & n = 8\\\\\n\\hline\n352 & l & 8 & 5 & 0.0204082 & 7 & 5 & 0.0000001 & 0.0000002 & n = 8\\\\\n\\hline\n353 & l & 8 & 5 & 0.0408163 & 7 & 5 & 0.0000022 & 0.0000056 & n = 8\\\\\n\\hline\n354 & l & 8 & 5 & 0.0612245 & 7 & 5 & 0.0000159 & 0.0000399 & n = 8\\\\\n\\hline\n355 & l & 8 & 5 & 0.0816327 & 7 & 5 & 0.0000642 & 0.0001572 & n = 8\\\\\n\\hline\n356 & l & 8 & 5 & 0.1020408 & 7 & 5 & 0.0001873 & 0.0004486 & n = 8\\\\\n\\hline\n357 & l & 8 & 5 & 0.1224490 & 7 & 5 & 0.0004452 & 0.0010418 & n = 8\\\\\n\\hline\n358 & l & 8 & 5 & 0.1428571 & 7 & 5 & 0.0009180 & 0.0020983 & n = 8\\\\\n\\hline\n359 & l & 8 & 5 & 0.1632653 & 7 & 5 & 0.0017055 & 0.0038056 & n = 8\\\\\n\\hline\n360 & l & 8 & 5 & 0.1836735 & 7 & 5 & 0.0029254 & 0.0063681 & n = 8\\\\\n\\hline\n361 & l & 8 & 5 & 0.2040816 & 7 & 5 & 0.0047095 & 0.0099957 & n = 8\\\\\n\\hline\n362 & l & 8 & 5 & 0.2244898 & 7 & 5 & 0.0072007 & 0.0148913 & n = 8\\\\\n\\hline\n363 & l & 8 & 5 & 0.2448980 & 7 & 5 & 0.0105477 & 0.0212388 & n = 8\\\\\n\\hline\n364 & l & 8 & 5 & 0.2653061 & 7 & 5 & 0.0148994 & 0.0291906 & n = 8\\\\\n\\hline\n365 & l & 8 & 5 & 0.2857143 & 7 & 5 & 0.0203997 & 0.0388565 & n = 8\\\\\n\\hline\n366 & l & 8 & 5 & 0.3061224 & 7 & 5 & 0.0271807 & 0.0502936 & n = 8\\\\\n\\hline\n367 & l & 8 & 5 & 0.3265306 & 7 & 5 & 0.0353569 & 0.0634982 & n = 8\\\\\n\\hline\n368 & l & 8 & 5 & 0.3469388 & 7 & 5 & 0.0450185 & 0.0783996 & n = 8\\\\\n\\hline\n369 & l & 8 & 5 & 0.3673469 & 7 & 5 & 0.0562254 & 0.0948565 & n = 8\\\\\n\\hline\n370 & l & 8 & 5 & 0.3877551 & 7 & 5 & 0.0690013 & 0.1126552 & n = 8\\\\\n\\hline\n371 & l & 8 & 5 & 0.4081633 & 7 & 5 & 0.0833283 & 0.1315114 & n = 8\\\\\n\\hline\n372 & l & 8 & 5 & 0.4285714 & 7 & 5 & 0.0991424 & 0.1510741 & n = 8\\\\\n\\hline\n373 & l & 8 & 5 & 0.4489796 & 7 & 5 & 0.1163289 & 0.1709323 & n = 8\\\\\n\\hline\n374 & l & 8 & 5 & 0.4693878 & 7 & 5 & 0.1347202 & 0.1906245 & n = 8\\\\\n\\hline\n375 & l & 8 & 5 & 0.4897959 & 7 & 5 & 0.1540930 & 0.2096504 & n = 8\\\\\n\\hline\n376 & l & 8 & 5 & 0.5102041 & 7 & 5 & 0.1741684 & 0.2274852 & n = 8\\\\\n\\hline\n377 & l & 8 & 5 & 0.5306122 & 7 & 5 & 0.1946118 & 0.2435957 & n = 8\\\\\n\\hline\n378 & l & 8 & 5 & 0.5510204 & 7 & 5 & 0.2150359 & 0.2574579 & n = 8\\\\\n\\hline\n379 & l & 8 & 5 & 0.5714286 & 7 & 5 & 0.2350041 & 0.2685761 & n = 8\\\\\n\\hline\n380 & l & 8 & 5 & 0.5918367 & 7 & 5 & 0.2540368 & 0.2765027 & n = 8\\\\\n\\hline\n381 & l & 8 & 5 & 0.6122449 & 7 & 5 & 0.2716190 & 0.2808578 & n = 8\\\\\n\\hline\n382 & l & 8 & 5 & 0.6326531 & 7 & 5 & 0.2872104 & 0.2813490 & n = 8\\\\\n\\hline\n383 & l & 8 & 5 & 0.6530612 & 7 & 5 & 0.3002579 & 0.2777896 & n = 8\\\\\n\\hline\n384 & l & 8 & 5 & 0.6734694 & 7 & 5 & 0.3102104 & 0.2701152 & n = 8\\\\\n\\hline\n385 & l & 8 & 5 & 0.6938776 & 7 & 5 & 0.3165365 & 0.2583972 & n = 8\\\\\n\\hline\n386 & l & 8 & 5 & 0.7142857 & 7 & 5 & 0.3187447 & 0.2428531 & n = 8\\\\\n\\hline\n387 & l & 8 & 5 & 0.7346939 & 7 & 5 & 0.3164069 & 0.2238525 & n = 8\\\\\n\\hline\n388 & l & 8 & 5 & 0.7551020 & 7 & 5 & 0.3091846 & 0.2019165 & n = 8\\\\\n\\hline\n389 & l & 8 & 5 & 0.7755102 & 7 & 5 & 0.2968589 & 0.1777115 & n = 8\\\\\n\\hline\n390 & l & 8 & 5 & 0.7959184 & 7 & 5 & 0.2793636 & 0.1520346 & n = 8\\\\\n\\hline\n391 & l & 8 & 5 & 0.8163265 & 7 & 5 & 0.2568217 & 0.1257902 & n = 8\\\\\n\\hline\n392 & l & 8 & 5 & 0.8367347 & 7 & 5 & 0.2295863 & 0.0999559 & n = 8\\\\\n\\hline\n393 & l & 8 & 5 & 0.8571429 & 7 & 5 & 0.1982847 & 0.0755370 & n = 8\\\\\n\\hline\n394 & l & 8 & 5 & 0.8775510 & 7 & 5 & 0.1638670 & 0.0535076 & n = 8\\\\\n\\hline\n395 & l & 8 & 5 & 0.8979592 & 7 & 5 & 0.1276586 & 0.0347370 & n = 8\\\\\n\\hline\n396 & l & 8 & 5 & 0.9183673 & 7 & 5 & 0.0914175 & 0.0199004 & n = 8\\\\\n\\hline\n397 & l & 8 & 5 & 0.9387755 & 7 & 5 & 0.0573956 & 0.0093707 & n = 8\\\\\n\\hline\n398 & l & 8 & 5 & 0.9591837 & 7 & 5 & 0.0284051 & 0.0030917 & n = 8\\\\\n\\hline\n399 & l & 8 & 5 & 0.9795918 & 7 & 5 & 0.0078896 & 0.0004294 & n = 8\\\\\n\\hline\n400 & l & 8 & 5 & 1.0000000 & 7 & 5 & 0.0000000 & 0.0000000 & n = 8\\\\\n\\hline\n401 & w & 9 & 6 & 0.0000000 & 8 & 5 & 0.0000000 & 0.0000000 & n = 9\\\\\n\\hline\n402 & w & 9 & 6 & 0.0204082 & 8 & 5 & 0.0000002 & 0.0000000 & n = 9\\\\\n\\hline\n403 & w & 9 & 6 & 0.0408163 & 8 & 5 & 0.0000056 & 0.0000003 & n = 9\\\\\n\\hline\n404 & w & 9 & 6 & 0.0612245 & 8 & 5 & 0.0000399 & 0.0000037 & n = 9\\\\\n\\hline\n405 & w & 9 & 6 & 0.0816327 & 8 & 5 & 0.0001572 & 0.0000193 & n = 9\\\\\n\\hline\n406 & w & 9 & 6 & 0.1020408 & 8 & 5 & 0.0004486 & 0.0000687 & n = 9\\\\\n\\hline\n407 & w & 9 & 6 & 0.1224490 & 8 & 5 & 0.0010418 & 0.0001913 & n = 9\\\\\n\\hline\n408 & w & 9 & 6 & 0.1428571 & 8 & 5 & 0.0020983 & 0.0004496 & n = 9\\\\\n\\hline\n409 & w & 9 & 6 & 0.1632653 & 8 & 5 & 0.0038056 & 0.0009320 & n = 9\\\\\n\\hline\n410 & w & 9 & 6 & 0.1836735 & 8 & 5 & 0.0063681 & 0.0017545 & n = 9\\\\\n\\hline\n411 & w & 9 & 6 & 0.2040816 & 8 & 5 & 0.0099957 & 0.0030599 & n = 9\\\\\n\\hline\n412 & w & 9 & 6 & 0.2244898 & 8 & 5 & 0.0148913 & 0.0050144 & n = 9\\\\\n\\hline\n413 & w & 9 & 6 & 0.2448980 & 8 & 5 & 0.0212388 & 0.0078020 & n = 9\\\\\n\\hline\n414 & w & 9 & 6 & 0.2653061 & 8 & 5 & 0.0291906 & 0.0116167 & n = 9\\\\\n\\hline\n415 & w & 9 & 6 & 0.2857143 & 8 & 5 & 0.0388565 & 0.0166528 & n = 9\\\\\n\\hline\n416 & w & 9 & 6 & 0.3061224 & 8 & 5 & 0.0502936 & 0.0230940 & n = 9\\\\\n\\hline\n417 & w & 9 & 6 & 0.3265306 & 8 & 5 & 0.0634982 & 0.0311011 & n = 9\\\\\n\\hline\n418 & w & 9 & 6 & 0.3469388 & 8 & 5 & 0.0783996 & 0.0407998 & n = 9\\\\\n\\hline\n419 & w & 9 & 6 & 0.3673469 & 8 & 5 & 0.0948565 & 0.0522679 & n = 9\\\\\n\\hline\n420 & w & 9 & 6 & 0.3877551 & 8 & 5 & 0.1126552 & 0.0655239 & n = 9\\\\\n\\hline\n421 & w & 9 & 6 & 0.4081633 & 8 & 5 & 0.1315114 & 0.0805172 & n = 9\\\\\n\\hline\n422 & w & 9 & 6 & 0.4285714 & 8 & 5 & 0.1510741 & 0.0971191 & n = 9\\\\\n\\hline\n423 & w & 9 & 6 & 0.4489796 & 8 & 5 & 0.1709323 & 0.1151177 & n = 9\\\\\n\\hline\n424 & w & 9 & 6 & 0.4693878 & 8 & 5 & 0.1906245 & 0.1342152 & n = 9\\\\\n\\hline\n425 & w & 9 & 6 & 0.4897959 & 8 & 5 & 0.2096504 & 0.1540288 & n = 9\\\\\n\\hline\n426 & w & 9 & 6 & 0.5102041 & 8 & 5 & 0.2274852 & 0.1740958 & n = 9\\\\\n\\hline\n427 & w & 9 & 6 & 0.5306122 & 8 & 5 & 0.2435957 & 0.1938823 & n = 9\\\\\n\\hline\n428 & w & 9 & 6 & 0.5510204 & 8 & 5 & 0.2574579 & 0.2127968 & n = 9\\\\\n\\hline\n429 & w & 9 & 6 & 0.5714286 & 8 & 5 & 0.2685761 & 0.2302081 & n = 9\\\\\n\\hline\n430 & w & 9 & 6 & 0.5918367 & 8 & 5 & 0.2765027 & 0.2454667 & n = 9\\\\\n\\hline\n431 & w & 9 & 6 & 0.6122449 & 8 & 5 & 0.2808578 & 0.2579306 & n = 9\\\\\n\\hline\n432 & w & 9 & 6 & 0.6326531 & 8 & 5 & 0.2813490 & 0.2669945 & n = 9\\\\\n\\hline\n433 & w & 9 & 6 & 0.6530612 & 8 & 5 & 0.2777896 & 0.2721205 & n = 9\\\\\n\\hline\n434 & w & 9 & 6 & 0.6734694 & 8 & 5 & 0.2701152 & 0.2728715 & n = 9\\\\\n\\hline\n435 & w & 9 & 6 & 0.6938776 & 8 & 5 & 0.2583972 & 0.2689440 & n = 9\\\\\n\\hline\n436 & w & 9 & 6 & 0.7142857 & 8 & 5 & 0.2428531 & 0.2601998 & n = 9\\\\\n\\hline\n437 & w & 9 & 6 & 0.7346939 & 8 & 5 & 0.2238525 & 0.2466946 & n = 9\\\\\n\\hline\n438 & w & 9 & 6 & 0.7551020 & 8 & 5 & 0.2019165 & 0.2287013 & n = 9\\\\\n\\hline\n439 & w & 9 & 6 & 0.7755102 & 8 & 5 & 0.1777115 & 0.2067256 & n = 9\\\\\n\\hline\n440 & w & 9 & 6 & 0.7959184 & 8 & 5 & 0.1520346 & 0.1815107 & n = 9\\\\\n\\hline\n441 & w & 9 & 6 & 0.8163265 & 8 & 5 & 0.1257902 & 0.1540288 & n = 9\\\\\n\\hline\n442 & w & 9 & 6 & 0.8367347 & 8 & 5 & 0.0999559 & 0.1254549 & n = 9\\\\\n\\hline\n443 & w & 9 & 6 & 0.8571429 & 8 & 5 & 0.0755370 & 0.0971191 & n = 9\\\\\n\\hline\n444 & w & 9 & 6 & 0.8775510 & 8 & 5 & 0.0535076 & 0.0704335 & n = 9\\\\\n\\hline\n445 & w & 9 & 6 & 0.8979592 & 8 & 5 & 0.0347370 & 0.0467887 & n = 9\\\\\n\\hline\n446 & w & 9 & 6 & 0.9183673 & 8 & 5 & 0.0199004 & 0.0274138 & n = 9\\\\\n\\hline\n447 & w & 9 & 6 & 0.9387755 & 8 & 5 & 0.0093707 & 0.0131955 & n = 9\\\\\n\\hline\n448 & w & 9 & 6 & 0.9591837 & 8 & 5 & 0.0030917 & 0.0044483 & n = 9\\\\\n\\hline\n449 & w & 9 & 6 & 0.9795918 & 8 & 5 & 0.0004294 & 0.0006309 & n = 9\\\\\n\\hline\n450 & w & 9 & 6 & 1.0000000 & 8 & 5 & 0.0000000 & 0.0000000 & n = 9\\\\\n\\hline\n\\end{tabular}\n\\end{table}\n:::\n:::\n\n\n\n##### Annotation (6): Normalizing {#sec-annotation-6-normalize}\n\nThe code lines in annotation 6 normalize the prior and the likelihood by\ngrouping the data by `n_trials`. Dividing every prior and likelihood\nvalues by their respective sum puts them both in a probability metric.\nThis metric is important for the comparisons of different probabilities.\n\n> If you don't normalize (i.e., divide the density by the sum of the\n> density), their respective heights don't match up with those in the\n> text. Furthermore, it's the normalization that makes them directly\n> comparable.\n\n\n\n::: {#tbl-bayesian-model-learning-anno6 .cell tbl-cap='Bayesian Updating: Normalizing prior and likelihood'}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: tbl-bayesian-model-learning-anno6\n#| tbl-cap: \"Bayesian Updating: Normalizing prior and likelihood\"\n\n# starting data\ntbl <- tibble(toss = c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\")) |> \n    mutate(n_trials  = 1:9, n_success = cumsum(toss == \"w\"))\n\n# start of bayesian modeling with grouping (as in the original)\nsequence_length <- 50\n\ntbl6 <- tbl %>% \n  expand_grid(p_water = seq(from = 0, to = 1, \n                            length.out = sequence_length)) %>% \n  group_by(p_water) %>% \n  mutate(lagged_n_trials  = lag(n_trials, default = 1),\n         lagged_n_success = lag(n_success, default = 1)) %>% \n  ungroup() %>% \n  mutate(prior      = ifelse(n_trials == 1, .5,\n                             dbinom(x    = lagged_n_success, \n                                    size = lagged_n_trials, \n                                    prob = p_water)),\n         likelihood = dbinom(x    = n_success, \n                             size = n_trials, \n                             prob = p_water),\n         strip      = str_c(\"n = \", n_trials)) |> \n    \n  ### add code lines of annotation <6> ################## \n  group_by(n_trials) %>% \n  mutate(prior      = prior / sum(prior),\n         likelihood = likelihood / sum(likelihood)) %>% \n    \n  # provide table scrool box for better comparison of the results\n  mutate(ID = row_number()) |> \n  relocate(ID, .before = toss) |> \n  kableExtra::kbl() %>%\n  kableExtra::kable_classic() %>%\n  kableExtra::scroll_box(height = \"600px\")\ntbl6\n```\n````\n\n::: {.cell-output-display}\n\\begin{table}\n\\centering\n\\begin{tabular}[t]{r|l|r|r|r|r|r|r|r|l}\n\\hline\nID & toss & n\\_trials & n\\_success & p\\_water & lagged\\_n\\_trials & lagged\\_n\\_success & prior & likelihood & strip\\\\\n\\hline\n1 & w & 1 & 1 & 0.0000000 & 1 & 1 & 0.0200000 & 0.0000000 & n = 1\\\\\n\\hline\n2 & w & 1 & 1 & 0.0204082 & 1 & 1 & 0.0200000 & 0.0008163 & n = 1\\\\\n\\hline\n3 & w & 1 & 1 & 0.0408163 & 1 & 1 & 0.0200000 & 0.0016327 & n = 1\\\\\n\\hline\n4 & w & 1 & 1 & 0.0612245 & 1 & 1 & 0.0200000 & 0.0024490 & n = 1\\\\\n\\hline\n5 & w & 1 & 1 & 0.0816327 & 1 & 1 & 0.0200000 & 0.0032653 & n = 1\\\\\n\\hline\n6 & w & 1 & 1 & 0.1020408 & 1 & 1 & 0.0200000 & 0.0040816 & n = 1\\\\\n\\hline\n7 & w & 1 & 1 & 0.1224490 & 1 & 1 & 0.0200000 & 0.0048980 & n = 1\\\\\n\\hline\n8 & w & 1 & 1 & 0.1428571 & 1 & 1 & 0.0200000 & 0.0057143 & n = 1\\\\\n\\hline\n9 & w & 1 & 1 & 0.1632653 & 1 & 1 & 0.0200000 & 0.0065306 & n = 1\\\\\n\\hline\n10 & w & 1 & 1 & 0.1836735 & 1 & 1 & 0.0200000 & 0.0073469 & n = 1\\\\\n\\hline\n11 & w & 1 & 1 & 0.2040816 & 1 & 1 & 0.0200000 & 0.0081633 & n = 1\\\\\n\\hline\n12 & w & 1 & 1 & 0.2244898 & 1 & 1 & 0.0200000 & 0.0089796 & n = 1\\\\\n\\hline\n13 & w & 1 & 1 & 0.2448980 & 1 & 1 & 0.0200000 & 0.0097959 & n = 1\\\\\n\\hline\n14 & w & 1 & 1 & 0.2653061 & 1 & 1 & 0.0200000 & 0.0106122 & n = 1\\\\\n\\hline\n15 & w & 1 & 1 & 0.2857143 & 1 & 1 & 0.0200000 & 0.0114286 & n = 1\\\\\n\\hline\n16 & w & 1 & 1 & 0.3061224 & 1 & 1 & 0.0200000 & 0.0122449 & n = 1\\\\\n\\hline\n17 & w & 1 & 1 & 0.3265306 & 1 & 1 & 0.0200000 & 0.0130612 & n = 1\\\\\n\\hline\n18 & w & 1 & 1 & 0.3469388 & 1 & 1 & 0.0200000 & 0.0138776 & n = 1\\\\\n\\hline\n19 & w & 1 & 1 & 0.3673469 & 1 & 1 & 0.0200000 & 0.0146939 & n = 1\\\\\n\\hline\n20 & w & 1 & 1 & 0.3877551 & 1 & 1 & 0.0200000 & 0.0155102 & n = 1\\\\\n\\hline\n21 & w & 1 & 1 & 0.4081633 & 1 & 1 & 0.0200000 & 0.0163265 & n = 1\\\\\n\\hline\n22 & w & 1 & 1 & 0.4285714 & 1 & 1 & 0.0200000 & 0.0171429 & n = 1\\\\\n\\hline\n23 & w & 1 & 1 & 0.4489796 & 1 & 1 & 0.0200000 & 0.0179592 & n = 1\\\\\n\\hline\n24 & w & 1 & 1 & 0.4693878 & 1 & 1 & 0.0200000 & 0.0187755 & n = 1\\\\\n\\hline\n25 & w & 1 & 1 & 0.4897959 & 1 & 1 & 0.0200000 & 0.0195918 & n = 1\\\\\n\\hline\n26 & w & 1 & 1 & 0.5102041 & 1 & 1 & 0.0200000 & 0.0204082 & n = 1\\\\\n\\hline\n27 & w & 1 & 1 & 0.5306122 & 1 & 1 & 0.0200000 & 0.0212245 & n = 1\\\\\n\\hline\n28 & w & 1 & 1 & 0.5510204 & 1 & 1 & 0.0200000 & 0.0220408 & n = 1\\\\\n\\hline\n29 & w & 1 & 1 & 0.5714286 & 1 & 1 & 0.0200000 & 0.0228571 & n = 1\\\\\n\\hline\n30 & w & 1 & 1 & 0.5918367 & 1 & 1 & 0.0200000 & 0.0236735 & n = 1\\\\\n\\hline\n31 & w & 1 & 1 & 0.6122449 & 1 & 1 & 0.0200000 & 0.0244898 & n = 1\\\\\n\\hline\n32 & w & 1 & 1 & 0.6326531 & 1 & 1 & 0.0200000 & 0.0253061 & n = 1\\\\\n\\hline\n33 & w & 1 & 1 & 0.6530612 & 1 & 1 & 0.0200000 & 0.0261224 & n = 1\\\\\n\\hline\n34 & w & 1 & 1 & 0.6734694 & 1 & 1 & 0.0200000 & 0.0269388 & n = 1\\\\\n\\hline\n35 & w & 1 & 1 & 0.6938776 & 1 & 1 & 0.0200000 & 0.0277551 & n = 1\\\\\n\\hline\n36 & w & 1 & 1 & 0.7142857 & 1 & 1 & 0.0200000 & 0.0285714 & n = 1\\\\\n\\hline\n37 & w & 1 & 1 & 0.7346939 & 1 & 1 & 0.0200000 & 0.0293878 & n = 1\\\\\n\\hline\n38 & w & 1 & 1 & 0.7551020 & 1 & 1 & 0.0200000 & 0.0302041 & n = 1\\\\\n\\hline\n39 & w & 1 & 1 & 0.7755102 & 1 & 1 & 0.0200000 & 0.0310204 & n = 1\\\\\n\\hline\n40 & w & 1 & 1 & 0.7959184 & 1 & 1 & 0.0200000 & 0.0318367 & n = 1\\\\\n\\hline\n41 & w & 1 & 1 & 0.8163265 & 1 & 1 & 0.0200000 & 0.0326531 & n = 1\\\\\n\\hline\n42 & w & 1 & 1 & 0.8367347 & 1 & 1 & 0.0200000 & 0.0334694 & n = 1\\\\\n\\hline\n43 & w & 1 & 1 & 0.8571429 & 1 & 1 & 0.0200000 & 0.0342857 & n = 1\\\\\n\\hline\n44 & w & 1 & 1 & 0.8775510 & 1 & 1 & 0.0200000 & 0.0351020 & n = 1\\\\\n\\hline\n45 & w & 1 & 1 & 0.8979592 & 1 & 1 & 0.0200000 & 0.0359184 & n = 1\\\\\n\\hline\n46 & w & 1 & 1 & 0.9183673 & 1 & 1 & 0.0200000 & 0.0367347 & n = 1\\\\\n\\hline\n47 & w & 1 & 1 & 0.9387755 & 1 & 1 & 0.0200000 & 0.0375510 & n = 1\\\\\n\\hline\n48 & w & 1 & 1 & 0.9591837 & 1 & 1 & 0.0200000 & 0.0383673 & n = 1\\\\\n\\hline\n49 & w & 1 & 1 & 0.9795918 & 1 & 1 & 0.0200000 & 0.0391837 & n = 1\\\\\n\\hline\n50 & w & 1 & 1 & 1.0000000 & 1 & 1 & 0.0200000 & 0.0400000 & n = 1\\\\\n\\hline\n1 & l & 2 & 1 & 0.0000000 & 1 & 1 & 0.0000000 & 0.0000000 & n = 2\\\\\n\\hline\n2 & l & 2 & 1 & 0.0204082 & 1 & 1 & 0.0008163 & 0.0024490 & n = 2\\\\\n\\hline\n3 & l & 2 & 1 & 0.0408163 & 1 & 1 & 0.0016327 & 0.0047959 & n = 2\\\\\n\\hline\n4 & l & 2 & 1 & 0.0612245 & 1 & 1 & 0.0024490 & 0.0070408 & n = 2\\\\\n\\hline\n5 & l & 2 & 1 & 0.0816327 & 1 & 1 & 0.0032653 & 0.0091837 & n = 2\\\\\n\\hline\n6 & l & 2 & 1 & 0.1020408 & 1 & 1 & 0.0040816 & 0.0112245 & n = 2\\\\\n\\hline\n7 & l & 2 & 1 & 0.1224490 & 1 & 1 & 0.0048980 & 0.0131633 & n = 2\\\\\n\\hline\n8 & l & 2 & 1 & 0.1428571 & 1 & 1 & 0.0057143 & 0.0150000 & n = 2\\\\\n\\hline\n9 & l & 2 & 1 & 0.1632653 & 1 & 1 & 0.0065306 & 0.0167347 & n = 2\\\\\n\\hline\n10 & l & 2 & 1 & 0.1836735 & 1 & 1 & 0.0073469 & 0.0183673 & n = 2\\\\\n\\hline\n11 & l & 2 & 1 & 0.2040816 & 1 & 1 & 0.0081633 & 0.0198980 & n = 2\\\\\n\\hline\n12 & l & 2 & 1 & 0.2244898 & 1 & 1 & 0.0089796 & 0.0213265 & n = 2\\\\\n\\hline\n13 & l & 2 & 1 & 0.2448980 & 1 & 1 & 0.0097959 & 0.0226531 & n = 2\\\\\n\\hline\n14 & l & 2 & 1 & 0.2653061 & 1 & 1 & 0.0106122 & 0.0238776 & n = 2\\\\\n\\hline\n15 & l & 2 & 1 & 0.2857143 & 1 & 1 & 0.0114286 & 0.0250000 & n = 2\\\\\n\\hline\n16 & l & 2 & 1 & 0.3061224 & 1 & 1 & 0.0122449 & 0.0260204 & n = 2\\\\\n\\hline\n17 & l & 2 & 1 & 0.3265306 & 1 & 1 & 0.0130612 & 0.0269388 & n = 2\\\\\n\\hline\n18 & l & 2 & 1 & 0.3469388 & 1 & 1 & 0.0138776 & 0.0277551 & n = 2\\\\\n\\hline\n19 & l & 2 & 1 & 0.3673469 & 1 & 1 & 0.0146939 & 0.0284694 & n = 2\\\\\n\\hline\n20 & l & 2 & 1 & 0.3877551 & 1 & 1 & 0.0155102 & 0.0290816 & n = 2\\\\\n\\hline\n21 & l & 2 & 1 & 0.4081633 & 1 & 1 & 0.0163265 & 0.0295918 & n = 2\\\\\n\\hline\n22 & l & 2 & 1 & 0.4285714 & 1 & 1 & 0.0171429 & 0.0300000 & n = 2\\\\\n\\hline\n23 & l & 2 & 1 & 0.4489796 & 1 & 1 & 0.0179592 & 0.0303061 & n = 2\\\\\n\\hline\n24 & l & 2 & 1 & 0.4693878 & 1 & 1 & 0.0187755 & 0.0305102 & n = 2\\\\\n\\hline\n25 & l & 2 & 1 & 0.4897959 & 1 & 1 & 0.0195918 & 0.0306122 & n = 2\\\\\n\\hline\n26 & l & 2 & 1 & 0.5102041 & 1 & 1 & 0.0204082 & 0.0306122 & n = 2\\\\\n\\hline\n27 & l & 2 & 1 & 0.5306122 & 1 & 1 & 0.0212245 & 0.0305102 & n = 2\\\\\n\\hline\n28 & l & 2 & 1 & 0.5510204 & 1 & 1 & 0.0220408 & 0.0303061 & n = 2\\\\\n\\hline\n29 & l & 2 & 1 & 0.5714286 & 1 & 1 & 0.0228571 & 0.0300000 & n = 2\\\\\n\\hline\n30 & l & 2 & 1 & 0.5918367 & 1 & 1 & 0.0236735 & 0.0295918 & n = 2\\\\\n\\hline\n31 & l & 2 & 1 & 0.6122449 & 1 & 1 & 0.0244898 & 0.0290816 & n = 2\\\\\n\\hline\n32 & l & 2 & 1 & 0.6326531 & 1 & 1 & 0.0253061 & 0.0284694 & n = 2\\\\\n\\hline\n33 & l & 2 & 1 & 0.6530612 & 1 & 1 & 0.0261224 & 0.0277551 & n = 2\\\\\n\\hline\n34 & l & 2 & 1 & 0.6734694 & 1 & 1 & 0.0269388 & 0.0269388 & n = 2\\\\\n\\hline\n35 & l & 2 & 1 & 0.6938776 & 1 & 1 & 0.0277551 & 0.0260204 & n = 2\\\\\n\\hline\n36 & l & 2 & 1 & 0.7142857 & 1 & 1 & 0.0285714 & 0.0250000 & n = 2\\\\\n\\hline\n37 & l & 2 & 1 & 0.7346939 & 1 & 1 & 0.0293878 & 0.0238776 & n = 2\\\\\n\\hline\n38 & l & 2 & 1 & 0.7551020 & 1 & 1 & 0.0302041 & 0.0226531 & n = 2\\\\\n\\hline\n39 & l & 2 & 1 & 0.7755102 & 1 & 1 & 0.0310204 & 0.0213265 & n = 2\\\\\n\\hline\n40 & l & 2 & 1 & 0.7959184 & 1 & 1 & 0.0318367 & 0.0198980 & n = 2\\\\\n\\hline\n41 & l & 2 & 1 & 0.8163265 & 1 & 1 & 0.0326531 & 0.0183673 & n = 2\\\\\n\\hline\n42 & l & 2 & 1 & 0.8367347 & 1 & 1 & 0.0334694 & 0.0167347 & n = 2\\\\\n\\hline\n43 & l & 2 & 1 & 0.8571429 & 1 & 1 & 0.0342857 & 0.0150000 & n = 2\\\\\n\\hline\n44 & l & 2 & 1 & 0.8775510 & 1 & 1 & 0.0351020 & 0.0131633 & n = 2\\\\\n\\hline\n45 & l & 2 & 1 & 0.8979592 & 1 & 1 & 0.0359184 & 0.0112245 & n = 2\\\\\n\\hline\n46 & l & 2 & 1 & 0.9183673 & 1 & 1 & 0.0367347 & 0.0091837 & n = 2\\\\\n\\hline\n47 & l & 2 & 1 & 0.9387755 & 1 & 1 & 0.0375510 & 0.0070408 & n = 2\\\\\n\\hline\n48 & l & 2 & 1 & 0.9591837 & 1 & 1 & 0.0383673 & 0.0047959 & n = 2\\\\\n\\hline\n49 & l & 2 & 1 & 0.9795918 & 1 & 1 & 0.0391837 & 0.0024490 & n = 2\\\\\n\\hline\n50 & l & 2 & 1 & 1.0000000 & 1 & 1 & 0.0400000 & 0.0000000 & n = 2\\\\\n\\hline\n1 & w & 3 & 2 & 0.0000000 & 2 & 1 & 0.0000000 & 0.0000000 & n = 3\\\\\n\\hline\n2 & w & 3 & 2 & 0.0204082 & 2 & 1 & 0.0024490 & 0.0001000 & n = 3\\\\\n\\hline\n3 & w & 3 & 2 & 0.0408163 & 2 & 1 & 0.0047959 & 0.0003915 & n = 3\\\\\n\\hline\n4 & w & 3 & 2 & 0.0612245 & 2 & 1 & 0.0070408 & 0.0008621 & n = 3\\\\\n\\hline\n5 & w & 3 & 2 & 0.0816327 & 2 & 1 & 0.0091837 & 0.0014994 & n = 3\\\\\n\\hline\n6 & w & 3 & 2 & 0.1020408 & 2 & 1 & 0.0112245 & 0.0022907 & n = 3\\\\\n\\hline\n7 & w & 3 & 2 & 0.1224490 & 2 & 1 & 0.0131633 & 0.0032237 & n = 3\\\\\n\\hline\n8 & w & 3 & 2 & 0.1428571 & 2 & 1 & 0.0150000 & 0.0042857 & n = 3\\\\\n\\hline\n9 & w & 3 & 2 & 0.1632653 & 2 & 1 & 0.0167347 & 0.0054644 & n = 3\\\\\n\\hline\n10 & w & 3 & 2 & 0.1836735 & 2 & 1 & 0.0183673 & 0.0067472 & n = 3\\\\\n\\hline\n11 & w & 3 & 2 & 0.2040816 & 2 & 1 & 0.0198980 & 0.0081216 & n = 3\\\\\n\\hline\n12 & w & 3 & 2 & 0.2244898 & 2 & 1 & 0.0213265 & 0.0095752 & n = 3\\\\\n\\hline\n13 & w & 3 & 2 & 0.2448980 & 2 & 1 & 0.0226531 & 0.0110954 & n = 3\\\\\n\\hline\n14 & w & 3 & 2 & 0.2653061 & 2 & 1 & 0.0238776 & 0.0126697 & n = 3\\\\\n\\hline\n15 & w & 3 & 2 & 0.2857143 & 2 & 1 & 0.0250000 & 0.0142857 & n = 3\\\\\n\\hline\n16 & w & 3 & 2 & 0.3061224 & 2 & 1 & 0.0260204 & 0.0159309 & n = 3\\\\\n\\hline\n17 & w & 3 & 2 & 0.3265306 & 2 & 1 & 0.0269388 & 0.0175927 & n = 3\\\\\n\\hline\n18 & w & 3 & 2 & 0.3469388 & 2 & 1 & 0.0277551 & 0.0192586 & n = 3\\\\\n\\hline\n19 & w & 3 & 2 & 0.3673469 & 2 & 1 & 0.0284694 & 0.0209163 & n = 3\\\\\n\\hline\n20 & w & 3 & 2 & 0.3877551 & 2 & 1 & 0.0290816 & 0.0225531 & n = 3\\\\\n\\hline\n21 & w & 3 & 2 & 0.4081633 & 2 & 1 & 0.0295918 & 0.0241566 & n = 3\\\\\n\\hline\n22 & w & 3 & 2 & 0.4285714 & 2 & 1 & 0.0300000 & 0.0257143 & n = 3\\\\\n\\hline\n23 & w & 3 & 2 & 0.4489796 & 2 & 1 & 0.0303061 & 0.0272137 & n = 3\\\\\n\\hline\n24 & w & 3 & 2 & 0.4693878 & 2 & 1 & 0.0305102 & 0.0286422 & n = 3\\\\\n\\hline\n25 & w & 3 & 2 & 0.4897959 & 2 & 1 & 0.0306122 & 0.0299875 & n = 3\\\\\n\\hline\n26 & w & 3 & 2 & 0.5102041 & 2 & 1 & 0.0306122 & 0.0312370 & n = 3\\\\\n\\hline\n27 & w & 3 & 2 & 0.5306122 & 2 & 1 & 0.0305102 & 0.0323782 & n = 3\\\\\n\\hline\n28 & w & 3 & 2 & 0.5510204 & 2 & 1 & 0.0303061 & 0.0333986 & n = 3\\\\\n\\hline\n29 & w & 3 & 2 & 0.5714286 & 2 & 1 & 0.0300000 & 0.0342857 & n = 3\\\\\n\\hline\n30 & w & 3 & 2 & 0.5918367 & 2 & 1 & 0.0295918 & 0.0350271 & n = 3\\\\\n\\hline\n31 & w & 3 & 2 & 0.6122449 & 2 & 1 & 0.0290816 & 0.0356102 & n = 3\\\\\n\\hline\n32 & w & 3 & 2 & 0.6326531 & 2 & 1 & 0.0284694 & 0.0360225 & n = 3\\\\\n\\hline\n33 & w & 3 & 2 & 0.6530612 & 2 & 1 & 0.0277551 & 0.0362516 & n = 3\\\\\n\\hline\n34 & w & 3 & 2 & 0.6734694 & 2 & 1 & 0.0269388 & 0.0362849 & n = 3\\\\\n\\hline\n35 & w & 3 & 2 & 0.6938776 & 2 & 1 & 0.0260204 & 0.0361100 & n = 3\\\\\n\\hline\n36 & w & 3 & 2 & 0.7142857 & 2 & 1 & 0.0250000 & 0.0357143 & n = 3\\\\\n\\hline\n37 & w & 3 & 2 & 0.7346939 & 2 & 1 & 0.0238776 & 0.0350854 & n = 3\\\\\n\\hline\n38 & w & 3 & 2 & 0.7551020 & 2 & 1 & 0.0226531 & 0.0342107 & n = 3\\\\\n\\hline\n39 & w & 3 & 2 & 0.7755102 & 2 & 1 & 0.0213265 & 0.0330779 & n = 3\\\\\n\\hline\n40 & w & 3 & 2 & 0.7959184 & 2 & 1 & 0.0198980 & 0.0316743 & n = 3\\\\\n\\hline\n41 & w & 3 & 2 & 0.8163265 & 2 & 1 & 0.0183673 & 0.0299875 & n = 3\\\\\n\\hline\n42 & w & 3 & 2 & 0.8367347 & 2 & 1 & 0.0167347 & 0.0280050 & n = 3\\\\\n\\hline\n43 & w & 3 & 2 & 0.8571429 & 2 & 1 & 0.0150000 & 0.0257143 & n = 3\\\\\n\\hline\n44 & w & 3 & 2 & 0.8775510 & 2 & 1 & 0.0131633 & 0.0231029 & n = 3\\\\\n\\hline\n45 & w & 3 & 2 & 0.8979592 & 2 & 1 & 0.0112245 & 0.0201583 & n = 3\\\\\n\\hline\n46 & w & 3 & 2 & 0.9183673 & 2 & 1 & 0.0091837 & 0.0168680 & n = 3\\\\\n\\hline\n47 & w & 3 & 2 & 0.9387755 & 2 & 1 & 0.0070408 & 0.0132195 & n = 3\\\\\n\\hline\n48 & w & 3 & 2 & 0.9591837 & 2 & 1 & 0.0047959 & 0.0092003 & n = 3\\\\\n\\hline\n49 & w & 3 & 2 & 0.9795918 & 2 & 1 & 0.0024490 & 0.0047980 & n = 3\\\\\n\\hline\n50 & w & 3 & 2 & 1.0000000 & 2 & 1 & 0.0000000 & 0.0000000 & n = 3\\\\\n\\hline\n1 & w & 4 & 3 & 0.0000000 & 3 & 2 & 0.0000000 & 0.0000000 & n = 4\\\\\n\\hline\n2 & w & 4 & 3 & 0.0204082 & 3 & 2 & 0.0001000 & 0.0000034 & n = 4\\\\\n\\hline\n3 & w & 4 & 3 & 0.0408163 & 3 & 2 & 0.0003915 & 0.0000266 & n = 4\\\\\n\\hline\n4 & w & 4 & 3 & 0.0612245 & 3 & 2 & 0.0008621 & 0.0000880 & n = 4\\\\\n\\hline\n5 & w & 4 & 3 & 0.0816327 & 3 & 2 & 0.0014994 & 0.0002041 & n = 4\\\\\n\\hline\n6 & w & 4 & 3 & 0.1020408 & 3 & 2 & 0.0022907 & 0.0003897 & n = 4\\\\\n\\hline\n7 & w & 4 & 3 & 0.1224490 & 3 & 2 & 0.0032237 & 0.0006581 & n = 4\\\\\n\\hline\n8 & w & 4 & 3 & 0.1428571 & 3 & 2 & 0.0042857 & 0.0010207 & n = 4\\\\\n\\hline\n9 & w & 4 & 3 & 0.1632653 & 3 & 2 & 0.0054644 & 0.0014873 & n = 4\\\\\n\\hline\n10 & w & 4 & 3 & 0.1836735 & 3 & 2 & 0.0067472 & 0.0020660 & n = 4\\\\\n\\hline\n11 & w & 4 & 3 & 0.2040816 & 3 & 2 & 0.0081216 & 0.0027632 & n = 4\\\\\n\\hline\n12 & w & 4 & 3 & 0.2244898 & 3 & 2 & 0.0095752 & 0.0035835 & n = 4\\\\\n\\hline\n13 & w & 4 & 3 & 0.2448980 & 3 & 2 & 0.0110954 & 0.0045300 & n = 4\\\\\n\\hline\n14 & w & 4 & 3 & 0.2653061 & 3 & 2 & 0.0126697 & 0.0056038 & n = 4\\\\\n\\hline\n15 & w & 4 & 3 & 0.2857143 & 3 & 2 & 0.0142857 & 0.0068046 & n = 4\\\\\n\\hline\n16 & w & 4 & 3 & 0.3061224 & 3 & 2 & 0.0159309 & 0.0081302 & n = 4\\\\\n\\hline\n17 & w & 4 & 3 & 0.3265306 & 3 & 2 & 0.0175927 & 0.0095769 & n = 4\\\\\n\\hline\n18 & w & 4 & 3 & 0.3469388 & 3 & 2 & 0.0192586 & 0.0111390 & n = 4\\\\\n\\hline\n19 & w & 4 & 3 & 0.3673469 & 3 & 2 & 0.0209163 & 0.0128094 & n = 4\\\\\n\\hline\n20 & w & 4 & 3 & 0.3877551 & 3 & 2 & 0.0225531 & 0.0145792 & n = 4\\\\\n\\hline\n21 & w & 4 & 3 & 0.4081633 & 3 & 2 & 0.0241566 & 0.0164376 & n = 4\\\\\n\\hline\n22 & w & 4 & 3 & 0.4285714 & 3 & 2 & 0.0257143 & 0.0183724 & n = 4\\\\\n\\hline\n23 & w & 4 & 3 & 0.4489796 & 3 & 2 & 0.0272137 & 0.0203696 & n = 4\\\\\n\\hline\n24 & w & 4 & 3 & 0.4693878 & 3 & 2 & 0.0286422 & 0.0224134 & n = 4\\\\\n\\hline\n25 & w & 4 & 3 & 0.4897959 & 3 & 2 & 0.0299875 & 0.0244864 & n = 4\\\\\n\\hline\n26 & w & 4 & 3 & 0.5102041 & 3 & 2 & 0.0312370 & 0.0265694 & n = 4\\\\\n\\hline\n27 & w & 4 & 3 & 0.5306122 & 3 & 2 & 0.0323782 & 0.0286417 & n = 4\\\\\n\\hline\n28 & w & 4 & 3 & 0.5510204 & 3 & 2 & 0.0333986 & 0.0306807 & n = 4\\\\\n\\hline\n29 & w & 4 & 3 & 0.5714286 & 3 & 2 & 0.0342857 & 0.0326621 & n = 4\\\\\n\\hline\n30 & w & 4 & 3 & 0.5918367 & 3 & 2 & 0.0350271 & 0.0345601 & n = 4\\\\\n\\hline\n31 & w & 4 & 3 & 0.6122449 & 3 & 2 & 0.0356102 & 0.0363470 & n = 4\\\\\n\\hline\n32 & w & 4 & 3 & 0.6326531 & 3 & 2 & 0.0360225 & 0.0379934 & n = 4\\\\\n\\hline\n33 & w & 4 & 3 & 0.6530612 & 3 & 2 & 0.0362516 & 0.0394684 & n = 4\\\\\n\\hline\n34 & w & 4 & 3 & 0.6734694 & 3 & 2 & 0.0362849 & 0.0407392 & n = 4\\\\\n\\hline\n35 & w & 4 & 3 & 0.6938776 & 3 & 2 & 0.0361100 & 0.0417714 & n = 4\\\\\n\\hline\n36 & w & 4 & 3 & 0.7142857 & 3 & 2 & 0.0357143 & 0.0425288 & n = 4\\\\\n\\hline\n37 & w & 4 & 3 & 0.7346939 & 3 & 2 & 0.0350854 & 0.0429736 & n = 4\\\\\n\\hline\n38 & w & 4 & 3 & 0.7551020 & 3 & 2 & 0.0342107 & 0.0430663 & n = 4\\\\\n\\hline\n39 & w & 4 & 3 & 0.7755102 & 3 & 2 & 0.0330779 & 0.0427656 & n = 4\\\\\n\\hline\n40 & w & 4 & 3 & 0.7959184 & 3 & 2 & 0.0316743 & 0.0420286 & n = 4\\\\\n\\hline\n41 & w & 4 & 3 & 0.8163265 & 3 & 2 & 0.0299875 & 0.0408107 & n = 4\\\\\n\\hline\n42 & w & 4 & 3 & 0.8367347 & 3 & 2 & 0.0280050 & 0.0390654 & n = 4\\\\\n\\hline\n43 & w & 4 & 3 & 0.8571429 & 3 & 2 & 0.0257143 & 0.0367449 & n = 4\\\\\n\\hline\n44 & w & 4 & 3 & 0.8775510 & 3 & 2 & 0.0231029 & 0.0337993 & n = 4\\\\\n\\hline\n45 & w & 4 & 3 & 0.8979592 & 3 & 2 & 0.0201583 & 0.0301772 & n = 4\\\\\n\\hline\n46 & w & 4 & 3 & 0.9183673 & 3 & 2 & 0.0168680 & 0.0258255 & n = 4\\\\\n\\hline\n47 & w & 4 & 3 & 0.9387755 & 3 & 2 & 0.0132195 & 0.0206893 & n = 4\\\\\n\\hline\n48 & w & 4 & 3 & 0.9591837 & 3 & 2 & 0.0092003 & 0.0147121 & n = 4\\\\\n\\hline\n49 & w & 4 & 3 & 0.9795918 & 3 & 2 & 0.0047980 & 0.0078356 & n = 4\\\\\n\\hline\n50 & w & 4 & 3 & 1.0000000 & 3 & 2 & 0.0000000 & 0.0000000 & n = 4\\\\\n\\hline\n1 & w & 5 & 4 & 0.0000000 & 4 & 3 & 0.0000000 & 0.0000000 & n = 5\\\\\n\\hline\n2 & w & 5 & 4 & 0.0204082 & 4 & 3 & 0.0000034 & 0.0000001 & n = 5\\\\\n\\hline\n3 & w & 5 & 4 & 0.0408163 & 4 & 3 & 0.0000266 & 0.0000016 & n = 5\\\\\n\\hline\n4 & w & 5 & 4 & 0.0612245 & 4 & 3 & 0.0000880 & 0.0000081 & n = 5\\\\\n\\hline\n5 & w & 5 & 4 & 0.0816327 & 4 & 3 & 0.0002041 & 0.0000250 & n = 5\\\\\n\\hline\n6 & w & 5 & 4 & 0.1020408 & 4 & 3 & 0.0003897 & 0.0000597 & n = 5\\\\\n\\hline\n7 & w & 5 & 4 & 0.1224490 & 4 & 3 & 0.0006581 & 0.0001209 & n = 5\\\\\n\\hline\n8 & w & 5 & 4 & 0.1428571 & 4 & 3 & 0.0010207 & 0.0002188 & n = 5\\\\\n\\hline\n9 & w & 5 & 4 & 0.1632653 & 4 & 3 & 0.0014873 & 0.0003644 & n = 5\\\\\n\\hline\n10 & w & 5 & 4 & 0.1836735 & 4 & 3 & 0.0020660 & 0.0005694 & n = 5\\\\\n\\hline\n11 & w & 5 & 4 & 0.2040816 & 4 & 3 & 0.0027632 & 0.0008462 & n = 5\\\\\n\\hline\n12 & w & 5 & 4 & 0.2244898 & 4 & 3 & 0.0035835 & 0.0012071 & n = 5\\\\\n\\hline\n13 & w & 5 & 4 & 0.2448980 & 4 & 3 & 0.0045300 & 0.0016647 & n = 5\\\\\n\\hline\n14 & w & 5 & 4 & 0.2653061 & 4 & 3 & 0.0056038 & 0.0022309 & n = 5\\\\\n\\hline\n15 & w & 5 & 4 & 0.2857143 & 4 & 3 & 0.0068046 & 0.0029173 & n = 5\\\\\n\\hline\n16 & w & 5 & 4 & 0.3061224 & 4 & 3 & 0.0081302 & 0.0037346 & n = 5\\\\\n\\hline\n17 & w & 5 & 4 & 0.3265306 & 4 & 3 & 0.0095769 & 0.0046924 & n = 5\\\\\n\\hline\n18 & w & 5 & 4 & 0.3469388 & 4 & 3 & 0.0111390 & 0.0057989 & n = 5\\\\\n\\hline\n19 & w & 5 & 4 & 0.3673469 & 4 & 3 & 0.0128094 & 0.0070607 & n = 5\\\\\n\\hline\n20 & w & 5 & 4 & 0.3877551 & 4 & 3 & 0.0145792 & 0.0084827 & n = 5\\\\\n\\hline\n21 & w & 5 & 4 & 0.4081633 & 4 & 3 & 0.0164376 & 0.0100673 & n = 5\\\\\n\\hline\n22 & w & 5 & 4 & 0.4285714 & 4 & 3 & 0.0183724 & 0.0118150 & n = 5\\\\\n\\hline\n23 & w & 5 & 4 & 0.4489796 & 4 & 3 & 0.0203696 & 0.0137231 & n = 5\\\\\n\\hline\n24 & w & 5 & 4 & 0.4693878 & 4 & 3 & 0.0224134 & 0.0157864 & n = 5\\\\\n\\hline\n25 & w & 5 & 4 & 0.4897959 & 4 & 3 & 0.0244864 & 0.0179963 & n = 5\\\\\n\\hline\n26 & w & 5 & 4 & 0.5102041 & 4 & 3 & 0.0265694 & 0.0203408 & n = 5\\\\\n\\hline\n27 & w & 5 & 4 & 0.5306122 & 4 & 3 & 0.0286417 & 0.0228044 & n = 5\\\\\n\\hline\n28 & w & 5 & 4 & 0.5510204 & 4 & 3 & 0.0306807 & 0.0253673 & n = 5\\\\\n\\hline\n29 & w & 5 & 4 & 0.5714286 & 4 & 3 & 0.0326621 & 0.0280058 & n = 5\\\\\n\\hline\n30 & w & 5 & 4 & 0.5918367 & 4 & 3 & 0.0345601 & 0.0306916 & n = 5\\\\\n\\hline\n31 & w & 5 & 4 & 0.6122449 & 4 & 3 & 0.0363470 & 0.0333915 & n = 5\\\\\n\\hline\n32 & w & 5 & 4 & 0.6326531 & 4 & 3 & 0.0379934 & 0.0360675 & n = 5\\\\\n\\hline\n33 & w & 5 & 4 & 0.6530612 & 4 & 3 & 0.0394684 & 0.0386764 & n = 5\\\\\n\\hline\n34 & w & 5 & 4 & 0.6734694 & 4 & 3 & 0.0407392 & 0.0411692 & n = 5\\\\\n\\hline\n35 & w & 5 & 4 & 0.6938776 & 4 & 3 & 0.0417714 & 0.0434915 & n = 5\\\\\n\\hline\n36 & w & 5 & 4 & 0.7142857 & 4 & 3 & 0.0425288 & 0.0455824 & n = 5\\\\\n\\hline\n37 & w & 5 & 4 & 0.7346939 & 4 & 3 & 0.0429736 & 0.0473751 & n = 5\\\\\n\\hline\n38 & w & 5 & 4 & 0.7551020 & 4 & 3 & 0.0430663 & 0.0487961 & n = 5\\\\\n\\hline\n39 & w & 5 & 4 & 0.7755102 & 4 & 3 & 0.0427656 & 0.0497650 & n = 5\\\\\n\\hline\n40 & w & 5 & 4 & 0.7959184 & 4 & 3 & 0.0420286 & 0.0501944 & n = 5\\\\\n\\hline\n41 & w & 5 & 4 & 0.8163265 & 4 & 3 & 0.0408107 & 0.0499896 & n = 5\\\\\n\\hline\n42 & w & 5 & 4 & 0.8367347 & 4 & 3 & 0.0390654 & 0.0490481 & n = 5\\\\\n\\hline\n43 & w & 5 & 4 & 0.8571429 & 4 & 3 & 0.0367449 & 0.0472598 & n = 5\\\\\n\\hline\n44 & w & 5 & 4 & 0.8775510 & 4 & 3 & 0.0337993 & 0.0445064 & n = 5\\\\\n\\hline\n45 & w & 5 & 4 & 0.8979592 & 4 & 3 & 0.0301772 & 0.0406610 & n = 5\\\\\n\\hline\n46 & w & 5 & 4 & 0.9183673 & 4 & 3 & 0.0258255 & 0.0355883 & n = 5\\\\\n\\hline\n47 & w & 5 & 4 & 0.9387755 & 4 & 3 & 0.0206893 & 0.0291440 & n = 5\\\\\n\\hline\n48 & w & 5 & 4 & 0.9591837 & 4 & 3 & 0.0147121 & 0.0211748 & n = 5\\\\\n\\hline\n49 & w & 5 & 4 & 0.9795918 & 4 & 3 & 0.0078356 & 0.0115176 & n = 5\\\\\n\\hline\n50 & w & 5 & 4 & 1.0000000 & 4 & 3 & 0.0000000 & 0.0000000 & n = 5\\\\\n\\hline\n1 & l & 6 & 4 & 0.0000000 & 5 & 4 & 0.0000000 & 0.0000000 & n = 6\\\\\n\\hline\n2 & l & 6 & 4 & 0.0204082 & 5 & 4 & 0.0000001 & 0.0000004 & n = 6\\\\\n\\hline\n3 & l & 6 & 4 & 0.0408163 & 5 & 4 & 0.0000016 & 0.0000055 & n = 6\\\\\n\\hline\n4 & l & 6 & 4 & 0.0612245 & 5 & 4 & 0.0000081 & 0.0000265 & n = 6\\\\\n\\hline\n5 & l & 6 & 4 & 0.0816327 & 5 & 4 & 0.0000250 & 0.0000803 & n = 6\\\\\n\\hline\n6 & l & 6 & 4 & 0.1020408 & 5 & 4 & 0.0000597 & 0.0001873 & n = 6\\\\\n\\hline\n7 & l & 6 & 4 & 0.1224490 & 5 & 4 & 0.0001209 & 0.0003710 & n = 6\\\\\n\\hline\n8 & l & 6 & 4 & 0.1428571 & 5 & 4 & 0.0002188 & 0.0006557 & n = 6\\\\\n\\hline\n9 & l & 6 & 4 & 0.1632653 & 5 & 4 & 0.0003644 & 0.0010660 & n = 6\\\\\n\\hline\n10 & l & 6 & 4 & 0.1836735 & 5 & 4 & 0.0005694 & 0.0016252 & n = 6\\\\\n\\hline\n11 & l & 6 & 4 & 0.2040816 & 5 & 4 & 0.0008462 & 0.0023548 & n = 6\\\\\n\\hline\n12 & l & 6 & 4 & 0.2244898 & 5 & 4 & 0.0012071 & 0.0032731 & n = 6\\\\\n\\hline\n13 & l & 6 & 4 & 0.2448980 & 5 & 4 & 0.0016647 & 0.0043949 & n = 6\\\\\n\\hline\n14 & l & 6 & 4 & 0.2653061 & 5 & 4 & 0.0022309 & 0.0057305 & n = 6\\\\\n\\hline\n15 & l & 6 & 4 & 0.2857143 & 5 & 4 & 0.0029173 & 0.0072856 & n = 6\\\\\n\\hline\n16 & l & 6 & 4 & 0.3061224 & 5 & 4 & 0.0037346 & 0.0090602 & n = 6\\\\\n\\hline\n17 & l & 6 & 4 & 0.3265306 & 5 & 4 & 0.0046924 & 0.0110490 & n = 6\\\\\n\\hline\n18 & l & 6 & 4 & 0.3469388 & 5 & 4 & 0.0057989 & 0.0132408 & n = 6\\\\\n\\hline\n19 & l & 6 & 4 & 0.3673469 & 5 & 4 & 0.0070607 & 0.0156182 & n = 6\\\\\n\\hline\n20 & l & 6 & 4 & 0.3877551 & 5 & 4 & 0.0084827 & 0.0181582 & n = 6\\\\\n\\hline\n21 & l & 6 & 4 & 0.4081633 & 5 & 4 & 0.0100673 & 0.0208321 & n = 6\\\\\n\\hline\n22 & l & 6 & 4 & 0.4285714 & 5 & 4 & 0.0118150 & 0.0236053 & n = 6\\\\\n\\hline\n23 & l & 6 & 4 & 0.4489796 & 5 & 4 & 0.0137231 & 0.0264384 & n = 6\\\\\n\\hline\n24 & l & 6 & 4 & 0.4693878 & 5 & 4 & 0.0157864 & 0.0292870 & n = 6\\\\\n\\hline\n25 & l & 6 & 4 & 0.4897959 & 5 & 4 & 0.0179963 & 0.0321027 & n = 6\\\\\n\\hline\n26 & l & 6 & 4 & 0.5102041 & 5 & 4 & 0.0203408 & 0.0348337 & n = 6\\\\\n\\hline\n27 & l & 6 & 4 & 0.5306122 & 5 & 4 & 0.0228044 & 0.0374254 & n = 6\\\\\n\\hline\n28 & l & 6 & 4 & 0.5510204 & 5 & 4 & 0.0253673 & 0.0398215 & n = 6\\\\\n\\hline\n29 & l & 6 & 4 & 0.5714286 & 5 & 4 & 0.0280058 & 0.0419650 & n = 6\\\\\n\\hline\n30 & l & 6 & 4 & 0.5918367 & 5 & 4 & 0.0306916 & 0.0437995 & n = 6\\\\\n\\hline\n31 & l & 6 & 4 & 0.6122449 & 5 & 4 & 0.0333915 & 0.0452699 & n = 6\\\\\n\\hline\n32 & l & 6 & 4 & 0.6326531 & 5 & 4 & 0.0360675 & 0.0463243 & n = 6\\\\\n\\hline\n33 & l & 6 & 4 & 0.6530612 & 5 & 4 & 0.0386764 & 0.0469153 & n = 6\\\\\n\\hline\n34 & l & 6 & 4 & 0.6734694 & 5 & 4 & 0.0411692 & 0.0470016 & n = 6\\\\\n\\hline\n35 & l & 6 & 4 & 0.6938776 & 5 & 4 & 0.0434915 & 0.0465495 & n = 6\\\\\n\\hline\n36 & l & 6 & 4 & 0.7142857 & 5 & 4 & 0.0455824 & 0.0455350 & n = 6\\\\\n\\hline\n37 & l & 6 & 4 & 0.7346939 & 5 & 4 & 0.0473751 & 0.0439454 & n = 6\\\\\n\\hline\n38 & l & 6 & 4 & 0.7551020 & 5 & 4 & 0.0487961 & 0.0417817 & n = 6\\\\\n\\hline\n39 & l & 6 & 4 & 0.7755102 & 5 & 4 & 0.0497650 & 0.0390604 & n = 6\\\\\n\\hline\n40 & l & 6 & 4 & 0.7959184 & 5 & 4 & 0.0501944 & 0.0358159 & n = 6\\\\\n\\hline\n41 & l & 6 & 4 & 0.8163265 & 5 & 4 & 0.0499896 & 0.0321027 & n = 6\\\\\n\\hline\n42 & l & 6 & 4 & 0.8367347 & 5 & 4 & 0.0490481 & 0.0279983 & n = 6\\\\\n\\hline\n43 & l & 6 & 4 & 0.8571429 & 5 & 4 & 0.0472598 & 0.0236053 & n = 6\\\\\n\\hline\n44 & l & 6 & 4 & 0.8775510 & 5 & 4 & 0.0445064 & 0.0190543 & n = 6\\\\\n\\hline\n45 & l & 6 & 4 & 0.8979592 & 5 & 4 & 0.0406610 & 0.0145067 & n = 6\\\\\n\\hline\n46 & l & 6 & 4 & 0.9183673 & 5 & 4 & 0.0355883 & 0.0101575 & n = 6\\\\\n\\hline\n47 & l & 6 & 4 & 0.9387755 & 5 & 4 & 0.0291440 & 0.0062387 & n = 6\\\\\n\\hline\n48 & l & 6 & 4 & 0.9591837 & 5 & 4 & 0.0211748 & 0.0030218 & n = 6\\\\\n\\hline\n49 & l & 6 & 4 & 0.9795918 & 5 & 4 & 0.0115176 & 0.0008218 & n = 6\\\\\n\\hline\n50 & l & 6 & 4 & 1.0000000 & 5 & 4 & 0.0000000 & 0.0000000 & n = 6\\\\\n\\hline\n1 & w & 7 & 5 & 0.0000000 & 6 & 4 & 0.0000000 & 0.0000000 & n = 7\\\\\n\\hline\n2 & w & 7 & 5 & 0.0204082 & 6 & 4 & 0.0000004 & 0.0000000 & n = 7\\\\\n\\hline\n3 & w & 7 & 5 & 0.0408163 & 6 & 4 & 0.0000055 & 0.0000004 & n = 7\\\\\n\\hline\n4 & w & 7 & 5 & 0.0612245 & 6 & 4 & 0.0000265 & 0.0000026 & n = 7\\\\\n\\hline\n5 & w & 7 & 5 & 0.0816327 & 6 & 4 & 0.0000803 & 0.0000105 & n = 7\\\\\n\\hline\n6 & w & 7 & 5 & 0.1020408 & 6 & 4 & 0.0001873 & 0.0000306 & n = 7\\\\\n\\hline\n7 & w & 7 & 5 & 0.1224490 & 6 & 4 & 0.0003710 & 0.0000727 & n = 7\\\\\n\\hline\n8 & w & 7 & 5 & 0.1428571 & 6 & 4 & 0.0006557 & 0.0001499 & n = 7\\\\\n\\hline\n9 & w & 7 & 5 & 0.1632653 & 6 & 4 & 0.0010660 & 0.0002785 & n = 7\\\\\n\\hline\n10 & w & 7 & 5 & 0.1836735 & 6 & 4 & 0.0016252 & 0.0004776 & n = 7\\\\\n\\hline\n11 & w & 7 & 5 & 0.2040816 & 6 & 4 & 0.0023548 & 0.0007689 & n = 7\\\\\n\\hline\n12 & w & 7 & 5 & 0.2244898 & 6 & 4 & 0.0032731 & 0.0011756 & n = 7\\\\\n\\hline\n13 & w & 7 & 5 & 0.2448980 & 6 & 4 & 0.0043949 & 0.0017221 & n = 7\\\\\n\\hline\n14 & w & 7 & 5 & 0.2653061 & 6 & 4 & 0.0057305 & 0.0024326 & n = 7\\\\\n\\hline\n15 & w & 7 & 5 & 0.2857143 & 6 & 4 & 0.0072856 & 0.0033306 & n = 7\\\\\n\\hline\n16 & w & 7 & 5 & 0.3061224 & 6 & 4 & 0.0090602 & 0.0044377 & n = 7\\\\\n\\hline\n17 & w & 7 & 5 & 0.3265306 & 6 & 4 & 0.0110490 & 0.0057726 & n = 7\\\\\n\\hline\n18 & w & 7 & 5 & 0.3469388 & 6 & 4 & 0.0132408 & 0.0073500 & n = 7\\\\\n\\hline\n19 & w & 7 & 5 & 0.3673469 & 6 & 4 & 0.0156182 & 0.0091797 & n = 7\\\\\n\\hline\n20 & w & 7 & 5 & 0.3877551 & 6 & 4 & 0.0181582 & 0.0112655 & n = 7\\\\\n\\hline\n21 & w & 7 & 5 & 0.4081633 & 6 & 4 & 0.0208321 & 0.0136046 & n = 7\\\\\n\\hline\n22 & w & 7 & 5 & 0.4285714 & 6 & 4 & 0.0236053 & 0.0161865 & n = 7\\\\\n\\hline\n23 & w & 7 & 5 & 0.4489796 & 6 & 4 & 0.0264384 & 0.0189925 & n = 7\\\\\n\\hline\n24 & w & 7 & 5 & 0.4693878 & 6 & 4 & 0.0292870 & 0.0219952 & n = 7\\\\\n\\hline\n25 & w & 7 & 5 & 0.4897959 & 6 & 4 & 0.0321027 & 0.0251581 & n = 7\\\\\n\\hline\n26 & w & 7 & 5 & 0.5102041 & 6 & 4 & 0.0348337 & 0.0284357 & n = 7\\\\\n\\hline\n27 & w & 7 & 5 & 0.5306122 & 6 & 4 & 0.0374254 & 0.0317734 & n = 7\\\\\n\\hline\n28 & w & 7 & 5 & 0.5510204 & 6 & 4 & 0.0398215 & 0.0351079 & n = 7\\\\\n\\hline\n29 & w & 7 & 5 & 0.5714286 & 6 & 4 & 0.0419650 & 0.0383681 & n = 7\\\\\n\\hline\n30 & w & 7 & 5 & 0.5918367 & 6 & 4 & 0.0437995 & 0.0414755 & n = 7\\\\\n\\hline\n31 & w & 7 & 5 & 0.6122449 & 6 & 4 & 0.0452699 & 0.0443460 & n = 7\\\\\n\\hline\n32 & w & 7 & 5 & 0.6326531 & 6 & 4 & 0.0463243 & 0.0468916 & n = 7\\\\\n\\hline\n33 & w & 7 & 5 & 0.6530612 & 6 & 4 & 0.0469153 & 0.0490218 & n = 7\\\\\n\\hline\n34 & w & 7 & 5 & 0.6734694 & 6 & 4 & 0.0470016 & 0.0506467 & n = 7\\\\\n\\hline\n35 & w & 7 & 5 & 0.6938776 & 6 & 4 & 0.0465495 & 0.0516795 & n = 7\\\\\n\\hline\n36 & w & 7 & 5 & 0.7142857 & 6 & 4 & 0.0455350 & 0.0520400 & n = 7\\\\\n\\hline\n37 & w & 7 & 5 & 0.7346939 & 6 & 4 & 0.0439454 & 0.0516583 & n = 7\\\\\n\\hline\n38 & w & 7 & 5 & 0.7551020 & 6 & 4 & 0.0417817 & 0.0504792 & n = 7\\\\\n\\hline\n39 & w & 7 & 5 & 0.7755102 & 6 & 4 & 0.0390604 & 0.0484668 & n = 7\\\\\n\\hline\n40 & w & 7 & 5 & 0.7959184 & 6 & 4 & 0.0358159 & 0.0456104 & n = 7\\\\\n\\hline\n41 & w & 7 & 5 & 0.8163265 & 6 & 4 & 0.0321027 & 0.0419301 & n = 7\\\\\n\\hline\n42 & w & 7 & 5 & 0.8367347 & 6 & 4 & 0.0279983 & 0.0374835 & n = 7\\\\\n\\hline\n43 & w & 7 & 5 & 0.8571429 & 6 & 4 & 0.0236053 & 0.0323731 & n = 7\\\\\n\\hline\n44 & w & 7 & 5 & 0.8775510 & 6 & 4 & 0.0190543 & 0.0267538 & n = 7\\\\\n\\hline\n45 & w & 7 & 5 & 0.8979592 & 6 & 4 & 0.0145067 & 0.0208422 & n = 7\\\\\n\\hline\n46 & w & 7 & 5 & 0.9183673 & 6 & 4 & 0.0101575 & 0.0149253 & n = 7\\\\\n\\hline\n47 & w & 7 & 5 & 0.9387755 & 6 & 4 & 0.0062387 & 0.0093707 & n = 7\\\\\n\\hline\n48 & w & 7 & 5 & 0.9591837 & 6 & 4 & 0.0030218 & 0.0046376 & n = 7\\\\\n\\hline\n49 & w & 7 & 5 & 0.9795918 & 6 & 4 & 0.0008218 & 0.0012881 & n = 7\\\\\n\\hline\n50 & w & 7 & 5 & 1.0000000 & 6 & 4 & 0.0000000 & 0.0000000 & n = 7\\\\\n\\hline\n1 & l & 8 & 5 & 0.0000000 & 7 & 5 & 0.0000000 & 0.0000000 & n = 8\\\\\n\\hline\n2 & l & 8 & 5 & 0.0204082 & 7 & 5 & 0.0000000 & 0.0000000 & n = 8\\\\\n\\hline\n3 & l & 8 & 5 & 0.0408163 & 7 & 5 & 0.0000004 & 0.0000010 & n = 8\\\\\n\\hline\n4 & l & 8 & 5 & 0.0612245 & 7 & 5 & 0.0000026 & 0.0000073 & n = 8\\\\\n\\hline\n5 & l & 8 & 5 & 0.0816327 & 7 & 5 & 0.0000105 & 0.0000289 & n = 8\\\\\n\\hline\n6 & l & 8 & 5 & 0.1020408 & 7 & 5 & 0.0000306 & 0.0000824 & n = 8\\\\\n\\hline\n7 & l & 8 & 5 & 0.1224490 & 7 & 5 & 0.0000727 & 0.0001913 & n = 8\\\\\n\\hline\n8 & l & 8 & 5 & 0.1428571 & 7 & 5 & 0.0001499 & 0.0003854 & n = 8\\\\\n\\hline\n9 & l & 8 & 5 & 0.1632653 & 7 & 5 & 0.0002785 & 0.0006990 & n = 8\\\\\n\\hline\n10 & l & 8 & 5 & 0.1836735 & 7 & 5 & 0.0004776 & 0.0011697 & n = 8\\\\\n\\hline\n11 & l & 8 & 5 & 0.2040816 & 7 & 5 & 0.0007689 & 0.0018359 & n = 8\\\\\n\\hline\n12 & l & 8 & 5 & 0.2244898 & 7 & 5 & 0.0011756 & 0.0027351 & n = 8\\\\\n\\hline\n13 & l & 8 & 5 & 0.2448980 & 7 & 5 & 0.0017221 & 0.0039010 & n = 8\\\\\n\\hline\n14 & l & 8 & 5 & 0.2653061 & 7 & 5 & 0.0024326 & 0.0053615 & n = 8\\\\\n\\hline\n15 & l & 8 & 5 & 0.2857143 & 7 & 5 & 0.0033306 & 0.0071369 & n = 8\\\\\n\\hline\n16 & l & 8 & 5 & 0.3061224 & 7 & 5 & 0.0044377 & 0.0092376 & n = 8\\\\\n\\hline\n17 & l & 8 & 5 & 0.3265306 & 7 & 5 & 0.0057726 & 0.0116629 & n = 8\\\\\n\\hline\n18 & l & 8 & 5 & 0.3469388 & 7 & 5 & 0.0073500 & 0.0143999 & n = 8\\\\\n\\hline\n19 & l & 8 & 5 & 0.3673469 & 7 & 5 & 0.0091797 & 0.0174226 & n = 8\\\\\n\\hline\n20 & l & 8 & 5 & 0.3877551 & 7 & 5 & 0.0112655 & 0.0206918 & n = 8\\\\\n\\hline\n21 & l & 8 & 5 & 0.4081633 & 7 & 5 & 0.0136046 & 0.0241551 & n = 8\\\\\n\\hline\n22 & l & 8 & 5 & 0.4285714 & 7 & 5 & 0.0161865 & 0.0277483 & n = 8\\\\\n\\hline\n23 & l & 8 & 5 & 0.4489796 & 7 & 5 & 0.0189925 & 0.0313957 & n = 8\\\\\n\\hline\n24 & l & 8 & 5 & 0.4693878 & 7 & 5 & 0.0219952 & 0.0350126 & n = 8\\\\\n\\hline\n25 & l & 8 & 5 & 0.4897959 & 7 & 5 & 0.0251581 & 0.0385072 & n = 8\\\\\n\\hline\n26 & l & 8 & 5 & 0.5102041 & 7 & 5 & 0.0284357 & 0.0417830 & n = 8\\\\\n\\hline\n27 & l & 8 & 5 & 0.5306122 & 7 & 5 & 0.0317734 & 0.0447420 & n = 8\\\\\n\\hline\n28 & l & 8 & 5 & 0.5510204 & 7 & 5 & 0.0351079 & 0.0472882 & n = 8\\\\\n\\hline\n29 & l & 8 & 5 & 0.5714286 & 7 & 5 & 0.0383681 & 0.0493303 & n = 8\\\\\n\\hline\n30 & l & 8 & 5 & 0.5918367 & 7 & 5 & 0.0414755 & 0.0507862 & n = 8\\\\\n\\hline\n31 & l & 8 & 5 & 0.6122449 & 7 & 5 & 0.0443460 & 0.0515861 & n = 8\\\\\n\\hline\n32 & l & 8 & 5 & 0.6326531 & 7 & 5 & 0.0468916 & 0.0516763 & n = 8\\\\\n\\hline\n33 & l & 8 & 5 & 0.6530612 & 7 & 5 & 0.0490218 & 0.0510225 & n = 8\\\\\n\\hline\n34 & l & 8 & 5 & 0.6734694 & 7 & 5 & 0.0506467 & 0.0496130 & n = 8\\\\\n\\hline\n35 & l & 8 & 5 & 0.6938776 & 7 & 5 & 0.0516795 & 0.0474607 & n = 8\\\\\n\\hline\n36 & l & 8 & 5 & 0.7142857 & 7 & 5 & 0.0520400 & 0.0446056 & n = 8\\\\\n\\hline\n37 & l & 8 & 5 & 0.7346939 & 7 & 5 & 0.0516583 & 0.0411157 & n = 8\\\\\n\\hline\n38 & l & 8 & 5 & 0.7551020 & 7 & 5 & 0.0504792 & 0.0370867 & n = 8\\\\\n\\hline\n39 & l & 8 & 5 & 0.7755102 & 7 & 5 & 0.0484668 & 0.0326409 & n = 8\\\\\n\\hline\n40 & l & 8 & 5 & 0.7959184 & 7 & 5 & 0.0456104 & 0.0279247 & n = 8\\\\\n\\hline\n41 & l & 8 & 5 & 0.8163265 & 7 & 5 & 0.0419301 & 0.0231043 & n = 8\\\\\n\\hline\n42 & l & 8 & 5 & 0.8367347 & 7 & 5 & 0.0374835 & 0.0183592 & n = 8\\\\\n\\hline\n43 & l & 8 & 5 & 0.8571429 & 7 & 5 & 0.0323731 & 0.0138741 & n = 8\\\\\n\\hline\n44 & l & 8 & 5 & 0.8775510 & 7 & 5 & 0.0267538 & 0.0098279 & n = 8\\\\\n\\hline\n45 & l & 8 & 5 & 0.8979592 & 7 & 5 & 0.0208422 & 0.0063803 & n = 8\\\\\n\\hline\n46 & l & 8 & 5 & 0.9183673 & 7 & 5 & 0.0149253 & 0.0036552 & n = 8\\\\\n\\hline\n47 & l & 8 & 5 & 0.9387755 & 7 & 5 & 0.0093707 & 0.0017211 & n = 8\\\\\n\\hline\n48 & l & 8 & 5 & 0.9591837 & 7 & 5 & 0.0046376 & 0.0005679 & n = 8\\\\\n\\hline\n49 & l & 8 & 5 & 0.9795918 & 7 & 5 & 0.0012881 & 0.0000789 & n = 8\\\\\n\\hline\n50 & l & 8 & 5 & 1.0000000 & 7 & 5 & 0.0000000 & 0.0000000 & n = 8\\\\\n\\hline\n1 & w & 9 & 6 & 0.0000000 & 8 & 5 & 0.0000000 & 0.0000000 & n = 9\\\\\n\\hline\n2 & w & 9 & 6 & 0.0204082 & 8 & 5 & 0.0000000 & 0.0000000 & n = 9\\\\\n\\hline\n3 & w & 9 & 6 & 0.0408163 & 8 & 5 & 0.0000010 & 0.0000001 & n = 9\\\\\n\\hline\n4 & w & 9 & 6 & 0.0612245 & 8 & 5 & 0.0000073 & 0.0000007 & n = 9\\\\\n\\hline\n5 & w & 9 & 6 & 0.0816327 & 8 & 5 & 0.0000289 & 0.0000039 & n = 9\\\\\n\\hline\n6 & w & 9 & 6 & 0.1020408 & 8 & 5 & 0.0000824 & 0.0000140 & n = 9\\\\\n\\hline\n7 & w & 9 & 6 & 0.1224490 & 8 & 5 & 0.0001913 & 0.0000391 & n = 9\\\\\n\\hline\n8 & w & 9 & 6 & 0.1428571 & 8 & 5 & 0.0003854 & 0.0000918 & n = 9\\\\\n\\hline\n9 & w & 9 & 6 & 0.1632653 & 8 & 5 & 0.0006990 & 0.0001902 & n = 9\\\\\n\\hline\n10 & w & 9 & 6 & 0.1836735 & 8 & 5 & 0.0011697 & 0.0003581 & n = 9\\\\\n\\hline\n11 & w & 9 & 6 & 0.2040816 & 8 & 5 & 0.0018359 & 0.0006245 & n = 9\\\\\n\\hline\n12 & w & 9 & 6 & 0.2244898 & 8 & 5 & 0.0027351 & 0.0010234 & n = 9\\\\\n\\hline\n13 & w & 9 & 6 & 0.2448980 & 8 & 5 & 0.0039010 & 0.0015922 & n = 9\\\\\n\\hline\n14 & w & 9 & 6 & 0.2653061 & 8 & 5 & 0.0053615 & 0.0023707 & n = 9\\\\\n\\hline\n15 & w & 9 & 6 & 0.2857143 & 8 & 5 & 0.0071369 & 0.0033985 & n = 9\\\\\n\\hline\n16 & w & 9 & 6 & 0.3061224 & 8 & 5 & 0.0092376 & 0.0047131 & n = 9\\\\\n\\hline\n17 & w & 9 & 6 & 0.3265306 & 8 & 5 & 0.0116629 & 0.0063472 & n = 9\\\\\n\\hline\n18 & w & 9 & 6 & 0.3469388 & 8 & 5 & 0.0143999 & 0.0083265 & n = 9\\\\\n\\hline\n19 & w & 9 & 6 & 0.3673469 & 8 & 5 & 0.0174226 & 0.0106669 & n = 9\\\\\n\\hline\n20 & w & 9 & 6 & 0.3877551 & 8 & 5 & 0.0206918 & 0.0133722 & n = 9\\\\\n\\hline\n21 & w & 9 & 6 & 0.4081633 & 8 & 5 & 0.0241551 & 0.0164321 & n = 9\\\\\n\\hline\n22 & w & 9 & 6 & 0.4285714 & 8 & 5 & 0.0277483 & 0.0198202 & n = 9\\\\\n\\hline\n23 & w & 9 & 6 & 0.4489796 & 8 & 5 & 0.0313957 & 0.0234934 & n = 9\\\\\n\\hline\n24 & w & 9 & 6 & 0.4693878 & 8 & 5 & 0.0350126 & 0.0273908 & n = 9\\\\\n\\hline\n25 & w & 9 & 6 & 0.4897959 & 8 & 5 & 0.0385072 & 0.0314344 & n = 9\\\\\n\\hline\n26 & w & 9 & 6 & 0.5102041 & 8 & 5 & 0.0417830 & 0.0355297 & n = 9\\\\\n\\hline\n27 & w & 9 & 6 & 0.5306122 & 8 & 5 & 0.0447420 & 0.0395678 & n = 9\\\\\n\\hline\n28 & w & 9 & 6 & 0.5510204 & 8 & 5 & 0.0472882 & 0.0434279 & n = 9\\\\\n\\hline\n29 & w & 9 & 6 & 0.5714286 & 8 & 5 & 0.0493303 & 0.0469812 & n = 9\\\\\n\\hline\n30 & w & 9 & 6 & 0.5918367 & 8 & 5 & 0.0507862 & 0.0500952 & n = 9\\\\\n\\hline\n31 & w & 9 & 6 & 0.6122449 & 8 & 5 & 0.0515861 & 0.0526388 & n = 9\\\\\n\\hline\n32 & w & 9 & 6 & 0.6326531 & 8 & 5 & 0.0516763 & 0.0544886 & n = 9\\\\\n\\hline\n33 & w & 9 & 6 & 0.6530612 & 8 & 5 & 0.0510225 & 0.0555347 & n = 9\\\\\n\\hline\n34 & w & 9 & 6 & 0.6734694 & 8 & 5 & 0.0496130 & 0.0556880 & n = 9\\\\\n\\hline\n35 & w & 9 & 6 & 0.6938776 & 8 & 5 & 0.0474607 & 0.0548865 & n = 9\\\\\n\\hline\n36 & w & 9 & 6 & 0.7142857 & 8 & 5 & 0.0446056 & 0.0531019 & n = 9\\\\\n\\hline\n37 & w & 9 & 6 & 0.7346939 & 8 & 5 & 0.0411157 & 0.0503458 & n = 9\\\\\n\\hline\n38 & w & 9 & 6 & 0.7551020 & 8 & 5 & 0.0370867 & 0.0466737 & n = 9\\\\\n\\hline\n39 & w & 9 & 6 & 0.7755102 & 8 & 5 & 0.0326409 & 0.0421888 & n = 9\\\\\n\\hline\n40 & w & 9 & 6 & 0.7959184 & 8 & 5 & 0.0279247 & 0.0370430 & n = 9\\\\\n\\hline\n41 & w & 9 & 6 & 0.8163265 & 8 & 5 & 0.0231043 & 0.0314344 & n = 9\\\\\n\\hline\n42 & w & 9 & 6 & 0.8367347 & 8 & 5 & 0.0183592 & 0.0256030 & n = 9\\\\\n\\hline\n43 & w & 9 & 6 & 0.8571429 & 8 & 5 & 0.0138741 & 0.0198202 & n = 9\\\\\n\\hline\n44 & w & 9 & 6 & 0.8775510 & 8 & 5 & 0.0098279 & 0.0143742 & n = 9\\\\\n\\hline\n45 & w & 9 & 6 & 0.8979592 & 8 & 5 & 0.0063803 & 0.0095487 & n = 9\\\\\n\\hline\n46 & w & 9 & 6 & 0.9183673 & 8 & 5 & 0.0036552 & 0.0055947 & n = 9\\\\\n\\hline\n47 & w & 9 & 6 & 0.9387755 & 8 & 5 & 0.0017211 & 0.0026930 & n = 9\\\\\n\\hline\n48 & w & 9 & 6 & 0.9591837 & 8 & 5 & 0.0005679 & 0.0009078 & n = 9\\\\\n\\hline\n49 & w & 9 & 6 & 0.9795918 & 8 & 5 & 0.0000789 & 0.0001288 & n = 9\\\\\n\\hline\n50 & w & 9 & 6 & 1.0000000 & 8 & 5 & 0.0000000 & 0.0000000 & n = 9\\\\\n\\hline\n\\end{tabular}\n\\end{table}\n:::\n:::\n\n\n\n##### Annotation (7): {#sec-annotation-7-ggplot}\n\nThe remainder of the code prepares the plot by using the 50 grid points\nin the range from 0 to 1 as the x-axis; prior and likelihood as y-axis.\nTo distinguish the prior from the likelihood is uses a dashed line for\nthe prior (`linetyp = 2`) and a full line (default) for the likelihood.\nThe x-axis has three breaks (`0, 0.5, 1`) whereas the y-axis has no\nbreak and no scale (`scales = \"free_y\"`).\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: fig-bayesian-model-learning-anno7\n#| fig-cap: \"Demonstrating Bayesian Updating\"\n\n# starting data\nd <- tibble(toss = c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\")) |> \n    mutate(n_trials  = 1:9, n_success = cumsum(toss == \"w\"))\n\nsequence_length <- 50\n\nd %>% \n  expand_grid(p_water = seq(from = 0, to = 1, \n                            length.out = sequence_length)) %>% # <1>\n  group_by(p_water) %>% # <2>\n  mutate(lagged_n_trials  = lag(n_trials, default = 1),\n         lagged_n_success = lag(n_success, default = 1)) %>% # <3>\n  ungroup() %>% # <4>\n  mutate(prior      = ifelse(n_trials == 1, .5,\n                             dbinom(x    = lagged_n_success, \n                                    size = lagged_n_trials, \n                                    prob = p_water)),\n         likelihood = dbinom(x    = n_success, \n                             size = n_trials, \n                             prob = p_water),\n         strip      = str_c(\"n = \", n_trials)) %>% # <5>\n  \n  group_by(n_trials) %>% # <6>\n  mutate(prior      = prior / sum(prior),\n         likelihood = likelihood / sum(likelihood)) %>% # <6>\n  \n  ### add code for annotation <7> for plotting the result ##############\n  ggplot(aes(x = p_water)) + # <7>\n  geom_line(aes(y = prior), \n            linetype = 2) + # <7>\n  geom_line(aes(y = likelihood)) + # <7>\n  scale_x_continuous(\"proportion water\", breaks = c(0, .5, 1)) + # <7>\n  scale_y_continuous(\"plausibility\", breaks = NULL) + # <7>\n  theme(panel.grid = element_blank()) + # <7>\n  facet_wrap(~ strip, scales = \"free_y\") # <7>\n```\n````\n\n::: {.cell-output-display}\n![Demonstrating Bayesian Updating](02-small-and-large-worlds_files/figure-pdf/fig-bayesian-model-learning-anno7-1.pdf){#fig-bayesian-model-learning-anno7 fig-pos='H'}\n:::\n:::\n\n\n\n## Components of the Model\n\n### Original\n\nWe observed three components of the model:\n\n1.  a **likelihood function**: \"the number of ways each conjecture could\n    produce an observation,\"\n\n2.  one or more **parameters**: \"the accumulated number of ways each\n    conjecture could produce the entire data,\" and\n\n3.  **a prior**: \"the initial plausibility of each conjectured cause of\n    the data\"\n\n#### List Variables\n\n> Variables are just symbols that can take on different values. In a\n> scientific context, variables include things we wish to infer, such as\n> proportions and rates, as well as things we might observe, the\n> data....\n>\n> Unobserved variables are usually called **parameters**. (emphasis in\n> the original)\n\nTake as example the globe tossing models: There are three variables: `W`\nand `L` (water or land) and the proportion of water and land `p`. We\nobserve the events of water or land but we calculate (do not observe\ndirectly) the proportion of water and land. So `p` is a parameter as\ndefined above.\n\n#### Define Variables\n\n> In defining each \\[variable\\], we build a model that relates the\n> variables to one another. Remember, the goal is to count all the ways\n> the data could arise, given the assumptions.\n\n##### Observed Variables\n\nFor each unobserved variable (parameter) we need to define the relative\nnumber of ways---the probability---that the values of each observed\nvariable could arise. And then for each unobserved variable, we need to\ndefine the prior plausibility of each value it could take.\n\nFor the count of water *W* and land *L* in the globe tossing model, we\n**define how plausible any combination of *W* and *L* would be, for a\nspecific value of *p*.** (emphasis is mine)\n\n**CHANGE OR MOVE THIS PARAGRAPH**: This idea is implemented in the\nfunctions of `expand()`, `expand_grid()` and `crossing()` of the\n{**tidyr**} package: They generate all combinations of variables found\nas is demonstrated in the section [Bayesian Updating of the\nbrms-variant](02b-small-and-large-worlds.html#sec-expand_grid).\n\nInstead of counting we can also use a mathematical function to calculate\nthe probability of all combinations. A distribution function assigned to\nan observed variable is usually called a **LIKELIHOOD**.\n\nIn the case of the globe-tossing model the appropriate distributional\nfunction is the **binomial distribution**. (Does this means that I have\nto know more on probability theory to decide when to choose which\ndistribution?)\n\n**Likelihood for prob = 0.5**\n\nThe likelihood in the globe-tossing example (9 trials, 6 with `W` and 3\nwith `L`) is easily computed:\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: likelihood-prob-0.5-a\n\n## R code 2.2\ndbinom(6, size = 9, prob = 0.5)\n```\n````\n\n```\n#> [1] 0.1640625\n```\n:::\n\n\n\nIn this example it is assumed that the probability of `W` and `L` are\nequal distributed. We calculated how plausible the combination of *6W*\nand *3L* would be, for the specific value of *p = 0.5*. The result is\nwith 16% a pretty low probability.\n\n**Likelihood for many prob values**\n\nTo get a better idea what the best estimation of the probability is, we\ncould vary systematically the `p` value and look for the maximum. A\ndemonstration how this is done can be seen in @sec-calcu-10-probs. It\nshows a maximum at *prob = 0.7*.\n\n##### Unobserved Variables\n\nEven variables that are not observed (= parameters) we need to define\nthem. In the globe-tossing model there is only one parameter (p), but\nmost models have more than one unobserved variables.\n\n> In future chapters, there will be more parameters in your models. In\n> statistical modeling, many of the most common questions we ask about\n> data are answered directly by parameters:\n>\n> -   What is the average difference between treatment groups?\n> -   How strong is the association between a treatment and an outcome?\n> -   Does the effect of the treatment depend upon a covariate?\n> -   How much variation is there among groups?\n>\n> \\[We will\\] see how these questions become extra parameters inside the\n> distribution function we assign to the data.\n\n::: callout-important\n###### Parameter & Prior\n\nFor every parameter we must provide a distribution of prior\nplausibility, its Prior. This is also true when the number of trials is\nnull (N = 0), e.g. even in the initial state of information we need a\nprior.\n:::\n\nWhen you have a previous estimate, that can become the prior. As a\nresult, each estimate (posterior probability) becomes then the prior for\nthe next step. Where do priors come from? They are both engineering\nassumptions, chosen to help the machine learn, and scientific\nassumptions, chosen to reflect what we know about a phenomenon. Because\nthe prior is an assumption, it should be interrogated like other\nassumptions: by altering it and checking how sensitive inference is to\nthe assumption.\n\n::: callout-note\n###### Data or Parameters\n\nData are measured and known; parameters are unknown and must be\nestimated from data. But there is a deep identity between certainty\n(data) and uncertainty (parameters): Sometimes we observe a variable\n(data), sometimes not (parameter) but it could be that the same\ndistribution function applies. An exploitation of the identity between\ndata & parameters is it to incorporate measurement error and missing\ndata into your modeling.\n\n::: callout-tip\nFor more in this topic, check out McElreath's lecture, [*Understanding\nBayesian statistics without frequentist\nlanguage*](https://youtu.be/yakg94HyWdE).\n:::\n:::\n\n#### A Model is Born\n\n> The observed variables *W* and *L* are given relative counts through\n> the binomial distribution.\n\n$$W∼Binomial(n,p)\\space where\\space N = W + L$$\n\nThe above is just a convention for communicating the assumption that the\nrelative counts of ways to realize *W* in *N* trials with probability\n*p* on each trial comes from the binomial distribution.\n\nOur binomial likelihood contains a parameter for an unobserved variable,\n*p*. Parameters in Bayesian models are assigned priors:\n\n$$p∼Uniform(0,1)$$\n\nwhich expresses the model assumption that the entire range of possible\nvalues for p are equally plausible.\n\n### Tidyverse\n\nGiven a probability of .5, (e.g. equal probability to both events `W`\nand `L`) we can use the `dbinom()` function to determine the likelihood\nof 6 out of 9 tosses coming out water.\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: likelihood-prob-0.5-b\n\ndbinom(x = 6, size = 9, prob = 0.5)\n```\n````\n\n```\n#> [1] 0.1640625\n```\n:::\n\n\n\nMcElreath suggests:\n\n> Change the 0.5 to any other value, to see how the value changes.\n\n#### Calculation likelihood with 10 different values of prob {#sec-calcu-10-probs}\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: likelihood-10-probs\n\n(d <- tibble(prob = seq(from = 0, to = 1, by = .1)) |> \n    mutate(likelihood = dbinom(x = 6, size = 9, prob = prob))\n)\n```\n````\n\n```\n#> # A tibble: 11 x 2\n#>     prob likelihood\n#>    <dbl>      <dbl>\n#>  1   0    0        \n#>  2   0.1  0.0000612\n#>  3   0.2  0.00275  \n#>  4   0.3  0.0210   \n#>  5   0.4  0.0743   \n#>  6   0.5  0.164    \n#>  7   0.6  0.251    \n#>  8   0.7  0.267    \n#>  9   0.8  0.176    \n#> 10   0.9  0.0446   \n#> 11   1    0\n```\n:::\n\n\n\nFilter the row with the `prob` maximum.\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: filter-max\n\nd  |>  \n    filter(likelihood == max(likelihood))\n```\n````\n\n```\n#> # A tibble: 1 x 2\n#>    prob likelihood\n#>   <dbl>      <dbl>\n#> 1   0.7      0.267\n```\n:::\n\n\n\nIn the series of values you will notice several point:\n\n1.  The values start with zero until a maximum of 0.267 and decline to\n    zero again. The maximum is with `prob = 0.7`, a proportion of `W`\n    and `L` that is --- as we know from our large world knowledge\n    (knowledge outside the small world of the model) --- already pretty\n    near the real distribution of about 0.71. (see [How Much of the\n    Earth Is Covered by\n    Water?](https://www.thedailyeco.com/how-much-of-the-earth-is-covered-by-water-122.html))\n2.  You see that the first prob (0) and last prob (1) values are both\n    zero. From the result (6 `W` and 3 `L`) prob cannot be 0 or 1\n    because there a both `W` and `L` in the observed sample.\n\nThe above code chunk is my interpretation from the quote \"Change the 0.5\nto any other value, to see how the value changes.\" Kurz has another\ninterpretation when he draws a graph of 100 prob values from 0 to 1:\n\n#### Plot likelihood for 100 values of prob {#sec-likelihood-for-many-p-values-b}\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: fig-likelihood-100-prob\n#| fig-cap: \"Likelihood for 100 values of prob, from 0 to 1, by steps of 0.01\"\n\ntibble(prob = seq(from = 0, to = 1, by = .01)) %>% \n  ggplot(aes(x = prob, y = dbinom(x = 6, size = 9, prob = prob))) +\n  geom_line() +\n  labs(x = \"probability\",\n       y = \"binomial likelihood\") +\n  theme(panel.grid = element_blank())\n```\n````\n\n::: {.cell-output-display}\n![Likelihood for 100 values of prob, from 0 to 1, by steps of 0.01](02-small-and-large-worlds_files/figure-pdf/fig-likelihood-100-prob-1.pdf){#fig-likelihood-100-prob fig-pos='H'}\n:::\n:::\n\n\n\nIn contrast to *p = 0.5* with a probability of 0.16 the\n@fig-likelihood-100-prob shows a maximum at about *p = 0.7* and a\nprobability estimated from the graph of about 0.26-0.28. We will get\nmore detailed data later in the book.\n\nIt is interesting to see that even the maximum probability is not very\nhigh. The reason is that there are many other configurations\n(distributions of `W`s and `L`s) to produce the result of *6W* and *3L*.\nEven if all these other distributions have a small probability they\n\"eat\" all with their share from the maximum.\n\n(I wanted to write \"from the maximum of 1.0\" but I think this would not\nbe correct as the above graph displays the rate of change in cumulative\nprobability (the **probability density**) and not the probability itself\n(the **probability mass**). See the following quote:\n\n> For mathematical reasons, probability densities can be greater than 1.\n> ... Probability *density* is the rate of change in cumulative\n> probability. So where cumulative probability is increasing rapidly,\n> density can easily exceed 1. But if we calculate the area under the\n> density function, it will never exceed 1. Such areas are also called\n> *probability mass*. (11.47 in calibre ebook-viewer reference mode)\n\nIn the literature the abbreviation **PDF ([probability density\nfunction](https://en.wikipedia.org/wiki/Probability_density_function))**\nis often used for the (probability) density. See also th Wikipedia entry\nabout [probability\ndistribution](https://en.wikipedia.org/wiki/Probability_distribution).\n\nMcElreath says:\n\n> The prior is a probability distribution for the parameter. In general,\n> for a uniform prior from *a* to *b*, the probability of any point in\n> the interval is 1/(*b -- a*). If you're bothered by the fact that the\n> probability of every value of *p* is 1, remember that every\n> probability distribution must sum (integrate) to 1. The expression\n> 1/(*b -- a*) ensures that the area under the flat line from *a* to *b*\n> is equal to 1.\n\nKurz demonstrated the truth of this quote with several *b* values while\nholding *a* constant:\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: uniform-prior1\n\ntibble(a = 0,\n       b = c(1, 1.5, 2, 3, 9)) %>% \n  mutate(prob = 1 / (b - a))\n```\n````\n\n```\n#> # A tibble: 5 x 3\n#>       a     b  prob\n#>   <dbl> <dbl> <dbl>\n#> 1     0   1   1    \n#> 2     0   1.5 0.667\n#> 3     0   2   0.5  \n#> 4     0   3   0.333\n#> 5     0   9   0.111\n```\n:::\n\n\n\nVerified with a plot he divides the range of the *b* parameter (*0-9*)\ninto 500 segments (*parameter_space*) and uses the `dunif()`\ndistribution to calculate the probabilities for a uniform distribution:\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: uniform-prior2\n\ntibble(a = 0,\n       b = c(1, 1.5, 2, 3, 9)) %>% \n  expand_grid(parameter_space = seq(from = 0, to = 9, length.out = 500)) %>% \n  mutate(prob = dunif(parameter_space, a, b),\n         b    = str_c(\"b = \", b)) %>% \n  \n  ggplot(aes(x = parameter_space, y = prob)) +\n  geom_area() +\n  scale_x_continuous(breaks = c(0, 1:3, 9)) +\n  scale_y_continuous(breaks = c(0, 1/9, 1/3, 1/2, 2/3, 1),\n                     labels = c(\"0\", \"1/9\", \"1/3\", \"1/2\", \"2/3\", \"1\")) +\n  theme(panel.grid.minor = element_blank(),\n        panel.grid.major.x = element_blank()) +\n  facet_wrap(~ b, ncol = 5)\n```\n````\n\n::: {.cell-output-display}\n![](02-small-and-large-worlds_files/figure-pdf/uniform-prior2-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nThis figure demonstrates that the area in the whole paramater space is\n*1.0*. It is a nice example how to calculate the probability *mass* (in\ncontrast to the curve of the probability *density*).\n\nI will skip the excursus and plot about the equivalence of\n`Uniform(0,1)` and the beta distribution calculated with `dbeta().`\n\n## Making the Model Go\n\n### Bayes' Theorem\n\n#### Original\n\n> Once you have named all the variables and chosen definitions for each,\n> a Bayesian model can update all of the prior distributions to their\n> purely logical consequences: the **POSTERIOR DISTRIBUTION**. For every\n> unique combination of data, likelihood, parameters, and prior, there\n> is a unique posterior distribution. This distribution contains the\n> relative plausibility of different parameter values, conditional on\n> the data and model. The posterior distribution takes the form of the\n> probability of the parameters, conditional on the data.\n\nIn the case of the globe-tossing model we can write:\n\n$$\nPr(p|W, L)\n$$\n\nThis has to be interpreted as \"the probability of each possible value of\n*p*, conditional on the specific *W* and *L* that we observed.\"\n\n$$\nPr(p|W,L) = \\frac{Pr(W,L|p)Pr(p)}{Pr(W,L)}\n$$\n\n> And this is Bayes' theorem. It says that the probability of any\n> particular value of *p*, considering the data, is equal to the product\n> of the relative plausibility of the data, conditional on *p*, and the\n> prior plausibility of *p*, divided by this thing Pr(*W, L*), which\n> I'll call the *average probability of the data*.\n\nExpressed in words:\n\n$$\nPosterior = \\frac{Probability\\space of\\space the\\space data\\space ✕\\space Prior}{Average\\space probability\\space of\\space the\\space data}\n$$\n\nOther names for the *average probability of the data*:\n\n-   evidence\n-   average likelihood\n-   marginal likelihood\n\nThe job of the average probability of the data is just to standardize\nthe posterior, to ensure it sums (integrates) to one.\n\n::: callout-important\n###### Key lesson\n\nThe posterior is proportional to the product of the prior and the\nprobability of the data.\n:::\n\n> \\[E\\]ach specific value of *p*, the number of paths through the garden\n> of forking data is the product of the prior number of paths and the\n> new number of paths. **Multiplication is just compressed counting.**\n> The average probability on the bottom just standardizes the counts so\n> they sum to one. (emphasis is mine)\n\n> \\[The following graph\\] illustrates the multiplicative interaction of\n> a prior and a probability of data. On each row, a prior on the left is\n> multiplied by the probability of data in the middle to produce a\n> posterior on the right. The probability of data in each case is the\n> same. The priors however vary. As a result, the posterior\n> distributions vary.\n\n![Figure 2.6 of the original book](img/SR2-fig2_6-min.jpg){#fig-2-6-book\nfig-alt=\"The posterior distribution as a product of the prior distribution and likelihood. Top: A flat prior constructs a posterior that is simply proportional to the likelihood. Middle: A step prior, assigning zero probability to all values less than 0.5, results in a truncated posterior. Bottom: A peaked prior that shifts and skews the posterior, relative to the likelihood.\"}\n\nFor my understanding it is important to reproduce the above graph with R\ncode as it is shown in the next section.\n\n#### Tidyverse\n\n> \\[The following graph\\] illustrates the multiplicative interaction of\n> a prior and a probability of data. On each row, a prior on the left is\n> multiplied by the probability of data in the middle to produce a\n> posterior on the right. The probability of data in each case is the\n> same. The priors however vary. As a result, the posterior\n> distributions vary.\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: prepare-model\n\nsequence_length <- 1e3\n\nd <-\n  tibble(probability = seq(from = 0, to = 1, length.out = sequence_length)) %>% \n  expand_grid(row = c(\"flat\", \"stepped\", \"Laplace\"))  %>% \n  arrange(row, probability) %>% \n  mutate(prior = ifelse(row == \"flat\", 1,\n                        ifelse(row == \"stepped\", rep(0:1, each = sequence_length / 2),\n                               exp(-abs(probability - 0.5) / .25) / ( 2 * 0.25))),\n         likelihood = dbinom(x = 6, size = 9, prob = probability)) %>% \n  group_by(row) %>% \n  mutate(posterior = prior * likelihood / sum(prior * likelihood)) %>% \n  pivot_longer(prior:posterior)  %>% \n  ungroup() %>% \n  mutate(name = factor(name, levels = c(\"prior\", \"likelihood\", \"posterior\")),\n         row  = factor(row, levels = c(\"flat\", \"stepped\", \"Laplace\")))\n```\n````\n:::\n\n\n\nIn comparison to my very detailed code annotations of\n@fig-bayesian-model-learning there are different lines of code, but\ngenerally there is nothing conceptually new: We use again\n`expand_grid()` to create a tibble of input combinations and create with\n`mutate()` two columns for prior and likelihood. We do not use the\n`lag()` functions as we calculate only for one prior and one likelihood.\nKurz advises us that is \"easier to just make each column of the plot\nseparately. We can then use the elegant and powerful syntax from [Thomas\nLin Pedersen](https://www.data-imaginist.com/)'s (2022) [patchwork\npackage](https://patchwork.data-imaginist.com/) to combine them.\"\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: fig-plot-model\n#| fig-cap: \"Figure 2.6 from book reproduced with tidyverse code shows the posterior distribution as a product of the prior distribution and likelihood. Top: A flat prior constructs a posterior that is simply proportional to the likelihood. Middle: A step prior, assigning zero probability to all values less than 0.5, results in a truncated posterior. Bottom: A peaked prior that shifts and skews the posterior, relative to the likelihood. Cormpare it with @fig-2-6-book.\"\n\np1 <-\n  d %>%\n  filter(row == \"flat\") %>% \n  ggplot(aes(x = probability, y = value)) +\n  geom_line() +\n  scale_x_continuous(NULL, breaks = NULL) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free_y\")\n\np2 <-\n  d %>%\n  filter(row == \"stepped\") %>% \n  ggplot(aes(x = probability, y = value)) +\n  geom_line() +\n  scale_x_continuous(NULL, breaks = NULL) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank(),\n        strip.background = element_blank(),\n        strip.text = element_blank()) +\n  facet_wrap(~ name, scales = \"free_y\")\n\np3 <-\n  d %>%\n  filter(row == \"Laplace\") %>% \n  ggplot(aes(x = probability, y = value)) +\n  geom_line() +\n  scale_x_continuous(NULL, breaks = c(0, .5, 1)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank(),\n        strip.background = element_blank(),\n        strip.text = element_blank()) +\n  facet_wrap(~ name, scales = \"free_y\")\n\n# combine\n# library(patchwork) # defined in setup chunk\np1 / p2 / p3\n```\n````\n\n::: {.cell-output-display}\n![Figure 2.6 from book reproduced with tidyverse code shows the posterior distribution as a product of the prior distribution and likelihood. Top: A flat prior constructs a posterior that is simply proportional to the likelihood. Middle: A step prior, assigning zero probability to all values less than 0.5, results in a truncated posterior. Bottom: A peaked prior that shifts and skews the posterior, relative to the likelihood. Cormpare it with @fig-2-6-book.](02-small-and-large-worlds_files/figure-pdf/fig-plot-model-1.pdf){#fig-plot-model fig-pos='H'}\n:::\n:::\n\n\n\n@fig-plot-model shows that the same likelihood with a different prior\nresults in a different posterior.\n\n### Motors\n\n#### Original\n\n> Recall that your Bayesian model is a machine, a figurative golem. It\n> has built-in definitions for the likelihood, the parameters, and the\n> prior. And then at its heart lies a motor that processes data,\n> producing a posterior distribution. The action of this motor can be\n> thought of as *conditioning* the prior on the data. As explained in\n> the previous section, this conditioning is governed by the rules of\n> probability theory, which defines a uniquely logical posterior for set\n> of assumptions and observations.\n>\n> However, knowing the mathematical rule is often of little help,\n> because many of the interesting models in contemporary science cannot\n> be conditioned formally, no matter your skill in mathematics. And\n> while some broadly useful models like linear regression can be\n> conditioned formally, this is only possible if you constrain your\n> choice of prior to special forms that are easy to do mathematics with.\n> We'd like to avoid forced modeling choices of this kind, instead\n> favoring conditioning engines that can accommodate whichever prior is\n> most useful for inference.\n>\n> What this means is that various numerical techniques are needed to\n> approximate the mathematics that follows from the definition of Bayes'\n> theorem. In this book, you'll meet three different conditioning\n> engines, numerical techniques for computing posterior distributions:\n\nWhat are the numerical techniques for computing posterior distributions\nexplained in the book?\n\n1.  Grid approximation\n2.  Quadratic approximation\n3.  Markov chain Monte Carlo (MCMC)\n\n> There are many other engines, and new ones are being invented all the\n> time. But the three you'll get to know here are common and widely\n> useful. (p. 39)\n\n::: callout-tip\n**Rethinking: How you fit the model is part of the model**. Earlier in\nthis chapter, I implicitly defined the model as a composite of a prior\nand a likelihood. That definition is typical. But in practical terms, we\nshould also consider how the model is fit to data as part of the model.\nIn very simple problems, like the globe tossing example that consumes\nthis chapter, calculation of the posterior density is trivial and\nfoolproof. In even moderately complex problems, however, the details of\nfitting the model to data force us to recognize that our numerical\ntechnique influences our inferences. This is because different mistakes\nand compromises arise under different techniques. The same model fit to\nthe same data using different techniques may produce different answers.\nWhen something goes wrong, every piece of the machine may be suspect.\nAnd so our golems carry with them their updating engines, as much slaves\nto their engineering as they are to the priors and likelihoods we\nprogram into them.\n:::\n\n#### Tidyverse\n\nIn my own words: Processing the built-in definitions for the likelihood,\nthe parameters, and the prior produces the posterior distribution. This\nprocess is governed by the rule of probability theory. But knowing the\nmathematics does generally not help for two reasons:\n\n-   Many of the interesting models in contemporary science cannot be\n    conditioned formally\n-   Though some broadly useful models like linear regression can be\n    conditioned formally, this is only possible if you constrain your\n    choice of prior to special forms that are easy to do mathematics\n    with.\n\nTherefore are various numerical techniques needed to approximate the\nmathematics that follows from the definition of Bayes' theorem. From the\nthree widely useful methods (grid approximation, quadratic approximation\nand MCMC) covered in the SR2-book the tidyverse version of this material\nconcentrates on using the [{**brms**}\npackage](https://paul-buerkner.github.io/brms/).\n\n::: {.callout-caution style=\"color: orange;\"}\n###### Jumping quickly into MCMC\n\n> The consequence is that this version will jump rather quickly into\n> MCMC. This will be awkward at times because it will force us to\n> contend with technical issues in earlier problems in the text than\n> McElreath originally did.\n:::\n\n### Grid approximation\n\n#### Original\n\n> While most parameters are *continuous*, capable of taking on an\n> infinite number of values, it turns out that we can achieve an\n> excellent approximation of the continuous posterior distribution by\n> considering only a finite grid of parameter values. At any particular\n> value of a parameter, *p*', it's a simple matter to compute the\n> posterior probability: just multiply the prior probability of *p*' by\n> the likelihood at *p*'. Repeating this procedure for each value in the\n> grid generates an approximate picture of the exact posterior\n> distribution. This procedure is called **GRID APPROXIMATION**.\n\nGrid approximation is very useful as a pedagogical tool. But in most of\nyour real modeling, grid approximation isn't practical because it scales\npoorly, as the number of parameters increases.\n\n1.  Define the grid. This means you decide how many points to use in\n    estimating the posterior, and then you make a list of the parameter\n    values on the grid.\n2.  Compute the value of the prior at each parameter value on the grid.\n3.  Compute the likelihood at each parameter value.\n4.  Compute the unstandardized posterior at each parameter value, by\n    multiplying the prior by the likelihood.\n5.  Finally, standardize the posterior, by dividing each value by the\n    sum of all values.\n\nIn the globe tossing model the five steps are as follows:\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: fig-grid-approx-base1\n#| fig-cap: \"Grid Approximation with 20 points with `prior = 1`\"\n\n# 1. define grid\np_grid <- seq(from = 0, to = 1, length.out = 20)\n\n# 2. define prior\nprior <- rep(1, 20)\n\n# 3. compute likelihood at each value in grid\nlikelihood <- dbinom(x = 6, size = 9, prob = p_grid)\n\n# 4. compute product of likelihood and prior\nunstd.posterior <- likelihood * prior\n\n# 5. standardize the posterior, so it sums to 1\nposterior <- unstd.posterior / sum(unstd.posterior)\n\n# 6 display the posterior distribution\n## R code 2.4\nplot(p_grid, posterior,\n  type = \"b\",\n  xlab = \"probability of water\", ylab = \"posterior probability\"\n)\nmtext('20 points with \"prior = 1\"')\n\n```\n````\n\n::: {.cell-output-display}\n![Grid Approximation with 20 points with `prior = 1`](02-small-and-large-worlds_files/figure-pdf/fig-grid-approx-base1-1.pdf){#fig-grid-approx-base1 fig-pos='H'}\n:::\n:::\n\n\n\n\n``` r\n#| label: lst-own-tidyverse\n#| lst-cap: \"My own tidyverse plot <-  just for learning purposes\"\n\n# # instead of books R code 2.4 I will use a tidyverse approach\n# df <- dplyr::bind_cols(\"prob\" = p_grid, \"post\" = posterior)\n# ggplot2::ggplot(df, ggplot2::aes(x = prob, y = post)) +\n#     ggplot2::geom_line() +\n#     ggplot2::geom_point()\n```\n\n\n##### Change likelihood parameters\n\nThe parameters for the calculated likelihood is based on the binomial\ndistribution and is shaping the above plot:\n\n-   x = number of water events `W`\n-   size = number of sample trials = number of observations\n-   prob = success probability on each trial = probability of `W` (water\n    event)\n\nIt does not matter in the code for @fig-grid-approx-base1 what prior\nprobability is chosen in the range from 0 to 1, if the probability of\n`W` is greater than zero and --- and by definition of the binomial\nfunction --- equal for all 20 events. This does not only conform to\nvalues but also for functions.\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: fig-grid-approx-base1a\n#| fig-cap: \"Grid Approximation with 20 points with `prior = 0.1`\"\n\n# 1. define grid\np_grid <- seq(from = 0, to = 1, length.out = 20)\n\n# 2. define prior\nprior <- rep(0.1, 20)\n\n# 3. compute likelihood at each value in grid\nlikelihood <- dbinom(x = 6, size = 9, prob = p_grid)\n\n# 4. compute product of likelihood and prior\nunstd.posterior <- likelihood * prior\n\n# 5. standardize the posterior, so it sums to 1\nposterior <- unstd.posterior / sum(unstd.posterior)\n\n# 6 display the posterior distribution\n## R code 2.4\nplot(p_grid, posterior,\n  type = \"b\",\n  xlab = \"probability of water\", ylab = \"posterior probability\"\n)\nmtext('20 points with \"prior = 0.1\"')\n```\n````\n\n::: {.cell-output-display}\n![Grid Approximation with 20 points with `prior = 0.1`](02-small-and-large-worlds_files/figure-pdf/fig-grid-approx-base1a-1.pdf){#fig-grid-approx-base1a fig-pos='H'}\n:::\n:::\n\n\n\n##### Changing prior parameters\n\nTo see the influence of the prior probability on the posterior\nprobability by using the same likelihood the book offers two code\nsnippets. Replace the definition of the prior from the `grid-approx-a`\nchunk (number 2 in the code snippet) --- one at a time --- with the\nfollowing lines of code:\n\n``` r\n#| label: lst-different-priors\n#| lst-cap: \"Using two different priors\"\nprior <- ifelse(p_grid < 0.5, 0, 1)\nprior <- exp(-5 * abs(p_grid - 0.5))\n```\n\nThe rest of the code remains the same.\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: fig-grid-approx-base2\n#| fig-cap: \"Grid Approximation with prior of `ifelse(p_grid < 0.5, 0, 1)`\"\n\n# 1. define grid\np_grid <- seq(from = 0, to = 1, length.out = 20)\n\n# 2. define prior\nprior <- ifelse(p_grid < 0.5, 0, 1)\n\n# 3. compute likelihood at each value in grid\nlikelihood <- dbinom(x = 6, size = 9, prob = p_grid)\n\n# 4. compute product of likelihood and prior\nunstd.posterior <- likelihood * prior\n\n# 5. standardize the posterior, so it sums to 1\nposterior <- unstd.posterior / sum(unstd.posterior)\n\n# 6 display the posterior distribution \n## R code 2.4\nplot(p_grid, posterior,\n  type = \"b\",\n  xlab = \"probability of water\", ylab = \"posterior probability\"\n)\nmtext('20 points with a prior of \"ifelse(p_grid < 0.5, 0, 1)\"')\n```\n````\n\n::: {.cell-output-display}\n![Grid Approximation with prior of `ifelse(p_grid < 0.5, 0, 1)`](02-small-and-large-worlds_files/figure-pdf/fig-grid-approx-base2-1.pdf){#fig-grid-approx-base2 fig-pos='H'}\n:::\n:::\n\n\n\n@fig-grid-approx-base2 employs as prior `ifelse(p_grid < 0.5, 0, 1)`,\nmeaning that if `prob` is smaller than 0.5 use zero as prior otherwise\n1.\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: fig-grid-approx-base3\n#| fig-cap: \"Grid Approximation with prior of `exp(-5 * abs(p_grid - 0.5))`\"\n\n# 1. define grid\np_grid <- seq(from = 0, to = 1, length.out = 20)\n\n# 2. define prior\nprior <- exp(-5 * abs(p_grid - 0.5))\n\n# 3. compute likelihood at each value in grid\nlikelihood <- dbinom(x = 6, size = 9, prob = p_grid)\n\n# 4. compute product of likelihood and prior\nunstd.posterior <- likelihood * prior\n\n# 5. standardize the posterior, so it sums to 1\nposterior <- unstd.posterior / sum(unstd.posterior)\n\n# 6 display the posterior distribution \n## R code 2.4\nplot(p_grid, posterior,\n  type = \"b\",\n  xlab = \"probability of water\", ylab = \"posterior probability\"\n)\nmtext('20 points with prior of \"exp(-5 * abs(p_grid - 0.5))\"')\n```\n````\n\n::: {.cell-output-display}\n![Grid Approximation with prior of `exp(-5 * abs(p_grid - 0.5))`](02-small-and-large-worlds_files/figure-pdf/fig-grid-approx-base3-1.pdf){#fig-grid-approx-base3 fig-pos='H'}\n:::\n:::\n\n\n\n##### Disadvantage\n\nGrid approximation is very expansive. The number of unique values to\nconsider in the grid grows rapidly as the number of parameters in the\nmodel increases. For the single-parameter globe tossing model, it's no\nproblem to compute a grid of 100 or 1000 values. But for two parameters\napproximated by 100 values each, that's already 100^2^ = 10.000 values\nto compute. For 10 parameters, the grid becomes many billions of values.\nThese days, it's routine to have models with hundreds or thousands of\nparameters. The grid approximation strategy scales very poorly with\nmodel complexity, so it won't get us very far. But it is a very useful\ndidactically as it help to understand the general principle.\n\n#### Rethinking myself\n\n::: callout-caution\n###### Puzzlement\n\nAs demonstrated in @fig-plot-model a different prior with the same\nlikelihood has different posteriors. So the curves of all three examples\nare different.\n\nBut what I do not understand is the fact that in the third example the\nresult is very different from the other two priors: About 0.5 and not\n0.7. My explication: I have only 9 trials. With many more trials the\ndifference in the density would balance out bit by bit.\n\nBut it turns out, that all three show about the same maximum I thought\nthat the chosen prior did not have any effect the outcome. But it turns\nout that different priors results in different PDFs.\\\nMy explication: I have only 9 trials. With many more trials the\ndifference in the density would balance out bit by bit.\n:::\n\nI am going to test my hypothesis. To demonstrate that with more Bayesian\nupdates we will approach the correct result 0b about 0.7 I will work\nwith 1000 samples.\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: fig-grid-approx-base4\n#| fig-cap: \"Grid Approximation with prior of `exp(-5 * abs(p_grid - 0.5))`\"\n\n# 1. define grid\np_grid <- seq(from = 0, to = 1, length.out = 20)\n\n# 2. define prior\nprior <- exp(-5 * abs(p_grid - 0.5))\n\n# 3. compute likelihood at each value in grid\nlikelihood <- dbinom(x = 600, size = 1e3, prob = p_grid)\n\n# 4. compute product of likelihood and prior\nunstd.posterior <- likelihood * prior\n\n# 5. standardize the posterior, so it sums to 1\nposterior <- unstd.posterior / sum(unstd.posterior)\n\n# 6 display the posterior distribution \n## R code 2.4\nplot(p_grid, posterior,\n  type = \"b\",\n  xlab = \"probability of water\", ylab = \"posterior probability\"\n)\nmtext('20 points for a 1000 samples with prior of \"exp(-5 * abs(p_grid - 0.5))\"')\n```\n````\n\n::: {.cell-output-display}\n![Grid Approximation with prior of `exp(-5 * abs(p_grid - 0.5))`](02-small-and-large-worlds_files/figure-pdf/fig-grid-approx-base4-1.pdf){#fig-grid-approx-base4 fig-pos='H'}\n:::\n:::\n\n\n\nIn fact the posterior distribution is nearer to the correct result of\n0.7. In @fig-grid-approx-base3 we have a maximum at about 0.52 and in\n@fig-grid-approx-base4 we have already 0.58. We should also taking into\naccount that with more samples our proportion of W to L will approach\nthe real Water:Land proportion of about 0.71 too.\n\nI will demonstrate this with 10000 samples and a W:L proportion of 7:3.\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: fig-grid-approx-base5\n#| fig-cap: \"Grid Approximation with prior of `exp(-5 * abs(p_grid - 0.5))`\"\n\n# 1. define grid\np_grid <- seq(from = 0, to = 1, length.out = 20)\n\n# 2. define prior\nprior <- exp(-5 * abs(p_grid - 0.5))\n\n# 3. compute likelihood at each value in grid\nlikelihood <- dbinom(x = 7e3, size = 1e4, prob = p_grid)\n\n# 4. compute product of likelihood and prior\nunstd.posterior <- likelihood * prior\n\n# 5. standardize the posterior, so it sums to 1\nposterior <- unstd.posterior / sum(unstd.posterior)\n\n# 6 display the posterior distribution \n## R code 2.4\nplot(p_grid, posterior,\n  type = \"b\",\n  xlab = \"probability of water\", ylab = \"posterior probability\"\n)\nmtext('20 points for a 10000 samples with prior of \"exp(-5 * abs(p_grid - 0.5))\" and W:L = 7:3.' )\n```\n````\n\n::: {.cell-output-display}\n![Grid Approximation with prior of `exp(-5 * abs(p_grid - 0.5))`](02-small-and-large-worlds_files/figure-pdf/fig-grid-approx-base5-1.pdf){#fig-grid-approx-base5 fig-pos='H'}\n:::\n:::\n\n\n\nIn this example the maximum of probability is already 0.68! We can say\nthat **with every chosen prior we will get finally the correct\nresult!**.\n\nBut the choice of the prior is still important as it determines how many\nBayesian updates we need to get the right result. If we have an awkward\nprior and not the appropriate size of the sample we will get a posterior\ndistribution showing us a wrong maximum of probability. In that case the\nprocess of approximation has not reached a state where the probability\nmaximum is near the correct result. The problem is: Most time we do not\nknow the correct solution and can't therefore decide if we have had\nenough Bayesian updates.\n\n#### Tidyverse\n\n**STOPPED HERE! (2023-07-21)**\n\n### Quadratic Approximation\n\n#### Original\n\n##### Concept\n\nUnder quite general conditions, the region near the peak of the\nposterior distribution will be nearly Gaussian---or \"normal\"---in shape.\nThis means the posterior distribution can be usefully approximated by a\nGaussian distribution. A Gaussian distribution is convenient, because it\ncan be completely described by only two numbers: the location of its\ncenter (mean) and its spread (variance).\n\nA Gaussian approximation is called \"quadratic approximation\" because the\nlogarithm of a Gaussian distribution forms a parabola. And a parabola is\na quadratic function.\n\n1.  Find the posterior mode with some algorithm. The procedure does not\n    know where the peak is but it knows the slope under it feet.\n2.  Estimate the curvature near the peak to calculate a quadratic\n    approximation. This computation is done by some numerical technique.\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: quadratic-approx\n\nlibrary(rethinking)\n\nglobe.qa <- quap(\n  alist(\n    W ~ dbinom(W + L, p), # binomial likelihood\n    p ~ dunif(0, 1) # uniform prior\n  ),\n  data = list(W = 6, L = 3)\n)\n\n# display summary of quadratic approximation\n# precis(globe.qa)\nprint(precis(globe.qa))\n```\n````\n\n```\n#>        mean        sd      5.5%     94.5%\n#> p 0.6666665 0.1571338 0.4155364 0.9177967\n```\n:::\n\n\n\n::: {.callout-warning style=\"color: red;\"}\n## precis results not displayed correctly\n\nThe result of `precis()` does not display visually correctly in\nQuarto/RStudio. . The columns of the table are too narrow so that you\ncan't see the header and inspect the values. Printing to the console or\nthe display in the RStudio environment variable is correct.\n\nA workaround is wrapping the result with `print()`.\n\nSee my [bug report](https://github.com/rstudio/rstudio/issues/13227).\n:::\n\nRead the result as: *Assuming the posterior is Gaussian, it is maximized\nat 0.67, and its standard deviation is 0.16*.\n\n##### Compare quadratic approximation with analytic calculation\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: analytical-calc\n\n# analytical calculation\nW <- 6\nL <- 3\ncurve(dbeta(x, W + 1, L + 1), from = 0, to = 1)\n\n# quadratic approximation\ncurve(dnorm(x, 0.67, 0.16), lty = 2, add = TRUE)\n```\n````\n\n::: {.cell-output-display}\n![](02-small-and-large-worlds_files/figure-pdf/analytical-calc-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n##### Disadvantage\n\nAs the amount of data increases, however, the quadratic approximation\ngets better. This phenomenon, where the quadratic approximation improves\nwith the amount of data, is very common. It's one of the reasons that so\nmany classical statistical procedures are nervous about small samples.\n\nUsing the quadratic approximation in a Bayesian context brings with it\nall the same concerns. But you can always lean on some algorithm other\nthan quadratic approximation, if you have doubts. Indeed, grid\napproximation works very well with small samples, because in such cases\nthe model must be simple and the computations will be quite fast. You\ncan also use MCMC.\n\nSometimes the quadratic approximation fails and you will get an error\nmessage about the \"Hessian\". A *Hessian* --- named after mathematician\nLudwig Otto Hesse (1811--1874) --- is a square matrix of second\nderivatives. The standard deviation is typically computed from the\nHessian, so computing the Hessian is nearly always a necessary step. But\nsometimes the computation goes wrong, and your golem will choke while\ntrying to compute the Hessian.\n\nSome other drawbacks will be explicated in later chapter. Therefore MCMC\nseems generally the best option for complex models.\n\n#### Markov Chain Monte Carlo (MCMC)\n\nThere are lots of important model types, like multilevel (mixed-effects)\nmodels, for which neither grid approximation nor quadratic approximation\nis always satisfactory. As a result, various counterintuitive model\nfitting techniques have arisen. The most popular of these is **MARKOV\nCHAIN MONTE CARLO** (MCMC), which is a family of conditioning engines\ncapable of handling highly complex models.\n\nThe understanding of this not intuitive technique is postponed to\nchapter 9. Instead of attempting to compute or approximate the posterior\ndistribution directly, MCMC techniques merely draw samples from the\nposterior. You end up with a collection of parameter values, and the\nfrequencies of these values correspond to the posterior plausibilities.\nYou can then build a picture of the posterior from the histogram of\nthese samples.\n\n\n\n::: {.cell}\n\n````{.cell-code  code-line-numbers=\"false\"}\n```{{r}}\n#| label: MCMC-globe-tossing\n\nn_samples <- 1000\np <- rep(NA, n_samples)\np[1] <- 0.5\nW <- 6\nL <- 3\n\nfor (i in 2:n_samples) {\n  p_new <- rnorm(1, p[i - 1], 0.1)\n  if (p_new < 0) p_new <- abs(p_new)\n  if (p_new > 1) p_new <- 2 - p_new\n  q0 <- dbinom(W, W + L, p[i - 1])\n  q1 <- dbinom(W, W + L, p_new)\n  p[i] <- ifelse(runif(1) < q1 / q0, p_new, p[i - 1])\n}\n\nrethinking::dens(p, xlim = c(0, 1))\ncurve(dbeta(x, W + 1, L + 1), lty = 2, add = TRUE)\n```\n````\n\n::: {.cell-output-display}\n![](02-small-and-large-worlds_files/figure-pdf/MCMC-globe-tossing-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nIt's weird. But it works. The above **METROPOLIS ALGORITHM** is\nexplained in [Chapter 9](javascript:void(0)).\n\n## 2.4a Bayesian Inference: Some Lessons to Draw\n\nThe following list summarizes differences between Bayesian and\nNon-Bayesian inference (\"Frequentism\"):\n\n1.  **No minimum sampling size**: The minimum sampling size in Bayesian\n    inference is one. You are going to update each data point at its\n    time. For instance you got an estimate every time when you toss the\n    globe and the estimate is updated. --- Well, the sample size of one\n    is not very informative but that is the power of Bayesian inference\n    in not getting over confident. It is always accurately representing\n    the relative confidence of plausability we should assign to each of\n    the possible proportions.\n2.  **Shape embodies sample size**: The shape of the posterior\n    distribution embodies all the information that the sample has about\n    the process of the proportions. Therefore you do not need to go back\n    to the original dataset for new observations. Just take the\n    posterior distribution and update it by multiplying the number fo\n    ways the new data could produce.\n3.  **No point estimates**: The estimate is the whole distribution. It\n    may be fine for communication purposes to talk about some summary\n    points of the distribution like the mode and mean. But neither of\n    these points is special as a point of estimate. When we do\n    calculations we draw predictions from the whole distribution, never\n    just from a point of it.\n4.  **No one true interval**: Intervals are not important in Bayesian\n    inference. They are merely summaries of the shape of the\n    distribution. There is nothing special in any of these intervals\n    because the endpoints of the intervals are not special. Nothing\n    happens of the endpoints of the intervals because the interval is\n    arbitrary. (The 95% in Non-Bayesian inference is essentially a\n    dogma, a superstition. Even in Non-Bayesian statistics it is\n    conceptualized as an arbitrary interval.)\n\n## Practice\n",
    "supporting": [
      "02-small-and-large-worlds_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{\"knit_meta_id\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]}},\"value\":[{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"booktabs\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"longtable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"array\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"multirow\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"wrapfig\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"float\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"colortbl\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"pdflscape\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"tabu\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttablex\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"ulem\"]},{\"type\":\"character\",\"attributes\":{},\"value\":[\"normalem\"]},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"makecell\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"xcolor\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]}]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}
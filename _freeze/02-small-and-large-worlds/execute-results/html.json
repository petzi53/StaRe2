{
  "hash": "05c27f6e8d42998ad76f0e7d15d9bf48",
  "result": {
    "markdown": "---\nformat: html\nexecute: \n  cache: true\nfilters: \n  - quarto\n  - nameref\n---\n\n\n# Small and Large Worlds {#sec-chap02}\n\n## ORIGINAL {.unnumbered}\n\n::: my-objectives\n::: my-objectives-header\nLearning Objectives\n:::\n\n::: my-objectives-container\n> \"This chapter focuses on the small world. It explains probability\n> theory in its essential form: counting the ways things can happen.\n> Bayesian inference arises automatically from this perspective. Then\n> the chapter presents the stylized components of a Bayesian statistical\n> model, a model for learning from data. Then it shows you how to\n> animate the model, to produce estimates.\n\n> All this work provides a foundation for the next chapter, in which\n> you'll learn to summarize Bayesian estimates, as well as begin to\n> consider large world obligations.\" ([McElreath, 2020, p.\n> 20](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=39&annotation=I8UG2HET))\n:::\n:::\n\nThe text references many times to the notions of small and large worlds:\n\n-   The **small world** is the self-contained logical world of the\n    model.\n-   The **large world** is the broader context in which one deploys a\n    model.\n\n## TIDYVERSE {.unnumbered}\n\nThe work by Solomon Kurz has many references to R specifics, so that\npeople new to R can follow the course. Most of these references are not\nnew to me, so I will not include them in my personal notes. There are\nalso very important references to other relevant articles I do not know.\nBut I will put these kind of references for now aside and will me mostly\nconcentrate on the replication and understanding of the code examples.\n\nOne challenge for Kurz was to replicate *all* the graphics of the\noriginal version, even if they were produced just for understanding of\nprocedures and argumentation without underlying R code. By contrast I\nwill use only those code lines that are essential to display Bayesian\nresults. Therefore I will not replicate for example the very extensive\nexplication how to produce with `tidyverse` means the graphics of the\ngarden of forking data.\n\n## The Garden of Forking Data {#sec-chap02-forking-data}\n\n### ORIGINAL\n\n**Bayesian inference is counting of possibilities**\n\n> \"Bayesian inference is really just counting and comparing of\n> possibilities.\" ([McElreath, 2020, p.\n> 20](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=39&annotation=JYNC25AE))\n\n> \"In order to make good inference about what actually happened, it\n> helps to consider everything that could have happened. A Bayesian\n> analysis is a garden of forking data, in which alternative sequences\n> of events are cultivated. As we learn about what did happen, some of\n> these alternative sequences are pruned. In the end, what remains is\n> only what is logically consistent with our knowledge.\" ([McElreath,\n> 2020, p. 21](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=40&annotation=Q2R6DERJ))\n\n#### Counting possibilities\n\n> \"Suppose there's a bag, and it contains four marbles. These marbles\n> come in two colors: blue and white. We know there are four marbles in\n> the bag, but we don't know how many are of each color. We do know that\n> there are five possibilities: (1) \\[⚪⚪⚪⚪\\], (2) \\[⚫⚪⚪⚪\\],\n> (3)\\[⚫⚫⚪⚪\\], (4) \\[⚫⚫⚫⚪\\], (5) \\[⚫⚫⚫⚫\\]. These are t These\n> These are the only possibilities consistent with what we know about\n> the contents of the bag. Call these five possibilities the\n> *conjectures*.\n>\n> Our goal is to figure out which of these conjectures is most\n> plausible, given some evidence about the contents of the bag. We do\n> have some evidence: A sequence of three marbles is pulled from the\n> bag, one at a time, replacing the marble each time and shaking the bag\n> before drawing another marble. The sequence that emerges is: ⚫⚪⚫,\n> in that order. These are the data.\n>\n> So now let's plant the garden and see how to use the data to infer\n> what's in the bag. Let's begin by considering just the single\n> conjecture, \\[ \\], that the bag contains one blue and three white\n> marbles.\" ([McElreath, 2020, p.\n> 21](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=40&annotation=RRD6HU9Y))\n\n|              |                        |\n|--------------|------------------------|\n| Conjecture   | Ways to produce ⚫⚪⚫ |\n| \\[⚪⚪⚪⚪\\] | 0 × 4 × 0 = 0          |\n| \\[⚫⚪⚪⚪\\] | 1 × 3 × 1 = 3          |\n| \\[⚫⚫⚪⚪\\] | 2 × 2 × 2 = 8          |\n| \\[⚫⚫⚫⚪\\] | 3 × 1 × 3 = 9          |\n| \\[⚫⚫⚫⚫\\] | 4 × 0 × 4 = 0          |\n\nI have bypassed the counting procedure related with the step-by-step\nvisualization of the garden of forking data. It is important to\nunderstand that the multiplication in the above table is still a\nsummarized counting:\n\n::: my-important\n::: my-important-header\nMultiplication is just a shortcut for counting\n:::\n\n::: my-important-container\n> \"Notice that the number of ways to produce the data, for each\n> conjecture, can be computed by first counting the number of paths in\n> each\"ring\" of the garden and then by multiplying these counts\n> together.\" ([McElreath, 2020, p.\n> 23](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=42&annotation=R9UIIW9R))\n\n> \"Multiplication is just a shortcut to enumerating and counting up all\n> of the paths through the garden that could produce all the\n> observations.\" ([McElreath, 2020, p.\n> 25](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=44&annotation=KUC4ZHP4))\n\n> \"Multiplication is just compressed counting.\" ([McElreath, 2020, p.\n> 37](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=56&annotation=IH6782AE))\n:::\n:::\n\nThe multiplication in the table above has to be interpreted the\nfollowing way:\n\n1.  The possibility of the conjecture that the bag contains four white\n    marbles is zero because the result shows also black marbles. This is\n    the other way around for the last conjecture of four blue/black\n    marbles.\n2.  The possibility of the conjecture that the bag contains one black\n    and three white marbles is calculated the following way: The first\n    marble of the result is black and --- according to our conjecture\n    --- there is only one way (=1) to produce this black marble. The\n    next marble we have drawn is white. This is consistent with three\n    (=3) different ways( marbles) of our conjecture. The last drawn\n    marble is again black which corresponds again with just one way\n    (possibility) following our conjecture. So we get as result of the\n    garden of forking data: `1 x 3 x 1`.\n3.  The calculation of the other conjectures follows the same pattern.\n\n#### Combining Other Information\n\n> \"We may have additional information about the relative plausibility of\n> each conjecture. This information could arise from knowledge of how\n> the contents of the bag were generated. It could also arise from\n> previous data. Whatever the source, it would help to have a way to\n> combine different sources of information to update the plausibilities.\n> Luckily there is a natural solution: Just multiply the counts.\n>\n> To grasp this solution, suppose we're willing to say each conjecture\n> is equally plausible at the start. So we just compare the counts of\n> ways in which each conjecture is compatible with the observed data.\n> This comparison suggests that \\[⚫⚫⚫⚪\\] is slightly more plausible\n> than \\[⚫⚫⚪⚪\\], and both are about three times more plausible than\n> \\[⚫⚪⚪⚪\\]. Since these are our initial counts, and we are going to\n> update them next, let's label them *prior*.\" ([McElreath, 2020, p.\n> 25](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=44&annotation=WECXDWEV))\n\n::: my-procedure\n::: my-procedure-header\n::: {#prp-bayesian-updating}\n: Bayesian Updating\n:::\n:::\n\n::: my-procedure-container\n1.  First we count the numbers of ways each conjecture could produce the\n    new observation, for instance drawing a blue marble.\n2.  Then we multiply each of these new counts by the prior numbers of\n    ways for each conjecture.\n:::\n:::\n\n|     |              |     |                    |     |              |     |            |     |\n|--------|--------|--------|--------|--------|--------|--------|--------|--------|\n|     |              |     |                    |     |              |     |            |     |\n|     | Conjecture   |     | Ways to produce ⚫ |     | Prior counts |     | New count  |     |\n|     | \\[⚪⚪⚪⚪\\] |     | 0                  |     | 0            |     | 0 × 0 = 0  |     |\n|     | \\[⚫⚪⚪⚪\\] |     | 1                  |     | 3            |     | 3 × 1 = 3  |     |\n|     | \\[⚫⚫⚪⚪\\] |     | 2                  |     | 8            |     | 8 × 2 = 16 |     |\n|     | \\[⚫⚫⚫⚪\\] |     | 3                  |     | 9            |     | 9 × 3 = 27 |     |\n|     | \\[⚫⚫⚫⚫\\] |     | 4                  |     | 0            |     | 0 × 4 = 0  |     |\n\n::: callout-caution\nIn the book the table header \"Ways to produce\" includes ⚪ instead of\n--- as I think is correct --- ⚫.\n:::\n\n#### From Counts to Probability\n\n::: my-important\n::: my-important-header\nPrinciple of honest ignorance (McElreath)\n:::\n\n::: my-important-container\n> \"When we don't know what caused the data, potential causes that may\n> produce the data in more ways are more plausible.\" ([McElreath, 2020,\n> p. 26](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=45&annotation=9EKVZNHR))\n:::\n:::\n\nTwo reasons for using probabilities instead of counts:\n\n1.  Only relative value matters.\n2.  Counts will fast grow very large and difficult to manipulate.\n\n$$\n\\begin{align*}\n\\text{plausibility of p after } D_{New} = \\frac{\\text{ways p can produce }D_{New} \\times \\text{prior plausibility p}}{\\text{sum of product}}\n\\end{align*}\n$$\n\n::: my-example\n::: my-example-header\nConstructing plausibilities by standardizing\n:::\n\n::: my-example-container\n| Possible composition | p    | Ways to produce data | Plausibility |\n|----------------------|------|----------------------|--------------|\n| \\[⚪⚪⚪⚪\\]         | 0    | 0                    | 0            |\n| \\[⚫⚪⚪⚪\\]         | 0.25 | 3                    | 0.15         |\n| \\[⚫⚫⚪⚪\\]         | 0.5  | 8                    | 0.40         |\n| \\[⚫⚫⚫⚪\\]         | 0.75 | 9                    | 0.45         |\n| \\[⚫⚫⚫⚫\\]         | 1    | 0                    | 0            |\n:::\n:::\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-2-1}\n: Compute these plausibilities in R\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/code-2-1_5bb905b93a59cbf23df215cdb0978be8'}\n\n```{.r .cell-code}\n## R code 2.1 #############\nways <- c(0, 3, 8, 9, 0)\nways / sum(ways)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.00 0.15 0.40 0.45 0.00\n```\n\n\n:::\n:::\n\n:::\n:::\n\n> \"These plausibilities are also *probabilities*---they are non-negative\n> (zero or positive) real numbers that sum to one. And all of the\n> mathematical things you can do with probabilities you can also do with\n> these values. Specifically, each piece of the calculation has a direct\n> partner in applied probability theory. These partners have stereotyped\n> names, so it's worth learning them, as you'll see them again and\n> again.\n>\n> -   A conjectured proportion of blue marbles, p, is usually called a\n>     <a class='glossary' title='Unobserved variables are usually called Parameters. (SR2, Chap.2) A parameter is an unknown numerical characteristics of a population that must be estimated. (CDS). They are also numbers that govern statistical models (stats.stackexchange)'>parameter</a> value. It's just a way of indexing\n>     possible explanations of the data.\n> -   The relative number of ways that a value p can produce the data is\n>     usually called a <a class='glossary' title='The likelihood function (often simply called the likelihood) is the joint probability (or probability density) of observed data viewed as a function of the parameters of a statistical model. (Wikipedia) It indicates how likely a particular population is to produce an observed sample. (&lt;a href=“https://www.statistics.com/glossary/likelihood-function/&gt;statistics.com) It is the probability of the data given our beliefs about the data: P(data | belief). (BF, Chap.8)'>likelihood</a>. It is derived by\n>     enumerating all the possible data sequences that could have\n>     happened and then eliminating those sequences inconsistent with\n>     the data.\n> -   The prior plausibility of any specific p is usually called the\n>     <a class='glossary' title='The Prior Probability, also called the Prior, is the assumed probability distribution before we have seen the data. (Wikipedia) It quantifies how likely our initial belief is: P(belief). (BF, Chap.8)'>prior probability</a>.\n> -   The new, updated plausibility of any specific p is usually called\n>     the <a class='glossary' title='It is the revised or updated probability of an event occurring after taking into consideration new information. (Investopedia). Posterior probability = prior probability + new evidence (called likelihood). (Statistics How To) The posterior distribution will be a distribution of Gaussian distributions. (SR, Chap.4). It quantifies exactly how much our observed data changes our beliefs: P(belief | data) (BF, Chap.8)'>posterior probability</a>.\" ([McElreath, 2020, p.\n>     27](zotero://select/groups/5243560/items/NFUEVASQ))\n>     ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=46&annotation=ZDYLMTTI))\n\n### TIDYVERSE (empty)\n\n## Building a Model\n\n### ORIGINAL\n\nWe are going to use a toy example, but it has the same structure as a\ntypical statistical analyses. The first nine samples produce the\nfollowing data:\n\n`W L W W W L W L W` (W indicates water and L indicates land.)\n\n::: my-procedure\n::: my-procedure-header\n::: {#prp-designing-bayesian-model}\n: Designing a simple Bayesian model\n:::\n:::\n\n::: my-procedure-container\n> \"Designing a simple Bayesian model benefits from a design loop with\n> three steps.\n>\n> 1.  Data story: Motivate the model by narrating how the data might\n>     arise.\n> 2.  Update: Educate your model by feeding it the data.\n> 3.  Evaluate: All statistical models require supervision, leading to\n>     model revision.\" ([McElreath, 2020, p.\n>     28](zotero://select/groups/5243560/items/NFUEVASQ))\n>     ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=47&annotation=KFNYKUC2))\n:::\n:::\n\n#### A Data Story\n\n> \"Bayesian data analysis usually means producing a story for how the\n> data came to be. This story may be *descriptive*, specifying\n> associations that can be used to predict outcomes, given observations.\n> Or it may be *causal*, a theory of how some events produce other\n> events.\" ([McElreath, 2020, p.\n> 28](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=47&annotation=D2IR5ZX3))\n\n> \"... all data stories are complete, in the sense that they are\n> sufficient for specifying an algorithm for simulating new data.\"\n> ([McElreath, 2020, p.\n> 28](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=47&annotation=A3EQXFBY))\n\n::: my-example\n::: my-example-header\nData story for our case\n:::\n\n::: my-example-container\n> \"The data story in this case is simply a restatement of the sampling\n> process:\n>\n> (1) The true proportion of water covering the globe is $p$.\n> (2) A single toss of the globe has a probability p of producing a\n>     water ($W$) observation. It has a probability $1 − p$ of producing\n>     a land ($L$) observation.\n> (3) Each toss of the globe is independent of the others.\" ([McElreath,\n>     2020, p. 29](zotero://select/groups/5243560/items/NFUEVASQ))\n>     ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=48&annotation=YPPK73D9))\n:::\n:::\n\nData stories are important:\n\n> \"Most data stories are much more specific than are the verbal\n> hypotheses that inspire data collection. Hypotheses can be vague, such\n> as\"it's more likely to rain on warm days.\" When you are forced to\n> consider sampling and measurement and make a precise statement of how\n> temperature predicts rain, many stories and resulting models will be\n> consistent with the same vague hypothesis. Resolving that ambiguity\n> often leads to important realizations and model revisions, before any\n> model is fit to data.\" ([McElreath, 2020, p.\n> 29](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=48&annotation=852Q3WN5))\n\n#### Bayesian Updating\n\n> \"Each possible proportion may be more or less plausible, given the\n> evidence. A Bayesian model begins with one set of plausibilities\n> assigned to each of these possibilities. These are the prior\n> plausibilities. Then it updates them in light of the data, to produce\n> the posterior plausibilities. This updating process is a kind of\n> learning, called <a class='glossary' title='A Bayesian model begins with one set of plausibilities assigned to each of these possibilities. These are the prior plausibilities. Then it updates them in light of the data, to produce the posterior plausibilities. This updating process is a kind of learning. (Chap.2)'>Bayesian updating</a>.\" ([McElreath,\n> 2020, p. 29](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=48&annotation=XIYIZHVD))\n\n::: my-important\n::: my-important-header\nHow a Bayesian model learns\n:::\n\n::: my-important-container\n@fig-2-5-book-copy helps to understand the Bayesian updating process. In\n@cnj-fig-bayesian-update we will learn how to write R code to reproduce\nthe book's figure as @fig-bayesian-update. To inspect the different\nsteps of the updating process is essential to understand Bayesian\nstatistics!\n:::\n:::\n\n![Copy of Figure 2.5: **How a Bayesian model learns**. In each plot,\nprevious plausibilities (dashed curve) are updated in light of the\nlatest observation to produce a new set of plausibilities (solid\ncurve).](img/bayesian_model_learns_step_by_step-min.png){#fig-2-5-book-copy\nfig-alt=\"Nine small diagrams to show the relationship between plausibility against proportion of water after each sample.\"}\n\n#### Evaluate\n\nKeep in mind two cautious principles:\n\n> \"First, the model's certainty is no guarantee that the model is a good\n> one.\" ([McElreath, 2020, p.\n> 31](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=50&annotation=QYALLSPY))\n> \"Second, it is important to supervise and critique your model's work.\"\n> ([McElreath, 2020, p.\n> 31](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=50&annotation=M7UKAL3D))\n\n::: my-watch-out\n::: my-watch-out-header\nTest Before You Est(imate)\n:::\n\n::: my-watch-out-container\nIn the planned third version of the book McElreath wants to include from\nthe beginning the evaluation part of the process. It is mentioned in the\nbook already in this chapter 2 but without practical implementation and\ncode examples. But we can find some remarks in his [Statistical\nRethinking Videos 2023b\n37:40](https://www.youtube.com/watch?v=R1vcdhPBlXA&list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus&index=2&t=37m40s).\n\nI will not go into details here, because my focus is on the second\nedition. But I will add as a kind of summary general advises for the\ntesting procedure:\n\n::: my-procedure\n::: my-procedure-header\n<div>\n\n: Model evaluation\n\n</div>\n:::\n\n::: my-procedure-container\n1.  Code a generative simulation\n2.  Code an estimator\n3.  Test the estimator with (1)\n    -   where the answer is known\n    -   at extreme values\n    -   explore different sampling designs\n    -   develop generally an intuition for sampling and estimation\n:::\n:::\n\nHere are some references to get some ideas of the necessary R Code:\n\n::: my-resource\n::: my-resource-header\nR Code snippets for testing procedure\n:::\n\n::: my-resource-container\n-   **Simulating globe tossing**: Statistical Rethinking 2023 - Lecture\n    02, [Slide\n    54](https://speakerdeck.com/rmcelreath/statistical-rethinking-2023-lecture-02?slide=54)\n-   **Simulate the experiment arbitrary times for any particular\n    proportion**: This is a way to explore the design of an experiment\n    as well as debug the code. ([Video 2023-02\n    39:55](https://www.youtube.com/watch?v=R1vcdhPBlXA&list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus&index=2&t=39m55s))\n-   **Test the simulation at extreme values**: R code snippets at [Slide\n    57](https://speakerdeck.com/rmcelreath/statistical-rethinking-2023-lecture-02?slide=57).\n:::\n:::\n:::\n:::\n\n### TIDYVERSE\n\nInstead of vectors the tidyverse approach works best with data frames\nrespectively tibbles. So let's save the globe-tossing data\n`W L W W W L W L W` into a tibble:\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-globe-tossing-data}\n: Save globe tossing data into a tibble\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/globe-tossing-data_347be1c8629101512b085d5820953623'}\n\n```{.r .cell-code}\n(d <- tibble::tibble(toss = c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 9 × 1\n#>   toss \n#>   <chr>\n#> 1 w    \n#> 2 l    \n#> 3 w    \n#> 4 w    \n#> 5 w    \n#> 6 l    \n#> 7 w    \n#> 8 l    \n#> 9 w\n```\n\n\n:::\n:::\n\n:::\n:::\n\n#### A Data Story\n\n#### Bayesian Updating {#sec-expand_grid}\n\nFor the updating process we need to add to the data the cumulative\nnumber of trials, `n_trials`, and the cumulative number of successes,\n`n_successes` with `toss == \"w\"`.\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-bayesian-updating-start}\n: Bayesian updating preparation\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/bayesian-updating-start_77e1d5b864b303565ad7d6078e7a3686'}\n\n```{.r .cell-code}\n(\n  d <-\n  d |>  \n  dplyr::mutate(n_trials  = 1:9,\n         n_success = cumsum(toss == \"w\"))\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 9 × 3\n#>   toss  n_trials n_success\n#>   <chr>    <int>     <int>\n#> 1 w            1         1\n#> 2 l            2         1\n#> 3 w            3         2\n#> 4 w            4         3\n#> 5 w            5         4\n#> 6 l            6         4\n#> 7 w            7         5\n#> 8 l            8         5\n#> 9 w            9         6\n```\n\n\n:::\n:::\n\n:::\n:::\n\nThe program code for reproducing the Figure 2.5 of the book (here in\nthis document it is @fig-2-5-book-copy) is pretty complex. I have to\ninspect the results line by line. At first I will give a short\nintroduction what each line does. In the next steps I will explain each\nstep more in detail and show the result of the corresponding lines of\ncode.\n\n::: my-watch-out\n::: my-watch-out-header\nParameter $k$ in `lag()` changed to $default$\n:::\n\n::: my-watch-out-container\nIn the following listing I had to change in the `lag()` function the\nparameter $k$ of the Kurz'sche version to $default$ as it is described\nin the corresponding [help\nfile](https://dplyr.tidyverse.org/reference/lead-lag.html). I don't\nunderstand why $k$ was used. Maybe $k$ was the name of the parameter of\na previous {**dplyr**} version?\n:::\n:::\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-fig-bayesian-update}\n: Bayesian updating: How a model learns\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/fig-bayesian-update_ff3b39cf47ea838f9d7137fb8a79703f'}\n\n```{.r .cell-code  code-summary=\"Code: **How a Bayesian model learns**\"}\n## (0) starting with tibble from the previous two code chunks #####\nd <- tibble::tibble(toss = c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\")) |>\n    dplyr::mutate(n_trials  = 1:9, n_success = cumsum(toss == \"w\"))\n\n## (1) create tibble from all input combinations ################\nsequence_length <- 50  \nd |>                  \n  tidyr::expand_grid(p_water = seq(from = 0, to = 1,  \n                            length.out = sequence_length)) |>   \n    \n## (2) group data by the `p-water` parameter ####################\n  dplyr::group_by(p_water) |>  \n\n## (3) create columns filled with the value of the previous rows #####\n  dplyr::mutate(lagged_n_trials  = dplyr::lag(n_trials, default = 1), \n         lagged_n_success = dplyr::lag(n_success, default = 1)) |>  \n\n## (4) restore the original ungrouped data structure #######\n  dplyr::ungroup() |>  \n\n## (5) calculate prior and likelihood & store values ######\n  dplyr::mutate(prior      = ifelse(n_trials == 1, .5, \n                             dbinom(x    = lagged_n_success, \n                                    size = lagged_n_trials,  \n                                    prob = p_water)),        \n         likelihood = dbinom(x    = n_success,  \n                             size = n_trials,   \n                             prob = p_water),   \n         strip      = stringr::str_c(\"n = \", n_trials)) |>   \n  \n## (6) normalize prior and likelihood ##########################\n  dplyr::group_by(n_trials) |>  \n  dplyr::mutate(prior      = prior / sum(prior),               \n         likelihood = likelihood / sum(likelihood)) |>  \n  \n  ## (7) plot the result ########################################\n  ggplot2::ggplot(ggplot2::aes(x = p_water)) + \n  ggplot2::geom_line(ggplot2::aes(y = prior), linetype = 2) +  \n  ggplot2::geom_line(ggplot2::aes(y = likelihood)) +           \n  ggplot2::scale_x_continuous(\"proportion water\",     \n                     breaks = c(0, .5, 1)) + \n  ggplot2::scale_y_continuous(\"plausibility\", breaks = NULL) +  \n  ggplot2::theme(panel.grid = ggplot2::element_blank()) +                \n  ggplot2::facet_wrap(~ strip, scales = \"free_y\") +            \n  ggplot2::theme_bw()                                  \n```\n\n::: {.cell-output-display}\n![How a Bayesian model learns: Replicating book figure 2.5](02-small-and-large-worlds_files/figure-html/fig-bayesian-update-1.png){#fig-bayesian-update width=672}\n:::\n:::\n\n:::\n:::\n\n##### Annotation (1): `tidyr::expand_grid()` {#sec-annotation-1-expand-grid}\n\n`tidyr::expand_grid()` creates a tibble from all combinations of inputs.\nInput are generalized vectors in contrast to `tidyr::expand()` that\ngenerates all combination of variables as well but needs as input a\ndataset. The range between 0 and 1 is divided into 50 part and then it\ngenerates all combinations by varying all columns from left to right.\nThe first column is the slowest, the second is faster and so on.). It\ngenerates 450 data points ($50 \\times 9$ trials).\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-bayesian-update-1}\n: Bayesian Update: Create tibble with all Input combinations\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/bayesian-update-1_3e7adfff72a7634eb46fa5fc7807b3a4'}\n\n```{.r .cell-code}\ntbl <- tibble::tibble(toss = c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\")) |> \n    dplyr::mutate(n_trials  = 1:9, n_success = cumsum(toss == \"w\"))\nsequence_length <- 50\n\ntbl |>  \n  tidyr::expand_grid(p_water = seq(from = 0, to = 1, \n                            length.out = sequence_length))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 450 × 4\n#>    toss  n_trials n_success p_water\n#>    <chr>    <int>     <int>   <dbl>\n#>  1 w            1         1  0     \n#>  2 w            1         1  0.0204\n#>  3 w            1         1  0.0408\n#>  4 w            1         1  0.0612\n#>  5 w            1         1  0.0816\n#>  6 w            1         1  0.102 \n#>  7 w            1         1  0.122 \n#>  8 w            1         1  0.143 \n#>  9 w            1         1  0.163 \n#> 10 w            1         1  0.184 \n#> # ℹ 440 more rows\n```\n\n\n:::\n:::\n\n:::\n:::\n\n##### Annotation (2): `dplyr::group_by()` {#sec-annotation-2-group_by}\n\nAt first I did not understand the line `group_by(p_water)`. Why has the\ndata to be grouped when every row has a different value --- as I have\nthought from a cursory inspection of the result? But it turned out that\nafter 50 records the parameter `p_water` is repeating its value.\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-bayesian-update-2}\n: Bayesian Updating: Lagged with grouping\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/bayesian-update-2_55727e25108983030bb65654f9c74ff5'}\n\n```{.r .cell-code}\ntbl1 <- tbl |>  \n  tidyr::expand_grid(p_water = seq(from = 0, to = 1, \n                            length.out = sequence_length)) |>  \n    \n  dplyr::group_by(p_water) |>  \n  dplyr::mutate(lagged_n_trials  = dplyr::lag(n_trials, default = 1),\n         lagged_n_success = dplyr::lag(n_success, default = 1)) |>  \n  dplyr::ungroup() |> \n    \n\n  # add new column ID with row numbers \n  # and relocate it to be the first column\n  dplyr::mutate(ID = dplyr::row_number()) |> \n  dplyr::relocate(ID, .before = toss) \n\n# show 2 records from different groups\ntbl1[c(1:2, 50:52, 100:102, 150:152), ] \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 11 × 7\n#>       ID toss  n_trials n_success p_water lagged_n_trials lagged_n_success\n#>    <int> <chr>    <int>     <int>   <dbl>           <int>            <int>\n#>  1     1 w            1         1  0                    1                1\n#>  2     2 w            1         1  0.0204               1                1\n#>  3    50 w            1         1  1                    1                1\n#>  4    51 l            2         1  0                    1                1\n#>  5    52 l            2         1  0.0204               1                1\n#>  6   100 l            2         1  1                    1                1\n#>  7   101 w            3         2  0                    2                1\n#>  8   102 w            3         2  0.0204               2                1\n#>  9   150 w            3         2  1                    2                1\n#> 10   151 w            4         3  0                    3                2\n#> 11   152 w            4         3  0.0204               3                2\n```\n\n\n:::\n:::\n\n:::\n:::\n\nI want to see the differences in detail. So I will provide also the\nungrouped version.\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-bayesian-update-2a}\n: Bayesian Updating: Lagged without grouping\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/bayesian-update-2a_eda1a025234d190d796e43a6e8803a09'}\n\n```{.r .cell-code}\ntbl2 <- tbl |>  \n  tidyr::expand_grid(p_water = seq(from = 0, to = 1, \n                            length.out = sequence_length)) |> \n    \n  dplyr::mutate(lagged_n_trials  = dplyr::lag(n_trials, default = 1),\n         lagged_n_success = dplyr::lag(n_success, default = 1)) |> \n  \n  # add new column ID with row numbers and relocate it as first column\n  dplyr::mutate(ID = dplyr::row_number()) |> \n  dplyr::relocate(ID, .before = toss) \n\n# show the same records without grouping\ntbl2[c(1:2, 50:52, 100:102, 150:152), ] \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 11 × 7\n#>       ID toss  n_trials n_success p_water lagged_n_trials lagged_n_success\n#>    <int> <chr>    <int>     <int>   <dbl>           <int>            <int>\n#>  1     1 w            1         1  0                    1                1\n#>  2     2 w            1         1  0.0204               1                1\n#>  3    50 w            1         1  1                    1                1\n#>  4    51 l            2         1  0                    1                1\n#>  5    52 l            2         1  0.0204               2                1\n#>  6   100 l            2         1  1                    2                1\n#>  7   101 w            3         2  0                    2                1\n#>  8   102 w            3         2  0.0204               3                2\n#>  9   150 w            3         2  1                    3                2\n#> 10   151 w            4         3  0                    3                2\n#> 11   152 w            4         3  0.0204               4                3\n```\n\n\n:::\n:::\n\n:::\n:::\n\nIt turned out that the two version differ after 51 records in the lagged\nvariables. (Not after 50 as I would have assumed. Apparently this has to\ndo with the `lag()` command because the first 51 records are identical.\nBeginning with row number 52 there are differences in the column\n`lagged_n_trials` and `lagged_n_success`. This pattern is repeated:\nOriginal version always changes after 100 records. The version without\ngrouping changes after 50 rows starting with row 51.\n\n##### Annotation (3): `dplyr::lag()` {#sec-annotation-3-lag}\n\nThe function `dplyr::lag()` finds the \"previous\" values in a vector\n(time series). This is useful for comparing values behind of the current\nvalues. See [Compute lagged or leading\nvalues](https://dplyr.tidyverse.org/reference/lead-lag.html).\n\nWe need to get the immediately previous values for drawing the prior\nprobabilities in the current graph (= dashed line or `linetype = 2` in\n{**ggplot2**} parlance). In the relation with the posterior\nprobabilities the difference form the prior possibility is always $1$\n(this is the option `default = 1` in the `dplyr::lag()` function. This\nis now the correct explanation for the differences starting after rows\n51 (and not 50).\n\n##### Annotation (4): `dplyr::ungroup()` {#sec-annotation-4-ungroup}\n\nThis is just the reversion of the grouping command `group_by(p_water)`.\n\n##### Annotation (5): `dbinom()` {#sec-annotation-5-dbinom}\n\nThis is the core of the prior and likelihood calculation. It uses\n`base::dbinom()`, to calculate two alternative events. `dbinom()` is the\nR function for the binomial distribution, a distribution provided by the\nprobability theory for \"coin tossing\" problems.\n\n::: my-resource\n::: my-resource-header\nExploring the `dbinom()` function\n:::\n\n::: my-resource-container\nThe [distribution zoo](https://ben18785.shinyapps.io/distribution-zoo/)\nis a nice resource to explore the shapes and properties of important\nBayesian distributions. This interactive {**shiny**} app was developed\nby [Ben Lambert](https://ben-lambert.com/bayesian/) and [Fergus\nCooper](https://www.cs.ox.ac.uk/people/fergus.cooper/site/).\n\nChoose \"Discrete Univariate\" as the \"Category of Distribution\" and then\nselect as \"Binomial\" as the \"Distribution Type\".\n\nSee also the two blog entries [An Introduction to the Binomial\nDistribution](https://www.statology.org/binomial-distribution/) and [A\nGuide to dbinom, pbinom, qbinom, and rbinom in\nR](https://www.statology.org/dbinom-pbinom-qbinom-rbinom-in-r/) of the\nStatology website.\n:::\n:::\n\nThe \"`d`\" in `dbinom()` stands for *density*. Functions named in this\nway almost always have corresponding partners that begin with \"`r`\" for\nrandom samples and that begin with \"`p`\" for cumulative probabilities.\nSee for example the [help\nfile](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Binomial.html).\n\nThe results of each of the different calculation (prior and likelihood)\nare collected with `dplyr::mutate()` into two new generated columns.\n\nThere is no prior for the first trial, so it is assumed that it is 0.5.\nThe formula for the binomial distribution uses for the prior the\nlagged-version whereas the likelihood uses the current version. These\ntwo lines provide the essential calculations: They match the 50 grid\npoints as assumed water probabilities of every trial to their trial\noutcome (`W` or `L`) probabilities.\n\nThe last `dplyr::mutate()` command generates the $strip$ variable\nconsisting of the prefix $n =$ followed by the counts of the number of\ntrials. This will later provide the title for the the different facets\nof the plot.\n\n::: my-checklist\n::: my-checklist-header\nTo-do: Construct labelling specification for toss results\n:::\n\n::: my-checklist-container\nTo get a better replication I would need to change the labels for the\nfacets from `n = n_trials` to the appropriate string length of the\nresults, e.g., for $n = 3$ I would need $W, L, W$.\n\nThis was not done by Kurz. I have tried it but didn't succeed up to now\n(2023-10-25).\n:::\n:::\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-bayesian.update-5}\n: Bayesian Updating: Calculating prior and likelihood\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/bayesian-update-5_f0ac216bccdf4b296b150c1bd7f2b1e4'}\n\n```{.r .cell-code}\ntbl5 <- tbl1 |>  \n  dplyr::mutate(prior      = ifelse(n_trials == 1, .5,\n                             dbinom(x    = lagged_n_success, \n                                    size = lagged_n_trials, \n                                    prob = p_water)),\n         likelihood = dbinom(x    = n_success, \n                             size = n_trials, \n                             prob = p_water),\n         strip      = stringr::str_c(\"n = \", n_trials)) \n \ntbl5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 450 × 10\n#>       ID toss  n_trials n_success p_water lagged_n_trials lagged_n_success prior\n#>    <int> <chr>    <int>     <int>   <dbl>           <int>            <int> <dbl>\n#>  1     1 w            1         1  0                    1                1   0.5\n#>  2     2 w            1         1  0.0204               1                1   0.5\n#>  3     3 w            1         1  0.0408               1                1   0.5\n#>  4     4 w            1         1  0.0612               1                1   0.5\n#>  5     5 w            1         1  0.0816               1                1   0.5\n#>  6     6 w            1         1  0.102                1                1   0.5\n#>  7     7 w            1         1  0.122                1                1   0.5\n#>  8     8 w            1         1  0.143                1                1   0.5\n#>  9     9 w            1         1  0.163                1                1   0.5\n#> 10    10 w            1         1  0.184                1                1   0.5\n#> # ℹ 440 more rows\n#> # ℹ 2 more variables: likelihood <dbl>, strip <chr>\n```\n\n\n:::\n:::\n\n:::\n:::\n\n##### Annotation (6): Normalizing {#sec-annotation-6-normalize}\n\nThe code lines in annotation 6 normalize the prior and the likelihood by\ngrouping the data by `n\\_trials`. Dividing every prior and likelihood\nvalues by their respective sum puts them both in a probability metric.\nThis metric is important for the comparisons of different probabilities.\n\n> If you don't normalize (i.e., divide the density by the sum of the\n> density), their respective heights don't match up with those in the\n> text. Furthermore, it's the normalization that makes them directly\n> comparable. (Kurz in section [Bayesian\n> Updating](https://bookdown.org/content/4857/small-worlds-and-large-worlds.html#bayesian-updating.)\n> of chapter 2)\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-bayesian-update-6}\n: Bayesian Updating: Normalizing prior and likelihood\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/bayesian-update-6_98a7024135ce76e35e53137d2cf3d69b'}\n\n```{.r .cell-code}\ntbl6 <- tbl5 |>  \n  dplyr::group_by(n_trials) |>  \n  dplyr::mutate(prior      = prior / sum(prior),\n         likelihood = likelihood / sum(likelihood))\n    \ntbl6\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 450 × 10\n#> # Groups:   n_trials [9]\n#>       ID toss  n_trials n_success p_water lagged_n_trials lagged_n_success prior\n#>    <int> <chr>    <int>     <int>   <dbl>           <int>            <int> <dbl>\n#>  1     1 w            1         1  0                    1                1  0.02\n#>  2     2 w            1         1  0.0204               1                1  0.02\n#>  3     3 w            1         1  0.0408               1                1  0.02\n#>  4     4 w            1         1  0.0612               1                1  0.02\n#>  5     5 w            1         1  0.0816               1                1  0.02\n#>  6     6 w            1         1  0.102                1                1  0.02\n#>  7     7 w            1         1  0.122                1                1  0.02\n#>  8     8 w            1         1  0.143                1                1  0.02\n#>  9     9 w            1         1  0.163                1                1  0.02\n#> 10    10 w            1         1  0.184                1                1  0.02\n#> # ℹ 440 more rows\n#> # ℹ 2 more variables: likelihood <dbl>, strip <chr>\n```\n\n\n:::\n:::\n\n:::\n:::\n\n##### Annotation (7): Graphical demonstration of Bayesian updating {#sec-annotation-7-ggplot}\n\nThe remainder of the code prepares the plot by using the 50 grid points\nin the range from $0$ to $1$ as the x-axis; prior and likelihood as\ny-axis. To distinguish the prior from the likelihood it uses a dashed\nline for the prior (`linetyp = 2`) and a full line (default) for the\nlikelihood. The x-axis has three breaks ($0, 0.5, 1$) whereas the y-axis\nhas no break and no scale (`scales = \"free_y\"`).\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-bayesian-update-7}\n: Bayesian updating: Graphical demonstration\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/fig-bayesian-update-7_4f13f88a78a2ab19a8c31866183071d3'}\n\n```{.r .cell-code}\ntbl6 |>  \n  ggplot2::ggplot(ggplot2::aes(x = p_water)) + \n  ggplot2::geom_line(ggplot2::aes(y = prior), \n            linetype = 2) + \n  ggplot2::geom_line(ggplot2::aes(y = likelihood)) + \n  ggplot2::scale_x_continuous(\"proportion water\", breaks = c(0, .5, 1)) + \n  ggplot2::scale_y_continuous(\"plausibility\", breaks = NULL) + \n  ggplot2::theme(panel.grid = ggplot2::element_blank()) + \n  ggplot2::theme_bw() +\n  ggplot2::facet_wrap(~ strip, scales = \"free_y\") \n```\n\n::: {.cell-output-display}\n![Graphical demonstration: 9 steps of Bayesian updating](02-small-and-large-worlds_files/figure-html/fig-bayesian-update-7-1.png){#fig-bayesian-update-7 width=672}\n:::\n:::\n\n:::\n:::\n\n## Components of the Model\n\n### ORIGINAL\n\nWe observed three components of the model:\n\n> \"(1) The number of ways each conjecture could produce an observation\n> (2) The accumulated number of ways each conjecture could produce the\n> entire data (3) The initial plausibility of each conjectured cause of\n> the data\" ([McElreath, 2020, p.\n> 32](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=51&annotation=R4M54D6K))\n\n#### Variables\n\n> \"Variables are just symbols that can take on different values. In a\n> scientific context, variables include things we wish to infer, such as\n> proportions and rates, as well as things we might observe, the data.\n> ... Unobserved variables are usually called\n> <a class='glossary' title='Unobserved variables are usually called Parameters. (SR2, Chap.2) A parameter is an unknown numerical characteristics of a population that must be estimated. (CDS). They are also numbers that govern statistical models (stats.stackexchange)'>parameters</a>.\" ([McElreath, 2020, p.\n> 32](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=51&annotation=C7C4PCB6))\n\nTake as example the globe tossing models: There are three variables: `W`\nand `L` (water or land) and the proportion of water and land `p`. We\nobserve the events of water or land but we calculate (do not observe\ndirectly) the proportion of water and land. So `p` is a parameter as\ndefined above.\n\n#### Definitions\n\n> \"Once we have the variables listed, we then have to define each of\n> them. In defining each, we build a model that relates the variables to\n> one another. Remember, the goal is to count all the ways the data\n> could arise, given the assumptions. This means, as in the globe\n> tossing model, that for each possible value of the unobserved\n> variables, such as $p$, we need to define the relative number of\n> ways---the probability---that the values of each observed variable\n> could arise. And then for each unobserved variable, we need to define\n> the prior plausibility of each value it could take.\" ([McElreath,\n> 2020, p. 33](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=52&annotation=EWFRB9S7))\n\n::: my-note\n::: my-note-header\n::: {#cor-expand-grid}\n: Calculate the prior plausibility of the values of each observed\nvariable in R\n:::\n:::\n\n::: my-note-container\nThere are different functions in R that generate all combination of\nvariables supplied by vectors, factors or data columns:\n`base::expand.grid()` or in the tidyverse `tidyr::expand()`,\n`tidyr::expand_grid()` and `tidyr::crossing()`. These functions can be\nused to calculate and generate the relative number of ways (= the\nprobability) that the values of each observed variable could arise. We\nhave seen a first demonstration in @cnj-bayesian-update-1. We will see\nmany more examples in later sections and chapters.\n:::\n:::\n\n##### Observed Variables\n\n> \"For the count of water W and land L, we define how plausible any\n> combination of W and L would be, for a specific value of p. This is\n> very much like the marble counting we did earlier in the chapter. Each\n> specific value of p corresponds to a specific plausibility of the\n> data, as in @fig-bayesian-update-7.\n>\n> So that we don't have to literally count, we can use a mathematical\n> function that tells us the right plausibility. In conventional\n> statistics, a distribution function assigned to an observed variable\n> is usually called a <a class='glossary' title='The likelihood function (often simply called the likelihood) is the joint probability (or probability density) of observed data viewed as a function of the parameters of a statistical model. (Wikipedia) It indicates how likely a particular population is to produce an observed sample. (&lt;a href=“https://www.statistics.com/glossary/likelihood-function/&gt;statistics.com) It is the probability of the data given our beliefs about the data: P(data | belief). (BF, Chap.8)'>likelihood</a>. That term has special\n> meaning in nonBayesian statistics, however.\" ([McElreath, 2020, p.\n> 33](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=52&annotation=TLDIVS8K))\n\nFor our globe tossing procedure we use instead of counting a\nmathematical function to calculate the probability of all combinations.\n\n> \"In this case, once we add our assumptions that (1) every toss is\n> independent of the other tosses and (2) the probability of W is the\n> same on every toss, probability theory provides a unique answer, known\n> as the <a class='glossary' title='It is used to calculate the probability of a certain number of successful outcomes, given a number of trials and the probability of the successful outcome. The “bi” in the term binomial refers to the two possible outcomes: an event happening and an event not happening. (BF, Chap.4)'>binomial distribution</a>. This is the common\"coin\n> tossing\" distribution.\" ([McElreath, 2020, p.\n> 33](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=52&annotation=E5HEDDW4))\n\n::: my-resource\n::: my-resource-container\nSee also @sec-annotation-5-dbinom for a resource to explore the binomial\ndistribution.\n:::\n:::\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-likelihood-globe-tossing}\n: Likelihood for prob = 0.5 in the globe tossing example\n:::\n:::\n\n::: my-r-code-container\nThe likelihood in the globe-tossing example (9 trials, 6 with `W` and 3\nwith `L`) is easily computed:\n\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/likelihood-prob-0.5-a_60732265876d2e8cc2f28199ad0b3d02'}\n\n```{.r .cell-code}\n## R code 2.2 ################\ndbinom(6, size = 9, prob = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.1640625\n```\n\n\n:::\n:::\n\n\n> \"Change the 0.5 to any other value, to see how the value changes.\"\n> ([McElreath, 2020, p.\n> 34](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=53&annotation=7USE7U6W))\n\nIn this example it is assumed that the probability of `W` and `L` are\nequal distributed. We calculated how plausible the combination of $6W$\nand $3L$ would be, for the specific value of $p = 0.5$. The result is\nwith $16\\%$ a pretty low probability.\n\nTo get a better idea what the best estimation of the probability is, we\ncould vary systematically the $p$ value and look for the maximum. A\ndemonstration how this is done can be seen in @cnj-calcu-10-probs. It\nshows a maximum at $prob = 0.7$.\n:::\n:::\n\n##### Unobserved Variables\n\nEven variables that are not observed (= parameters) we need to define\nthem. In the globe-tossing model there is only one parameter ($p$), but\nmost models have more than one unobserved variables.\n\n::: my-important\n::: my-important-header\nParameter & Prior\n:::\n\n::: my-important-container\nFor every parameter we must provide a distribution of prior\nplausibility, its <a class='glossary' title='The Prior Probability, also called the Prior, is the assumed probability distribution before we have seen the data. (Wikipedia) It quantifies how likely our initial belief is: P(belief). (BF, Chap.8)'>Prior Probability</a>. This is also true\nwhen the number of trials is null ($N = 0$), e.g. even in the initial\nstate of information we need a prior. (See @cnj-fig-bayesian-update,\nwhere a flat prior was used.)\n:::\n:::\n\nWhen you have a previous estimate, that can become the prior. As a\nresult, each estimate (<a class='glossary' title='It is the revised or updated probability of an event occurring after taking into consideration new information. (Investopedia). Posterior probability = prior probability + new evidence (called likelihood). (Statistics How To) The posterior distribution will be a distribution of Gaussian distributions. (SR, Chap.4). It quantifies exactly how much our observed data changes our beliefs: P(belief | data) (BF, Chap.8)'>posterior probability</a>) becomes\nthen the prior for the next step (as you have seen in\n@cnj-fig-bayesian-update).\n\n> \"So where do priors come from? They are both engineering assumptions,\n> chosen to help the machine learn, and scientific assumptions, chosen\n> to reflect what we know about a phenomenon.\" ([McElreath, 2020, p.\n> 35](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=54&annotation=C5Q2WBUL))\n\n> \"Within Bayesian data analysis in the natural and social sciences, the\n> prior is considered to be just part of the model. As such it should be\n> chosen, evaluated, and revised just like all of the other components\n> of the model.\" ([McElreath, 2020, p.\n> 35](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=54&annotation=GQX776IE))\n\n> \"Beyond all of the above, there's no law mandating we use only one\n> prior. If you don't have a strong argument for any particular prior,\n> then try different ones. Because the prior is an assumption, it should\n> be interrogated like other assumptions: by altering it and checking\n> how sensitive inference is to the assumption.\" ([McElreath, 2020, p.\n> 35](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=54&annotation=H9FQKLCU))\n\n::: my-resource\n::: my-resource-header\nMore on the difference between Bayesian and frequentist statistics?\n:::\n\n::: my-resource-container\nCheck out McElreath's lecture, [*Understanding Bayesian statistics\nwithout frequentist language*](https://youtu.be/yakg94HyWdE) at\nBayes\\@Lund2017 (20 April 2017).\n:::\n:::\n\n#### A Model is Born\n\n> \"The observed variables W and L are given relative counts through the\n> binomial distribution.\" ([McElreath, 2020, p.\n> 36](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=55&annotation=8FL6YY2Q))\n\n$$W∼Binomial(n,p) \\space where\\space N = W + L$$ {#eq-globe-tossing-binomial-dist}\n\n> \"The above is just a convention for communicating the assumption that\n> the relative counts of ways to realize $W$ in $N$ trials with\n> probability $p$ on each trial comes from the binomial distribution.\"\n> ([McElreath, 2020, p.\n> 36](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=55&annotation=H9UKQXP2))\n\nOur binomial likelihood contains a parameter for an unobserved variable,\n*p*.\n\n$$p∼Uniform(0,1)$$ {#eq-uniform-prior}\n\nThe formula expresses the model assumption that the entire range of\npossible values for $p$ are equally plausible.\n\n###TIDYVERSE\n\nGiven a probability of .5, (e.g. equal probability to both events `W`\nand `L`) we use the `dbinom()` function to determine the likelihood of 6\nout of 9 tosses coming out water in @cnj-likelihood-globe-tossing.\n\nMcElreath suggests:\n\n> \"Change the 0.5 to any other value, to see how the value changes.\"\n> ([McElreath, 2020, p.\n> 34](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=53&annotation=7USE7U6W))\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-calcu-10-probs}\n: Calculation likelihood with 10 different values of `prob`\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/likelihood-10-probs_d6754de174031f42e1bf82d14110b695'}\n\n```{.r .cell-code}\n(d <- tibble::tibble(prob = seq(from = 0, to = 1, by = .1)) |> \n    dplyr::mutate(likelihood = dbinom(x = 6, size = 9, prob = prob))\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 11 × 2\n#>     prob likelihood\n#>    <dbl>      <dbl>\n#>  1   0    0        \n#>  2   0.1  0.0000612\n#>  3   0.2  0.00275  \n#>  4   0.3  0.0210   \n#>  5   0.4  0.0743   \n#>  6   0.5  0.164    \n#>  7   0.6  0.251    \n#>  8   0.7  0.267    \n#>  9   0.8  0.176    \n#> 10   0.9  0.0446   \n#> 11   1    0\n```\n\n\n:::\n:::\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/filter-max_065d1eb63b1ed81ba20cdd3dad89b6b7'}\n\n```{.r .cell-code}\nd  |>  \n    dplyr::filter(likelihood == max(likelihood))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 1 × 2\n#>    prob likelihood\n#>   <dbl>      <dbl>\n#> 1   0.7      0.267\n```\n\n\n:::\n:::\n\n:::\n:::\n\nIn the series of values you will notice several point:\n\n1.  The values start with zero until a maximum of $0.267$ and decline to\n    zero again. The maximum is with $prob = 0.7$, a proportion of $W$\n    and $L$ that is --- as we know from our large world knowledge\n    (knowledge outside the small world of the model) --- already pretty\n    near the real distribution of about $0.71$. (see [How Much of the\n    Earth Is Covered by\n    Water?](https://www.thedailyeco.com/how-much-of-the-earth-is-covered-by-water-122.html))\n2.  You see that the first `prob` ($0$) and last `prob` ($1$) values are\n    both zero. From the result (6 $W$ and 3 $L$) `prob` cannot be 0 or 1\n    because there a both $W$ and $L$ in the observed sample.\n\n@cnj-calcu-10-probs is my interpretation from the quote \"Change the 0.5\nto any other value, to see how the value changes.\" Kurz has another\ninterpretation when he draws a graph of 100 `prob` values from 0 to 1:\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-calcu-100-probs}\n: Plot likelihood for 100 values of `prob` from $0$ to $1$, by steps of\n$0.01$\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/fig-likelihood-100-prob_16f3b7b52873d0ac6ab10a7f2456d95f'}\n\n```{.r .cell-code}\ntibble::tibble(prob = seq(from = 0, to = 1, by = .01)) |>  \n  ggplot2::ggplot(ggplot2::aes(x = prob, y = dbinom(x = 6, size = 9, prob = prob))) +\n  ggplot2::geom_line() +\n  ggplot2::labs(x = \"probability\",\n       y = \"binomial likelihood\") +\n  ggplot2::theme(panel.grid = ggplot2::element_blank()) +\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Likelihood for 100 values of prob, from 0 to 1, by steps of 0.01](02-small-and-large-worlds_files/figure-html/fig-likelihood-100-prob-1.png){#fig-likelihood-100-prob width=672}\n:::\n:::\n\n:::\n:::\n\nIn contrast to $p = 0.5$ with a probability of $0.16$ the\n@fig-likelihood-100-prob shows a maximum at about $p = 0.7$ and a\nprobability estimated from the graph of about $0.26-0.28$. We will get\nmore detailed data later in the book.\n\nIt is interesting to see that even the maximum probability is not very\nhigh. The reason is that there are many other configurations\n(distributions of $W$s and $L$s) to produce the result of $6W$ and $3L$.\nEven if all these other distributions have a small probability they\n\"eat\" all with their share from the maximum.\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-piror-prob-dens}\n: Prior as a probability distribution for the parameter\n:::\n:::\n\n::: my-r-code-container\n> \"The prior is a <a class='glossary' title='It is a way of describing all possible events and the probability of each one happening. Probability distributions are also very useful for asking questions about ranges of possible values. (BF, Chap.4) The two defining features are: (1) All values of the distribution must be real and non-negative. (2) The sum (for discrete random variables) or integral (for continuous random variables) across all possible values of the random variable must be 1. (BS, Chap.3)'>probability distribution</a> for the\n> parameter. In general, for a uniform prior from $a$ to $b$, the\n> probability of any point in the interval is $1/(b − a)$. If you're\n> bothered by the fact that the probability of every value of $p$ is\n> $1$, remember that every probability distribution must sum (integrate)\n> to $1$.\" ([McElreath, 2020, p.\n> 35](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=54&annotation=3TQ9MKDM))\n\nKurz demonstrates the truth of this quote with several $b$ values while\nholding $a$ constant:\n\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/uniform-prior1_0805e5b6fd3cc080096540d61a27b9fe'}\n\n```{.r .cell-code}\ntibble::tibble(a = 0,\n       b = c(1, 1.5, 2, 3, 9)) |>  \n  dplyr::mutate(prob = 1 / (b - a))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 5 × 3\n#>       a     b  prob\n#>   <dbl> <dbl> <dbl>\n#> 1     0   1   1    \n#> 2     0   1.5 0.667\n#> 3     0   2   0.5  \n#> 4     0   3   0.333\n#> 5     0   9   0.111\n```\n\n\n:::\n:::\n\n\nVerified with a plot Kurz divides the range of the $b$ parameter ($0-9$)\ninto 500 segments (*parameter_space*) and uses the `dunif()`\ndistribution to calculate the probabilities for a uniform distribution:\n\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/fig-uniform-prior2_859c73fc4e373e68d91132a13b8ba98f'}\n\n```{.r .cell-code}\ntibble::tibble(a = 0,\n       b = c(1, 1.5, 2, 3, 9)) |>  \n  tidyr::expand_grid(parameter_space = seq(from = 0, to = 9, length.out = 500)) |>  \n  dplyr::mutate(prob = dunif(parameter_space, a, b),\n         b    = stringr::str_c(\"b = \", b)) |>  \n  \n  ggplot2::ggplot(ggplot2::aes(x = parameter_space, y = prob)) +\n  ggplot2::geom_area() +\n  ggplot2::scale_x_continuous(breaks = c(0, 1:3, 9)) +\n  ggplot2::scale_y_continuous(breaks = c(0, 1/9, 1/3, 1/2, 2/3, 1),\n                     labels = c(\"0\", \"1/9\", \"1/3\", \"1/2\", \"2/3\", \"1\")) +\n  ggplot2::theme(panel.grid.minor = ggplot2::element_blank(),\n        panel.grid.major.x = ggplot2::element_blank()) +\n  ggplot2::theme_bw() +\n  ggplot2::facet_wrap(~ b, ncol = 5)\n```\n\n::: {.cell-output-display}\n![Graphical demonstration that the probability of every value of $p$ is $1$](02-small-and-large-worlds_files/figure-html/fig-uniform-prior2-1.png){#fig-uniform-prior2 width=672}\n:::\n:::\n\n\nThis figure demonstrates that the area in the whole parameter space is\n*1.0*. It is a nice example how to calculate the probability *mass* (in\ncontrast to the curve of the probability *density*).\n:::\n:::\n\n## Making the Model Go\n\n> \"Once you have named all the variables and chosen definitions for\n> each, a Bayesian model can update all of the prior distributions to\n> their purely logical consequences: the ´r glossary(\"posterior\n> distribution\")\\`. For every unique combination of data, likelihood,\n> parameters, and prior, there is a unique posterior distribution. This\n> distribution contains the relative plausibility of different parameter\n> values, conditional on the data and model. The posterior distribution\n> takes the form of the probability of the parameters, conditional on\n> the data.\" ([McElreath, 2020, p.\n> 36](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=55&annotation=P5JNFN9Y))\n\nIn the case of the globe-tossing model we can write:\n\n$$\nPr(p|W, L)\n$$ {#eq-prob-globe-tossing}\n\nThis has to be interpreted as \"the probability of each possible value of\n*p*, conditional on the specific $W$ and $L$ that we observed.\"\n\n### Bayes' Theorem\n\n#### ORIGINAL\n\nThe Bayes' theorem or Bayes' rule gives Bayesian data analysis its name.\nWhat follows is a quick derivation of it. At first glance this might be\ntoo theoretical but after reading different books on Bayesian statistics\nI got the impression that it is not only important to know where the\nformula comes from but also what it stands for.\n\n::: my-theorem\n::: my-theorem-header\n::: {#thm-bayes-theorem}\n: Bayes Theorem\n:::\n:::\n\n::: my-theorem-container\n> \"The joint probability of the data W and L and any particular value of\n> p is:\" ([McElreath, 2020, p.\n> 37](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=56&annotation=TTB5EEEJ))\n\n$$\nPr(W, L, p) = Pr(W, L \\mid p) Pr(p)\n$$ {#eq-joint-prob-globe-tossing}\n\n> “This just says that the probability of W, L and p is the product of Pr(W, L|p) and the prior probability Pr(p). This is like saying that the probability of rain and cold on the same day is equal to the probability of rain, when it’s cold, times the probability that it’s cold. This much is just definition. But it’s just as true that:” ([McElreath, 2020, p. 37](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=56&annotation=Y4JHGXCG))\n\n\n$$\nPr(W, L, p) = Pr(p \\mid W, L) Pr(W, L)\n$$ {#eq-reverse-cond-prob}\n\n\n> “All I’ve done is reverse which probability is conditional, on the right-hand side. It is still a true definition. It’s like saying that the probability of rain and cold on the same day is equal to the probability that it’s cold, when it’s raining, times the probability of rain.” ([McElreath, 2020, p. 37](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=56&annotation=KC37E7XA))\n\n> \"Now since both right-hand sides above are equal to the same thing,\n> $Pr(W, L, p)$, they are also equal to one another:\" ([McElreath, 2020,\n> p. 37](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=56&annotation=WEIB5BKN))\n\n$$\n\\begin{align*}\nPr(W, L \\mid p) Pr(p) = Pr(p \\mid W, L) Pr(W, L) \\\\\nPr(p\\mid W, L) = \\frac{Pr(W, L \\mid p) Pr(p)}{Pr(W, L)}\n\\end{align*} \n$$ {#eq-bayes-theorem}\n\n> \"And this is <a class='glossary' title='This is the theorem that gives Bayesian data analysis its name. But the theorem itself is a trivial implication of probability theory. The mathematical definition of the posterior distribution arises from Bayes’ Theorem. The key lesson is that the posterior is proportional to the product of the prior and the probability of the data. (Chap.2)'>Bayes’ theorem</a>. It says that the\n> probability of any particular value of $p$, considering the data, is\n> equal to the product of the relative plausibility of the data,\n> conditional on $p$, and the prior plausibility of $p$, divided by this\n> thing $Pr(W, L)$, which I'll call the *average probability of the\n> data*.\" ([McElreath, 2020, p.\n> 37](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=56&annotation=EECURSC4))\n\nExpressed in words:\n\n$$\nPosterior = \\frac{Probability\\space of\\space the\\space data\\space ✕\\space Prior}{Average\\space probability\\space of\\space the\\space data}\n$$ {#eq-in-words}\n\n\nOther names for the somewhat confusing *average probability of the\ndata*:\n\n-   evidence\n-   average likelihood\n-   marginal likelihood\n\n> \"The probability $Pr(W, L)$ is literally the average probability of\n> the data. Averaged over what? Averaged over the prior. It's job is\n> just to standardize the posterior, to ensure it sums (integrates) to\n> one. In mathematical form:\" ([McElreath, 2020, p.\n> 37](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=56&annotation=6J5GL8BK))\n\n$$\nPR(W, L) = E(Pr(W, L \\mid p)) = \\int Pr(W, L \\mid p) Pr(p) dp\n$$ {#eq-math-form}\n\n> \"The operator E means to take an *expectation.* Such averages are\n> commonly called *marginals* in mathematical statistics, and so you may\n> also see this same probability called a *marginal likelihood*. And the\n> integral above just defines the proper way to compute the average over\n> a continuous distribution of values, like the infinite possible values\n> of $p$.\" ([McElreath, 2020, p.\n> 37](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=56&annotation=QPMLS5NG))\n:::\n:::\n\n::: my-important\n::: my-important-header\nKey lesson about Bayes' theorem\n:::\n\n::: my-important-container\nThe posterior is proportional to the product of the prior and the\nprobability of the data.\n:::\n:::\n\n> @fig-2-6-book \"illustrates the multiplicative interaction of a prior\n> and a probability of data. On each row, a prior on the left is\n> multiplied by the probability of data in the middle to produce a\n> posterior on the right. The probability of data in each case is the\n> same. The priors however vary. As a result, the posterior\n> distributions vary.\" ([McElreath, 2020, p.\n> 38](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=57&annotation=MRV5EKMJ))\n\n![The original Figure 2.6 from the\nbook](img/SR2-fig2_6-min.jpg){#fig-2-6-book\nfig-alt=\"The posterior distribution as a product of the prior distribution and likelihood. Top: A flat prior constructs a posterior that is simply proportional to the likelihood. Middle: A step prior, assigning zero probability to all values less than 0.5, results in a truncated posterior. Bottom: A peaked prior that shifts and skews the posterior, relative to the likelihood.\"}\n\n#### TIDYVERSE\n\nFor my understanding it is important to reproduce the @fig-2-6-book\ngraph with {**tidyverse**} R code as it is shown in the next section.\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-fig-2-6-book}\n: The posterior distribution as a product of the prior distribution and\nlikelihood\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/prepare-fig-2-6-book_e5c967294ea3aa0ebec7fec47b25e6c9'}\n\n```{.r .cell-code}\nsequence_length <- 1e3\n\nd <-\n  tibble::tibble(probability = seq(from = 0, to = 1, \n                                   length.out = sequence_length)) |>  \n  tidyr::expand_grid(row = c(\"flat\", \"stepped\", \"Laplace\"))  |>  \n  dplyr::arrange(row, probability) |>  \n  dplyr::mutate(\n      prior = ifelse(row == \"flat\", 1,\n              ifelse(row == \"stepped\", rep(0:1, each = sequence_length / 2),\n                    exp(-abs(probability - 0.5) / .25) / ( 2 * 0.25))),\n      likelihood = dbinom(x = 6, size = 9, prob = probability)) |>  \n  dplyr::group_by(row) |>  \n  dplyr::mutate(posterior = prior * likelihood / sum(prior * likelihood)) |>  \n  tidyr::pivot_longer(prior:posterior)  |>  \n  dplyr::ungroup() |>  \n  dplyr::mutate(\n      name = forcats::fct(name, levels = c(\"prior\", \"likelihood\", \"posterior\")),\n      row  = forcats::fct(row, levels = c(\"flat\", \"stepped\", \"Laplace\")))\n```\n:::\n\n:::\n:::\n\nIn comparison to my very detailed code annotations of\n@fig-bayesian-update there are different lines of code, but generally\nthere is nothing conceptually new: We use again `expand_grid()` to\ncreate a tibble of input combinations and create with `mutate()` two\ncolumns for prior and likelihood. We do not use the `lag()` functions as\nwe calculate only for one prior and one likelihood. Kurz advises us that\nis \"easier to just make each column of the plot separately. We can then\nuse the elegant and powerful syntax from [Thomas Lin\nPedersen](https://www.data-imaginist.com/)'s (2022) [patchwork\npackage](https://patchwork.data-imaginist.com/) to combine them.\"\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-fig-plot-2-6-book}\n: Reproduction of book's @fig-2-6-book\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/fig-plot-2-6-book_851854b710b5a3167d99c88d06737700'}\n\n```{.r .cell-code}\np1 <-\n  d |> \n  dplyr::filter(row == \"flat\") |>  \n  ggplot2::ggplot(ggplot2::aes(x = probability, y = value)) +\n  ggplot2::geom_line() +\n  ggplot2::scale_x_continuous(NULL, breaks = NULL) +\n  ggplot2::scale_y_continuous(NULL, breaks = NULL) +\n  ggplot2::theme(panel.grid = ggplot2::element_blank()) +\n  ggplot2::theme_bw() +\n  ggplot2::facet_wrap(~ name, scales = \"free_y\")\n\np2 <-\n  d |> \n  dplyr::filter(row == \"stepped\") |>  \n  ggplot2::ggplot(ggplot2::aes(x = probability, y = value)) +\n  ggplot2::geom_line() +\n  ggplot2::scale_x_continuous(NULL, breaks = NULL) +\n  ggplot2::scale_y_continuous(NULL, breaks = NULL) +\n  ggplot2::theme(panel.grid = ggplot2::element_blank(),\n        strip.background = ggplot2::element_blank(),\n        strip.text = ggplot2::element_blank()) +\n  ggplot2::theme_bw() +\n  ggplot2::facet_wrap(~ name, scales = \"free_y\")\n\np3 <-\n  d |> \n  dplyr::filter(row == \"Laplace\") |>  \n  ggplot2::ggplot(ggplot2::aes(x = probability, y = value)) +\n  ggplot2::geom_line() +\n  ggplot2::scale_x_continuous(NULL, breaks = c(0, .5, 1)) +\n  ggplot2::scale_y_continuous(NULL, breaks = NULL) +\n  ggplot2::theme(panel.grid = ggplot2::element_blank(),\n        strip.background = ggplot2::element_blank(),\n        strip.text = ggplot2::element_blank()) +\n  ggplot2::theme_bw() +\n  ggplot2::facet_wrap(~ name, scales = \"free_y\")\n\n# combine\nlibrary(patchwork) \np1 / p2 / p3\n```\n\n::: {.cell-output-display}\n![This is the reproduction of @fig-2-6-book from the book coded with tidyverse approach. It shows the posterior distribution as a product of the prior distribution and likelihood. TOP: A flat prior constructs a posterior that is simply proportional to the likelihood. MIDDLE: A step prior, assigning zero probability to all values less than 0.5, results in a truncated posterior. BOTTOM: A peaked prior that shifts and skews the posterior, relative to the likelihood.](02-small-and-large-worlds_files/figure-html/fig-plot-2-6-book-1.png){#fig-plot-2-6-book width=672}\n:::\n:::\n\n:::\n:::\n\n@fig-plot-2-6-book replicates @fig-2-6-book. It shows that the same\nlikelihood with a different prior results in a different posterior.\n\n### Motors\n\n#### ORIGINAL\n\nThe production of the posterior distribution is driven by a motor that\nconditions with it calculation the prior to the data. However, besides\nof simple and special restrictive models, often there is no formal\nmathematical solution. Therefore we need numerical techniques to\napproximate the mathematics.\n\n::: my-objectives\n::: my-objectives-header\nLearning three different conditioning engines\n:::\n\n::: my-objectives-container\n> \"In this book, you'll meet three different conditioning engines,\n> numerical techniques for computing posterior distributions:\n>\n> (1) Grid approximation\n> (2) Quadratic approximation\n> (3) Markov chain Monte Carlo (MCMC)\n>\n> There are many other engines, and new ones are being invented all the\n> time. But the three you'll get to know here are common and widely\n> useful. In addition, as you learn them, you'll also learn principles\n> that will help you understand other techniques.\" ([McElreath, 2020, p.\n> 39](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=58&annotation=4CYVRVT6))\n:::\n:::\n\n::: my-watch-out\n::: my-watch-out-header\nFitting the model is part of the model\n:::\n\n::: my-watch-out-container\n> \"... the details of fitting the model to data force us to recognize\n> that our numerical technique influences our inferences. This is\n> because different mistakes and compromises arise under different\n> techniques. The same model fit to the same data using different\n> techniques may produce different answers.\" ([McElreath, 2020, p.\n> 39](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=58&annotation=KY35YK8M))\n:::\n:::\n\n#### TIDYVERSE (empty)\n\n### Grid approximation {#sec-grid-approx}\n\n#### ORIGINAL\n\n> \"One of the simplest conditioning techniques is grid approximation.\n> While most parameters are continuous, capable of taking on an infinite\n> number of values, it turns out that we can achieve an excellent\n> approximation of the continuous posterior distribution by considering\n> only a finite grid of parameter values.\" ([McElreath, 2020, p.\n> 39](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=58&annotation=53UNNMMU))\n\nGrid approximation is very useful as a pedagogical tool. But often it\nisn't practical because it scales poorly, as the number of parameters\nincreases.\n\n::: my-procedure\n::: my-procedure-header\n::: {#prp-grid-approx}\n: Grid Approximation\n:::\n:::\n\n::: my-procedure-container\n> \"Here is the recipe:\n>\n> (1) Define the grid. This means you decide how many points to use in\n>     estimating the posterior, and then you make a list of the\n>     parameter values on the grid.\n> (2) Compute the value of the prior at each parameter value on the\n>     grid.\n> (3) Compute the likelihood at each parameter value.\n> (4) Compute the unstandardized posterior at each parameter value, by\n>     multiplying the prior by the likelihood.\n> (5) Finally, standardize the posterior, by dividing each value by the\n>     sum of all values.\" ([McElreath, 2020, p.\n>     40](zotero://select/groups/5243560/items/NFUEVASQ))\n>     ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=59&annotation=23IXIX8K))\n:::\n:::\n\nI will outline in the globe tossing model the necessary steps for grid\napproximation. In addition to the 5 and 20 points approximation I will\nadd 100 and 1000 points to see how the inferences changes with higher\nvalues.\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-globe-tossing-steps}\n: 6 Steps for Grid Approximation for the Globe Tossing Model\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/fig-grid-approx-5-and-20_63edd96ac9446b487b2c184e9f8e304d'}\n\n```{.r .cell-code}\n## R code 2.3  adapted #########################\n\n# 1. define grid\np_grid5 <- seq(from = 0, to = 1, length.out = 5)\np_grid20 <- seq(from = 0, to = 1, length.out = 20)\np_grid100 <- seq(from = 0, to = 1, length.out = 100)\np_grid1000 <- seq(from = 0, to = 1, length.out = 1000)\n\n# 2. define prior\nprior5 <- rep(1, 5)\nprior20 <- rep(1, 20)\nprior100 <- rep(1, 100)\nprior1000 <- rep(1, 1000)\n\n# 3. compute likelihood at each value in grid\nlikelihood5 <- dbinom(x = 6, size = 9, prob = p_grid5)\nlikelihood20 <- dbinom(x = 6, size = 9, prob = p_grid20)\nlikelihood100 <- dbinom(x = 6, size = 9, prob = p_grid100)\nlikelihood1000 <- dbinom(x = 6, size = 9, prob = p_grid1000)\n\n# 4. compute product of likelihood and prior\nunstd.posterior5 <- likelihood5 * prior5\nunstd.posterior20 <- likelihood20 * prior20\nunstd.posterior100 <- likelihood100 * prior100\nunstd.posterior1000 <- likelihood1000 * prior1000\n\n# 5. standardize the posterior, so it sums to 1\nposterior5 <- unstd.posterior5 / sum(unstd.posterior5)\nposterior20 <- unstd.posterior20 / sum(unstd.posterior20)\nposterior100 <- unstd.posterior100 / sum(unstd.posterior100)\nposterior1000 <- unstd.posterior1000 / sum(unstd.posterior1000)\n\n\n# 6. display posterior distributions\nop <- par(mfrow = c(2, 2))\n\n\n## R code 2.4 ######################\n## R code 2.4 for 5 points\nplot(p_grid5, posterior5,\n  type = \"b\",\n  xlab = \"probability of water\", ylab = \"posterior probability\"\n)\nmtext('5 points with \"prior = 1\"')\n\n## R code 2.4 for 20 points\nplot(p_grid20, posterior20,\n  type = \"b\",\n  xlab = \"probability of water\", ylab = \"posterior probability\"\n)\nmtext('20 points with \"prior = 1\"')\n\n## R code 2.4 for 100 points\nplot(p_grid100, posterior100,\n  type = \"b\",\n  xlab = \"probability of water\", ylab = \"posterior probability\"\n)\nmtext('100 points with \"prior = 1\"')\n\n## R code 2.4 for 1000 points\nplot(p_grid1000, posterior1000,\n  type = \"b\",\n  xlab = \"probability of water\", ylab = \"posterior probability\"\n)\nmtext('1000 points with \"prior = 1\"')\n\npar(op)\n```\n\n::: {.cell-output-display}\n![Grid Approximation with 5 and 20 points. You can see that the approximation gets a lot better with 20 than with 5 points. After 100 points there is no much improvement. This conforms to McElreath remark that 'there won’t be much change in inference after the first 100.' (McElreath, 2020, p. 40)](02-small-and-large-worlds_files/figure-html/fig-grid-approx-5-and-20-1.png){#fig-grid-approx-5-and-20 width=672}\n:::\n:::\n\n:::\n:::\n\n#### TIDYVERSE\n\nInstead of just to replicate the globe-tossing steps with tidyverse code\nas in @cnj-globe-tossing-steps I will experiment with different priors\nto see their influences against the posterior distribution.\n\n@cnj-globe-tossing-different-priors uses Kurz' last code chunk in his [grid\napproximation\nsection](https://bookdown.org/content/4857/small-worlds-and-large-worlds.html#grid-approximation).\nBut I have adapted it to include the three uniform priors and to show\nalso the (small) effect of 100 points grids.\n\n::: my-experiment\n::: my-experiment-header\nExperiment: The effect of different priors and of different numbers of\ngrid points.\n:::\n\n::: my-experiment-container\nThe parameters for the calculated likelihood is based on the binomial\ndistribution and is shaping the above plot:\n\n-   `x` = number of water events $W$\n-   `size` = number of sample trials = number of observations\n-   `prob` = success probability on each trial = probability of $W$\n    (water event)\n\nI will only change `prob` and approximate the five different prior\nvariants with 5, 20 and 100 points. I want to see how the effects of\ndifferent priors on the posterior distribution by different number of\nevents (grid points). The priors I will use are uniform priors of $0.1$,\n$0.5$ and $0.9$, and the two suggestion by McElreath\n$ifelse(p\\_grid < 0.5, 0, 1)$ and $exp(-5 * abs(p\\_grid - 0.5))$.\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-globe-tossing-different-priors}\n: Globe tossing with different priors and grid points\n:::\n:::\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/fig-different-priors_e18e235306bd52d4323e1ae0620df9ea'}\n\n```{.r .cell-code}\n## R code 2.5 tidyverse integrated ################\n# prepare the plot by producing the data\n\ntib <- \ntibble::tibble(n_points = c(5, 20, 100)) |>  \n  dplyr::mutate(p_grid = purrr::map(n_points, ~seq(from = 0, to = 1, \n                                            length.out = .))) |>  \n  tidyr::unnest(p_grid) |>  \n  tidyr::expand_grid(priors = c(\"0.1\",\n                         \"0.5\",\n                         \"0.9\",\n                         \"ifelse(p_grid < 0.5, 0, 1)\", \n                         \"exp(-5 * abs(p_grid - 0.5))\")) |>  \n  dplyr::mutate(prior = dplyr::case_when(\n    priors == \"0.1\" ~ 0.1,\n    priors == \"0.5\" ~ 0.5,\n    priors == \"0.9\" ~ 0.9,\n    priors == \"ifelse(p_grid < 0.5, 0, 1)\" ~ ifelse(p_grid < 0.5, 0, 1),\n    priors == \"exp(-5 * abs(p_grid - 0.5))\" ~ exp(-5 * abs(p_grid - 0.5))\n  )) |> \n  dplyr::mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) |>  \n  dplyr::mutate(posterior = likelihood * prior / sum(likelihood * prior)) |>  \n  dplyr::mutate(n_points = stringr::str_c(\"# points = \", n_points),\n         priors   = stringr::str_c(\"prior = \", priors)) |>  \n  \n  # plot the data\n  ggplot2::ggplot(ggplot2::aes(x = p_grid, y = posterior)) +\n  ggplot2::geom_line() +\n  ggplot2::geom_point() +\n  ggplot2::labs(x = \"probability of water\",\n       y = \"posterior probability\") +\n  ggplot2::theme(panel.grid = ggplot2::element_blank()) +\n  ggplot2::theme_bw() +\n  ggplot2::facet_grid(priors ~ n_points, scales = \"free\")\n```\n:::\n\n:::\n:::\n\nInspecting the output we can draw some conclusions:\n\nIt does not matter what uniform prior probability is chosen in the range\nfrom 0 to 1, if the probability of $W$ is greater than zero and --- and\nby definition of the binomial function --- equal for all events (grid\npoints). This does not only conform to values but also for functions\nthat generates values.\n\nThe only outlier in this sequence of different priors is the exponential\nfunction. But even this prior will finally give way if the number of\nobserved events (approaching the real probability of 0.71) rises.\n\nI will demonstrate this with 10000 samples and a W:L proportion of 7:3.\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-globe-tossing-many-samples}\n: Diminishing effect of the prior with rising amout of data\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/fig-grid-many-samples_2725b9fff52606d42fe159f4c0e0201b'}\n\n```{.r .cell-code}\nd <-\n    # 1.  define grid\n    tibble::tibble(p_grid = seq(from = 0, to = 1, length.out = 100),\n    # 2. define prior          \n           prior  = 1) |>       \n    # 3. compute likelihood at each value in grid\n    dplyr::mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) |>   \n    # 4. compute product of likelihood and prior\n    dplyr::mutate(unstd_posterior = likelihood * prior) |>  \n    # 5. standardize the posterior, so it sums to 1\n    dplyr::mutate(posterior = unstd_posterior / sum(unstd_posterior))   \n\nd |>  \n  ggplot2::ggplot(ggplot2::aes(x = p_grid, y = posterior)) +\n  ggplot2::geom_point() +\n  ggplot2::geom_line() +\n  ggplot2::labs(title = \"100 point grid approximation of 10000 samples with prior \n       of exp(-5 * abs(p_grid - 0.5)) and a proportion of W:L = 7:3.\",\n       x = \"probability of water\",\n       y = \"posterior probability\") +\n  ggplot2::theme(panel.grid = ggplot2::element_blank()) +\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Grid Approximation with prior of exp(-5 * abs(p_grid - 0.5)) drawing 10,000 samples with a W proportion of 0.7](02-small-and-large-worlds_files/figure-html/fig-grid-many-samples-1.png){#fig-grid-many-samples width=672}\n:::\n:::\n\n:::\n:::\n\nIn this example the maximum of probability is already 0.68! We can say\nthat **with every chosen prior we will get finally the correct\nresult!**.\n\nBut the choice of the prior is still important as it determines how many\nBayesian updates we need to get the right result. If we have an awkward\nprior and not the appropriate size of the sample we will get a posterior\ndistribution showing us a wrong maximum of probability. In that case the\nprocess of approximation has not reached a state where the probability\nmaximum is near the correct result. The problem is: Most time we do not\nknow the correct solution and can't therefore decide if we have had\nenough Bayesian updates.\n:::\n:::\n\n### Quadratic Approximation\n\nGrid approximation is very costly.\n\n> \"The reason is that the number of unique values to consider in the\n> grid grows rapidly as the number of parameters in your model\n> increases. For the single-parameter globe tossing model, it's no\n> problem to compute a grid of 100 or 1000 values. But for two\n> parameters approximated by 100 values each, that's already 100\\^2 =\n> 10,000 values to compute. For 10 parameters, the grid becomes many\n> billions of values. These days, it's routine to have models with\n> hundreds or thousands of parameters.\" \" ([McElreath, 2020, p.\n> 41f](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=60&annotation=7E79NMUP))\n\nEven grid approximation strategy scales poorly with model complexity it\nis very valuable for pedagogical reason. The two other strategies\n(<a class='glossary' title='Quadratic approximation is a way to approximate a curve. Quadratic approximation is an extension of linear approximation – we’re adding one more term, which is related to the second derivative. Linear approximation uses the first derivative to find the straight line that most closely resembles a curve at some point. Quadratic approximation uses the first and second derivatives to find the parabola closest to the curve near a point. (Statistics How To and MIT OpenCourseWare)'>Quadratic approximation</a> and <a class='glossary' title='Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a MARKOV CHAIN that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the chain. The more steps that are included, the more closely the distribution of the sample matches the actual desired distribution. Various algorithms exist for constructing chains. (Wikipedia)'>MCMC</a>\nMethods) are better understood after examining in detail how grid\napproximation works.\n\n#### ORIGINAL\n\n##### Concept\n\n> \"Under quite general conditions, the region near the peak of the\n> posterior distribution will be nearly Gaussian---or\"normal\"---in\n> shape. This means the posterior distribution can be usefully\n> approximated by a Gaussian distribution. A Gaussian distribution is\n> convenient, because it can be completely described by only two\n> numbers: the location of its center (mean) and its spread (variance).\n>\n> A Gaussian approximation is called \"quadratic approximation\" because\n> the logarithm of a Gaussian distribution forms a parabola. And a\n> parabola is a quadratic function. So this approximation essentially\n> represents any log-posterior with a parabola.\" ([McElreath, 2020, p.\n> 42](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=61&annotation=2PTP9XSY))\n\n::: my-procedure\n::: my-procedure-header\n::: {#prp-quap}\n: Quadratic Approximation\n:::\n:::\n\n::: my-procedure-container\n1.  **Find the posterior mode** with some optimization algorithm. The\n    procedure does not know where the peak is but it knows the slope\n    under its feet.\n2.  **Estimate the curvature near the peak** to calculate a quadratic\n    approximation. This computation is done by some numerical technique.\n:::\n:::\n\n##### Computing the quadratic approximation\n\nTo compute the <a class='glossary' title='Quadratic approximation is a way to approximate a curve. Quadratic approximation is an extension of linear approximation – we’re adding one more term, which is related to the second derivative. Linear approximation uses the first derivative to find the straight line that most closely resembles a curve at some point. Quadratic approximation uses the first and second derivatives to find the parabola closest to the curve near a point. (Statistics How To and MIT OpenCourseWare)'>quadratic approximation</a> for the globe\ntossing data, we'll use a tool in the {**rethinking**} package:\n`rethinking::quap()`.\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-quap-globe-tossing}\n: Quadratic approximation of the globe tossing data\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/quap-globe-tossing_46812e50ef763d724c745d14d1b47097'}\n\n```{.r .cell-code}\n## R code 2.6 ####################\nglobe.qa <- rethinking::quap(\n  alist(\n    W ~ dbinom(W + L, p), # binomial likelihood\n    p ~ dunif(0, 1) # uniform prior\n  ),\n  data = list(W = 6, L = 3)\n)\n\n# display summary of quadratic approximation\nrethinking::precis(globe.qa)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>        mean        sd      5.5%     94.5%\n#> p 0.6666667 0.1571338 0.4155366 0.9177968\n```\n\n\n:::\n:::\n\n:::\n:::\n\n> \"You can read this kind of approximation like: *Assuming the posterior\n> is Gaussian, it is maximized at* $0.67$, and its standard deviation is\n> $0.16$.\" ([McElreath, 2020, p.\n> 43](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=62&annotation=RMZS6PQW))\n\n:::::{.my-watch-out}\n:::{.my-watch-out-header}\nSometimes the `quap()` functions returns an error\n:::\n::::{.my-watch-out-container}\n \nIf the `quap()` function does not compile because of an error, just try rendering the function or file again. The reason for the error? The function has chosen randomly a bad starting point for the approximation. (Later we will learn how to set start values to prevent such errors.) \n::::\n:::::\n\n\n\n::: my-procedure\n::: my-procedure-header\n::: {#prp-explain-quap}\n: Usage of `rethinking::quap()`\n:::\n:::\n\n::: my-procedure-container\n1.  `rethinking::quap()` needs a *formula*, e.g., a list of *data*\n    provided through `base::alist()`.\n2.  `alist()` handles its arguments as if they described function\n    arguments. So the values are not evaluated, and tagged arguments\n    with no value are allowed. It is most often used in conjunction with\n    `base::formals()`.\n3.  The function `rethinking::precis` presents a brief summary of the\n    quadratic approximation.\n\n<!-- -->\n\na)  **mean**: It shows the posterior mean value of $p = 0.67$. This is\n    the peak of the posterior distribution.\nb)  **sd**: The curvature is labeled \"sd\" This stands for\n    <a class='glossary' title='The standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is the square root of its variance. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data. Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter \\(\\sigma\\) (sigma), for the population standard deviation, or the Latin letter \\(s\\) for the sample standard deviation. (Wikipedia)'>standard deviation</a>. This value is the standard\n    deviation of the posterior distribution.\nc)  **5.5% and 94.5%**: These two values are the\n    <a class='glossary' title='The set of divisions that produce exactly 100 equal parts in a series of continuous values, such as blood pressure, weight, height, etc. Thus a person with blood pressure above the 80th percentile has a greater blood pressure value than over 80% of the other recorded values.” (CDS, p.323)'>percentile intervals</a> which will\n    explained in @sec-chap03.\n:::\n:::\n\n##### Computing analytical solution\n\nWe want to compare the quadratic approximation with the analytic\ncalculation.\n\n> \"I'll use the analytical approach here, which uses dbeta. I won't\n> explain this calculation, but it ensures that we have exactly the\n> right answer. You can find an explanation and derivation of it in just\n> about any mathematical textbook on Bayesian inference.\" ([McElreath,\n> 2020, p. 43](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=62&annotation=FHWLARTT))\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-globe-tossing-analytical}\n: Comparing quadratic approximation with analytic calculation using\n`dbeta()`\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/fig-analytical-calc_c0c43ccd29347396cddf9f181df33f9c'}\n\n```{.r .cell-code}\n## R code 2.7 ################################\n# analytic calculation\nW <- 6\nL <- 3\ncurve(dbeta(x, W + 1, L + 1), from = 0, to = 1)\n\n# quadratic approximation\ncurve(dnorm(x, 0.67, 0.16), lty = 2, add = TRUE)\n```\n\n::: {.cell-output-display}\n![Comparing quadratic approximation with analytic calculation](02-small-and-large-worlds_files/figure-html/fig-analytical-calc-1.png){#fig-analytical-calc width=672}\n:::\n:::\n\n\n> \"The ~~blue~~ solid line curve is the analytical posterior and the\n> ~~black~~ dashed curve is the quadratic approximation. The ~~black~~\n> solid curve does alright on its left side, but looks pretty bad on its\n> right side. It even assigns positive probability to $p = 1$, which we\n> know is impossible, since we saw at least one land sample\"\n> ([McElreath, 2020, p.\n> 43](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=62&annotation=UJVX3WCP))\n:::\n:::\n\n#### TIDYVERSE\n\nIn the book the calculation is only done for $n = 9$ but McElreath also\ndisplay the graphs for $n = 18$ and $n = 36$ with the same proportion of\n`W` and `L`. Kurz shows how this is done and results into Figure 2.8 in\nthe book.\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-quap-different-sample-size}\n: Quadratic approximation showing the effect of different sample size\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/quap-different-sample-size_dba40f447bd9ec33bf2974d19b539088'}\n\n```{.r .cell-code}\n### quap() with 9 sample size #################################\nglobe_qa_9 <- rethinking::quap(\n  alist(\n    W ~ dbinom(W + L, p), # binomial likelihood\n    p ~ dunif(0, 1) # uniform prior\n  ),\n  data = list(W = 6, L = 3)\n)\n\n# display summary of quadratic approximation\nrethinking::precis(globe_qa_9)\n\n### quap() with 18 sample size ###################################\nglobe_qa_18 <- rethinking::quap(\n  alist(\n    W ~ dbinom(W + L, p), # binomial likelihood\n    p ~ dunif(0, 1) # uniform prior\n  ),\n  data = list(W = 12, L = 6)\n)\n\n# display summary of quadratic approximation\nrethinking::precis(globe_qa_18)\n\n### quap() with 36 sample size ###################################\nglobe_qa_36 <- rethinking::quap(\n  alist(\n    W ~ dbinom(W + L, p), # binomial likelihood\n    p ~ dunif(0, 1) # uniform prior\n  ),\n  data = list(W = 24, L = 12)\n)\n\n# display summary of quadratic approximation\nrethinking::precis(globe_qa_36)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>        mean        sd      5.5%     94.5%\n#> p 0.6666665 0.1571338 0.4155363 0.9177967\n#>        mean        sd      5.5%     94.5%\n#> p 0.6666663 0.1111104 0.4890903 0.8442422\n#>       mean         sd     5.5%    94.5%\n#> p 0.666667 0.07856685 0.541102 0.792232\n```\n\n\n:::\n:::\n\n\nThe results above displays values for sample size $n = 9$ (top),\n$n = 18$ (middle) and $n = 36$ bottom. You can see that as the amount of\ndata increases\n\n1.  the mean approximates the real value\n2.  the standard deviation and the percentile intervals gets smaller.\n:::\n:::\n\n> \"As the amount of data increases, ... the quadratic approximation gets\n> better.\" ([McElreath, 2020, p.\n> 43](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=62&annotation=YQE3PX52))\n\n> \"This phenomenon, where the quadratic approximation improves with the\n> amount of data, is very common. It's one of the reasons that so many\n> classical statistical procedures are nervous about small samples:\n> Those procedures use quadratic (or other) approximations that are only\n> known to be safe with infinite data. Often, these approximations are\n> useful with less than infinite data, obviously. But the rate of\n> improvement as sample size increases varies greatly depending upon the\n> details. In some models, the quadratic approximation can remain\n> terrible even with thousands of samples.\" ([McElreath, 2020, p.\n> 44](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=63&annotation=PD39BQDF))\n\n@fig-quap-compare-analytic-different-n demonstrate that with more data\nthe curve gets smaller and higher, signifying that the model gets more\nsecure about the probability.\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-quap-compare-analytic-different-n}\n: Accuracy of the quadratic approximation with different sample sizes\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/fig-quap-compare-analytic-different-n_2d702ae6406c7d904aa1ea4ca4fb3d00'}\n\n```{.r .cell-code}\nn_grid <- 100\n\n# wrangle\ntibble::tibble(w = c(6, 12, 24),\n       n = c(9, 18, 36),\n       s = c(.16, .11, .08)) |>  \n  tidyr::expand_grid(p_grid = seq(from = 0, to = 1, length.out = n_grid)) |>  \n  dplyr::mutate(prior = 1,\n         m     = .67)  |> \n  dplyr::mutate(likelihood = dbinom(w, size = n, prob = p_grid)) |> \n  dplyr::mutate(unstd_grid_posterior = likelihood * prior,\n         unstd_quad_posterior = dnorm(p_grid, m, s)) |> \n  dplyr::group_by(w) |>  \n  dplyr::mutate(grid_posterior = unstd_grid_posterior / sum(unstd_grid_posterior),\n         quad_posterior = unstd_quad_posterior / sum(unstd_quad_posterior),\n         n              = stringr::str_c(\"n = \", n)) |>  \n dplyr::mutate(n = forcats::fct(n, levels = c(\"n = 9\", \"n = 18\", \"n = 36\"))) |>  \n  \n  # plot\n  ggplot2::ggplot(ggplot2::aes(x = p_grid)) +\n  ggplot2::geom_line(ggplot2::aes(y = grid_posterior)) +\n  ggplot2::geom_line(ggplot2::aes(y = quad_posterior),\n            color = \"grey50\") +\n  ggplot2::labs(x = \"proportion water\",\n       y = \"density\") +\n  ggplot2::theme(panel.grid = ggplot2::element_blank()) +\n  ggplot2::theme_bw() +\n  ggplot2::facet_wrap(~ n, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![Accuracy of the quadratic approximation. In each plot, the exact posterior distribution is plotted as solid curve, and the quadratic approximation is plotted as the dashed curve.](02-small-and-large-worlds_files/figure-html/fig-quap-compare-analytic-different-n-1.png){#fig-quap-compare-analytic-different-n width=672}\n:::\n:::\n\n:::\n:::\n\n::: my-resource\n::: my-resource-header\nTextbooks highlighting MLE for GLMs\n:::\n\nKurz refers to three textbooks that highlight the maximum likelihood\nestimate (<a class='glossary' title='Maximum likelihood estimation (MLE) is a method of estimating the parameters of an assumed probability distribution, given some observed data. This is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. (Wkipedia)'>MLE</a>) for the generalized linear model\n(<a class='glossary' title='generalized linear model (GLM) is a flexible generalization of ordinary linear regression. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value. (Wikipedia)'>GLM</a>).\n\n::: my-resource-container\n-   Agresti. (2015). Foundations of Linear and Generalized Linear\n    Models. Wiley.\n-   Dobson, A. J., & Barnett, A. G. (2018). An Introduction to\n    Generalized Linear Models (4th ed.). Chapman and Hall/CRC.\n-   Dunn, P. K., & Smyth, G. K. (2018). Generalized Linear Models With\n    Examples in R (1st ed. 2018 Edition). Springer.\n\n[@agresti2015; @dobson2018; @dunn2018]\n:::\n:::\n\n::: callout-note\n####### Maximum Likelihood Estimation (MLE)\n\n> \"The quadratic approximation, either with a uniform prior or with a\n> lot of data, is often equivalent to a\n> <a class='glossary' title='Maximum likelihood estimation (MLE) is a method of estimating the parameters of an assumed probability distribution, given some observed data. This is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. (Wkipedia)'>maximum likelihood estimate</a> (MLE) and its\n> <a class='glossary' title='The standard error (SE) of a statistic (usually an estimate of a PARAMETER) is the STANDARD DEVIATION of its sampling distribution or an estimate of that standard deviation. … The standard error is a key ingredient in producing CONFIDENCE INTERVALs. (Wikipedia)'>standard error</a>. The MLE is a very common non-Bayesian\n> parameter estimate. This correspondence between a Bayesian\n> approximation and a common non-Bayesian estimator is both a blessing\n> and a curse. It is a blessing, because it allows us to re-interpret a\n> wide range of published non-Bayesian model fits in Bayesian terms. It\n> is a curse, because maximum likelihood estimates have some curious\n> drawbacks, and the quadratic approximation can share them.\"\n> ([McElreath, 2020, p.\n> 44](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=63&annotation=KDENIT9G))\n:::\n\n### Markov Chain Monte Carlo (MCMC)\n\n#### ORIGINAL\n\n> \"There are lots of important model types, like multilevel\n> (mixed-effects) models, for which neither grid approximation nor\n> quadratic approximation is always satisfactory.\" ([McElreath, 2020, p.\n> 45](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=64&annotation=FQU3V949))\n\n> \"As a result, various counterintuitive model fitting techniques have\n> arisen. The most popular of these is Markov chain Monte Carlo (MCMC),\n> which is a family of conditioning engines capable of handling highly\n> complex models. It is fair to say that MCMC is largely responsible for\n> the insurgence of Bayesian data analysis that began in the 1990s.\n> While MCMC is older than the 1990s, affordable computer power is not,\n> so we must also thank the engineers.\" ([McElreath, 2020, p.\n> 45](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=64&annotation=K3L7AAW2))\n\n> \"The conceptual challenge with MCMC lies in its highly non-obvious\n> strategy. Instead of attempting to compute or approximate the\n> posterior distribution directly, MCMC techniques merely draw samples\n> from the posterior. You end up with a collection of parameter values,\n> and the frequencies of these values correspond to the posterior\n> plausibilities. You can then build a picture of the posterior from the\n> histogram of these samples.\" ([McElreath, 2020, p.\n> 45](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=64&annotation=637IUB2W))\n\nThe understanding of this not intuitive technique is postponed to\n@sec-chap09. What follows is just a demonstration of the technique.\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-fig-demo-mcmc-rethinking}\n: Demo of the Markov Chain Monte Carlo (MCMC) method\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/fig-demo-MCMC-rethinking_47fc05bbee91a96a922f40871942dea5'}\n\n```{.r .cell-code}\n## R code 2.8 #######################\nn_samples <- 1000\np <- rep(NA, n_samples)\np[1] <- 0.5\nW <- 6\nL <- 3\nfor (i in 2:n_samples) {\n  p_new <- rnorm(1, p[i - 1], 0.1)\n  if (p_new < 0) p_new <- abs(p_new)\n  if (p_new > 1) p_new <- 2 - p_new\n  q0 <- dbinom(W, W + L, p[i - 1])\n  q1 <- dbinom(W, W + L, p_new)\n  p[i] <- ifelse(runif(1) < q1 / q0, p_new, p[i - 1])\n}\n\n## R code 2.9 #######################\nrethinking::dens(p, xlim = c(0, 1))\ncurve(dbeta(x, W + 1, L + 1), lty = 2, add = TRUE)\n```\n\n::: {.cell-output-display}\n![Demo of the Markov Chain Monte Carlo (MCMC) method using the globe-tossing data and calculated and diplayed with the {**rethinking**} package.](02-small-and-large-worlds_files/figure-html/fig-demo-MCMC-rethinking-1.png){#fig-demo-MCMC-rethinking width=672}\n:::\n:::\n\n:::\n:::\n\nIt's weird. But it works. The <a class='glossary' title='The Metropolis Algorithm often also called Metropolis–Hastings algorithm is a Markov chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a probability distribution from which direct sampling is difficult. This sequence can be used to approximate the distribution (e.g. to generate a histogram) or to compute an integral (e.g. an expected value). (a href=“https://en.wikipedia.org/w/index.php?title=Metropolis%E2%80%93Hastings_algorithm&amp;oldid=1172902257&quot;&gt;Wikipedia)'>metropolis algorithm</a> used\nin #cnj-fig-demo-mcmc-rethinking is explained in @sec-chap09.\n\n#### TIDYVERSE\n\nThe {**brms**} package uses a version of <a class='glossary' title='Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a MARKOV CHAIN that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the chain. The more steps that are included, the more closely the distribution of the sample matches the actual desired distribution. Various algorithms exist for constructing chains. (Wikipedia)'>MCMC</a> to fit\nBayesian models. <a class='glossary' title='brms stands for Bayesina Regression Models using Stan. brms provides an interface to fit Bayesian generalized (non-)linear multivariate multilevel models using Stan. The formula syntax is very similar to that of the package lme4 to provide a familiar and simple interface for performing regression analyses. (brms website)'>brms</a> stands for Bayesian Regression\nModels using <a class='glossary' title='Stan is a state-of-the-art platform for statistical modeling and high-performance statistical computation. Stan interfaces with the most popular data analysis languages (R, Python, shell, MATLAB, Julia, Stata) and runs on all major platforms (Linux, Mac, Windows). Users specify log density functions in Stan’s probabilistic programming language and get (a) full Bayesian statistical inference with MCMC sampling (NUTS, HMC), (b) approximate Bayesian inference with variational inference (ADVI), © penalized maximum likelihood estimation with optimization (L-BFGS). Stan is named in honor of Stanislaw Ulam (1909-1984), co-inventor of the Monte Carlo method. (STAN website and R-Bloggers)'>Stan</a>.\n\nThe main goals of the Kurz' version is to highlight the {**brms**}\npackage. Therefore we will use it instead just replicating the\n{**rethinking**}-version of MCMC.\n\nIf you haven't already installed {**brms**}, you can find instructions\non how to do on\n[GitHub](https://github.com/paul-buerkner/brms#how-do-i-install-brms) or\non the [corresponding website](https://paul-buerkner.github.io/brms/).)\n\nAs an exercise we will re-fit the model with $W = 24$ and $n = 36$ of\n@cnj-quap-different-sample-size and @cnj-quap-globe-tossing.\n\n::: my-watch-out\n::: my-watch-out-header\n{**brms**} and {**rethinking**} are conflicting packages\n:::\n\n::: my-watch-out-container\n**Preventing code conflicts**\n\n{**brms**} and {**rethinking**} are packages designed for similar\npurposes and therefore overlap in some of their function names. If you\nhave loaded {**rethinking**} then you need to detach the package before\nloading {**brms**}. This is not necessary in my case, because I haven't\nused the `library()` command.\n\nTo learn more on the topic, see [this R-bloggers\npost](https://www.r-bloggers.com/2015/04/r-and-package-masking-a-real-life-example/).\n(This remark comes from [section 4.3.1 of the Kurz\nversion](https://bookdown.org/content/4857/geocentric-models.html#the-data)).\n\n**First compiling**\n\nBe patient when you render the following chunk the first time. It need\nsome time. Furthermore it results in a very long processing message\nunder the compiled chunk. Again this happens only the first time because\nthe result is stored in the \"fits\" folder which you have to create\nbefore running the chunk.\n:::\n:::\n\nTo resume the necessary preparing steps I collected all to dos in a\nprocedure callout:\n\n::: my-procedure\n::: my-procedure-header\n::: {#prp-chap02-using-brms-first-time}\n: Steps to use {**brms**} the first time\n:::\n:::\n\n::: my-procedure-container\n1.  Prevent code conflicts as explained in the previous watch-out\n    callout.\n2.  Download the {**brms**} package\n3.  Load the package with `library()` or use the package name followed\n    by a double colon immediately before the function\n4.  Create a folder in the working directory of your project. The\n    following code chunks suggest \"fits\" as folder name. If you are\n    using another name then you have to change the scripts accordingly.\n5.  Render the script and be patient especially the first time you ar\n    compiling the script.\n:::\n:::\n\n::: my-r-code\n::: my-r-code-header\n::: {#cnj-brms-clobe-tossing}\n: Demonstration of the `brm()` function of the {**brms**} package.\n:::\n:::\n\n::: my-r-code-container\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/brms-globe-tossing_2acfa7261f0fe318f8f52caadd9ebb38'}\n\n```{.r .cell-code}\nb2.1 <-\n  brms::brm(data = list(w = 24), \n      family = binomial(link = \"identity\"),\n      w | trials(36) ~ 0 + Intercept,\n      brms::prior(beta(1, 1), class = b, lb = 0, ub = 1),\n      seed = 2,\n      file = \"fits/b02.01\")\n\nprint(b2.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  Family: binomial \n#>   Links: mu = identity \n#> Formula: w | trials(36) ~ 0 + Intercept \n#>    Data: list(w = 24) (Number of observations: 1) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept     0.66      0.08     0.50     0.80 1.00     1537     1456\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\nA detailed explanation of the result is postponed to @sec-chap04. Here I\nwill just copy the notes by Kurz to get a first understanding and a\nstarting point for further exploration in the following chapters.\n\n> For now, focus on the 'Intercept' line. As we'll also learn in Chapter\n> 4, the intercept of a typical regression model with no predictors is\n> the same as its mean. In the special case of a model using the\n> binomial likelihood, the mean is the probability of a 1 in a given\n> trial, $\\theta$.\n>\n> Also, with {**brms**}, there are many ways to summarize the results of\n> a model. The `brms::posterior_summary()` function is an analogue to\n> `rethinking::precis()`. We will, however, need to use `round()` to\n> reduce the output to a reasonable number of decimal places.\n\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/print-brms-summary_e389d1e6ca6742c6a8c608eee533d544'}\n\n```{.r .cell-code}\nbrms::posterior_summary(b2.1) |>  \n  round(digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>             Estimate Est.Error  Q2.5 Q97.5\n#> b_Intercept     0.66      0.08  0.50  0.80\n#> lprior          0.00      0.00  0.00  0.00\n#> lp__           -3.98      0.75 -6.03 -3.46\n```\n\n\n:::\n:::\n\n\n> The `b_Intercept` row is the probability. Don't worry about the second\n> line, for now. We'll cover the details of {**brms}** model fitting in\n> later chapters. To finish up, why not plot the results of our model\n> and compare them with those from `rethinking::quap()`, above? (See\n> @fig-demo-MCMC-rethinking)\n\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/fig-demo-MCMC-brms_19b66f0915e11f468adb5916594b0003'}\n\n```{.r .cell-code}\nbrms::as_draws_df(b2.1) |>  \n  dplyr::mutate(n = \"n = 36\") |> \n  \n  ggplot2::ggplot(ggplot2::aes(x = b_Intercept)) +\n  ggplot2::geom_density(fill = \"gray\") +\n  ggplot2::scale_x_continuous(\"proportion water\", limits = c(0, 1)) +\n  ggplot2::theme(panel.grid = ggplot2::element_blank()) +\n  ggplot2::theme_bw() +\n  ggplot2::facet_wrap(~ n)\n```\n\n::: {.cell-output-display}\n![Demo of the Markov Chain Monte Carlo (MCMC) method using the globe-tossing data with the {**brms**} package.](02-small-and-large-worlds_files/figure-html/fig-demo-MCMC-brms-1.png){#fig-demo-MCMC-brms width=672}\n:::\n:::\n\n\n> If you're still confused, cool. This is just a preview. We'll start\n> walking through fitting models with **brms** in [Chapter\n> 4](https://bookdown.org/content/4857/geocentric-models.html#geocentric-models)\n> and we'll learn a lot about regression with the binomial likelihood in\n> [Chapter\n> 11](https://bookdown.org/content/4857/god-spiked-the-integers.html#god-spiked-the-integers).\n:::\n:::\n\n::: my-resource\n::: my-resource-header\nUsing Bayesian Regression Models using Stan {**brms**}\n:::\n\n::: my-resource-container\n-   Website: [Bayesian regression models using\n    Stan](https://paul-buerkner.github.io/brms/)\n-   Bürkner, P-C. (2017). brms: An R Package for Bayesian Multilevel\n    Models Using Stan. Journal of Statistical Software, 80, 1--28.\n    [https://doi.org/10.18637/jss.v080.i01](Ellis,%20A.%20(2021).%20Learn%20multilevel%20models:%20An%20Introduction%20to%20brms.%20https://awellis.github.io/learnmultilevelmodels/walkthrough-brms.html/)\n-   Bürkner, P-C. (2018). The R Journal: Advanced Bayesian Multilevel\n    Modeling with the R Package brms. The R Journal, 10(1), 395--411.\n    <https://doi.org/10.32614/RJ-2018-017>\n-   van de Schoot, R. (2019). BRMS. Series of tutorials how to run the\n    brms package. Rens van de Schoot Webpage.\n    <https://www.rensvandeschoot.com/tutorials/brms/>\n-   Nalborczyk, L. (2017). Appendix: An introduction to Bayesian\n    multilevel models using brms \\| Understanding rumination as a form\n    of inner speech. Retrieved October 31, 2023, from\n    <https://lnalborczyk.github.io/phd_thesis/appendix-brms.html>\n-   Nalborczyk, L., Batailler, C., Loevenbruck, H., Vilain, A., &\n    Bürkner, P.-C. (2017). An Introduction to Bayesian Multilevel Models\n    Using brms: A Case Study of Gender Effects on Vowel Variability in\n    Standard Indonesian. <https://doi.org/10.17605/OSF.IO/DPZCB>\n-   Ellis, A. (2021). Learn multilevel models: An Introduction to brms.\n    <https://awellis.github.io/learnmultilevelmodels/walkthrough-brms.html/>\n:::\n:::\n\n::: my-important\n::: my-important-header\nBayesian Inference: Some Lessons to Draw\n:::\n\n::: my-important-container\nThe following list summarizes differences between Bayesian and\nNon-Bayesian inference (<a class='glossary' title='Also known as frequentist interference, is a type of statistical approach where conclusions are made based on the frequency of an event. This statistical approach determines the probability of a long-term experiment, meaning the experiment is repeated under the same set of conditions to obtain an outcome. In frequentist statistics the population parameters are fixed, but unknown, and the data observed in experiments are random. (deepai.org)'>frequentist statistics</a> or\n\"Frequentism\"):\n\n1.  **No minimum sampling size**: The minimum sampling size in Bayesian\n    inference is one. You are going to update each data point at its\n    time. For instance you got an estimate every time when you toss the\n    globe and the estimate is updated. --- Well, the sample size of one\n    is not very informative but that is the power of Bayesian inference\n    in not getting over confident. It is always accurately representing\n    the relative confidence of plausability we should assign to each of\n    the possible proportions.\n2.  **Shape embodies sample size**: The shape of the posterior\n    distribution embodies all the information that the sample has about\n    the process of the proportions. Therefore you do not need to go back\n    to the original dataset for new observations. Just take the\n    posterior distribution and update it by multiplying the number of\n    ways the new data could produce.\n3.  **No point estimates**: The estimate is the whole distribution. It\n    may be fine for communication purposes to talk about some summary\n    points of the distribution like the mode and mean. But neither of\n    these points is special as a point of estimate. When we do\n    calculations we draw predictions from the whole distribution, never\n    just from a point of it.\n4.  **No one true interval**: Intervals are not important in Bayesian\n    inference. They are merely summaries of the shape of the\n    distribution. There is nothing special in any of these intervals\n    because the endpoints of the intervals are not special. Nothing\n    happens of the endpoints of the intervals because the interval is\n    arbitrary. (The 95% in Non-Bayesian inference is essentially a\n    dogma, a superstition. Even in Non-Bayesian statistics it is\n    conceptualized as an arbitrary interval.)\n:::\n:::\n\n\n\n## Practice\n\n### Easy\n\n#### 2E1 {#sec-chap02-e1}\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\nExercise 2E1: Which of the expressions below correspond to the statement: *the probability of rain on Monday*?\n:::\n::::{.my-exercise-container}\n\n(1) $Pr(rain)$\n(2) $Pr(rain \\mid Monday)$\n(3) $Pr(Monday \\mid rain)$\n(4) $\\frac{Pr(rain, Monday)}{Pr(Monday)}$\n\n***\n\n**My Solution**\n\n2.  and 4.\n\n::: callout-warning\nOriginally I believed that only 2. is true. But both [Erik\nKusch](https://www.erikkusch.com/courses/rethinking/chapter-02/#practice-e1)\nand [Jake\nThompson](https://sr2-solutions.wjakethompson.com/bayesian-inference#chapter-2)\nalso mentioned choice 4.\n\nThe explication by [Erik\nKusch](https://www.erikkusch.com/courses/rethinking/chapter-02/#practice-e1)\nconvinced me:\n\n(4) $\\frac{Pr(rain, Monday)}{Pr(Monday)}$ - reads as \"the probability\n    that is raining and a Monday, divided by the probability of it being\n    a Monday\" which is the same as \"the probability of rain, given that\n    it is Monday. This is simply just the Bayes theorem in action.\"\n:::\n\n\n::::\n:::::\n\n\n#### 2E2 {#sec-chap02-e2}\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\nExercise 2E2: Which of the following statements corresponds to the expression: $Pr(Monday|rain)$?\n:::\n::::{.my-exercise-container}\n\n(1) The probability of rain on Monday.\n(2) The probability of rain, given that it is Monday.\n(3) The probability that it is Monday, given that it is raining.\n(4) The probability that it is Monday and that it is raining.\n\n\n***\n\n**My Solution**\n\n3. \n\n::::\n:::::\n\n\n#### 2E3 {#sec-chap02-e3}\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\nExercise 2E3: Which of the expressions below correspond to the statement: *the probability that it is Monday, given that it is raining*?\n:::\n::::{.my-exercise-container}\n\n$(1) Pr(Monday|rain)$ $(2) Pr(rain|Monday)$\n$(3) Pr(rain|Monday) Pr(Monday)$\n$(4) \\frac{Pr(rain|Monday) Pr(Monday)} {Pr(rain)}$\n$(5) \\frac{Pr(Monday|rain) Pr(rain)} {Pr(Monday)}$\n\n***\n\n**My Solution**\n\n1. and 4. \n\n::: callout-warning\nAgain I did not see/know that 4. the <a class='glossary' title='This is the theorem that gives Bayesian data analysis its name. But the theorem itself is a trivial implication of probability theory. The mathematical definition of the posterior distribution arises from Bayes’ Theorem. The key lesson is that the posterior is proportional to the product of the prior and the probability of the data. (Chap.2)'>bayes’ theorem</a> is\nalso a correct solution. Again\n[Kusch](https://www.erikkusch.com/courses/rethinking/chapter-02/#practice-e3)\nand Thompson had 1. and 4. as solution. This time [Jake\nThompson](https://sr2-solutions.wjakethompson.com/bayesian-inference#chapter-2)\nhas a short explanation:\n\n> Answer option (1) is the standard notation for the conditional\n> probability. Answer option (4) is equivalent, as this is [Bayes'\n> Theorem](https://en.wikipedia.org/wiki/Bayes'_theorem#Statement_of_theorem).\n:::\n\n::::\n:::::\n\n#### 2E4 {#sec-chap02-e4}\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\nExercise 2E4: Does probability exist?\n:::\n::::{.my-exercise-container}\n\nThe Bayesian statistician Bruno de Finetti (1906--1985) began his 1973\nbook on probability theory with the declaration: \"PROBABILITY DOES NOT\nEXIST.\" The capitals appeared in the original, so I imagine de Finetti\nwanted us to shout this statement. What he meant is that probability is\na device for describing uncertainty from the perspective of an observer\nwith limited knowledge; it has no objective reality. Discuss the globe\ntossing example from the chapter, in light of this statement. What does\nit mean to say \"the probability of water is 0.7\"?\n\nSee [@finetti2017; @galavotti2008]\n\n***\n\n**My Solution**\n\nAs an observer we collected evidence (tossing the globe and collecting how many often our index finger hold the globe over water) that \"the probability of water is 0.7\". But the word \"probability\" is only necessary as we have only limited knowledge. Beings with unlimited knowledge (e.g., \"God\") would say \"the earth has a proportion of 0.71:0.29 of water to land\". The word \"probability\" would not be necessary.\n\n::: {.callout-warning}\nThe answers to this question differ widely. One answer I like especially is from [Jake Thompson](https://sr2-solutions.wjakethompson.com/bayesian-inference#chapter-2): \n\n> The idea is that probability is only a subjective perception of the likelihood that something will happen. In the globe tossing example, the result will always be either “land” or “water” (i.e., 0 or 1). When we toss the globe, we don’t know what the result will be, but we know it will always be “land” or “water.” To express our uncertainty in the outcome, we use probability. Because we know that water is more likely than land, we may say that the probability of “water” is 0.7; however, we’ll never actually observe a result of 0.7 waters, or observe any probability. We will only ever observe the two results of “land” and “water.”\n\n:::\n\n\n::::\n:::::\n\n\n### Middle\n\n#### 2M1 {#sec-chap02-m1}\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\nExercise 2M1: Globe tossing model -- Compute and plot the grid approximate posterior distribution for each of the following sets of observations\n:::\n::::{.my-exercise-container}\n\nIn each case, assume a uniform prior for $p$.\n\n(1) $W, W, W$ \n(2) $W, W, W, L$ \n(3) $L, W, W, L, W, W, W$\n\n***\n\n**My Solution**\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-2M1}\n: Compute and plot the grid approximate posterior distribution\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/fig-exercise-2m1_9a2846d3f32d248dcaade7c49556ba8a'}\n\n```{.r .cell-code}\n# prepare the plot by producing the data \ndf_2m1 <- tibble::tibble(grid_2m1 = \n                 seq(from = 0, to = 1, length.out = 20)) |> \n  dplyr::mutate(prior = 1,\n                likelihood_1 = dbinom(3, size = 3, prob = grid_2m1),\n                likelihood_2 = dbinom(3, size = 4, prob = grid_2m1),\n                likelihood_3 = dbinom(5, size = 7, prob = grid_2m1),\n                posterior_1 = likelihood_1 * \n                  prior / sum(likelihood_1 * prior),\n                posterior_2 = likelihood_2 * \n                  prior / sum(likelihood_2 * prior),\n                posterior_3 = likelihood_3 * \n                  prior / sum(likelihood_3 * prior))\n\n  \n  # plot the data\n  ggplot2::ggplot(data = df_2m1) + \n    ggplot2::geom_line(ggplot2::aes(x = grid_2m1, y = posterior_1, \n                                linetype = \"(1) W, W, W\")) +\n    ggplot2::geom_line(ggplot2::aes(x = grid_2m1, y = posterior_2,\n                                linetype = \"(2) W, W, W, L\")) +\n    ggplot2::geom_line(ggplot2::aes(x = grid_2m1, y = posterior_3,\n              linetype = \"(3) L, W, W, L, W, W, W\")) +\n    ggplot2::scale_linetype_manual(\"Data:\", \n                        values = c(\"solid\", \"dashed\", \"dotted\")) +\n  ggplot2::labs(title = \n  \"Exercise 2M1: Grid approximation with uniform prior = 1, 20 points\",\n                x = \"probability of water\",\n                y = \"posterior probability\") +\n  ggplot2::theme(panel.grid = ggplot2::element_blank()) +\n  ggplot2::theme_bw() \n```\n\n::: {.cell-output-display}\n![Globe tossing model -- Compute and plot the grid approximate posterior distribution with a uniform prior](02-small-and-large-worlds_files/figure-html/fig-exercise-2m1-1.png){#fig-exercise-2m1 width=672}\n:::\n:::\n\n\n::::\n:::::\n\nMy original code was with 100 points and a uniform prior of 0.5. The shape of the curves conformed to both [Kusch](https://www.erikkusch.com/courses/rethinking/chapter-02/#practice-m1--m2) and [Thompson](https://sr2-solutions.wjakethompson.com/bayesian-inference#chapter-2), but the values of the y-axis were different. For a better comparison I chanced my code to the prior 0f 1 with 20 points.\n\nBTW: The code from Thompson is very advanced. Obviously he is very experienced with R.\n\n:::\n\n\n::::\n\n\n#### 2M2 {#sec-chap02-m2}\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\nExercise 2M2: Now assume a prior for $p$ that is equal to zero when $p < 0.5$ and is a positive constant when $p ≥ 0.5$. \n:::\n::::{.my-exercise-container}\n\nAgain compute and plot the grid approximate posterior distribution for each of the sets of the observations\n\n(1) $W, W, W$ \n(2) $W, W, W, L$ \n(3) $L, W, W, L, W, W, W$\n\n***\n\n**My Solution**\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-2M1}\n: Compute and plot the grid approximate posterior distribution\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/fig-exercise-2m2_891975fa8fa2d18885d946202459ff73'}\n\n```{.r .cell-code}\n# prepare the plot by producing the data \ndf_2m2 <- tibble::tibble(grid_2m2 = \n                 seq(from = 0, to = 1, length.out = 20)) |> \n  dplyr::mutate(prior = ifelse(grid_2m2 < 0.5, 0, 1)) |> \n  dplyr::mutate(likelihood_1 = dbinom(3, size = 3, prob = grid_2m2),\n                likelihood_2 = dbinom(3, size = 4, prob = grid_2m2),\n                likelihood_3 = dbinom(5, size = 7, prob = grid_2m2),\n                posterior_1 = likelihood_1 * \n                  prior / sum(likelihood_1 * prior),\n                posterior_2 = likelihood_2 * \n                  prior / sum(likelihood_2 * prior),\n                posterior_3 = likelihood_3 * \n                  prior / sum(likelihood_3 * prior))\n\n  \n  # plot the data\n  ggplot2::ggplot(data = df_2m2) + \n    ggplot2::geom_line(ggplot2::aes(x = grid_2m2, y = posterior_1, \n                                linetype = \"(1) W, W, W\")) +\n    ggplot2::geom_line(ggplot2::aes(x = grid_2m2, y = posterior_2,\n                                linetype = \"(2) W, W, W, L\")) +\n    ggplot2::geom_line(ggplot2::aes(x = grid_2m2, y = posterior_3,\n              linetype = \"(3) L, W, W, L, W, W, W\")) +\n    ggplot2::scale_linetype_manual(\"Data:\", \n                        values = c(\"solid\", \"dashed\", \"dotted\")) +\n  ggplot2::labs(title = \n  \"Exercise 2M2: Grid approximation with 20 points a prior for p that is \nequal to zero when p < 0.5 and is a positive constant when p ≥ 0.5\",\n                x = \"probability of water\",\n                y = \"posterior probability\") +\n  ggplot2::theme(panel.grid = ggplot2::element_blank()) +\n  ggplot2::theme_bw() \n```\n\n::: {.cell-output-display}\n![Globe tossing model -- Compute and plot the grid approximate posterior distribution with a prior for p that is equal to zero when p < 0.5 and is a positive constant when p ≥ 0.5](02-small-and-large-worlds_files/figure-html/fig-exercise-2m2-1.png){#fig-exercise-2m2 width=672}\n:::\n:::\n\n\n::::\n:::::\n\n\n::::\n\n#### 2M3 {#sec-chap02-m3}\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\nExercise 2M3: Suppose there are two globes, one for Earth and one for Mars.\n:::\n::::{.my-exercise-container}\n\nThe Earth globe is $70\\%$ covered in water. The Mars globe is $100\\%$ land. Further suppose that one of these globes—you don’t know which—was tossed in the air and produced a “land” observation. Assume that each globe was equally likely to be tossed. Show that the posterior probability that the globe was the Earth, conditional on seeing “land” ($Pr(Earth \\mid land)$), is $0.23$.\n\n***\n\n**My Solution**\n\nWhat do we know?\n\n$$\n\\begin{align*}\nPr(land \\mid Earth) = 1 - 0.7 = 0.3 \\\\\nPr(land \\mid Mars) = 1 \\\\\nPr(Earth) = Pr(Mars) = 0.5 \\\\\n\\text{Bayes’ theorem} = Pr(Earth \\mid land) = \\frac{Pr(land\\mid Earth) Pr(Earth)}{Pr(land)} \\\\\nPr(land) \\text{is the only value that is missing, can be caluclated by:} \\\\\nPr(land) = Pr(land \\mid Earth)Pr(Earth) + Pr(land \\mid Mars)Pr(Mars) \\\\\nPr(land) = 0.3 \\times 0.5 + 1 \\times 0.5 = 0.15 + 0.5 = 0.65 \\\\\n\\text{now we can plug in all values in Bayes’ theorem} \\\\\nPr(Earth \\mid land) = \\frac{0.3 \\times 0.5 }{0.65} = 0.2307692\n\\end{align*}\n$$ {#eq-2m3}\n\n::: {.callout-warning}\nI have to confess that I didn't know the solution. I had to cheat and look at the solution of [Aidan Marshall](https://github.com/AidanMar/Statistical-Rethinking-Solutions/blob/main/Chapter%202%20-%20SMALL%20WORLDS%20AND%20LARGE%20WORLDS.ipynb), [Taras Svirskyi](https://github.com/jffist/statistical-rethinking-solutions/blob/master/ch02_hw.R), [Jake Thompson](https://sr2-solutions.wjakethompson.com/bayesian-inference#chapter-2) and [Erik Kusch](https://www.erikkusch.com/courses/rethinking/chapter-02/#practice-m3). All of them knew the correct answer! \n\nMy answers follows the solution of Erik Kusch. By this occasion I detected an error in my writing down the derivation of the Bayes' theorem (@thm-bayes-theorem). No surprise that I failed this exercise and all the others with the Bayes’ theorem as part of the solution (\\nameref{sec-chap02-e1} and \\nameref{sec-chap02-e3}).\n:::\n\n\n\n\n\n\n\n:::\n:::\n\n\n::::\n\n\n\n#### 2M4 {#sec-chap02-m4}\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\nExercise 2M4: Suppose you have a deck with only three cards {#exercise-2m4}\n:::\n::::{.my-exercise-container}\n\nEach card has two sides, and each side is either black or white. One\ncard has two black sides. The second card has one black and one white\nside. The third card has two white sides.\n\nNow suppose all three cards are placed in a bag and shuffled. Someone\nreaches into the bag and pulls out a card and places it flat on a table.\nA black side is shown facing up, but you don't know the color of the\nside facing down.\n\nShow that the probability that the other side is also black is 2/3. Use\nthe counting method from section \\nameref{sec-chap02-forking-data} to\napproach this problem. This means counting up the ways that each card\ncould produce the observed data (a black side facing up on the table).\n\n***\n\n**My Solution**\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-2M4}\n: Deck with three cards B/B, B/W, and W/W\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/exercise-2m4_81a429a2208a6a66427f6fc97ea17d1f'}\n\n```{.r .cell-code}\nways_card_bb <- 2\nways_card_bw <- 1\nways_card_ww <- 0\n\nlikelihood <- c(ways_card_bb, ways_card_bw, ways_card_ww)\nprior <- c(1, 1, 1)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nposterior[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.6666667\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n:::\n\n::: {.callout-warning}\nAgain it seems that the way I tried to solve the exercise was wrong even if I got the correct result 1/3. I thought it would be enough to count the whole cards as ways and not the sides of the cards. Why? I thought that every card has just one side to make it visible for the observer. \n\nSo my solution counted the sample space as {B/B, B/W, W/W} and ways to produce a cards with a black side in front as {B/B, B/W}. $\\frac{Ways}{Sample Space} = \\frac{2}{3}$\n\nBut with the next example my method yields $\\frac{3}{4}$ whereas the solution of other people resulted in $0.8$.\n:::\n\n\n:::\n\n#### 2M5 {#sec-chap02-m5}\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\nExercise 2M5: Deck with four cards B/B, B/W, W/W, and B/B\n:::\n::::{.my-exercise-container}\n\nAgain suppose a card is drawn from the bag and a black side appears face up. Again calculate the probability that the other side is black.\n\n***\n\n**My Solution**\n\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-2M5}\n: Deck with four cards B/B, B/W, W/W, and B/B\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/exercise-2m5_4418f1c627fe9b036097440ce7ad7f33'}\n\n```{.r .cell-code}\nways_card_bb <- 2\nways_card_bw <- 1\nways_card_ww <- 0\n\nlikelihood <- c(ways_card_bb, ways_card_bw, \n                ways_card_ww, ways_card_bb)\nprior <- c(1, 1, 1, 1)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nposterior[1] + posterior[4]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.8\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n\n::::\n:::::\n\n#### 2M6 {#sec-chap02-m6}\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\nExercise 2M6: Card deck where cards with a black side are heavier\n:::\n::::{.my-exercise-container}\n\nImagine that black ink is heavy, and so cards with black sides are heavier than cards with white sides. As a result, it’s less likely that a card with black sides is pulled from the bag. \n\nSo again assume there are three cards: B/B, B/W, and W/W. After experimenting a number of times, you conclude that for every way to pull the B/B card from the bag, there are 2 ways to pull the B/W card and 3 ways to pull the W/W card. \n\nAgain suppose that a card is pulled and a black side appears face up. Show that the probability the other side is black is now 0.5. Use the counting method, as before.\n\n***\n\n**My Solution**\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-2M6}\n: Deck with three cards with different probability to draw\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/exercise-2m6_8dca4c73fbace730c5b8a7448fc9eda8'}\n\n```{.r .cell-code}\nways_card_bb <- 2\nways_card_bw <- 1\nways_card_ww <- 0\n\nlikelihood <- c(ways_card_bb, ways_card_bw, ways_card_ww)\nprior <- c(1, 2, 3)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nposterior[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.5\n```\n\n\n:::\n:::\n\n\n\n:::\n:::\n\n:::\n::::\n\n#### 2M7 {#sec-chap02-m7}\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\nExercise 2M7: Card showing a black side face up before drawing a second card\n:::\n::::{.my-exercise-container}\n\nAssume again the original card problem, with a single card showing a black side face up. Before looking at the other side, we draw another card from the bag and lay it face up on the table. The face that is shown on the new card is white. Show that the probability that the first card, the one showing a black side, has black on its other side is now 0.75. Use the counting method, if you can. \n\nHint: Treat this like the sequence of globe tosses, counting all the ways to see each observation, for each possible first card.\n\n***\n\n**My Solution**\n\nWe have in the original card problem three cards: B/B, B/W, and W/W. Two cards show B and W. The probability that the first card is B/B is 0.75. \n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-2M7}\n: Card showing a black side face up before drawing a second card\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/exercise-2m7_009115e3635556090fa2d5c43ee77100'}\n\n```{.r .cell-code}\n# 2 choices for first card, with 3 options for second card: 2 W/W + 1 W/B\nways_card_bb <- 2 * 3\nways_card_bw <- 1 * 2\nways_card_ww <- 0\n\nlikelihood <- c(ways_card_bb, ways_card_bw, ways_card_ww)\nprior <- c(1, 1, 1)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nposterior[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.75\n```\n\n\n:::\n:::\n\n\n:::\n\n\n:::\n:::\n\n::: {.callout-note}\n[Aidan Marshall](https://github.com/AidanMar/Statistical-Rethinking-Solutions/blob/main/Chapter%202%20-%20SMALL%20WORLDS%20AND%20LARGE%20WORLDS.ipynb) has interesting different solutions of this and the previous exercises using the Bayes’ rule.\n:::\n\n\n::::\n\n\n\n\n### Hard\n\n#### 2H1 {#sec-chap02-h1}\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\nExercise 2H1: Two species of panda bear differ in their family size\n:::\n::::{.my-exercise-container}\n\nSuppose there are two species of panda bear. Both are equally common in the wild and live in the same places. They look exactly alike and eat the same food, and there is yet no genetic assay capable of telling them apart. \n\nThey differ however in their family sizes. Species A gives birth to twins 10% of the time, otherwise birthing a single infant. Species B births twins 20% of the time, otherwise birthing single infants. Assume these numbers are known with certainty, from many years of field research. \n\nNow suppose you are managing a captive panda breeding program. You have a new female panda of unknown species, and she has just given birth to twins. What is the probability that her next birth will also be twins?\n\n***\n\n**My Solution**\n\n\n\n$$\n\\begin{align*}\nPr(A) = 0.5 \\\\\nPr(B) = 0.5 \\\\\nPr(twins \\mid A) = 0.1 \\\\\nPr(twins \\mid B) = 0.2 \\\\\nPr(twins) = \\text{average of twins} \\\\\nPr(twins) = (0.1 * 0.5) + (0.2 * 0.5) \\text = (0.1 + 0.2) / 2 = 0.15 \\\\\n\\text{} \\\\\nPr(A \\mid twins) = \\frac{Pr(twins \\mid A)Pr(A)}{Pr(twins)} \\\\\nPr(A \\mid twins) = \\frac{0.1 \\times 0.5}{0.15} = 0.3333 \\\\\n\\text{} \\\\\nPr(B \\mid twins) = \\frac{Pr(twins \\mid B)Pr(B)}{Pr(twins)} \\\\\nPr(B \\mid twins) = \\frac{0.2 \\times 0.5}{0.15} = 0.6667 \\\\\n\\text{} \\\\\nPr(twins) = Pr(A \\mid twins)Pr(twins \\mid A) + Pr(B \\mid twins)Pr(twins \\mid B)\\\\\nPr(twins) = \\frac{1}{3}  \\times 0.1 + \\frac{2}{3} \\times 0.2 = \\frac{0.1}{3} + \\frac{0.4}{3} = \\frac{0.5}{3} = \\frac{1}{6} \\text{  or}\\\\    \nPr(twins) = (0.3333 * 0.1) + (0.6667 * 0.2) = 0.16667\n\\end{align*}\n$$ {#eq-2h1}\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-2H1}\n: Probability that the next birth are twins again\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/exercise-2H1_39bcfd9dd54918ce7adc890db2eadf42'}\n\n```{.r .cell-code}\n## species A and B are each likely\nprob_A <- 0.5\nprob_B <- 0.5\nprior <- c(prob_A, prob_B)\n\n## the likelihood to give birth twins for species A and B\ntwins_likelihood_A <-  0.1\ntwins_likelihood_B <-  0.2\nlikelihood <- c(twins_likelihood_A, twins_likelihood_B)\n\n## posterior probability of each species giving birth twins\nposterior <-  prior * likelihood\nposterior <- posterior / sum(posterior)\n\nglue::glue(\n  'Posterior probability of giving birth to twins: \n  for species A = {round(posterior[1], 4)},\n  for species B = {round(posterior[2], 4)}.')\n\nglue::glue(\n  'Probability that the next birth are twins:\n  Summarize the probability of each panda times \n  their appriopriate probability to give birth to twins = {(round(sum(posterior * likelihood), 4))}'\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Posterior probability of giving birth to twins: \n#> for species A = 0.3333,\n#> for species B = 0.6667.\n#> Probability that the next birth are twins:\n#> Summarize the probability of each panda times \n#> their appriopriate probability to give birth to twins = 0.1667\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n::::\n:::::\n\n\n\n#### 2H2 {#sec-chap02-h2}\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\nExercise 2H2: Probability for Panda A after first birth of twins to birth twins again with its second birth\n:::\n::::{.my-exercise-container}\n\nRecall all the facts from the problem above. Now compute the probability that the panda we have is from species A, assuming we have observed only the first birth and that it was twins.\n\n***\n\n**My Solution**\n\n$$\n\\begin{align*}\nPr(twins \\mid A) = Pr(A \\mid twins) \\times Pr(A \\mid twins) \\\\\nPr(twins \\mid A) = \\frac{1}{3} \\times \\frac{1}{3} = \\frac{1}{9} = 0.1111\n\\end{align*}\n$$ {#eq-2h2}\n\n***\n\nAll the other people have just taken the already computed for species A the probability of the first generation to give birth twins (= 0.3333). But I understood the question in the different: We should calculate the probability of twins for the second birth given that we know the panda is from species A and that the first birth were twins.\n::::\n:::::\n\n\n\n#### 2H3 {#sec-chap02-h3}\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\nExercise 2H3: Second birth is a single infant.\n:::\n::::{.my-exercise-container}\n\nContinuing on from the previous problem, suppose the same panda mother has a second birth and that it is not twins, but a single infant. Compute the posterior probability that this panda is species A.\n\n***\n\n**My Solution**\n\n$$\n\\begin{align*}\nPr(single \\mid A) = 1 - Pr(twins \\mid A) = 1 - 0.1 = 0.9 \\\\\nPr(single \\mid B) = 1 - Pr(twins \\mid B) = 1 - 0.2 = 0.8 \\\\\nPr(A) = \\frac{1}{3} \\text{ = first twin birth for A} \\\\\nPr(B) = \\frac{2}{3} \\text{ = first twin birth for B} \\\\\n\\text{second birth is singelton} \\\\\nPr(single) = Pr(single \\mid A)Pr(A)\n+ Pr(single \\mid B)Pr(B)\\\\\nPr(single) = (0.9 \\times \\frac{1}{3}) + (0.8 \\times \\frac{2}{3}) = \\frac{0.9}{3} + \\frac{1.6}{3} = \\frac{2.5}{3} = \\frac{5}{6} \\\\\n\\\\\nPr(A \\mid single) = \\frac{Pr(single \\mid A)Pr(A)}{Pr(single)} \\\\\nPr(A \\mid single) = \\frac{0.9 \\times \\frac{1}{3}}{\\frac{5}{6}} = \\frac{\\frac{0.9}{3}}{\\frac{5}{6}} = \\frac{5.4}{15} = 0.36\n\\end{align*}\n$$ {#eq-2h3}\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-2H3}\n: Probability of {twins, single} for species A\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/exercise-2H3_d381c0205678049447720e1ea58ccfa9'}\n\n```{.r .cell-code}\n## the likelihood to give birth twins for species A and B\ntwins_likelihood_A <-  0.1\ntwins_likelihood_B <-  0.2\n\n## the likelihood to single birth for species A and B\nsingle_likelihood_A <-  1 - twins_likelihood_A\nsingle_likelihood_B <-  1 - twins_likelihood_B\n\n\n# likelihood for each species is Pr(twins) * Pr(single)\nlikelihood_A <- twins_likelihood_A * single_likelihood_A\nlikelihood_B <- twins_likelihood_B * single_likelihood_B\n\n# posterior probabilities\nlikelihood <- c(likelihood_A, likelihood_B)\nprior <- c(1, 1)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\n# print result\nglue::glue('Posterior probability for single\nin second birth for species A = {posterior[1]}.')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Posterior probability for single\n#> in second birth for species A = 0.36.\n```\n\n\n:::\n:::\n\n\nI had it wrong. Instead of multiplying $Pr(single \\mid species)$ with the previous calculation of  $Pr(twins \\mid species)$ I computed the $Pr(species \\mid single)$ following the pattern of @eq-2h1.\n::::\n:::::\n\n\n\n::::\n:::::\n\n#### 2H4 {#sec-chap02-h4}\n\n:::::{.my-exercise}\n:::{.my-exercise-header}\nExercise 2H4: Bayesian inference with data of different type\n:::\n::::{.my-exercise-container}\n\nA common boast of Bayesian statisticians is that Bayesian inference makes it easy to use all of the data, even if the data are of different types. So suppose now that a veterinarian comes along who has a new genetic test that she claims can identify the species of our mother panda. But the test, like all tests, is imperfect. This is the information you have about the test: \n\n- The probability it correctly identifies a species A panda is 0.8. \n- The probability it correctly identifies a species B panda is 0.65. \n\nThe vet administers the test to your panda and tells you that the test is positive for species A. First ignore your previous information from the births and compute the posterior probability that your panda is species A. Then redo your calculation, now using the birth data as well.\n\n***\n\n**My Solution**\n\n$$\n\\begin{align*}\nPr(A) = Pr(B) = 0.50 \\\\\nPr(➕ \\mid A) = 0.80\\\\\nPr(➕ \\mid B) = 0.65\\\\\nPr(A \\mid ➕) = \\frac{Pr(➕ \\mid A)Pr(A)}{Pr(➕)} \\\\\nPr(➕) = Pr(➕ \\mid A) \\times Pr(A) + (1 - Pr(➕\\mid B) \\times Pr(B))\\\\\nPr(➕) = (0.8 \\times 0.5) + (0.35 \\times 0.5) = 0.575\\\\\nPr(A \\mid ➕) = \\frac{0.8 \\times 0.5}{0.575} = 0.696 \\\\\n\\\\\n\\\\\n\\text{include all birth data from previous exercise}\\\\\nPr(A) = 0.36 \\\\\nPr(B) = 1 - Pr(A) = 0.64 \\\\\nPr(➕) = Pr(➕ \\mid A) \\times Pr(A) + (1 - Pr(➕\\mid B) \\times Pr(B))\\\\\nPr(➕) = (0.8 \\times 0.36) + (0.35 \\times 0.64) = 0.512\\\\\nPr(A \\mid ➕) = \\frac{Pr(➕ \\mid A)Pr(A)}{Pr(➕)} \\\\\nPr(A \\mid ➕) = \\frac{0.8 \\times 0.36}{0.512} = 0.5625\n\\end{align*}\n$$ {#eq-2h4}\n\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-2h4}\n: Probability with genetic test without and with birth data\n::::::\n:::\n::::{.my-r-code-container}\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/exercise-2h4_9c9185dca00b7400cc32d926602e93b8'}\n\n```{.r .cell-code}\nlikelihood_test <- c(0.8, 0.35)\nprior <- c(1, 1)\nposterior_test <- prior * likelihood_test\nposterior_test <- posterior_test / sum(posterior_test)\n\n\n##### include all birth data from previous exercise\n\n# likelihood for each species is Pr(twins) * Pr(singleton)\nlikelihood_A <- 0.1 * (1 - 0.1)\nlikelihood_B <- 0.2 * (1 - 0.2)\n\n# compute posterior probabilities, using test result as prior\nlikelihood <- c(likelihood_A, likelihood_B)\nprior <- c(posterior_test[1], posterior_test[2])\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\n# print result\nglue::glue('Posterior probability for \npositive test result: {round(posterior_test[1], 4)}\npositive test and two births: {round(posterior[1], 4)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> Posterior probability for \n#> positive test result: 0.6957\n#> positive test and two births: 0.5625\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n::::\n:::::\n\n\n\n## Session Info\n\n\n::: {.cell hash='02-small-and-large-worlds_cache/html/session-info_89aeadf25e422faea31f1e3ac6dab1a3'}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> R version 4.3.2 (2023-10-31)\n#> Platform: x86_64-apple-darwin20 (64-bit)\n#> Running under: macOS Sonoma 14.1.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n#> \n#> locale:\n#> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n#> \n#> time zone: Europe/Vienna\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#> [1] patchwork_1.1.3     glossary_1.0.0.9000\n#> \n#> loaded via a namespace (and not attached):\n#>   [1] gridExtra_2.3        inline_0.3.19        sandwich_3.0-2      \n#>   [4] rlang_1.1.2          magrittr_2.0.3       multcomp_1.4-25     \n#>   [7] matrixStats_1.1.0    compiler_4.3.2       loo_2.6.0           \n#>  [10] callr_3.7.3          vctrs_0.6.4          reshape2_1.4.4      \n#>  [13] stringr_1.5.1        crayon_1.5.2         pkgconfig_2.0.3     \n#>  [16] shape_1.4.6          fastmap_1.1.1        backports_1.4.1     \n#>  [19] ellipsis_0.3.2       labeling_0.4.3       utf8_1.2.4          \n#>  [22] threejs_0.3.3        cmdstanr_0.5.3       promises_1.2.1      \n#>  [25] rmarkdown_2.25       markdown_1.11        ps_1.7.5            \n#>  [28] purrr_1.0.2          xfun_0.41            jsonlite_1.8.7      \n#>  [31] later_1.3.1          prettyunits_1.2.0    parallel_4.3.2      \n#>  [34] R6_2.5.1             dygraphs_1.1.1.6     StanHeaders_2.26.28 \n#>  [37] stringi_1.8.1        lubridate_1.9.3      estimability_1.4.1  \n#>  [40] Rcpp_1.0.11          rstan_2.32.3         knitr_1.45          \n#>  [43] zoo_1.8-12           base64enc_0.1-3      bayesplot_1.10.0    \n#>  [46] timechange_0.2.0     splines_4.3.2        httpuv_1.6.12       \n#>  [49] Matrix_1.6-3         igraph_1.5.1         tidyselect_1.2.0    \n#>  [52] rstudioapi_0.15.0    abind_1.4-5          yaml_2.3.7          \n#>  [55] codetools_0.2-19     rethinking_2.40      miniUI_0.1.1.1      \n#>  [58] processx_3.8.2       curl_5.1.0           pkgbuild_1.4.2      \n#>  [61] lattice_0.22-5       tibble_3.2.1         plyr_1.8.9          \n#>  [64] shiny_1.8.0          withr_2.5.2          bridgesampling_1.1-2\n#>  [67] posterior_1.5.0      coda_0.19-4          evaluate_0.23       \n#>  [70] survival_3.5-7       RcppParallel_5.1.7   xts_0.13.1          \n#>  [73] xml2_1.3.5           pillar_1.9.0         tensorA_0.36.2      \n#>  [76] checkmate_2.3.0      DT_0.30              stats4_4.3.2        \n#>  [79] shinyjs_2.1.0        distributional_0.3.2 generics_0.1.3      \n#>  [82] ggplot2_3.4.4        rstantools_2.3.1.1   munsell_0.5.0       \n#>  [85] commonmark_1.9.0     scales_1.2.1         gtools_3.9.4        \n#>  [88] xtable_1.8-4         glue_1.6.2           emmeans_1.8.9       \n#>  [91] tools_4.3.2          shinystan_2.6.0      colourpicker_1.3.0  \n#>  [94] forcats_1.0.0        mvtnorm_1.2-3        grid_4.3.2          \n#>  [97] tidyr_1.3.0          QuickJSR_1.0.7       crosstalk_1.2.0     \n#> [100] colorspace_2.1-0     nlme_3.1-163         cli_3.6.1           \n#> [103] fansi_1.0.5          Brobdingnag_1.2-9    dplyr_1.1.4         \n#> [106] V8_4.4.0             rversions_2.1.2      gtable_0.3.4        \n#> [109] digest_0.6.33        TH.data_1.1-2        brms_2.20.4         \n#> [112] htmlwidgets_1.6.2    farver_2.1.1         htmltools_0.5.7     \n#> [115] lifecycle_1.0.4      mime_0.12            shinythemes_1.2.0   \n#> [118] MASS_7.3-60\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
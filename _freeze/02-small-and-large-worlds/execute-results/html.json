{
  "hash": "177ef26b7e61c931e9b595ffe2a3c582",
  "result": {
    "markdown": "# Small and Large Worlds {#sec-small-and-large-worlds}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.3     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.2     \n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n:::\n\n\n## ORIGINAL {.unnumbered}\n\n::: my-objectives\n::: my-objectives-header\nLearning Objectives\n:::\n\n::: my-objectives-container\n> \"This chapter focuses on the small world. It explains probability\n> theory in its essential form: counting the ways things can happen.\n> Bayesian inference arises automatically from this perspective. Then\n> the chapter presents the stylized components of a Bayesian statistical\n> model, a model for learning from data. Then it shows you how to\n> animate the model, to produce estimates.\n\n> All this work provides a foundation for the next chapter, in which\n> you'll learn to summarize Bayesian estimates, as well as begin to\n> consider large world obligations.\" ([McElreath, 2020, p.\n> 20](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=39&annotation=I8UG2HET))\n:::\n:::\n\nThe text references many times to the notions of small and large worlds:\n\n-   The **small world** is the self-contained logical world of the\n    model.\n-   The **large world** is the broader context in which one deploys a\n    model.\n\n## TIDYVERSE {.unnumbered}\n\nThe work by Solomon Kurz has many references to R specifics, so that\npeople new to R can follow the course. Most of these references are not\nnew to me, so I will not include them in my personal notes. There are\nalso very important references to other relevant articles I do not know.\nBut I will put these kind of references for now aside and will me mostly\nconcentrate on the replication and understanding of the code examples.\n\nOne challenge for Kurz was to replicate *all* the graphics of the\noriginal version, even if they were produced just for understanding of\nprocedures and argumentation without underlying R code. By contrast I\nwill use only those code lines that are essential to display Bayesian\nresults. Therefore I will not replicate for example the very extensive\nexplication how to produce with `tidyverse` means the graphics of the\ngarden of forking data.\n\n## The Garden of Forking Data\n\n### ORIGINAL\n\n**Bayesian inference is counting of possibilities**\n\n> \"Bayesian inference is really just counting and comparing of\n> possibilities.\" ([McElreath, 2020, p.\n> 20](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=39&annotation=JYNC25AE))\n\n> \"In order to make good inference about what actually happened, it\n> helps to consider everything that could have happened. A Bayesian\n> analysis is a garden of forking data, in which alternative sequences\n> of events are cultivated. As we learn about what did happen, some of\n> these alternative sequences are pruned. In the end, what remains is\n> only what is logically consistent with our knowledge.\" ([McElreath,\n> 2020, p. 21](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=40&annotation=Q2R6DERJ))\n\n#### Counting possibilities\n\n> \"Suppose there's a bag, and it contains four marbles. These marbles\n> come in two colors: blue and white. We know there are four marbles in\n> the bag, but we don't know how many are of each color. We do know that\n> there are five possibilities: (1) \\[⚪⚪⚪⚪\\], (2) \\[⚫⚪⚪⚪\\],\n> (3)\\[⚫⚫⚪⚪\\], (4) \\[⚫⚫⚫⚪\\], (5) \\[⚫⚫⚫⚫\\]. These are t These\n> These are the only possibilities consistent with what we know about\n> the contents of the bag. Call these five possibilities the\n> *conjectures*.\n>\n> Our goal is to figure out which of these conjectures is most\n> plausible, given some evidence about the contents of the bag. We do\n> have some evidence: A sequence of three marbles is pulled from the\n> bag, one at a time, replacing the marble each time and shaking the bag\n> before drawing another marble. The sequence that emerges is: ⚫⚪⚫,\n> in that order. These are the data.\n>\n> So now let's plant the garden and see how to use the data to infer\n> what's in the bag. Let's begin by considering just the single\n> conjecture, \\[ \\], that the bag contains one blue and three white\n> marbles.\" ([McElreath, 2020, p.\n> 21](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=40&annotation=RRD6HU9Y))\n\n|              |                        |\n|--------------|------------------------|\n| Conjecture   | Ways to produce ⚫⚪⚫ |\n| \\[⚪⚪⚪⚪\\] | 0 × 4 × 0 = 0          |\n| \\[⚫⚪⚪⚪\\] | 1 × 3 × 1 = 3          |\n| \\[⚫⚫⚪⚪\\] | 2 × 2 × 2 = 8          |\n| \\[⚫⚫⚫⚪\\] | 3 × 1 × 3 = 9          |\n| \\[⚫⚫⚫⚫\\] | 4 × 0 × 4 = 0          |\n\nI have bypassed the counting procedure related with the step-by-step\nvisualization of the garden of forking data. It is important to\nunderstand that the multiplication in the above table is still a\nsummarized counting:\n\n::: my-important\n::: my-important-header\nMultiplication is just a shortcut for counting\n:::\n\n::: my-important-container\n\"Notice that the number of ways to produce the data, for each\nconjecture, can be computed by first counting the number of paths in\neach\"ring\" of the garden and then by multiplying these counts together.\"\n([McElreath, 2020, p.\n23](zotero://select/groups/5243560/items/NFUEVASQ))\n([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=42&annotation=R9UIIW9R))\n\n\"Multiplication is just a shortcut to enumerating and counting up all of\nthe paths through the garden that could produce all the observations.\"\n([McElreath, 2020, p.\n25](zotero://select/groups/5243560/items/NFUEVASQ))\n([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=44&annotation=KUC4ZHP4))\n:::\n:::\n\nThe multiplication in the table above has to be interpreted the\nfollowing way:\n\n1.  The possibility of the conjecture that the bag contains four white\n    marbles is zero because the result shows also black marbles. This is\n    the other way around for the last conjecture of four blue/black\n    marbles.\n2.  The possibility of the conjecture that the bag contains one black\n    and three white marbles is calculated the following way: The first\n    marble of the result is black and --- according to our conjecture\n    --- there is only one way (=1) to produce this black marble. The\n    next marble we have drawn is white. This is consistent with three\n    (=3) different ways( marbles) of our conjecture. The last drawn\n    marble is again black which corresponds again with just one way\n    (possibility) following our conjecture. So we get as result of the\n    garden of forking data: `1 x 3 x 1`.\n3.  The calculation of the other conjectures follows the same pattern.\n\n#### Combining Other Information\n\n> \"We may have additional information about the relative plausibility of\n> each conjecture. This information could arise from knowledge of how\n> the contents of the bag were generated. It could also arise from\n> previous data. Whatever the source, it would help to have a way to\n> combine different sources of information to update the plausibilities.\n> Luckily there is a natural solution: Just multiply the counts.\n>\n> To grasp this solution, suppose we're willing to say each conjecture\n> is equally plausible at the start. So we just compare the counts of\n> ways in which each conjecture is compatible with the observed data.\n> This comparison suggests that \\[⚫⚫⚫⚪\\] is slightly more plausible\n> than \\[⚫⚫⚪⚪\\], and both are about three times more plausible than\n> \\[⚫⚪⚪⚪\\]. Since these are our initial counts, and we are going to\n> update them next, let's label them *prior*.\" ([McElreath, 2020, p.\n> 25](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=44&annotation=WECXDWEV))\n\n::: my-procedure\n::: my-procedure-header\nBayesian Updating Process\n:::\n\n::: my-procedure-container\nHere's how we do the updating:\n\n1.  First we count the numbers of ways each conjecture could produce the\n    new observation, ⚫.\n2.  Then we multiply each of these new counts by the prior numbers of\n    ways for each conjecture.\n:::\n:::\n\n|     |              |     |                    |     |              |     |            |     |\n|--------|--------|--------|--------|--------|--------|--------|--------|--------|\n|     |              |     |                    |     |              |     |            |     |\n|     | Conjecture   |     | Ways to produce ⚫ |     | Prior counts |     | New count  |     |\n|     | \\[⚪⚪⚪⚪\\] |     | 0                  |     | 0            |     | 0 × 0 = 0  |     |\n|     | \\[⚫⚪⚪⚪\\] |     | 1                  |     | 3            |     | 3 × 1 = 3  |     |\n|     | \\[⚫⚫⚪⚪\\] |     | 2                  |     | 8            |     | 8 × 2 = 16 |     |\n|     | \\[⚫⚫⚫⚪\\] |     | 3                  |     | 9            |     | 9 × 3 = 27 |     |\n|     | \\[⚫⚫⚫⚫\\] |     | 4                  |     | 0            |     | 0 × 4 = 0  |     |\n\n::: callout-caution\nIn the book the table header \"Ways to produce\" includes ⚪ instead of\n--- as I think is correct --- ⚫.\n:::\n\n#### From Counts to Probability\n\n::: my-important\n::: my-important-header\nPrinciple of honest ignorance (McElreath)\n:::\n\n::: my-important-container\n> \"When we don't know what caused the data, potential causes that may\n> produce the data in more ways are more plausible.\" ([McElreath, 2020,\n> p. 26](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=45&annotation=9EKVZNHR))\n:::\n:::\n\nTwo reasons for using probabilities instead of counts:\n\n1.  Only relative value matters.\n2.  Counts will fast grow very large and difficult to manipulate.\n\n$$\n\\begin{align*}\n\\text{plausibility of p after } D_{New} = \\frac{\\text{ways p can produce }D_{New} \\times \\text{prior plausibility p}}{\\text{sum of product}}\n\\end{align*}\n$$\n\n::: my-example\n::: my-example-header\nConstructing plausibilities by standardizing\n:::\n\n::: my-example-container\n| Possible composition | p    | Ways to produce data | Plausibility |\n|----------------------|------|----------------------|--------------|\n| \\[⚪⚪⚪⚪\\]         | 0    | 0                    | 0            |\n| \\[⚫⚪⚪⚪\\]         | 0.25 | 3                    | 0.15         |\n| \\[⚫⚫⚪⚪\\]         | 0.5  | 8                    | 0.40         |\n| \\[⚫⚫⚫⚪\\]         | 0.75 | 9                    | 0.45         |\n| \\[⚫⚫⚫⚫\\]         | 1    | 0                    | 0            |\n:::\n:::\n\n::: {.my-r-code}\n::: {.my-r-code-header}\n::: {#cnj-2-1}\n: Compute these plausibilities in R\n:::\n:::\n\n::: {.my-r-code-container}\n\n::: {.cell}\n\n```{.r .cell-code}\n## R code 2.1 #############\nways <- c(0, 3, 8, 9, 0)\nways / sum(ways)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.00 0.15 0.40 0.45 0.00\n```\n\n\n:::\n:::\n\n:::\n:::\n\n> \"These plausibilities are also *probabilities*---they are non-negative\n> (zero or positive) real numbers that sum to one. And all of the\n> mathematical things you can do with probabilities you can also do with\n> these values. Specifically, each piece of the calculation has a direct\n> partner in applied probability theory. These partners have stereotyped\n> names, so it's worth learning them, as you'll see them again and\n> again.\n>\n> -   A conjectured proportion of blue marbles, p, is usually called a\n>     <a class='glossary' title='Unobserved variables are usually called Parameters. (SR2, Chap.2) A parameter is an unknown numerical characteristics of a population that must be estimated. (CDS). They are also numbers that govern statistical models (stats.stackexchange)'>parameter</a> value. It's just a way of indexing\n>     possible explanations of the data.\n> -   The relative number of ways that a value p can produce the data is\n>     usually called a <a class='glossary' title='The likelihood function (often simply called the likelihood) is the joint probability (or probability density) of observed data viewed as a function of the parameters of a statistical model. (Wikipedia) It indicates how likely a particular population is to produce an observed sample. (&lt;a href=“https://www.statistics.com/glossary/likelihood-function/&gt;statistics.com) It is the probability of the data given our beliefs about the data: P(data | belief). (BF, Chap.8)'>likelihood</a>. It is derived by\n>     enumerating all the possible data sequences that could have\n>     happened and then eliminating those sequences inconsistent with\n>     the data.\n> -   The prior plausibility of any specific p is usually called the\n>     <a class='glossary' title='The Prior Probability, also called the Prior, is the assumed probability distribution before we have seen the data. (Wikipedia) It quantifies how likely our initial belief is: P(belief). (BF, Chap.8)'>prior probability</a>.\n> -   The new, updated plausibility of any specific p is usually called\n>     the <a class='glossary' title='It is the revised or updated probability of an event occurring after taking into consideration new information. (Investopedia). Posterior probability = prior probability + new evidence (called likelihood). (Statistics How To) The posterior distribution will be a distribution of Gaussian distributions. (SR, Chap.4). It quantifies exactly how much our observed data changes our beliefs: P(belief | data) (BF, Chap.8)'>posterior probability</a>.\" ([McElreath, 2020, p.\n>     27](zotero://select/groups/5243560/items/NFUEVASQ))\n>     ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=46&annotation=ZDYLMTTI))\n\n### TIDYVERSE (empty)\n\n## Building a Model\n\n### ORIGINAL\n\nWe are going to use a toy example, but it has the same structure as a\ntypical statistical analyses. The first nine samples produce the\nfollowing data:\n\n`W L W W W L W L W` (W indicates water and L indicates land.)\n\n::: my-procedure\n::: my-procedure-header\nDesigning a simple Bayesian model\n:::\n\n::: my-procedure-container\n> \"Designing a simple Bayesian model benefits from a design loop with\n> three steps.\n>\n> 1.  Data story: Motivate the model by narrating how the data might\n>     arise.\n> 2.  Update: Educate your model by feeding it the data.\n> 3.  Evaluate: All statistical models require supervision, leading to\n>     model revision.\" ([McElreath, 2020, p.\n>     28](zotero://select/groups/5243560/items/NFUEVASQ))\n>     ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=47&annotation=KFNYKUC2))\n:::\n:::\n\n#### A Data Story\n\n> \"Bayesian data analysis usually means producing a story for how the\n> data came to be. This story may be *descriptive*, specifying\n> associations that can be used to predict outcomes, given observations.\n> Or it may be *causal*, a theory of how some events produce other\n> events.\" ([McElreath, 2020, p.\n> 28](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=47&annotation=D2IR5ZX3))\n\n> \"... all data stories are complete, in the sense that they are\n> sufficient for specifying an algorithm for simulating new data.\"\n> ([McElreath, 2020, p.\n> 28](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=47&annotation=A3EQXFBY))\n\n::: my-example\n::: my-example-header\nData story for our case\n:::\n\n::: my-example-container\n> \"The data story in this case is simply a restatement of the sampling\n> process:\n>\n> (1) The true proportion of water covering the globe is $p$.\n> (2) A single toss of the globe has a probability p of producing a\n>     water ($W$) observation. It has a probability $1 − p$ of producing\n>     a land ($L$) observation.\n> (3) Each toss of the globe is independent of the others.\" ([McElreath,\n>     2020, p. 29](zotero://select/groups/5243560/items/NFUEVASQ))\n>     ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=48&annotation=YPPK73D9))\n:::\n:::\n\nData stories are important:\n\n> \"Most data stories are much more specific than are the verbal\n> hypotheses that inspire data collection. Hypotheses can be vague, such\n> as\"it's more likely to rain on warm days.\" When you are forced to\n> consider sampling and measurement and make a precise statement of how\n> temperature predicts rain, many stories and resulting models will be\n> consistent with the same vague hypothesis. Resolving that ambiguity\n> often leads to important realizations and model revisions, before any\n> model is fit to data.\" ([McElreath, 2020, p.\n> 29](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=48&annotation=852Q3WN5))\n\n#### Bayesian Updating\n\n> \"Each possible proportion may be more or less plausible, given the\n> evidence. A Bayesian model begins with one set of plausibilities\n> assigned to each of these possibilities. These are the prior\n> plausibilities. Then it updates them in light of the data, to produce\n> the posterior plausibilities. This updating process is a kind of\n> learning, called <a class='glossary' title='A Bayesian model begins with one set of plausibilities assigned to each of these possibilities. These are the prior plausibilities. Then it updates them in light of the data, to produce the posterior plausibilities. This updating process is a kind of learning. (Chap.2)'>Bayesian updating</a>.\" ([McElreath,\n> 2020, p. 29](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=48&annotation=XIYIZHVD))\n\n::: my-important\n::: my-important-header\nHow a Bayesian model learns\n:::\n\n::: my-important-container\n@fig-2-5-book-copy helps to understand the Bayesian updating process. In\n@cnj-fig-bayesian-updating we will learn how to write R code to reproduce\nthe book's figure as @fig-bayesian-update. To inspect the different\nsteps of the updating process is essential to understand Bayesian\nstatistics!\n:::\n:::\n\n![Copy of Figure 2.5: **How a Bayesian model learns**. In each plot,\nprevious plausibilities (dashed curve) are updated in light of the\nlatest observation to produce a new set of plausibilities (solid\ncurve).](img/bayesian_model_learns_step_by_step-min.png){#fig-2-5-book-copy\nfig-alt=\"Nine small diagrams to show the relationship between plausibility against proportion of water after each sample.\"}\n\n#### Evaluate\n\nKeep in mind two cautious principles:\n\n> \"First, the model's certainty is no guarantee that the model is a good\n> one.\" ([McElreath, 2020, p.\n> 31](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=50&annotation=QYALLSPY))\n> \"Second, it is important to supervise and critique your model's work.\"\n> ([McElreath, 2020, p.\n> 31](zotero://select/groups/5243560/items/NFUEVASQ))\n> ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=50&annotation=M7UKAL3D))\n\n::: my-watch-out\n::: my-watch-out-header\nTest Before You Est(imate)\n:::\n\n::: my-watch-out-container\nIn the planned third version of the book McElreath wants to include from\nthe beginning the evaluation part of the process. It is mentioned in the\nbook already in this chapter 2 but without practical implementation and\ncode examples. But we can find some remarks in his [Statistical\nRethinking Videos 2023b\n37:40](https://www.youtube.com/watch?v=R1vcdhPBlXA&list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus&index=2&t=37m40s).\n\nI will not go into details here, because my focus is on the second\nedition. But I will add as a kind of summary general advises for the\ntesting procedure:\n\n::: my-procedure\n::: my-procedure-header\nEvaluation procedure\n:::\n\n::: my-procedure-container\n1.  Code a generative simulation\n2.  Code an estimator\n3.  Test the estimator with (1)\n    -   where the answer is known\n    -   at extreme values\n    -   explore different sampling designs\n    -   develop generally an intuition for sampling and estimation\n:::\n:::\n\nHere are some references to get some ideas of the necessary R Code:\n\n::: my-resource\n::: my-resource-header\nR Code snippets for testing procedure\n:::\n\n::: my-resource-container\n-   **Simulating globe tossing**: Statistical Rethinking 2023 - Lecture\n    02, [Slide\n    54](https://speakerdeck.com/rmcelreath/statistical-rethinking-2023-lecture-02?slide=54)\n-   **Simulate the experiment arbitrary times for any particular\n    proportion**: This is a way to explore the design of an experiment\n    as well as debug the code. ([Video 2023-02\n    39:55](https://www.youtube.com/watch?v=R1vcdhPBlXA&list=PLDcUM9US4XdPz-KxHM4XHt7uUVGWWVSus&index=2&t=39m55s))\n-   **Test the simulation at extreme values**: R code snippets at [Slide\n    57](https://speakerdeck.com/rmcelreath/statistical-rethinking-2023-lecture-02?slide=57).\n:::\n:::\n:::\n:::\n\n### TIDYVERSE\n\nInstead of vectors the tidyverse approach works best with data frames\nrespectively tibbles. So let's save the globe-tossing data\n`W L W W W L W L W` into a tibble:\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-globe-tossing-data}\n: Save globe tossing data into a tibble\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(d <- tibble::tibble(toss = c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 9 × 1\n#>   toss \n#>   <chr>\n#> 1 w    \n#> 2 l    \n#> 3 w    \n#> 4 w    \n#> 5 w    \n#> 6 l    \n#> 7 w    \n#> 8 l    \n#> 9 w\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n#### A Data Story\n\n#### Bayesian Updating {#sec-expand_grid}\n\nFor the updating process we need to add to the data the cumulative\nnumber of trials, `n_trials`, and the cumulative number of successes,\n`n_successes` with `toss == \"w\"`.\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-bayesian-updating-start}\n: Bayesian updating preparation\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(\n  d <-\n  d %>% \n  dplyr::mutate(n_trials  = 1:9,\n         n_success = cumsum(toss == \"w\"))\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 9 × 3\n#>   toss  n_trials n_success\n#>   <chr>    <int>     <int>\n#> 1 w            1         1\n#> 2 l            2         1\n#> 3 w            3         2\n#> 4 w            4         3\n#> 5 w            5         4\n#> 6 l            6         4\n#> 7 w            7         5\n#> 8 l            8         5\n#> 9 w            9         6\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\nThe program code for reproducing the Figure 2.5 of the book (here in\nthis document it is @fig-2-5-book-copy) is pretty complex. I have to\ninspect the results line by line. At first I will give a short\nintroduction what each line does. In the next steps I will explain each\nstep more in detail and show the result of the corresponding lines of\ncode.\n\n::: my-watch-out\n::: my-watch-out-header\nParameter $k$ in `lag()` changed to $default$\n:::\n\n::: my-watch-out-container\nIn the following listing I had to change in the `lag()` function the\nparameter $k$ of the Kurz'sche version to $default$ as it is described\nin the corresponding [help\nfile](https://dplyr.tidyverse.org/reference/lead-lag.html). I don't\nunderstand why $k$ was used. Maybe $k$ was the name of the parameter of\na previous {**dplyr**} version?\n:::\n:::\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-fig-bayesian-update}\n: Bayesian updating: How a model learns\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-summary=\"Code: **How a Bayesian model learns**\"}\n## (0) starting with tibble from the previous two code chunks #####\nd <- tibble::tibble(toss = c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\")) |>\n    dplyr::mutate(n_trials  = 1:9, n_success = cumsum(toss == \"w\"))\n\n## (1) create tibble from all input combinations ################\nsequence_length <- 50  \nd %>%                 \n  tidyr::expand_grid(p_water = seq(from = 0, to = 1,  \n                            length.out = sequence_length)) %>%  \n    \n## (2) group data by the `p-water` parameter ####################\n  dplyr::group_by(p_water) %>% \n\n## (3) create columns filled with the value of the previous rows #####\n  dplyr::mutate(lagged_n_trials  = dplyr::lag(n_trials, default = 1), \n         lagged_n_success = dplyr::lag(n_success, default = 1)) %>% \n\n## (4) restore the original ungrouped data structure #######\n  dplyr::ungroup() %>% \n\n## (5) calculate prior and likelihood & store values ######\n  dplyr::mutate(prior      = ifelse(n_trials == 1, .5, \n                             dbinom(x    = lagged_n_success, \n                                    size = lagged_n_trials,  \n                                    prob = p_water)),        \n         likelihood = dbinom(x    = n_success,  \n                             size = n_trials,   \n                             prob = p_water),   \n         strip      = stringr::str_c(\"n = \", n_trials)) %>%  \n  \n## (6) normalize prior and likelihood ##########################\n  dplyr::group_by(n_trials) %>% \n  dplyr::mutate(prior      = prior / sum(prior),               \n         likelihood = likelihood / sum(likelihood)) %>% \n  \n  ## (7) plot the result ########################################\n  ggplot2::ggplot(ggplot2::aes(x = p_water)) + \n  ggplot2::geom_line(ggplot2::aes(y = prior), linetype = 2) +  \n  ggplot2::geom_line(ggplot2::aes(y = likelihood)) +           \n  ggplot2::scale_x_continuous(\"proportion water\",     \n                     breaks = c(0, .5, 1)) + \n  ggplot2::scale_y_continuous(\"plausibility\", breaks = NULL) +  \n  ggplot2::theme(panel.grid = ggplot2::element_blank()) +                \n  ggplot2::facet_wrap(~ strip, scales = \"free_y\") +            \n  ggplot2::theme_bw()                                  \n```\n\n::: {.cell-output-display}\n![How a Bayesian model learns: Replicating book figure 2.5](02-small-and-large-worlds_files/figure-html/fig-bayesian-update-1.png){#fig-bayesian-update width=672}\n:::\n:::\n\n\n\n\n::::\n:::::\n\n##### Annotation (1): `tidyr::expand_grid()` {#sec-annotation-1-expand-grid}\n\n`tidyr::expand_grid()` creates a tibble from all combinations of inputs.\nInput are generalized vectors in contrast to `tidyr::expand()` that\ngenerates all combination of variables as well but needs as input a\ndataset. The range between 0 and 1 is divided into 50 part and then it\ngenerates all combinations by varying all columns from left to right.\nThe first column is the slowest, the second is faster and so on.). It\ngenerates 450 data points ($50 \\times 9$ trials).\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-bayesian-update-1}\n: Bayesian Update: Create tibble with all Input combinations\n::::::\n:::\n::::{.my-r-code-container}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl <- tibble::tibble(toss = c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\")) |> \n    dplyr::mutate(n_trials  = 1:9, n_success = cumsum(toss == \"w\"))\nsequence_length <- 50\n\ntbl %>% \n  tidyr::expand_grid(p_water = seq(from = 0, to = 1, \n                            length.out = sequence_length))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 450 × 4\n#>    toss  n_trials n_success p_water\n#>    <chr>    <int>     <int>   <dbl>\n#>  1 w            1         1  0     \n#>  2 w            1         1  0.0204\n#>  3 w            1         1  0.0408\n#>  4 w            1         1  0.0612\n#>  5 w            1         1  0.0816\n#>  6 w            1         1  0.102 \n#>  7 w            1         1  0.122 \n#>  8 w            1         1  0.143 \n#>  9 w            1         1  0.163 \n#> 10 w            1         1  0.184 \n#> # ℹ 440 more rows\n```\n\n\n:::\n:::\n\n\n\n::::\n:::::\n\n\n##### Annotation (2): `dplyr::group_by()` {#sec-annotation-2-group_by}\n\nAt first I did not understand the line `group_by(p_water)`. Why has the\ndata to be grouped when every row has a different value --- as I have\nthought from a cursory inspection of the result? But it turned out that\nafter 50 records the parameter `p_water` is repeating its value.\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-bayesian-update-2}\n: Bayesian Updating: Lagged with grouping\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl1 <- tbl %>% \n  tidyr::expand_grid(p_water = seq(from = 0, to = 1, \n                            length.out = sequence_length)) %>% \n    \n  dplyr::group_by(p_water) %>% \n  dplyr::mutate(lagged_n_trials  = dplyr::lag(n_trials, default = 1),\n         lagged_n_success = dplyr::lag(n_success, default = 1)) %>% \n  dplyr::ungroup() |> \n    \n\n  # add new column ID with row numbers \n  # and relocate it to be the first column\n  dplyr::mutate(ID = dplyr::row_number()) |> \n  dplyr::relocate(ID, .before = toss) \n\n# show 2 records from different groups\ntbl1[c(1:2, 50:52, 100:102, 150:152), ] \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 11 × 7\n#>       ID toss  n_trials n_success p_water lagged_n_trials lagged_n_success\n#>    <int> <chr>    <int>     <int>   <dbl>           <int>            <int>\n#>  1     1 w            1         1  0                    1                1\n#>  2     2 w            1         1  0.0204               1                1\n#>  3    50 w            1         1  1                    1                1\n#>  4    51 l            2         1  0                    1                1\n#>  5    52 l            2         1  0.0204               1                1\n#>  6   100 l            2         1  1                    1                1\n#>  7   101 w            3         2  0                    2                1\n#>  8   102 w            3         2  0.0204               2                1\n#>  9   150 w            3         2  1                    2                1\n#> 10   151 w            4         3  0                    3                2\n#> 11   152 w            4         3  0.0204               3                2\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\nI want to see the differences in detail. So I will provide also the\nungrouped version.\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-bayesian-update-2a}\n: Bayesian Updating: Lagged without grouping\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl2 <- tbl %>% \n  tidyr::expand_grid(p_water = seq(from = 0, to = 1, \n                            length.out = sequence_length)) |> \n    \n  dplyr::mutate(lagged_n_trials  = dplyr::lag(n_trials, default = 1),\n         lagged_n_success = dplyr::lag(n_success, default = 1)) |> \n  \n  # add new column ID with row numbers and relocate it as first column\n  dplyr::mutate(ID = dplyr::row_number()) |> \n  dplyr::relocate(ID, .before = toss) \n\n# show the same records without grouping\ntbl2[c(1:2, 50:52, 100:102, 150:152), ] \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 11 × 7\n#>       ID toss  n_trials n_success p_water lagged_n_trials lagged_n_success\n#>    <int> <chr>    <int>     <int>   <dbl>           <int>            <int>\n#>  1     1 w            1         1  0                    1                1\n#>  2     2 w            1         1  0.0204               1                1\n#>  3    50 w            1         1  1                    1                1\n#>  4    51 l            2         1  0                    1                1\n#>  5    52 l            2         1  0.0204               2                1\n#>  6   100 l            2         1  1                    2                1\n#>  7   101 w            3         2  0                    2                1\n#>  8   102 w            3         2  0.0204               3                2\n#>  9   150 w            3         2  1                    3                2\n#> 10   151 w            4         3  0                    3                2\n#> 11   152 w            4         3  0.0204               4                3\n```\n\n\n:::\n:::\n\n\n\n::::\n:::::\n\nIt turned out that the two version differ after 51 records in the lagged variables. (Not after 50 as I would have assumed. Apparently this has to do with the `lag()`\ncommand because the first 51 records are identical. Beginning with row\nnumber 52 there are differences in the column `lagged_n_trials` and `lagged_n_success`. This pattern is repeated: Original version always changes after 100 records. The version without grouping changes after 50 rows starting with row 51.\n\n##### Annotation (3): `dplyr::lag()` {#sec-annotation-3-lag}\n\nThe function `dplyr::lag()` finds the \"previous\" values in a vector (time series). This is useful for comparing values behind of the current values. See [Compute lagged or leading values](https://dplyr.tidyverse.org/reference/lead-lag.html).\n\nWe need to get the immediately previous values for drawing the prior\nprobabilities in the current graph (= dashed line or `linetype = 2` in\n{**ggplot2**} parlance). In the relation with the posterior probabilities the\ndifference form the prior possibility is always $1$ (this is the\noption `default = 1` in the `dplyr::lag()` function. This is now the correct\nexplanation for the differences starting after rows 51 (and not 50).\n\n##### Annotation (4): `dplyr::ungroup()` {#sec-annotation-4-ungroup}\n\nThis is just the reversion of the grouping command `group_by(p_water)`.\n\n##### Annotation (5): `dbinom()` {#sec-annotation-5-dbinom}\n\nThis is the core of the prior and likelihood calculation. It uses\n`base::dbinom()`, to calculate two alternative events. `dbinom()` is the\nR function for the binomial distribution, a distribution provided by the\nprobability theory for \"coin tossing\" problems.\n\n:::::{.my-resource}\n:::{.my-resource-header}\nExploring the `dbinom()` function\n:::\n::::{.my-resource-container}\nThe [distribution zoo](https://ben18785.shinyapps.io/distribution-zoo/) is a nice resource to explore the shapes and properties of important Bayesian distributions. This interactive {**shiny**} app was developed by [Ben Lambert](https://ben-lambert.com/bayesian/) and [Fergus Cooper](https://www.cs.ox.ac.uk/people/fergus.cooper/site/).\n\nChoose \"Discrete Univariate\" as the \"Category of Distribution\" and then select as \"Binomial\" as the \"Distribution Type\".\n\nSee also the two blog entries [An Introduction to the Binomial\nDistribution](https://www.statology.org/binomial-distribution/) and [A\nGuide to dbinom, pbinom, qbinom, and rbinom in\nR](https://www.statology.org/dbinom-pbinom-qbinom-rbinom-in-r/) of the\nStatology website.\n::::\n:::::\n\n\nThe \"`d`\" in `dbinom()` stands for *density*. Functions named in this\nway almost always have corresponding partners that begin with \"`r`\" for\nrandom samples and that begin with \"`p`\" for cumulative probabilities.\nSee for example the [help\nfile](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Binomial.html).\n\n\nThe results of each of the different calculation (prior and likelihood)\nare collected with `dplyr::mutate()` into two new generated columns.\n\nThere is no prior for the first trial, so it is assumed that it is 0.5.\nThe formula for the binomial distribution uses for the prior the\nlagged-version whereas the likelihood uses the current version. These\ntwo lines provide the essential calculations: They match the 50 grid\npoints as assumed water probabilities of every trial to their trial\noutcome (`W` or `L`) probabilities.\n\nThe last `dplyr::mutate()` command generates the $strip$ variable consisting of the\nprefix $n =$ followed by the counts of the number of trials. This will\nlater provide the title for the the different facets of the plot. \n\n::: my-checklist\n::: my-checklist-header\nTo-do: Construct labelling specification for toss results\n:::\n\n::: my-checklist-container\nTo get a better replication I would need to change the labels for the\nfacets from `n = n_trials` to the appropriate string length of the\nresults, e.g., for $n = 3$ I would need $W, L, W$.\n\nThis was not done by Kurz. I have tried it but didn't succeed up to now\n(2023-10-25).\n:::\n:::\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-bayesian.update-5}\n: Bayesian Updating: Calculating prior and likelihood\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl5 <- tbl1 %>% \n  dplyr::mutate(prior      = ifelse(n_trials == 1, .5,\n                             dbinom(x    = lagged_n_success, \n                                    size = lagged_n_trials, \n                                    prob = p_water)),\n         likelihood = dbinom(x    = n_success, \n                             size = n_trials, \n                             prob = p_water),\n         strip      = stringr::str_c(\"n = \", n_trials)) \n \ntbl5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 450 × 10\n#>       ID toss  n_trials n_success p_water lagged_n_trials lagged_n_success prior\n#>    <int> <chr>    <int>     <int>   <dbl>           <int>            <int> <dbl>\n#>  1     1 w            1         1  0                    1                1   0.5\n#>  2     2 w            1         1  0.0204               1                1   0.5\n#>  3     3 w            1         1  0.0408               1                1   0.5\n#>  4     4 w            1         1  0.0612               1                1   0.5\n#>  5     5 w            1         1  0.0816               1                1   0.5\n#>  6     6 w            1         1  0.102                1                1   0.5\n#>  7     7 w            1         1  0.122                1                1   0.5\n#>  8     8 w            1         1  0.143                1                1   0.5\n#>  9     9 w            1         1  0.163                1                1   0.5\n#> 10    10 w            1         1  0.184                1                1   0.5\n#> # ℹ 440 more rows\n#> # ℹ 2 more variables: likelihood <dbl>, strip <chr>\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n##### Annotation (6): Normalizing {#sec-annotation-6-normalize}\n\nThe code lines in annotation 6 normalize the prior and the likelihood by\ngrouping the data by `n\\_trials`. Dividing every prior and likelihood\nvalues by their respective sum puts them both in a probability metric.\nThis metric is important for the comparisons of different probabilities.\n\n> If you don't normalize (i.e., divide the density by the sum of the\n> density), their respective heights don't match up with those in the\n> text. Furthermore, it's the normalization that makes them directly\n> comparable. (Kurz in section [Bayesian Updating](https://bookdown.org/content/4857/small-worlds-and-large-worlds.html#bayesian-updating.) of chapter 2)\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-bayesian-update-6}\n: Bayesian Updating: Normalizing prior and likelihood\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl6 <- tbl5 %>% \n  dplyr::group_by(n_trials) %>% \n  dplyr::mutate(prior      = prior / sum(prior),\n         likelihood = likelihood / sum(likelihood))\n    \ntbl6\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 450 × 10\n#> # Groups:   n_trials [9]\n#>       ID toss  n_trials n_success p_water lagged_n_trials lagged_n_success prior\n#>    <int> <chr>    <int>     <int>   <dbl>           <int>            <int> <dbl>\n#>  1     1 w            1         1  0                    1                1  0.02\n#>  2     2 w            1         1  0.0204               1                1  0.02\n#>  3     3 w            1         1  0.0408               1                1  0.02\n#>  4     4 w            1         1  0.0612               1                1  0.02\n#>  5     5 w            1         1  0.0816               1                1  0.02\n#>  6     6 w            1         1  0.102                1                1  0.02\n#>  7     7 w            1         1  0.122                1                1  0.02\n#>  8     8 w            1         1  0.143                1                1  0.02\n#>  9     9 w            1         1  0.163                1                1  0.02\n#> 10    10 w            1         1  0.184                1                1  0.02\n#> # ℹ 440 more rows\n#> # ℹ 2 more variables: likelihood <dbl>, strip <chr>\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\n##### Annotation (7): Graphical demonstration of Bayesian updating  {#sec-annotation-7-ggplot}\n\nThe remainder of the code prepares the plot by using the 50 grid points\nin the range from $0$ to $1$ as the x-axis; prior and likelihood as y-axis.\nTo distinguish the prior from the likelihood it uses a dashed line for\nthe prior (`linetyp = 2`) and a full line (default) for the likelihood.\nThe x-axis has three breaks ($0, 0.5, 1$) whereas the y-axis has no\nbreak and no scale (`scales = \"free_y\"`).\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-bayesian-update-7}\n: Bayesian updating: Graphical demonstration\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl6 %>% \n  ggplot2::ggplot(ggplot2::aes(x = p_water)) + \n  ggplot2::geom_line(ggplot2::aes(y = prior), \n            linetype = 2) + \n  ggplot2::geom_line(ggplot2::aes(y = likelihood)) + \n  ggplot2::scale_x_continuous(\"proportion water\", breaks = c(0, .5, 1)) + \n  ggplot2::scale_y_continuous(\"plausibility\", breaks = NULL) + \n  ggplot2::theme(panel.grid = ggplot2::element_blank()) + \n  ggplot2::facet_wrap(~ strip, scales = \"free_y\") \n```\n\n::: {.cell-output-display}\n![Graphical demonstration: 9 steps of Bayesian updating](02-small-and-large-worlds_files/figure-html/fig-bayesian-update-7-1.png){#fig-bayesian-update-7 width=672}\n:::\n:::\n\n\n::::\n:::::\n\n\n\n## Components of the Model\n\n### ORIGINAL\n\nWe observed three components of the model:\n\n> “(1) The number of ways each conjecture could produce an observation \n> (2) The accumulated number of ways each conjecture could produce the entire data\n> (3) The initial plausibility of each conjectured cause of the data” ([McElreath, 2020, p. 32](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=51&annotation=R4M54D6K))\n\n#### Variables\n\n> “Variables are just symbols that can take on different values. In a scientific context, variables include things we wish to infer, such as proportions and rates, as well as things we might observe, the data. … Unobserved variables are usually called <a class='glossary' title='Unobserved variables are usually called Parameters. (SR2, Chap.2) A parameter is an unknown numerical characteristics of a population that must be estimated. (CDS). They are also numbers that govern statistical models (stats.stackexchange)'>parameters</a>.” ([McElreath, 2020, p. 32](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=51&annotation=C7C4PCB6))\n\nTake as example the globe tossing models: There are three variables: `W`\nand `L` (water or land) and the proportion of water and land `p`. We\nobserve the events of water or land but we calculate (do not observe\ndirectly) the proportion of water and land. So `p` is a parameter as\ndefined above.\n\n#### Definitions\n\n> “Once we have the variables listed, we then have to define each of them. In defining each, we build a model that relates the variables to one another. Remember, the goal is to count all the ways the data could arise, given the assumptions. This means, as in the globe tossing model, that for each possible value of the unobserved variables, such as $p$, we need to define the relative number of ways—the probability—that the values of each observed variable could arise. And then for each unobserved variable, we need to define the prior plausibility of each value it could take.” ([McElreath, 2020, p. 33](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=52&annotation=EWFRB9S7))\n\n:::::{.my-note}\n:::{.my-note-header}\n:::::: {#cor-expand-grid}\n: Calculate the prior plausibility of the values of each observed variable in R\n::::::\n:::\n::::{.my-note-container}\nThere are different functions in R that generate all combination of variables supplied by vectors, factors or data columns: `base::expand.grid()` or in the tidyverse `tidyr::expand()`, `tidyr::expand_grid()` and `tidyr::crossing()`.\nThese functions can be used to calculate and generate the relative number of ways (= the probability) that the values of each observed variable could arise.  We have seen a first demonstration in @cnj-bayesian-update-1. We will see many more examples in later sections and chapters.\n::::\n:::::\n\n\n##### Observed Variables\n\n> “For the count of water W and land L, we define how plausible any combination of W and L would be, for a specific value of p. This is very much like the marble counting we did earlier in the chapter. Each specific value of p corresponds to a specific plausibility of the data, as in @fig-bayesian-update-7.\n>\n>So that we don’t have to literally count, we can use a mathematical function that tells us the right plausibility. In conventional statistics, a distribution function assigned to an observed variable is usually called a <a class='glossary' title='The likelihood function (often simply called the likelihood) is the joint probability (or probability density) of observed data viewed as a function of the parameters of a statistical model. (Wikipedia) It indicates how likely a particular population is to produce an observed sample. (&lt;a href=“https://www.statistics.com/glossary/likelihood-function/&gt;statistics.com) It is the probability of the data given our beliefs about the data: P(data | belief). (BF, Chap.8)'>likelihood</a>. That term has special meaning in nonBayesian statistics, however.” ([McElreath, 2020, p. 33](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=52&annotation=TLDIVS8K))\n\nFor our globe tossing procedure we use instead of counting a mathematical function to calculate the probability of all combinations.\n\n > “In this case, once we add our assumptions that (1) every toss is independent of the other tosses and (2) the probability of W is the same on every toss, probability theory provides a unique answer, known as the <a class='glossary' title='It is used to calculate the probability of a certain number of successful outcomes, given a number of trials and the probability of the successful outcome. The “bi” in the term binomial refers to the two possible outcomes: an event happening and an event not happening. (BF, Chap.4)'>binomial distribution</a>. This is the common “coin tossing” distribution.” ([McElreath, 2020, p. 33](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=52&annotation=E5HEDDW4))\n\n:::::{.my-resource}\n::::{.my-resource-container}\nSee also @sec-annotation-5-dbinom for a resource to explore the binomial distribution.\n::::\n:::::\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-likelihood-globe-tossing}\n: Likelihood for prob = 0.5 in the globe tossing example\n::::::\n:::\n::::{.my-r-code-container}\nThe likelihood in the globe-tossing example (9 trials, 6 with `W` and 3\nwith `L`) is easily computed:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## R code 2.2 ################\ndbinom(6, size = 9, prob = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.1640625\n```\n\n\n:::\n:::\n\n\n> “Change the 0.5 to any other value, to see how the value changes.” ([McElreath, 2020, p. 34](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=53&annotation=7USE7U6W))\n\nIn this example it is assumed that the probability of `W` and `L` are\nequal distributed. We calculated how plausible the combination of $6W$\nand $3L$ would be, for the specific value of $p = 0.5$. The result is\nwith $16\\%$ a pretty low probability.\n\nTo get a better idea what the best estimation of the probability is, we\ncould vary systematically the $p$ value and look for the maximum. A\ndemonstration how this is done can be seen in @sec-calcu-10-probs. It\nshows a maximum at $prob = 0.7$.\n::::\n:::::\n\n\n\n##### Unobserved Variables\n\nEven variables that are not observed (= parameters) we need to define\nthem. In the globe-tossing model there is only one parameter ($p$), but\nmost models have more than one unobserved variables.\n\n:::::{.my-important}\n:::{.my-important-header}\nParameter & Prior\n:::\n::::{.my-important-container}\nFor every parameter we must provide a distribution of prior\nplausibility, its <a class='glossary' title='The Prior Probability, also called the Prior, is the assumed probability distribution before we have seen the data. (Wikipedia) It quantifies how likely our initial belief is: P(belief). (BF, Chap.8)'>Prior Probability</a>. This is also true when the number of trials is null ($N = 0$), e.g. even in the initial state of information we need a prior. (See @cnj-fig-bayesian-update, where a flat prior was used.)\n::::\n:::::\n\n\nWhen you have a previous estimate, that can become the prior. As a\nresult, each estimate (<a class='glossary' title='It is the revised or updated probability of an event occurring after taking into consideration new information. (Investopedia). Posterior probability = prior probability + new evidence (called likelihood). (Statistics How To) The posterior distribution will be a distribution of Gaussian distributions. (SR, Chap.4). It quantifies exactly how much our observed data changes our beliefs: P(belief | data) (BF, Chap.8)'>posterior probability</a>) becomes then the prior for the next step (as you have seen in @cnj-fig-bayesian-update).\n\n> “So where do priors come from? They are both engineering assumptions, chosen to help the machine learn, and scientific assumptions, chosen to reflect what we know about a phenomenon.” ([McElreath, 2020, p. 35](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=54&annotation=C5Q2WBUL))\n\n> “Within Bayesian data analysis in the natural and social sciences, the prior is considered to be just part of the model. As such it should be chosen, evaluated, and revised just like all of the other components of the model.” ([McElreath, 2020, p. 35](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=54&annotation=GQX776IE))\n\n> “Beyond all of the above, there’s no law mandating we use only one prior. If you don’t have a strong argument for any particular prior, then try different ones. Because the prior is an assumption, it should be interrogated like other assumptions: by altering it and checking how sensitive inference is to the assumption.” ([McElreath, 2020, p. 35](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=54&annotation=H9FQKLCU))\n\n\n\n\n:::::{.my-resource}\n:::{.my-resource-header}\nMore on the difference between Bayesian and frequentist statistics?\n:::\n::::{.my-resource-container}\nCheck out McElreath's lecture, [*Understanding\nBayesian statistics without frequentist language*](https://youtu.be/yakg94HyWdE) at Bayes@Lund2017 (20 April 2017).\n::::\n:::::\n\n\n\n#### A Model is Born\n\n> “The observed variables W and L are given relative counts through the binomial distribution.” ([McElreath, 2020, p. 36](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=55&annotation=8FL6YY2Q))\n\n$$W∼Binomial(n,p) \\space where\\space N = W + L$$ {#eq-globe-tossing-binomial-dist}\n\n> “The above is just a convention for communicating the assumption that the relative counts of ways to realize $W$ in $N$ trials with probability $p$ on each trial comes from the binomial distribution.” ([McElreath, 2020, p. 36](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=55&annotation=H9UKQXP2))\n\nOur binomial likelihood contains a parameter for an unobserved variable,\n*p*.\n\n$$p∼Uniform(0,1)$$ {#eq-uniform-prior}\n\nThe formula expresses the model assumption that the entire range of possible\nvalues for $p$ are equally plausible.\n\n\n### TIDYVERSE\n\n\n\nGiven a probability of .5, (e.g. equal probability to both events `W`\nand `L`) we use the `dbinom()` function to determine the likelihood\nof 6 out of 9 tosses coming out water in @cnj-likelihood-globe-tossing.\n\nMcElreath suggests:\n\n> “Change the 0.5 to any other value, to see how the value changes.” ([McElreath, 2020, p. 34](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=53&annotation=7USE7U6W))\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-calcu-10-probs}\n: Calculation likelihood with 10 different values of `prob`\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(d <- tibble::tibble(prob = seq(from = 0, to = 1, by = .1)) |> \n    dplyr::mutate(likelihood = dbinom(x = 6, size = 9, prob = prob))\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 11 × 2\n#>     prob likelihood\n#>    <dbl>      <dbl>\n#>  1   0    0        \n#>  2   0.1  0.0000612\n#>  3   0.2  0.00275  \n#>  4   0.3  0.0210   \n#>  5   0.4  0.0743   \n#>  6   0.5  0.164    \n#>  7   0.6  0.251    \n#>  8   0.7  0.267    \n#>  9   0.8  0.176    \n#> 10   0.9  0.0446   \n#> 11   1    0\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd  |>  \n    dplyr::filter(likelihood == max(likelihood))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 1 × 2\n#>    prob likelihood\n#>   <dbl>      <dbl>\n#> 1   0.7      0.267\n```\n\n\n:::\n:::\n\n\n::::\n:::::\n\n\nIn the series of values you will notice several point:\n\n1.  The values start with zero until a maximum of $0.267$ and decline to\n    zero again. The maximum is with $prob = 0.7$, a proportion of $W$\n    and $L$ that is --- as we know from our large world knowledge\n    (knowledge outside the small world of the model) --- already pretty\n    near the real distribution of about $0.71$. (see [How Much of the\n    Earth Is Covered by\n    Water?](https://www.thedailyeco.com/how-much-of-the-earth-is-covered-by-water-122.html))\n2.  You see that the first `prob` ($0$) and last `prob` ($1$) values are both\n    zero. From the result (6 $W$ and 3 $L$) `prob` cannot be 0 or 1\n    because there a both $W$ and $L$ in the observed sample.\n\n@cnj-calcu-10-probs is my interpretation from the quote \"Change the 0.5\nto any other value, to see how the value changes.\" Kurz has another\ninterpretation when he draws a graph of 100 `prob` values from 0 to 1:\n\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-calcu-100-probs}\n: Plot likelihood for 100 values of `prob` from $0$ to $1$, by steps of $0.01$\n::::::\n:::\n::::{.my-r-code-container}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble::tibble(prob = seq(from = 0, to = 1, by = .01)) %>% \n  ggplot2::ggplot(ggplot2::aes(x = prob, y = dbinom(x = 6, size = 9, prob = prob))) +\n  ggplot2::geom_line() +\n  ggplot2::labs(x = \"probability\",\n       y = \"binomial likelihood\") +\n  ggplot2::theme(panel.grid = ggplot2::element_blank()) +\n  ggplot2::theme_bw()\n```\n\n::: {.cell-output-display}\n![Likelihood for 100 values of prob, from 0 to 1, by steps of 0.01](02-small-and-large-worlds_files/figure-html/fig-likelihood-100-prob-1.png){#fig-likelihood-100-prob width=672}\n:::\n:::\n\n\n::::\n:::::\n\n\nIn contrast to $p = 0.5$ with a probability of $0.16$ the\n@fig-likelihood-100-prob shows a maximum at about $p = 0.7$ and a\nprobability estimated from the graph of about $0.26-0.28$. We will get\nmore detailed data later in the book.\n\nIt is interesting to see that even the maximum probability is not very\nhigh. The reason is that there are many other configurations\n(distributions of $W$s and $L$s) to produce the result of $6W$ and $3L$.\nEven if all these other distributions have a small probability they\n\"eat\" all with their share from the maximum.\n\n:::::{.my-r-code}\n:::{.my-r-code-header}\n:::::: {#cnj-piror-prob-dens}\n: Prior as a probability distribution for the parameter\n::::::\n:::\n::::{.my-r-code-container}\n> “The prior is a <a class='glossary' title='It is a way of describing all possible events and the probability of each one happening. Probability distributions are also very useful for asking questions about ranges of possible values. (BF, Chap.4) The two defining features are: (1) All values of the distribution must be real and non-negative. (2) The sum (for discrete random variables) or integral (for continuous random variables) across all possible values of the random variable must be 1. (BS, Chap.3)'>probability distribution</a> for the parameter. In general, for a uniform prior from $a$ to $b$, the probability of any point in the interval is $1/(b − a)$. If you’re bothered by the fact that the probability of every value of $p$ is $1$, remember that every probability distribution must sum (integrate) to $1$.” ([McElreath, 2020, p. 35](zotero://select/groups/5243560/items/NFUEVASQ)) ([pdf](zotero://open-pdf/groups/5243560/items/CPFRPHX8?page=54&annotation=3TQ9MKDM))\n\nKurz demonstrates the truth of this quote with several $b$ values while\nholding $a$ constant:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble::tibble(a = 0,\n       b = c(1, 1.5, 2, 3, 9)) %>% \n  dplyr::mutate(prob = 1 / (b - a))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 5 × 3\n#>       a     b  prob\n#>   <dbl> <dbl> <dbl>\n#> 1     0   1   1    \n#> 2     0   1.5 0.667\n#> 3     0   2   0.5  \n#> 4     0   3   0.333\n#> 5     0   9   0.111\n```\n\n\n:::\n:::\n\n\nVerified with a plot Kurz divides the range of the $b$ parameter ($0-9$)\ninto 500 segments (*parameter_space*) and uses the `dunif()`\ndistribution to calculate the probabilities for a uniform distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble::tibble(a = 0,\n       b = c(1, 1.5, 2, 3, 9)) %>% \n  tidyr::expand_grid(parameter_space = seq(from = 0, to = 9, length.out = 500)) %>% \n  dplyr::mutate(prob = dunif(parameter_space, a, b),\n         b    = str_c(\"b = \", b)) %>% \n  \n  ggplot2::ggplot(ggplot2::aes(x = parameter_space, y = prob)) +\n  ggplot2::geom_area() +\n  ggplot2::scale_x_continuous(breaks = c(0, 1:3, 9)) +\n  ggplot2::scale_y_continuous(breaks = c(0, 1/9, 1/3, 1/2, 2/3, 1),\n                     labels = c(\"0\", \"1/9\", \"1/3\", \"1/2\", \"2/3\", \"1\")) +\n  ggplot2::theme(panel.grid.minor = ggplot2::element_blank(),\n        panel.grid.major.x = ggplot2::element_blank()) +\n  ggplot2::theme_bw() +\n  ggplot2::facet_wrap(~ b, ncol = 5)\n```\n\n::: {.cell-output-display}\n![Graphical demonstration that the probability of every value of $p$ is $1$](02-small-and-large-worlds_files/figure-html/fig-uniform-prior2-1.png){#fig-uniform-prior2 width=672}\n:::\n:::\n\n\nThis figure demonstrates that the area in the whole parameter space is\n*1.0*. It is a nice example how to calculate the probability *mass* (in\ncontrast to the curve of the probability *density*).\n\n\n::::\n:::::\n\n\n\n## I STOPPED HERE!! SECOND PASS (2023-10-26) {.unnumbered}\n\n## Making the Model Go\n\n### Bayes' Theorem\n\n#### Original\n\n> Once you have named all the variables and chosen definitions for each,\n> a Bayesian model can update all of the prior distributions to their\n> purely logical consequences: the **POSTERIOR DISTRIBUTION**. For every\n> unique combination of data, likelihood, parameters, and prior, there\n> is a unique posterior distribution. This distribution contains the\n> relative plausibility of different parameter values, conditional on\n> the data and model. The posterior distribution takes the form of the\n> probability of the parameters, conditional on the data.\n\nIn the case of the globe-tossing model we can write:\n\n$$\nPr(p|W, L)\n$$\n\nThis has to be interpreted as \"the probability of each possible value of\n*p*, conditional on the specific *W* and *L* that we observed.\"\n\n$$\nPr(p|W,L) = \\frac{Pr(W,L|p)Pr(p)}{Pr(W,L)}\n$$\n\n> And this is Bayes' theorem. It says that the probability of any\n> particular value of *p*, considering the data, is equal to the product\n> of the relative plausibility of the data, conditional on *p*, and the\n> prior plausibility of *p*, divided by this thing Pr(*W, L*), which\n> I'll call the *average probability of the data*.\n\nExpressed in words:\n\n$$\nPosterior = \\frac{Probability\\space of\\space the\\space data\\space ✕\\space Prior}{Average\\space probability\\space of\\space the\\space data}\n$$\n\nOther names for the *average probability of the data*:\n\n-   evidence\n-   average likelihood\n-   marginal likelihood\n\nThe job of the average probability of the data is just to standardize\nthe posterior, to ensure it sums (integrates) to one.\n\n::: callout-important\n###### Key lesson\n\nThe posterior is proportional to the product of the prior and the\nprobability of the data.\n:::\n\n> \\[E\\]ach specific value of *p*, the number of paths through the garden\n> of forking data is the product of the prior number of paths and the\n> new number of paths. **Multiplication is just compressed counting.**\n> The average probability on the bottom just standardizes the counts so\n> they sum to one. (emphasis is mine)\n\n> \\[The following graph\\] illustrates the multiplicative interaction of\n> a prior and a probability of data. On each row, a prior on the left is\n> multiplied by the probability of data in the middle to produce a\n> posterior on the right. The probability of data in each case is the\n> same. The priors however vary. As a result, the posterior\n> distributions vary.\n\n![Figure 2.6 of the original book](img/SR2-fig2_6-min.jpg){#fig-2-6-book\nfig-alt=\"The posterior distribution as a product of the prior distribution and likelihood. Top: A flat prior constructs a posterior that is simply proportional to the likelihood. Middle: A step prior, assigning zero probability to all values less than 0.5, results in a truncated posterior. Bottom: A peaked prior that shifts and skews the posterior, relative to the likelihood.\"}\n\nFor my understanding it is important to reproduce the above graph with R\ncode as it is shown in the next section.\n\n#### Tidyverse\n\n> \\[The following graph\\] illustrates the multiplicative interaction of\n> a prior and a probability of data. On each row, a prior on the left is\n> multiplied by the probability of data in the middle to produce a\n> posterior on the right. The probability of data in each case is the\n> same. The priors however vary. As a result, the posterior\n> distributions vary.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-prepare-model lst-cap=\"The posterior distribution as a product of the prior distribution and likelihood.\"}\nsequence_length <- 1e3\n\nd <-\n  tibble(probability = seq(from = 0, to = 1, length.out = sequence_length)) %>% \n  expand_grid(row = c(\"flat\", \"stepped\", \"Laplace\"))  %>% \n  arrange(row, probability) %>% \n  mutate(prior = ifelse(row == \"flat\", 1,\n                        ifelse(row == \"stepped\", rep(0:1, each = sequence_length / 2),\n                               exp(-abs(probability - 0.5) / .25) / ( 2 * 0.25))),\n         likelihood = dbinom(x = 6, size = 9, prob = probability)) %>% \n  group_by(row) %>% \n  mutate(posterior = prior * likelihood / sum(prior * likelihood)) %>% \n  pivot_longer(prior:posterior)  %>% \n  ungroup() %>% \n  mutate(name = factor(name, levels = c(\"prior\", \"likelihood\", \"posterior\")),\n         row  = factor(row, levels = c(\"flat\", \"stepped\", \"Laplace\")))\n```\n:::\n\n\nIn comparison to my very detailed code annotations of\n@fig-bayesian-update there are different lines of code, but generally\nthere is nothing conceptually new: We use again `expand_grid()` to\ncreate a tibble of input combinations and create with `mutate()` two\ncolumns for prior and likelihood. We do not use the `lag()` functions as\nwe calculate only for one prior and one likelihood. Kurz advises us that\nis \"easier to just make each column of the plot separately. We can then\nuse the elegant and powerful syntax from [Thomas Lin\nPedersen](https://www.data-imaginist.com/)'s (2022) [patchwork\npackage](https://patchwork.data-imaginist.com/) to combine them.\"\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(patchwork)\n\np1 <-\n  d %>%\n  filter(row == \"flat\") %>% \n  ggplot(aes(x = probability, y = value)) +\n  geom_line() +\n  scale_x_continuous(NULL, breaks = NULL) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ name, scales = \"free_y\")\n\np2 <-\n  d %>%\n  filter(row == \"stepped\") %>% \n  ggplot(aes(x = probability, y = value)) +\n  geom_line() +\n  scale_x_continuous(NULL, breaks = NULL) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank(),\n        strip.background = element_blank(),\n        strip.text = element_blank()) +\n  facet_wrap(~ name, scales = \"free_y\")\n\np3 <-\n  d %>%\n  filter(row == \"Laplace\") %>% \n  ggplot(aes(x = probability, y = value)) +\n  geom_line() +\n  scale_x_continuous(NULL, breaks = c(0, .5, 1)) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank(),\n        strip.background = element_blank(),\n        strip.text = element_blank()) +\n  facet_wrap(~ name, scales = \"free_y\")\n\n# combine\n# library(patchwork) # defined in setup chunk\np1 / p2 / p3\n```\n\n::: {.cell-output-display}\n![Figure 2.6 from book reproduced with tidyverse code shows the posterior distribution as a product of the prior distribution and likelihood. Top: A flat prior constructs a posterior that is simply proportional to the likelihood. Middle: A step prior, assigning zero probability to all values less than 0.5, results in a truncated posterior. Bottom: A peaked prior that shifts and skews the posterior, relative to the likelihood. Cormpare it with @fig-2-6-book.](02-small-and-large-worlds_files/figure-html/fig-plot-model-1.png){#fig-plot-model width=672}\n:::\n:::\n\n\n@fig-plot-model shows that the same likelihood with a different prior\nresults in a different posterior.\n\n### Motors\n\n#### Original\n\n> Recall that your Bayesian model is a machine, a figurative golem. It\n> has built-in definitions for the likelihood, the parameters, and the\n> prior. And then at its heart lies a motor that processes data,\n> producing a posterior distribution. The action of this motor can be\n> thought of as *conditioning* the prior on the data. As explained in\n> the previous section, this conditioning is governed by the rules of\n> probability theory, which defines a uniquely logical posterior for set\n> of assumptions and observations.\n>\n> However, knowing the mathematical rule is often of little help,\n> because many of the interesting models in contemporary science cannot\n> be conditioned formally, no matter your skill in mathematics. And\n> while some broadly useful models like linear regression can be\n> conditioned formally, this is only possible if you constrain your\n> choice of prior to special forms that are easy to do mathematics with.\n> We'd like to avoid forced modeling choices of this kind, instead\n> favoring conditioning engines that can accommodate whichever prior is\n> most useful for inference.\n>\n> What this means is that various numerical techniques are needed to\n> approximate the mathematics that follows from the definition of Bayes'\n> theorem. In this book, you'll meet three different conditioning\n> engines, numerical techniques for computing posterior distributions:\n\nWhat are the numerical techniques for computing posterior distributions\nexplained in the book?\n\n1.  Grid approximation\n2.  Quadratic approximation\n3.  Markov chain Monte Carlo (MCMC)\n\n> There are many other engines, and new ones are being invented all the\n> time. But the three you'll get to know here are common and widely\n> useful. (p. 39)\n\n::: callout-tip\n**Rethinking: How you fit the model is part of the model**. Earlier in\nthis chapter, I implicitly defined the model as a composite of a prior\nand a likelihood. That definition is typical. But in practical terms, we\nshould also consider how the model is fit to data as part of the model.\nIn very simple problems, like the globe tossing example that consumes\nthis chapter, calculation of the posterior density is trivial and\nfoolproof. In even moderately complex problems, however, the details of\nfitting the model to data force us to recognize that our numerical\ntechnique influences our inferences. This is because different mistakes\nand compromises arise under different techniques. The same model fit to\nthe same data using different techniques may produce different answers.\nWhen something goes wrong, every piece of the machine may be suspect.\nAnd so our golems carry with them their updating engines, as much slaves\nto their engineering as they are to the priors and likelihoods we\nprogram into them.\n:::\n\n#### Tidyverse\n\nIn my own words: Processing the built-in definitions for the likelihood,\nthe parameters, and the prior produces the posterior distribution. This\nprocess is governed by the rule of probability theory. But knowing the\nmathematics does generally not help for two reasons:\n\n-   Many of the interesting models in contemporary science cannot be\n    conditioned formally\n-   Though some broadly useful models like linear regression can be\n    conditioned formally, this is only possible if you constrain your\n    choice of prior to special forms that are easy to do mathematics\n    with.\n\nTherefore are various numerical techniques needed to approximate the\nmathematics that follows from the definition of Bayes' theorem. From the\nthree widely useful methods (grid approximation, quadratic approximation\nand MCMC) covered in the SR2-book the tidyverse version of this material\nconcentrates on using the [{**brms**}\npackage](https://paul-buerkner.github.io/brms/).\n\n::: {.callout-caution style=\"color: orange;\"}\n###### Jumping quickly into MCMC\n\n> The consequence is that this version will jump rather quickly into\n> MCMC. This will be awkward at times because it will force us to\n> contend with technical issues in earlier problems in the text than\n> McElreath originally did.\n:::\n\n### Grid approximation\n\n#### Original\n\n> While most parameters are *continuous*, capable of taking on an\n> infinite number of values, it turns out that we can achieve an\n> excellent approximation of the continuous posterior distribution by\n> considering only a finite grid of parameter values. At any particular\n> value of a parameter, *p*', it's a simple matter to compute the\n> posterior probability: just multiply the prior probability of *p*' by\n> the likelihood at *p*'. Repeating this procedure for each value in the\n> grid generates an approximate picture of the exact posterior\n> distribution. This procedure is called **GRID APPROXIMATION**.\n\nGrid approximation is very useful as a pedagogical tool. But in most of\nyour real modeling, grid approximation isn't practical because it scales\npoorly, as the number of parameters increases.\n\n1.  Define the grid. This means you decide how many points to use in\n    estimating the posterior, and then you make a list of the parameter\n    values on the grid.\n2.  Compute the value of the prior at each parameter value on the grid.\n3.  Compute the likelihood at each parameter value.\n4.  Compute the unstandardized posterior at each parameter value, by\n    multiplying the prior by the likelihood.\n5.  Finally, standardize the posterior, by dividing each value by the\n    sum of all values.\n\nIn the globe tossing model the five steps are as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. define grid\np_grid <- seq(from = 0, to = 1, length.out = 20)\n\n# 2. define prior\nprior <- rep(1, 20)\n\n# 3. compute likelihood at each value in grid\nlikelihood <- dbinom(x = 6, size = 9, prob = p_grid)\n\n# 4. compute product of likelihood and prior\nunstd.posterior <- likelihood * prior\n\n# 5. standardize the posterior, so it sums to 1\nposterior <- unstd.posterior / sum(unstd.posterior)\n\n# 6 display the posterior distribution\n## R code 2.4\nplot(p_grid, posterior,\n  type = \"b\",\n  xlab = \"probability of water\", ylab = \"posterior probability\"\n)\nmtext('20 points with \"prior = 1\"')\n```\n\n::: {.cell-output-display}\n![Grid Approximation with 20 points with `prior = 1`](02-small-and-large-worlds_files/figure-html/fig-grid-approx-base1-1.png){#fig-grid-approx-base1 width=672}\n:::\n:::\n\n\n``` r\n#| label: lst-own-tidyverse\n#| lst-cap: \"My own tidyverse plot <-  just for learning purposes\"\n\n# # instead of books R code 2.4 I will use a tidyverse approach\n# df <- dplyr::bind_cols(\"prob\" = p_grid, \"post\" = posterior)\n# ggplot2::ggplot(df, ggplot2::aes(x = prob, y = post)) +\n#     ggplot2::geom_line() +\n#     ggplot2::geom_point()\n```\n\n##### Change likelihood parameters\n\nThe parameters for the calculated likelihood is based on the binomial\ndistribution and is shaping the above plot:\n\n-   x = number of water events `W`\n-   size = number of sample trials = number of observations\n-   prob = success probability on each trial = probability of `W` (water\n    event)\n\nIt does not matter in the code for @fig-grid-approx-base1 what prior\nprobability is chosen in the range from 0 to 1, if the probability of\n`W` is greater than zero and --- and by definition of the binomial\nfunction --- equal for all 20 events. This does not only conform to\nvalues but also for functions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. define grid\np_grid <- seq(from = 0, to = 1, length.out = 20)\n\n# 2. define prior\nprior <- rep(0.1, 20)\n\n# 3. compute likelihood at each value in grid\nlikelihood <- dbinom(x = 6, size = 9, prob = p_grid)\n\n# 4. compute product of likelihood and prior\nunstd.posterior <- likelihood * prior\n\n# 5. standardize the posterior, so it sums to 1\nposterior <- unstd.posterior / sum(unstd.posterior)\n\n# 6 display the posterior distribution\n## R code 2.4\nplot(p_grid, posterior,\n  type = \"b\",\n  xlab = \"probability of water\", ylab = \"posterior probability\"\n)\nmtext('20 points with \"prior = 0.1\"')\n```\n\n::: {.cell-output-display}\n![Grid Approximation with 20 points with `prior = 0.1`](02-small-and-large-worlds_files/figure-html/fig-grid-approx-base1a-1.png){#fig-grid-approx-base1a width=672}\n:::\n:::\n\n\n##### Changing prior parameters\n\nTo see the influence of the prior probability on the posterior\nprobability by using the same likelihood the book offers two code\nsnippets. Replace the definition of the prior from the `grid-approx-a`\nchunk (number 2 in the code snippet) --- one at a time --- with the\nfollowing lines of code:\n\n``` r\n#| label: lst-different-priors\n#| lst-cap: \"Using two different priors\"\nprior <- ifelse(p_grid < 0.5, 0, 1)\nprior <- exp(-5 * abs(p_grid - 0.5))\n```\n\nThe rest of the code remains the same.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. define grid\np_grid <- seq(from = 0, to = 1, length.out = 20)\n\n# 2. define prior\nprior <- ifelse(p_grid < 0.5, 0, 1)\n\n# 3. compute likelihood at each value in grid\nlikelihood <- dbinom(x = 6, size = 9, prob = p_grid)\n\n# 4. compute product of likelihood and prior\nunstd.posterior <- likelihood * prior\n\n# 5. standardize the posterior, so it sums to 1\nposterior <- unstd.posterior / sum(unstd.posterior)\n\n# 6 display the posterior distribution \n## R code 2.4\nplot(p_grid, posterior,\n  type = \"b\",\n  xlab = \"probability of water\", ylab = \"posterior probability\"\n)\nmtext('20 points with a prior of \"ifelse(p_grid < 0.5, 0, 1)\"')\n```\n\n::: {.cell-output-display}\n![Grid Approximation with prior of `ifelse(p_grid < 0.5, 0, 1)`](02-small-and-large-worlds_files/figure-html/fig-grid-approx-base2-1.png){#fig-grid-approx-base2 width=672}\n:::\n:::\n\n\n@fig-grid-approx-base2 employs as prior `ifelse(p_grid < 0.5, 0, 1)`,\nmeaning that if `prob` is smaller than 0.5 use zero as prior otherwise\n1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. define grid\np_grid <- seq(from = 0, to = 1, length.out = 20)\n\n# 2. define prior\nprior <- exp(-5 * abs(p_grid - 0.5))\n\n# 3. compute likelihood at each value in grid\nlikelihood <- dbinom(x = 6, size = 9, prob = p_grid)\n\n# 4. compute product of likelihood and prior\nunstd.posterior <- likelihood * prior\n\n# 5. standardize the posterior, so it sums to 1\nposterior <- unstd.posterior / sum(unstd.posterior)\n\n# 6 display the posterior distribution \n## R code 2.4\nplot(p_grid, posterior,\n  type = \"b\",\n  xlab = \"probability of water\", ylab = \"posterior probability\"\n)\nmtext('20 points with prior of \"exp(-5 * abs(p_grid - 0.5))\"')\n```\n\n::: {.cell-output-display}\n![Grid Approximation with prior of `exp(-5 * abs(p_grid - 0.5))`](02-small-and-large-worlds_files/figure-html/fig-grid-approx-base3-1.png){#fig-grid-approx-base3 width=672}\n:::\n:::\n\n\n##### Disadvantage\n\nGrid approximation is very expansive. The number of unique values to\nconsider in the grid grows rapidly as the number of parameters in the\nmodel increases. For the single-parameter globe tossing model, it's no\nproblem to compute a grid of 100 or 1000 values. But for two parameters\napproximated by 100 values each, that's already 100^2^ = 10.000 values\nto compute. For 10 parameters, the grid becomes many billions of values.\nThese days, it's routine to have models with hundreds or thousands of\nparameters. The grid approximation strategy scales very poorly with\nmodel complexity, so it won't get us very far. But it is a very useful\ndidactically as it help to understand the general principle.\n\n#### Reconsideration\n\n::: callout-caution\n###### Puzzlement\n\nAs demonstrated in @fig-plot-model a different prior with the same\nlikelihood has different posteriors. So the curves of all three examples\nare different.\n\nBut what I do not understand is the fact that in the third example the\nresult is very different from the other two priors: About 0.5 and not\n0.7. My explication: I have only 9 trials. With many more trials the\ndifference in the density would balance out bit by bit.\n\nBut it turns out, that all three show about the same maximum I thought\nthat the chosen prior did not have any effect the outcome. But it turns\nout that different priors results in different PDFs.\\\nMy explication: I have only 9 trials. With many more trials the\ndifference in the density would balance out bit by bit.\n:::\n\nI am going to test my hypothesis. To demonstrate that with more Bayesian\nupdates we will approach the correct result 0b about 0.7 I will work\nwith 1000 samples.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. define grid\np_grid <- seq(from = 0, to = 1, length.out = 20)\n\n# 2. define prior\nprior <- exp(-5 * abs(p_grid - 0.5))\n\n# 3. compute likelihood at each value in grid\nlikelihood <- dbinom(x = 600, size = 1e3, prob = p_grid)\n\n# 4. compute product of likelihood and prior\nunstd.posterior <- likelihood * prior\n\n# 5. standardize the posterior, so it sums to 1\nposterior <- unstd.posterior / sum(unstd.posterior)\n\n# 6 display the posterior distribution \n## R code 2.4\nplot(p_grid, posterior,\n  type = \"b\",\n  xlab = \"probability of water\", ylab = \"posterior probability\"\n)\nmtext('20 points for a 1000 samples with prior of \"exp(-5 * abs(p_grid - 0.5))\"')\n```\n\n::: {.cell-output-display}\n![Grid Approximation with prior of `exp(-5 * abs(p_grid - 0.5))`](02-small-and-large-worlds_files/figure-html/fig-grid-approx-base4-1.png){#fig-grid-approx-base4 width=672}\n:::\n:::\n\n\nIn fact the posterior distribution is nearer to the correct result of\n0.7. In @fig-grid-approx-base3 we have a maximum at about 0.52 and in\n@fig-grid-approx-base4 we have already 0.58. We should also taking into\naccount that with more samples our proportion of W to L will approach\nthe real Water:Land proportion of about 0.71 too.\n\nI will demonstrate this with 10000 samples and a W:L proportion of 7:3.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. define grid\np_grid <- seq(from = 0, to = 1, length.out = 20)\n\n# 2. define prior\nprior <- exp(-5 * abs(p_grid - 0.5))\n\n# 3. compute likelihood at each value in grid\nlikelihood <- dbinom(x = 7e3, size = 1e4, prob = p_grid)\n\n# 4. compute product of likelihood and prior\nunstd.posterior <- likelihood * prior\n\n# 5. standardize the posterior, so it sums to 1\nposterior <- unstd.posterior / sum(unstd.posterior)\n\n# 6 display the posterior distribution \n## R code 2.4\nplot(p_grid, posterior,\n  type = \"b\",\n  xlab = \"probability of water\", ylab = \"posterior probability\"\n)\nmtext('20 points for a 10000 samples with prior of \"exp(-5 * abs(p_grid - 0.5))\" and W:L = 7:3.' )\n```\n\n::: {.cell-output-display}\n![Grid Approximation with prior of `exp(-5 * abs(p_grid - 0.5))`](02-small-and-large-worlds_files/figure-html/fig-grid-approx-base5-1.png){#fig-grid-approx-base5 width=672}\n:::\n:::\n\n\nIn this example the maximum of probability is already 0.68! We can say\nthat **with every chosen prior we will get finally the correct\nresult!**.\n\nBut the choice of the prior is still important as it determines how many\nBayesian updates we need to get the right result. If we have an awkward\nprior and not the appropriate size of the sample we will get a posterior\ndistribution showing us a wrong maximum of probability. In that case the\nprocess of approximation has not reached a state where the probability\nmaximum is near the correct result. The problem is: Most time we do not\nknow the correct solution and can't therefore decide if we have had\nenough Bayesian updates.\n\n#### Tidyverse\n\nWe just employed grid approximation in the @fig-plot-model chunk. To get\nnice smooth lines, we computed in the @lst-prepare-model chunk the\nposterior over 1,000 evenly-spaced points on the probability space. Here\nwe'll prepare for the left panel of Figure 2.7 with just 5 evenly-spaced\npoints.\n\nAs a reminder I will add comments for the five steps for the grid\napproximation procedure. As it is explained in detail on several other\nplaces I will not produce code annotations.\n\n##### Produce grid data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(\n  d <-\n    # 1. define grid\n    tibble(p_grid = seq(from = 0, to = 1, length.out = 5), \n           \n    # 2. define (compute) prior  \n           prior  = 1) %>%\n        \n    # 3. compute likelihood at each value in grid\n    mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %>% \n        \n    # 4. compute product of likelihood and prior\n    mutate(unstd_posterior = likelihood * prior) %>%  \n        \n    # 5. standardize the posterior, so it sums to 1\n    mutate(posterior = unstd_posterior / sum(unstd_posterior))   \n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> # A tibble: 5 × 5\n#>   p_grid prior likelihood unstd_posterior posterior\n#>    <dbl> <dbl>      <dbl>           <dbl>     <dbl>\n#> 1   0        1    0               0          0     \n#> 2   0.25     1    0.00865         0.00865    0.0213\n#> 3   0.5      1    0.164           0.164      0.404 \n#> 4   0.75     1    0.234           0.234      0.575 \n#> 5   1        1    0               0          0\n```\n\n\n:::\n:::\n\n\n##### Plot with only 5 points\n\nThe next step is to plot the above results to get the left panel of\nFigure 2.5\n\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- \n    d |> \n    ggplot(aes(x = p_grid, y = posterior)) +\n      geom_point() +\n      geom_line() +\n      labs(subtitle = \"5 points\",\n           x = \"probability of water\",\n           y = \"posterior probability\") +\n      theme(panel.grid = element_blank())\np1\n```\n\n::: {.cell-output-display}\n![](02-small-and-large-worlds_files/figure-html/plot-grid-approx1-1.png){width=672}\n:::\n:::\n\n\n##### Produce grid with 20 points and plot result\n\nNow the same with 20 evenly spaced points to get the right panel of\nFigure 2.7.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np2 <-\n  tibble(p_grid = seq(from = 0, to = 1, length.out = 20),\n         prior  = 1) %>%\n  mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %>%\n  mutate(unstd_posterior = likelihood * prior) %>%\n  mutate(posterior = unstd_posterior / sum(unstd_posterior)) %>% \n  \n  ggplot(aes(x = p_grid, y = posterior)) +\n  geom_point() +\n  geom_line() +\n  labs(subtitle = \"20 points\",\n       x = \"probability of water\",\n       y = \"posterior probability\") +\n  theme(panel.grid = element_blank())\np2\n```\n\n::: {.cell-output-display}\n![](02-small-and-large-worlds_files/figure-html/grid-approx2-1.png){width=672}\n:::\n:::\n\n\n##### Combine plots with {patchwork}\n\nAnd finally we display the two graphics with the `+` operator of\n{**patchwork**} side by side and annotate the plot with\n`patchwork::plot_annotation()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# needs library(patchwork) = defined in setup chunk\np1 + p2 + plot_annotation(title = \"More grid points make for smoother approximations\")\n```\n\n::: {.cell-output-display}\n![](02-small-and-large-worlds_files/figure-html/plot-grid-approx-b2-1.png){width=672}\n:::\n:::\n\n\n##### Using differnt prior functions\n\nTo see the influence of the prior probability on the posterior\nprobability by using the same likelihood the book offers two code\nsnippets. Replace the definition of the prior (number 2 in the code\nsnippet) --- one at a time --- with the following lines of code:\n\n\n::: {.cell}\n\n```{.r .cell-code  caption=\"Two other prior functions to try out what happens with different priors.\"}\nprior <- ifelse(p_grid < 0.5, 0, 1)\nprior <- exp(-5 * abs(p_grid - 0.5))\n```\n:::\n\n\nWhat follows is a condensed way to make the four plots all at once. It\nis a pretty complex program snippet not only using\n`tidyr::expand_grid()` --- as already explained ---, but also\n`tidyr::unnest()` which expands a list-column containing data frames\ninto rows and columns.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# prepare the plot by producing the data\ntibble(n_points = c(5, 20)) %>% \n  mutate(p_grid = purrr::map(n_points, ~seq(from = 0, to = 1, length.out = .))) %>% \n  unnest(p_grid) %>% \n  expand_grid(priors = c(\"ifelse(p_grid < 0.5, 0, 1)\", \"exp(-5 * abs(p_grid - 0.5))\")) %>% \n  mutate(prior = ifelse(priors == \"ifelse(p_grid < 0.5, 0, 1)\", \n                        ifelse(p_grid < 0.5, 0, 1),\n                        exp(-5 * abs(p_grid - 0.5)))) %>% \n  mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %>% \n  mutate(posterior = likelihood * prior / sum(likelihood * prior)) %>% \n  mutate(n_points = str_c(\"# points = \", n_points),\n         priors   = str_c(\"prior = \", priors)) %>% \n  \n  # plot the data\n  ggplot(aes(x = p_grid, y = posterior)) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"probability of water\",\n       y = \"posterior probability\") +\n  theme(panel.grid = element_blank()) +\n  facet_grid(n_points ~ priors, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![The effect of different priors and of different amounts of grid points.](02-small-and-large-worlds_files/figure-html/fig-different-priors-5-and-20-points-1.png){#fig-different-priors-5-and-20-points width=672}\n:::\n:::\n\n\n### Quadratic Approximation\n\n#### Original\n\n##### Concept\n\n> Under quite general conditions, the region near the peak of the\n> posterior distribution will be nearly Gaussian---or \"normal\"---in\n> shape. This means the posterior distribution can be usefully\n> approximated by a Gaussian distribution. A Gaussian distribution is\n> convenient, because it can be completely described by only two\n> numbers: the location of its center (mean) and its spread (variance).\n\n> A Gaussian approximation is called \"quadratic approximation\" because\n> the logarithm of a Gaussian distribution forms a parabola. And a\n> parabola is a quadratic function.\n\nTwo steps: 1. Find the posterior mode with some algorithm. The procedure\ndoes not know where the peak is but it knows the slope under it feet. 2.\nEstimate the curvature near the peak to calculate a quadratic\napproximation. This computation is done by some numerical technique.\n\n##### Computing the quadratic approximation\n\nTo compute the quadratic approximation for the globe tossing data, we'll\nuse a tool in the {**rethinking**} package: `rethinking::quap()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobe.qa <- rethinking::quap(\n  alist(\n    W ~ dbinom(W + L, p), # binomial likelihood\n    p ~ dunif(0, 1) # uniform prior\n  ),\n  data = list(W = 6, L = 3)\n)\n\n# display summary of quadratic approximation\nrethinking::precis(globe.qa)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>        mean        sd      5.5%     94.5%\n#> p 0.6666668 0.1571337 0.4155368 0.9177969\n```\n\n\n:::\n:::\n\n\n::: callout-warning\n###### `precis()` results not printed correctly from visual mode\n\nThe result of `rethinking::precis()` does not display correctly after\nthe chunk in RStudio visual mode. But it works in source mode and it\ndisplayed correctly immediately after the chunk.\n\nThe columns of the table are too narrow so that you can't see the header\nand inspect the values. Printing to the console or to the web is\ncorrect.\n\nA workaround is wrapping the result with `print()` or to render the\ndocument in source mode. See my [bug\nreport](https://github.com/rstudio/rstudio/issues/13227).\n:::\n\nTo use `quap()`, you provide a *formula*, a list of *data* with\n`base::alist()`. `alist()` handles its arguments as if they described\nfunction arguments. So the values are not evaluated, and tagged\narguments with no value are allowed. It is most often used in\nconjunction with `base::formals()`.\n\nThe function `precis` presents a brief summary of the quadratic\napproximation. In this case, it shows the posterior mean value of\n$p = 0.67$, which it calls the \"Mean.\" The curvature is labeled\n\"StdDev\". This stands for *standard deviation*. This value is the\nstandard deviation of the posterior distribution, while the mean value\nis its peak. Finally, the last two values in the `precis` output show\nthe 89% percentile interval, which you'll learn more about in the next\nchapter. You can read this kind of approximation like: *Assuming the\nposterior is Gaussian, it is maximized at 0.67, and its standard\ndeviation is 0.16*.\n\n##### Computing analytical solution\n\nWe want to compare the quadratic approximation with the analytic\ncalculation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# analytic calculation\nW <- 6\nL <- 3\ncurve(dbeta(x, W + 1, L + 1), from = 0, to = 1)\n\n# quadratic approximation\ncurve(dnorm(x, 0.67, 0.16), lty = 2, add = TRUE)\n```\n\n::: {.cell-output-display}\n![](02-small-and-large-worlds_files/figure-html/analytical-calc-1.png){width=672}\n:::\n:::\n\n\nThe solid line curve is the analytical posterior and the dashed curve is\nthe quadratic approximation. The dashed curve does alright on its left\nside, but looks pretty bad on its right side. It even assigns positive\nprobability to $p = 1$, which we know is impossible, since we saw at\nleast one land sample. As the amount of data increases, however, the\nquadratic approximation gets better.\n\n##### Disadvantage\n\nThe phenomenon, where the quadratic approximation improves with the\namount of data, is very common. It's one of the reasons that so many\nclassical statistical procedures are nervous about small samples.\n\nUsing the quadratic approximation in a Bayesian context brings with it\nall the same concerns. But you can always lean on some algorithm other\nthan quadratic approximation, if you have doubts. Indeed, grid\napproximation works very well with small samples, because in such cases\nthe model must be simple and the computations will be quite fast. You\ncan also use MCMC.\n\nSometimes the quadratic approximation fails and you will get an error\nmessage about the \"Hessian\". A *Hessian* --- named after mathematician\nLudwig Otto Hesse (1811--1874) --- is a square matrix of second\nderivatives. The standard deviation is typically computed from the\nHessian, so computing the Hessian is nearly always a necessary step. But\nsometimes the computation goes wrong, and your golem will choke while\ntrying to compute the Hessian.\n\nSome other drawbacks will be explicated in later chapter. Therefore MCMC\nseems generally the best option for complex models.\n\n#### Tidyverse\n\n##### Quadratic approximation with different sample size\n\nIn the book the calculation is only done for $n = 9$ but McElreath also\ndisplay the graphs for $n = 18$ and $n = 36$ with the same proportion of\n`W` and `L`. Kurz shows how this is done and results into Figure 2.8 in\nthe book.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-quap-different-sample-size lst-cap=\"Quadratic approximation with quap() showing the effect of different sample size\"}\n### quap() with 9 sample size #################################\nglobe_qa_9 <- rethinking::quap(\n  alist(\n    W ~ dbinom(W + L, p), # binomial likelihood\n    p ~ dunif(0, 1) # uniform prior\n  ),\n  data = list(W = 6, L = 3)\n)\n\n# display summary of quadratic approximation\nrethinking::precis(globe_qa_9)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>        mean        sd      5.5%     94.5%\n#> p 0.6666666 0.1571338 0.4155364 0.9177967\n```\n\n\n:::\n\n```{.r .cell-code #lst-quap-different-sample-size lst-cap=\"Quadratic approximation with quap() showing the effect of different sample size\"}\n### quap() with 18 sample size ###################################\nglobe_qa_18 <- rethinking::quap(\n  alist(\n    W ~ dbinom(W + L, p), # binomial likelihood\n    p ~ dunif(0, 1) # uniform prior\n  ),\n  data = list(W = 12, L = 6)\n)\n\n# display summary of quadratic approximation\nrethinking::precis(globe_qa_18)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>        mean        sd      5.5%     94.5%\n#> p 0.6666662 0.1111104 0.4890902 0.8442421\n```\n\n\n:::\n\n```{.r .cell-code #lst-quap-different-sample-size lst-cap=\"Quadratic approximation with quap() showing the effect of different sample size\"}\n### quap() with 36 sample size ###################################\nglobe_qa_36 <- rethinking::quap(\n  alist(\n    W ~ dbinom(W + L, p), # binomial likelihood\n    p ~ dunif(0, 1) # uniform prior\n  ),\n  data = list(W = 24, L = 12)\n)\n\n# display summary of quadratic approximation\nrethinking::precis(globe_qa_36)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>        mean         sd      5.5%     94.5%\n#> p 0.6666665 0.07856691 0.5411014 0.7922316\n```\n\n\n:::\n:::\n\n\n::: callout-note\n## Slightly different code\n\nI used a slightly different code than Kurz. I have only changed the data\nvalues.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-fig-quadratic-approx lst-cap=\"Accuracy of the quadratic approximation\"}\nn_grid <- 100\n\n# wrangle\ntibble(w = c(6, 12, 24),\n       n = c(9, 18, 36),\n       s = c(.16, .11, .08)) %>% \n  expand_grid(p_grid = seq(from = 0, to = 1, length.out = n_grid)) %>% \n  mutate(prior = 1,\n         m     = .67)  %>%\n  mutate(likelihood = dbinom(w, size = n, prob = p_grid)) %>%\n  mutate(unstd_grid_posterior = likelihood * prior,\n         unstd_quad_posterior = dnorm(p_grid, m, s)) %>%\n  group_by(w) %>% \n  mutate(grid_posterior = unstd_grid_posterior / sum(unstd_grid_posterior),\n         quad_posterior = unstd_quad_posterior / sum(unstd_quad_posterior),\n         n              = str_c(\"n = \", n)) %>% \n  mutate(n = factor(n, levels = c(\"n = 9\", \"n = 18\", \"n = 36\"))) %>% \n  \n  # plot\n  ggplot(aes(x = p_grid)) +\n  geom_line(aes(y = grid_posterior)) +\n  geom_line(aes(y = quad_posterior),\n            color = \"grey50\") +\n  labs(x = \"proportion water\",\n       y = \"density\") +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ n, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![Accuracy of the quadratic approximation. In each plot, the exact posterior distribution is plotted as solid curve, and the quadratic approximation is plotted as the dashed curve.](02-small-and-large-worlds_files/figure-html/fig-quadratic-approx-1.png){#fig-quadratic-approx width=672}\n:::\n:::\n\n\n##### Maximum Likelihood Estimation (MLE)\n\n> The quadratic approximation, either with a uniform prior or with a lot\n> of data, is often equivalent to a maximum likelihood estimate (MLE)\n> and its standard error. The MLE is a very common non-Bayesian\n> parameter estimate. This correspondence between a Bayesian\n> approximation and a common non-Bayesian estimator is both a blessing\n> and a curse. It is a blessing, because it allows us to re-interpret a\n> wide range of published non-Bayesian model fits in Bayesian terms. It\n> is a curse, because maximum likelihood estimates have some curious\n> drawbacks, and the quadratic approximation can share them. (p. 44,\n> emphasis, in the original)\n\nTextbooks highlighting the maximum likelihood method for the generalized\nlinear model abound. If this is new to you and you'd like to learn more,\nperhaps check out Roback and Legler's (2021) Beyond multiple linear\nregression: Applied generalized linear models and multilevel models in\nR, Agresti's (2015) Foundations of linear and generalized linear models\nor Dunn and Smyth's (2018) Generalized linear models with examples in R.\n\n### Markov Chain Monte Carlo (MCMC)\n\n#### Original\n\n> There are lots of important model types, like multilevel\n> (mixed-effects) models, for which neither grid approximation nor\n> quadratic approximation is always satisfactory. ... As a result,\n> various counterintuitive model fitting techniques have arisen. The\n> most popular of these is **MARKOV CHAIN MONTE CARLO** (MCMC), which is\n> a family of conditioning engines capable of handling highly complex\n> models.\n\n> Instead of attempting to compute or approximate the posterior\n> distribution directly, MCMC techniques merely draw samples from the\n> posterior. You end up with a collection of parameter values, and the\n> frequencies of these values correspond to the posterior\n> plausibilities. You can then build a picture of the posterior from the\n> histogram of these samples.\n\nThe understanding of this not intuitive technique is postponed to\nchapter 9. What follows is just a demonstration of the technique.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## R code 2.8\nn_samples <- 1000\np <- rep(NA, n_samples)\np[1] <- 0.5\nW <- 6\nL <- 3\nfor (i in 2:n_samples) {\n  p_new <- rnorm(1, p[i - 1], 0.1)\n  if (p_new < 0) p_new <- abs(p_new)\n  if (p_new > 1) p_new <- 2 - p_new\n  q0 <- dbinom(W, W + L, p[i - 1])\n  q1 <- dbinom(W, W + L, p_new)\n  p[i] <- ifelse(runif(1) < q1 / q0, p_new, p[i - 1])\n}\n\n## R code 2.9\nrethinking::dens(p, xlim = c(0, 1))\ncurve(dbeta(x, W + 1, L + 1), lty = 2, add = TRUE)\n```\n\n::: {.cell-output-display}\n![Demo of the Markov Chain Monte Carlo (MCMC) method using the globe-tossing data and calculated and diplayed with the {**rethinking**} package.](02-small-and-large-worlds_files/figure-html/fig-demo-MCMC-rethinking-1.png){#fig-demo-MCMC-rethinking width=672}\n:::\n:::\n\n\nIt's weird. But it works. The above **METROPOLIS ALGORITHM** is\nexplained in Chapter 9.\n\n#### Tidyverse\n\nThe {**brms**} package uses a version of MCMC to fit Bayesian models.\nbrms stands for Bayesian Regression Models using 'Stan'.\n\nSince one of the main goals of \\[the Kurz version of SR2\\] is to\nhighlight {**brms**}, we may as well fit a model. This seems like an\nappropriately named subsection to do so. First we'll have to load the\npackage. (If you haven't already installed {**brms**}, you can find\ninstructions on how to do on\n[GitHub](https://github.com/paul-buerkner/brms#how-do-i-install-brms) or\non the [corresponding\nwebsite](https://paul-buerkner.github.io/brms/).)As an exercise we will\nre-fit the model with $W = 24$ and $n = 36$ of\n@lst-quap-different-sample-size and @lst-fig-quadratic-approx.\n\nBut before we use {**brms**} we need to detach the {**rethinking**}\npackage.\n\n> **R** will not allow us to use a function from one package that shares\n> the same name as a different function from another package if both\n> packages are open at the same time. The **rethinking** and **brms**\n> packages are designed for similar purposes and, unsurprisingly,\n> overlap in some of their function names. To prevent problems, we will\n> always make sure **rethinking** is detached before using **brms**. To\n> learn more on the topic, see [this R-bloggers\n> post](https://www.r-bloggers.com/2015/04/r-and-package-masking-a-real-life-example/).\n> (This remark comes from [section 4.3.1 of the Kurz\n> version](https://bookdown.org/content/4857/geocentric-models.html#the-data)).\n\nThis is the reason why I have not loading the rethinking packages in\ncode chunks of this file. Instead I referred to every function of the\n{**rethinking**} packages directly adding `rethinking::` before the\nfunction call. This has also the advantage to learn which functions come\nfrom {**rethinking**}.\n\n::: callout-warning\n## First compiling\n\nBe patient when you render the following chunk the first time. It need\nsome time. Furthermore it results in a very long processing message\nunder the compiled chunk. Again this happens only the first because the\nresult is stored in the \"fits\" folder which you have to create before\nrunning the chunk.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-demo-brms lst-cap=\"Demonstration of the `brm()` function of the {**brms**} package.\"}\nb2.1 <-\n  brms::brm(data = list(w = 24), \n      family = binomial(link = \"identity\"),\n      w | trials(36) ~ 0 + Intercept,\n      brms::prior(beta(1, 1), class = b, lb = 0, ub = 1),\n      seed = 2,\n      file = \"fits/b02.01\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code #lst-print-demo-brms lst-cap=\"Print the result of the demo of the {**brms**} package.\"}\nprint(b2.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>  Family: binomial \n#>   Links: mu = identity \n#> Formula: w | trials(36) ~ 0 + Intercept \n#>    Data: list(w = 24) (Number of observations: 1) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept     0.66      0.08     0.50     0.80 1.00     1537     1456\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\nA detailed explanation is postponed to chapter 4. Here I will just copy\nthe notes by Kurz to get a first understand and a starting point for a\nlater further exploration.\n\n> For now, focus on the 'Intercept' line. As we'll also learn in Chapter\n> 4, the intercept of a typical regression model with no predictors is\n> the same as its mean. In the special case of a model using the\n> binomial likelihood, the mean is the probability of a 1 in a given\n> trial, $\\theta$.\n>\n> Also, with {**brms**}, there are many ways to summarize the results of\n> a model. The `brms::posterior_summary()` function is an analogue to\n> `rethinking::precis()`. We will, however, need to use `round()` to\n> reduce the output to a reasonable number of decimal places.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrms::posterior_summary(b2.1) %>% \n  round(digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>             Estimate Est.Error  Q2.5 Q97.5\n#> b_Intercept     0.66      0.08  0.50  0.80\n#> lprior          0.00      0.00  0.00  0.00\n#> lp__           -3.98      0.75 -6.03 -3.46\n```\n\n\n:::\n:::\n\n\n> The `b_Intercept` row is the probability. Don't worry about the second\n> line, for now. We'll cover the details of {**brms}** model fitting in\n> later chapters. To finish up, why not plot the results of our model\n> and compare them with those from `rethinking::quap()`, above? (See\n> @fig-demo-MCMC-rethinking)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrms::as_draws_df(b2.1) %>% \n  mutate(n = \"n = 36\") %>%\n  \n  ggplot(aes(x = b_Intercept)) +\n  geom_density(fill = \"black\") +\n  scale_x_continuous(\"proportion water\", limits = c(0, 1)) +\n  theme(panel.grid = element_blank()) +\n  facet_wrap(~ n)\n```\n\n::: {.cell-output-display}\n![Demo of the Markov Chain Monte Carlo (MCMC) method using the globe-tossing data with the {**brms**} package.](02-small-and-large-worlds_files/figure-html/fig-demo-MCMC-brms-1.png){#fig-demo-MCMC-brms width=672}\n:::\n:::\n\n\n> If you're still confused, cool. This is just a preview. We'll start\n> walking through fitting models with **brms** in [Chapter\n> 4](https://bookdown.org/content/4857/geocentric-models.html#geocentric-models)\n> and we'll learn a lot about regression with the binomial likelihood in\n> [Chapter\n> 11](https://bookdown.org/content/4857/god-spiked-the-integers.html#god-spiked-the-integers).\n\n#### Reconsideration\n\n##### Bayesian Inference: Some Lessons to Draw\n\nThe following list summarizes differences between Bayesian and\nNon-Bayesian inference (\"Frequentism\"):\n\n1.  **No minimum sampling size**: The minimum sampling size in Bayesian\n    inference is one. You are going to update each data point at its\n    time. For instance you got an estimate every time when you toss the\n    globe and the estimate is updated. --- Well, the sample size of one\n    is not very informative but that is the power of Bayesian inference\n    in not getting over confident. It is always accurately representing\n    the relative confidence of plausability we should assign to each of\n    the possible proportions.\n2.  **Shape embodies sample size**: \\*The shape of the posterior\n    distribution embodies all the information that the sample has about\n    the process of the proportions. Therefore you do not need to go back\n    to the original dataset for new observations. Just take the\n    posterior distribution and update it by multiplying the number of\n    ways the new data could produce.\n3.  **No point estimates**: The estimate is the whole distribution. It\n    may be fine for communication purposes to talk about some summary\n    points of the distribution like the mode and mean. But neither of\n    these points is special as a point of estimate. When we do\n    calculations we draw predictions from the whole distribution, never\n    just from a point of it.\n4.  **No one true interval**: Intervals are not important in Bayesian\n    inference. They are merely summaries of the shape of the\n    distribution. There is nothing special in any of these intervals\n    because the endpoints of the intervals are not special. Nothing\n    happens of the endpoints of the intervals because the interval is\n    arbitrary. (The 95% in Non-Bayesian inference is essentially a\n    dogma, a superstition. Even in Non-Bayesian statistics it is\n    conceptualized as an arbitrary interval.)\n\n## Synopsis\n\n### Small worlds and the garden of forking data\n\nThe chapter starts to build Bayesian models and is focused on the small\nworld. It explains probability theory in its essential form: counting\nthe ways things can happen. This is shown with **the garden of forking\ndata**.\n\nBayesian inference is really just counting and comparing of\npossibilities. ... In order to make good inference about what actually\nhappened, it helps to consider everything that could have happened. A\nBayesian analysis is a garden of forking data, in which alternative\nsequences of events are cultivated. As we learn about what did happen,\nsome of these alternative sequences are pruned. In the end, what remains\nis only what is logically consistent with our knowledge.\n\n#### Bayesian updating: Counting\n\nWe may have additional information about the relative plausibility of\neach conjecture. This information could arise from knowledge of how the\ncontents of the bag were generated. It could also arise from previous\ndata. Whatever the source, it would help to have a way to combine\ndifferent sources of information to update the plausibilities. Luckily\nthere is a natural solution: Just multiply the counts. ...\nMultiplication is just a shortcut to enumerating and counting up all of\nthe paths through the garden that could produce all the observations.\n\nA Bayesian model begins with one set of plausibilities assigned to each\nof these possibilities. These are the prior plausibilities. Then it\nupdates them in light of the data, to produce the posterior\nplausibilities. This updating process is a kind of learning, called\nBayesian updating.\n\nHow to start the updating process?\n\nWhen there is no reason to say that one conjecture is more plausible\nthan another, weigh all of the conjectures equally.\n\n#### From counts to probability\n\nWhen we don't know what caused the data, potential causes that may\nproduce the data in more ways are more plausible.\n\n### Building a model\n\n#### Data story\n\nBayesian data analysis usually means producing a story for how the data\ncame to be. This story may be descriptive, specifying associations that\ncan be used to predict outcomes, given observations. Or it may be\ncausal, a theory of how some events produce other events.\n\n#### Bayesian updating again\n\nIn contrast to non-Bayesian statistical inference with it widespread\nsuperstition that 30 observations are needed before one can use a\nGaussian distribution (so-called *asymptotic* behavior), Bayesian\nestimates are valid for any sample size.\n\nThis does not mean that more data isn't helpful---it certainly is.\nRather, the estimates have a clear and valid interpretation, no matter\nthe sample size. But the price for this power is dependency upon the\ninitial plausibilities, the prior. If the prior is a bad one, then the\nresulting inference will be misleading.\n\n#### Evaluation\n\n1.  Model's certainty (curves narrow and tall) is no guarantee that the\n    model is a good one, because the inferences are conditional on the\n    model. Under a different model, things might look differently.\n2.  It is important to supervise and critique your model's work and to\n    check the model's inferences in light of aspects of the data it does\n    not know about. This usually means asking and answering additional\n    questions, beyond those that originally constructed the model.\n\n### Components of the model\n\n-   **List variables**, observed ones (data) and unobserved ones\n    (parameter like proportions of observed variables).\n-   **Define variables** by building a model that relates variables to\n    one another.\n    -   **Observed variables**: We don't have to literally count, we can\n        use a mathematical function that tells us the right\n        plausibility, the likelihood. For example in the case of the\n        globe tossing model this is the *binomial distribution*.\n    -   **Unobserved variables (parameters)**: In statistical modeling,\n        many of the most common questions we ask about data are answered\n        directly by parameters. For every parameter we intend our\n        Bayesian machine to consider, we must provide a distribution of\n        prior plausibility, its prior. Because the prior is an\n        assumption, it should be interrogated like other assumptions: by\n        altering it and checking how sensitive inference is to the\n        assumption.\n\n### Making the model go\n\nAs mathematical rules often cannot be conditioned formally or you have\nto constrain your choice of prior to special forms that are easy to do\nmathematics with, we need numerical techniques to approximate the\nmathematics.\n\n#### Grid approximation\n\nWe can achieve an excellent approximation of the continuous posterior\ndistribution by considering only a finite grid of parameter values. At\nany particular value of a parameter, `p`, it's a simple matter to\ncompute the posterior probability: just multiply the prior probability\nof `p` by the likelihood at `p`. Repeating this procedure for each value\nin the grid generates an approximate picture of the exact posterior\ndistribution.\n\n##### Base R\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-grid-approx-base-demo lst-cap=\"Five steps of the grid approximation technique using only functions of base R\"}\n## R code 2.3 ###########################################\n\np_grid <- seq(from = 0, to = 1, length.out = 20)    # <1>\nprior <- rep(1, 20)                                 # <2>\nlikelihood <- dbinom(6, size = 9, prob = p_grid)    # <3>\nunstd.posterior <- likelihood * prior               # <4>\nposterior <- unstd.posterior / sum(unstd.posterior) # <5>\n```\n:::\n\n\n1.  Define the grid. This means you decide how many points to use in\n    estimating the posterior, and then you make a list of the parameter\n    values on the grid.\n2.  Compute the value of the prior at each parameter value on the grid.\n3.  Compute the likelihood at each parameter value.\n4.  Compute the unstandardized posterior at each parameter value, by\n    multiplying the prior by the likelihood.\n5.  Finally, standardize the posterior, by dividing each value by the\n    sum of all values.\n\n##### Tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-grid-approx-tidyverse-demo lst-cap=\"Five steps of the grid approximation technique using tidyverse functions\"}\n(\n    df <-\n        tibble(p_grid = seq(from = 0, to = 1, length.out = 20),      # <1>\n               prior  = 1) %>%                                       # <2>\n        mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %>%  # <3>\n        mutate(unstd_posterior = likelihood * prior) %>%             # <4>\n        mutate(posterior = unstd_posterior / sum(unstd_posterior))   # <5>\n)\n```\n:::\n\n\n1.  Define grid.\n2.  Define prior.\n3.  Compute likelihood at each value in grid.\n4.  Compute product of likelihood and prior.\n5.  Standardize the posterior.\n\n#### Quadratic approximation\n\nGrid approximation has the disadvantage that the number of unique values\nto consider in the grid grows rapidly as the number of parameters in the\nmodel increases. In this case quadratic approximation may be an\nalternative.\n\nUnder quite general conditions, the region near the peak of the\nposterior distribution will be nearly Gaussian---or \"normal\"---in shape.\nThis means the posterior distribution can be usefully approximated by a\nGaussian distribution. A Gaussian distribution is convenient, because it\ncan be completely described by only two numbers: the location of its\ncenter (mean) and its spread (variance).\n\nA Gaussian approximation is called \"quadratic approximation\" because the\nlogarithm of a Gaussian distribution forms a parabola. And a parabola is\na quadratic function.\n\nTwo steps of the quadratic approximation technique:\n\n1.  Find the posterior mode. This is usually accomplished by some\n    optimization algorithm, a procedure that virtually \"climbs\" the\n    posterior distribution, as if it were a mountain. The golem doesn't\n    know where the peak is, but it does know the slope under its feet.\n    There are many well-developed optimization procedures, most of them\n    more clever than simple hill climbing. But all of them try to find\n    peaks.\n2.  Once you find the peak of the posterior, you must estimate the\n    curvature near the peak. This curvature is sufficient to compute a\n    quadratic approximation of the entire posterior distribution. In\n    some cases, these calculations can be done analytically, but usually\n    your computer uses some numerical technique instead.\n\n##### Rethinking\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-quadratic-approx-demo lst-cap=\"Two steps of the quadratic approximation technique\"}\nglobe_qa <- rethinking::quap( # <1>\n  alist(                      # <2>\n    W ~ dbinom(W + L, p),     # <3> \n    p ~ dunif(0, 1)           # <4> \n  ), \n  data = list(W = 6, L = 3)   # <5>\n)\n\nrethinking::precis(globe_qa)  # <6>\n```\n:::\n\n\n1.  To compute the quadratic approximation, the book will use as tool\n    the `rethinking::quap()` function in the rethinking package. With\n    `quap()` you will find the mode of posterior distribution and\n    produce an approximation of the full posterior using the quadratic\n    curvature at the mode.\n2.  To use `quap()`, you provide a formula, a list of data. The formula\n    defines the probability of the data and the prior. `base::alist()`\n    handles its arguments as if they described function arguments. So\n    the values are not evaluated, and tagged arguments with no value are\n    allowed whereas list simply ignores them. `alist()` is most often\n    used in conjunction with formulae.\n3.  Provide formula for the calculation of the binomial likelihood.\n4.  Provide the formula for the prior probability. In this example we\n    choose the uniform distribution between 0 and 1.\n5.  Provide the data for the calculation as a list of values. The chosen\n    names (symbols) correspond to the formula.\n6.  The function `rethinking::precis()` presents a brief summary of the\n    quadratic approximation.\n\nMcElreath is using the quadratic approximation for much of the first\nhalf of this book. For many of the most common procedures in applied\nstatistics---linear regression, for example---the approximation works\nvery well. Often, it is even exactly correct, not actually an\napproximation at all. Computationally, quadratic approximation is very\ninexpensive, at least compared to grid approximation and MCMC.\n\nAdditionally of using `rethinking::quap()`, I will also provide the\n`brms::brm()` alternative as suggested by the {**tidyverse**} version of\nKurz.\n\n#### Markov chain Monte Carlo (MCMC)\n\nThere are lots of important model types, like multilevel (mixed-effects)\nmodels, for which neither grid approximation nor quadratic approximation\nis always satisfactory. ... As a result, various counterintuitive model\nfitting techniques have arisen. The most popular of these is MCMC, which\nis a family of conditioning engines capable of handling highly complex\nmodels.\n\nThe conceptual challenge with MCMC lies in its highly non-obvious\nstrategy. Instead of attempting to compute or approximate the posterior\ndistribution directly, MCMC techniques merely draw samples from the\nposterior. You end up with a collection of parameter values, and the\nfrequencies of these values correspond to the posterior plausibilities.\nYou can then build a picture of the posterior from the histogram of\nthese samples.\n\n##### Base R and Rethinking\n\nI will add here without comments a demonstration of this important\ntechnique. In later chapters we will learn more about it.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-demo-MCMC lst-cap=\"Demonstration how to estimate the posterior with MCMC\"}\n## R code 2.8 ####################################\nn_samples <- 1000\np <- rep(NA, n_samples)\np[1] <- 0.5\nW <- 6\nL <- 3\nfor (i in 2:n_samples) {\n  p_new <- rnorm(1, p[i - 1], 0.1)\n  if (p_new < 0) p_new <- abs(p_new)\n  if (p_new > 1) p_new <- 2 - p_new\n  q0 <- dbinom(W, W + L, p[i - 1])\n  q1 <- dbinom(W, W + L, p_new)\n  p[i] <- ifelse(runif(1) < q1 / q0, p_new, p[i - 1])\n}\n\n## R code 2.9 ####################################\nrethinking::dens(p, xlim = c(0, 1))\ncurve(dbeta(x, W + 1, L + 1), lty = 2, add = TRUE)\n```\n:::\n\n\n##### brms\n\nAgain: The following demo comes without comments. It just will give a\nfeeling of the structure of the used function. Later I will dive with\nKurz into the details how to apply the functions of the {**brms**}\npackage.\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-demo-using-brms lst-cap=\"Demonstration of using {brms}\"}\nb2.1 <-\n  brms::brm(data = list(w = 24), \n      family = binomial(link = \"identity\"),\n      w | trials(36) ~ 0 + Intercept,\n      brms::prior(beta(1, 1), class = b, lb = 0, ub = 1),\n      seed = 2,\n      file = \"fits/b02.01\")\n\nprint(b2.1)\n\nbrms::posterior_summary(b2.1) %>% \n  round(digits = 2)\n```\n:::\n\n\n## Practice\n\n## I STOPPED HERE! (1st. pass, 2023-08-01) TO BE CONTINUED {.unnumbered}\n",
    "supporting": [
      "02-small-and-large-worlds_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
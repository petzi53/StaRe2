{
  "hash": "6020fa39623ab47cab692524544f7912",
  "result": {
    "markdown": "# Sampling the Imaginary {#sec-sampling-the-imaginary}\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: setup\n\nlibrary(tidyverse)\n```\n````\n\n```\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1     \n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n\n## Vampire Example {.unnumbered}\n\n### Original {.unnumbered}\n\n#### a) Medical Test Scenario with Bayes theorem {.unnumbered}\n\n1.  Suppose there is a blood test that correctly detects vampirism 95%\n    of the time.$Pr(positivetest|vampire) = 0.95$.\n2.  It's a very accurate test, nearly always catching real vampires. It\n    also make mistakes, though, in the form of false positives. One\n    percent of the time, it incorrectly diagnoses normal people as\n    vampires, $Pr(positive test result| mortal) = 0.01$.\n3.  The final bit of information we are told is that vampires are rather\n    rare, being only 0.1% of the population, implying\n    $Pr(vampire) = 0.001$.\n\nSuppose now that someone tests positive for vampirism. What's the\nprobability that he or she is a bloodsucking immortal?\n\nThe correct approach is just to use Bayes' theorem to invert the\nprobability, to compute $Pr(vampire | positive)$.\n\n$$\n\\Pr(\\text{vampire}|\\text{positive}) = \\frac{\\Pr(\\text{positive}|\\text{vampire})\\Pr(\\text{vampire})}{\\Pr(\\text{positive})}.\n$$\n\nwhere $Pr(positive)$ is the average probability of a positive test\nresult, that is,\n\n$$\nPr(positive) = Pr(positive|vampire)Pr(vampire) + Pr(positive|mortal)Pr(1-vampire)\n$$\n\n\n::: {.cell}\n\n````{.cell-code #lst-bayes-scenario lst-cap=\"Calculated with the Bayes formula\"}\n```{{r}}\n#| label: scenario-bayes-theorem-a\n#| attr-source: '#lst-bayes-scenario lst-cap=\"Calculated with the Bayes formula\"'\n\n## R code 3.1\nPr_Positive_Vampire_a <- 0.95\nPr_Positive_Mortal_a <- 0.01\nPr_Vampire_a <- 0.001\nPr_Positive_a <- Pr_Positive_Vampire_a * Pr_Vampire_a +\n  Pr_Positive_Mortal_a * (1 - Pr_Vampire_a)\n(Pr_Vampire_Positive_a <- Pr_Positive_Vampire_a * Pr_Vampire_a / Pr_Positive_a)\n```\n````\n\n```\n#> [1] 0.08683729\n```\n:::\n\n\nThere is only an 8.7% chance that the suspect is actually a vampire.\n\n> Most people find this result counterintuitive. And it's a very\n> important result, because it mimics the structure of many realistic\n> testing contexts, such as HIV and DNA testing, criminal profiling, and\n> even statistical significance testing ... . Whenever the condition of\n> interest is very rare, having a test that finds all the true cases is\n> still no guarantee that a positive result carries much information at\n> all. The reason is that most positive results are false positives,\n> even when all the true positives are detected correctly.\n\n#### b) Medical test scenario with natural frequencies {.unnumbered}\n\n```         \n(1)  In a population of 100,000 people, 100 of them are vampires.\n(2)  Of the 100 who are vampires, 95 of them will test positive for vampirism.\n(3)  Of the 99,900 mortals, 999 of them will test positive for vampirism.\n```\n\nThere are 999 + 95 = 1094 people tested positive. But from these\npeople only 95 / (999 + 95) = 8.6837294 % are actually\nvampires.\n\nOr with a slightly different wording it is still easier to\nunderstand: 1. We can just count up the number of people who test\npositive: $95 + 999 = 1094$. 2. Out of these $1094$ positive tests, $95$\nof them are real vampires, so that implies:\n\n$$\nPR(positive|vampire) = \\frac{95}{1094}\n$$\n\n\n::: {.cell}\n\n````{.cell-code #lst-common-sense-scenario lst-cap=\"Calculated with natural figures instead propabilities\"}\n```{{r}}\n#| label: scenario-common-sense-a\n#| attr-source: '#lst-common-sense-scenario lst-cap=\"Calculated with natural figures instead propabilities\"'\n\n95/1094\n```\n````\n\n```\n#> [1] 0.08683729\n```\n:::\n\n\nThe second presentation of the problem, using counts rather than\nprobabilities, is often called the *frequency format* or *natural\nfrequencies*. It is easier for people to understand because are\nconfronted with count in everyday life. Nobody has ever seen a\nprobability.\n\n::: callout-note\n## Meta remark: Study guide\n\nThis chapter teaches the basic skills for working with samples from the\nposterior distribution. We'll begin to use samples to summarize and\nsimulate model output. The skills learned here will apply to every\nproblem in the remainder of the book, even though the details of the\nmodels and how the samples are produced will vary.\n\nThe chapter exploits the fact that people are better in counts than in\nprobabilities. We will take the probability distributions from the\nprevious chapter and sampling from them to produce counts.\n:::\n\n> The posterior distribution is a probability distribution. And like all\n> probability distributions, we can imagine drawing *samples* from it.\n> The sampled events in this case are parameter values. Most parameters\n> have no exact empirical realization. The Bayesian formalism treats\n> parameter distributions as relative plausibility, not as any physical\n> random process. In any event, randomness is always a property of\n> information, never of the real world. But inside the computer,\n> parameters are just as empirical as the outcome of a coin flip or a\n> die toss or an agricultural experiment. The posterior defines the\n> expected frequency that different parameter values will appear, once\n> we start plucking parameters out of it.\n\nThere are two reasons more to use samples:\n\n1.  First, many scientists are uncomfortable with integral calculus,\n    even though they have strong and valid intuitions about how to\n    summarize data. Working with samples transforms a problem in\n    calculus into a problem in data summary, into a frequency format\n    problem. An integral in a typical Bayesian context is just the total\n    probability in some interval.\n2.  Second, some of the most capable methods of computing the posterior\n    produce nothing but samples. Many of these methods are variants of\n    Markov chain Monte Carlo techniques (MCMC).\n\nDrawing samples from the very simple posterior distribution of the\nglobe-tossing model might seem as overkill but using this technique from\nthe start has educational impact: When you inevitably must fit a model\nto data using MCMC, you will already know how to make sense of the\noutput.\n\n### Tidyverse {.unnumbered}\n\n#### a) Medical Test Scenario with Bayes theorem {.unnumbered}\n\nIf you would like to know the probability someone is a vampire given\nthey test positive to the blood-based vampire test, you compute\n\n$$\nPr(vampire|positive) = \\frac{Pr(positive|vampire) \\times Pr(vampire)} {Pr(positive)}\n$$ This is Bayes theorem.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: scenario-bayes-theorem-b\n\ntibble(pr_positive_vampire_b = .95,\n       pr_positive_mortal_b  = .01,\n       pr_vampire_b          = .001) %>% \n  mutate(pr_positive_b = pr_positive_vampire_b * pr_vampire_b + pr_positive_mortal_b * (1 - pr_vampire_b)) %>% \n  mutate(pr_vampire_positive_b = pr_positive_vampire_b * pr_vampire_b / pr_positive_b) %>% \n  glimpse()\n```\n````\n\n```\n#> Rows: 1\n#> Columns: 5\n#> $ pr_positive_vampire_b <dbl> 0.95\n#> $ pr_positive_mortal_b  <dbl> 0.01\n#> $ pr_vampire_b          <dbl> 0.001\n#> $ pr_positive_b         <dbl> 0.01094\n#> $ pr_vampire_positive_b <dbl> 0.08683729\n```\n:::\n\n\n#### b) Medical test scenario with natural frequencies {.unnumbered}\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: scenario-natural-frequencies\n\ntibble(pr_vampire_b2          = 100 / 100000,\n       pr_positive_vampire_b2 = 95 / 100,\n       pr_positive_mortal_b2  = 999 / 99900) %>% \n  mutate(pr_positive_b2 = 95 + 999) %>% \n  mutate(pr_vampire_positive_b2 = pr_positive_vampire_b2 * 100 / pr_positive_b2) %>% \n  glimpse()\n```\n````\n\n```\n#> Rows: 1\n#> Columns: 5\n#> $ pr_vampire_b2          <dbl> 0.001\n#> $ pr_positive_vampire_b2 <dbl> 0.95\n#> $ pr_positive_mortal_b2  <dbl> 0.01\n#> $ pr_positive_b2         <dbl> 1094\n#> $ pr_vampire_positive_b2 <dbl> 0.08683729\n```\n:::\n\n\n## Sampling from a grid-approximate posterior\n\n### Original\n\n#### Grid approximation\n\nBefore we are going to draw samples from the posterior distribution we\nneed to compute the distribution. Again we are using grid approximation.\n\n\n::: {.cell}\n\n````{.cell-code #lst-grid-approx-a lst-cap=\"Generate the posterior distribution form the globe-tossing example\"}\n```{{r}}\n#| label: grid-approx-a\n#| attr-source: '#lst-grid-approx-a lst-cap=\"Generate the posterior distribution form the globe-tossing example\"'\n\n### R code 3.2 ##########################\n# change prob_b to prior\n# change prob_data to likelihood\n# added variables: n, n_success, n_trials\n\nn_grid_a <- 1000L\nn_success_a <- 6L\nn_trials_a <-  9L\n\n\np_grid_a <- seq(from = 0, to = 1, length.out = n_grid_a)\nprior_a <- rep(1, n_grid_a) # = prior, assumed as uniform distribution\nlikelihood_a <- dbinom(n_success_a, size = n_trials_a, prob = p_grid_a) # = likelihood\nposterior_a <- likelihood_a * prior_a\nposterior_a <- posterior_a / sum(posterior_a)\n```\n````\n:::\n\n\n> Now we wish to draw 10,000 samples from this posterior. Imagine the\n> posterior is a bucket full of parameter values, numbers such as 0.1,\n> 0.7, 0.5, 1, etc. Within the bucket, each value exists in proportion\n> to its posterior probability, such that values near the peak are much\n> more common than those in the tails. We're going to scoop out 10,000\n> values from the bucket. Provided the bucket is well mixed, the\n> resulting samples will have the same proportions as the exact\n> posterior density. Therefore the individual values of *p* will appear\n> in our samples in proportion to the posterior plausibility of each\n> value.\n\n#### Drawing samples\n\n\n::: {.cell}\n\n````{.cell-code #lst-draw-samples-a lst-cap=\"Draw 1000 Samples from the posterior distribution, using `base::set.seed(3)`\"}\n```{{r}}\n#| label: draw-samples-a\n#| attr-source: '#lst-draw-samples-a lst-cap=\"Draw 1000 Samples from the posterior distribution, using `base::set.seed(3)`\"'\n\nn_samples_a <- 1e4\n\nset.seed(3) # <1>\n\n## R code 3.3 ##########################################\nsamples_a <- sample(p_grid_a, prob = posterior_a, \n                    size = n_samples_a, replace = TRUE) # <2>\n```\n````\n:::\n\n\n1.  I have included the `base::set.seed()` command myself to provide\n    reproducibility. With this code line you will get the same results\n    in your sampling process because it is not really a random procedure\n    but the outcome of a complex algorithm that can be configured by the\n    `set.seed()` function.\n2.  The workhorse here is `base::sample`, which randomly pulls values\n    from a vector. The vector in this case is `p_grid_a`, the grid of\n    1000 (1e3) parameter values. The probability of each value is given\n    by `posterior_a`, which we computed with @lst-grid-approx-a.\n\nTo compare the calculated values with variant b (the tidyverse version),\nI bound the three vectors with `base::cbind()` together into a matrix\nand displayed the first six lines with `utils::head()`. Additionally I\nalso displayed the first 10 values of `samples_a` vector.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: display-grid_result_a\n\n# display grid results to compare with variant b\nd_a <- cbind(p_grid_a, prior_a, likelihood_a, posterior_a) \nhead(d_a, 10)\n```\n````\n\n```\n#>          p_grid_a prior_a likelihood_a  posterior_a\n#>  [1,] 0.000000000       1 0.000000e+00 0.000000e+00\n#>  [2,] 0.001001001       1 8.425225e-17 8.433659e-19\n#>  [3,] 0.002002002       1 5.375951e-15 5.381333e-17\n#>  [4,] 0.003003003       1 6.105137e-14 6.111249e-16\n#>  [5,] 0.004004004       1 3.419945e-13 3.423368e-15\n#>  [6,] 0.005005005       1 1.300676e-12 1.301978e-14\n#>  [7,] 0.006006006       1 3.872087e-12 3.875963e-14\n#>  [8,] 0.007007007       1 9.734489e-12 9.744233e-14\n#>  [9,] 0.008008008       1 2.162473e-11 2.164638e-13\n#> [10,] 0.009009009       1 4.370695e-11 4.375070e-13\n```\n:::\n\n\nFurthermore I also displayed the first 10 values of `samples_a` vector.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: display-samples_result_a\n\n# display sample results to compare with variant b\nhead(samples_a, 10)\n```\n````\n\n```\n#>  [1] 0.5645646 0.6516517 0.5475475 0.5905906 0.5955956 0.7877878 0.7267267\n#>  [8] 0.4914915 0.7507508 0.4494494\n```\n:::\n\n\n#### Plot samples distribution\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-scatterplot-samples-a\n#| fig-cap: \"Scatterplot of the drawn samples (Version a)\"\n\n## R code 3.4 #########\nplot(samples_a)\n```\n````\n\n::: {.cell-output-display}\n![Scatterplot of the drawn samples (Version a)](03-sampling-the-imaginary_files/figure-html/fig-scatterplot-samples-a-1.png){#fig-scatterplot-samples-a width=672}\n:::\n:::\n\n\nIn @fig-scatterplot-samples-a, it is as if you are flying over the\nposterior distribution, looking down on it. There are many more samples\nfrom the dense region near 0.6 and very few samples below 0.25.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-density-samples-a\n#| fig-cap: \"Density estimate of the drawn samples (Version a)\"\n\n## R code 3.5 #############\nrethinking::dens(samples_a)\n```\n````\n\n::: {.cell-output-display}\n![Density estimate of the drawn samples (Version a)](03-sampling-the-imaginary_files/figure-html/fig-density-samples-a-1.png){#fig-density-samples-a width=672}\n:::\n:::\n\n\nPlot @fig-density-samples-a shows the density estimate computed from our\nsampling process. The estimated density is very similar to ideal\nposterior you computed via grid approximation. If you draw even more\nsamples, maybe 1e5 or 1e6, the density estimate will get more and more\nsimilar to the ideal. This is shown in the tidyverse version\n@fig-density-samples-b2.\n\n### Tidyverse\n\n#### Grid approximation\n\n\n::: {.cell}\n\n````{.cell-code #lst-grid-approx-b lst-cap=\"Grid aproximation\"}\n```{{r}}\n#| label: grid-approx-b\n#| attr-source: '#lst-grid-approx-b lst-cap=\"Grid aproximation\"'\n\n# how many grid points would you like?\nn_grid_b <- 1000L\nn_success_b <- 6L\nn_trials_b  <- 9L\n\n(\n  d_b <-\n  tibble(p_grid_b = seq(from = 0, to = 1, length.out = n_grid_b),\n         # note we're still using a flat uniform prior\n         prior_b  = 1) %>% \n  mutate(likelihood_b = dbinom(n_success_b, size = n_trials_b, prob = p_grid_b)) %>% \n  mutate(posterior_b = (likelihood_b * prior_b) / sum(likelihood_b * prior_b))\n)\n```\n````\n\n```\n#> # A tibble: 1,000 × 4\n#>    p_grid_b prior_b likelihood_b posterior_b\n#>       <dbl>   <dbl>        <dbl>       <dbl>\n#>  1  0             1     0           0       \n#>  2  0.00100       1     8.43e-17    8.43e-19\n#>  3  0.00200       1     5.38e-15    5.38e-17\n#>  4  0.00300       1     6.11e-14    6.11e-16\n#>  5  0.00400       1     3.42e-13    3.42e-15\n#>  6  0.00501       1     1.30e-12    1.30e-14\n#>  7  0.00601       1     3.87e-12    3.88e-14\n#>  8  0.00701       1     9.73e-12    9.74e-14\n#>  9  0.00801       1     2.16e-11    2.16e-13\n#> 10  0.00901       1     4.37e-11    4.38e-13\n#> # ℹ 990 more rows\n```\n:::\n\n\n#### Drawing samples\n\n::: callout-caution\n###### Changing variable names\n\nWe've renamed McElreath's `prob_p` and `prob_data` as `prior_b` and\n`likelihood_b`, respectively. Now we'll use the `dplyr::slice_sample()`\nfunction to sample rows from `d_b`, saving them as `samples_b`.\n\nTo get the same variable name of the sample results in version a and b I\nwill change in the following code chunk `p_grid_b` to `samples_b`. So I\ncan compare the vector `samples_a` with the column `samples_b`of the\ntibble with the same name (`samples_b`).\n\nAdditionally: To see the difference between grid and samples I will add\n\"\\_sample\" to all the other variable names. For reasons of consistence I\nwill also change the name of \"prior_b\"\n:::\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: draw-samples-b\n\n# how many samples would you like?\nn_samples_b <- 1e4\n\n# make it reproducible\nset.seed(3)\n\nsamples_b <-\n  d_b %>% \n    slice_sample(n = n_samples_b, weight_by = posterior_b, replace = T)\n\n\n( \n    samples_b <- samples_b |>\n        rename(samples_b = p_grid_b,\n               likelihood_samples_b = likelihood_b,\n               prior_samples_prior_b = prior_b,\n               posterior_samples_b = posterior_b)\n)\n```\n````\n\n```\n#> # A tibble: 10,000 × 4\n#>    samples_b prior_samples_prior_b likelihood_samples_b posterior_samples_b\n#>        <dbl>                 <dbl>                <dbl>               <dbl>\n#>  1     0.565                     1                0.225             0.00225\n#>  2     0.652                     1                0.272             0.00272\n#>  3     0.548                     1                0.210             0.00210\n#>  4     0.591                     1                0.245             0.00245\n#>  5     0.596                     1                0.248             0.00248\n#>  6     0.788                     1                0.192             0.00192\n#>  7     0.727                     1                0.253             0.00253\n#>  8     0.491                     1                0.156             0.00156\n#>  9     0.751                     1                0.233             0.00233\n#> 10     0.449                     1                0.116             0.00116\n#> # ℹ 9,990 more rows\n```\n:::\n\n\nThe column `samples_b` of the tibble with the same name (watch out not\nto confuse these two objects) is identical with the vector `samples_a`.\nThis `base::identical()` results in different sampling processes was the\nwork of `set.seed(3)`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: test-if-samples-identical\n\nidentical(samples_a, samples_b$samples_b)\n```\n````\n\n```\n#> [1] TRUE\n```\n:::\n\n\nWith the `utils::str()` function you will get a result with shorter\nfigures that is better adapted to a small screen.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: str-samples-b\n\nstr(samples_b)\n```\n````\n\n```\n#> tibble [10,000 × 4] (S3: tbl_df/tbl/data.frame)\n#>  $ samples_b            : num [1:10000] 0.565 0.652 0.548 0.591 0.596 ...\n#>  $ prior_samples_prior_b: num [1:10000] 1 1 1 1 1 1 1 1 1 1 ...\n#>  $ likelihood_samples_b : num [1:10000] 0.225 0.272 0.21 0.245 0.248 ...\n#>  $ posterior_samples_b  : num [1:10000] 0.00225 0.00272 0.0021 0.00245 0.00248 ...\n```\n:::\n\n\nAn alternative of the tidyverse approach is `dplyr::glimpse()`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: glimpse-samples-b\n\nglimpse(samples_b)\n```\n````\n\n```\n#> Rows: 10,000\n#> Columns: 4\n#> $ samples_b             <dbl> 0.5645646, 0.6516517, 0.5475475, 0.5905906, 0.59…\n#> $ prior_samples_prior_b <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n#> $ likelihood_samples_b  <dbl> 0.22455994, 0.27190272, 0.20966655, 0.24460869, …\n#> $ posterior_samples_b   <dbl> 0.0022478473, 0.0027217490, 0.0020987643, 0.0024…\n```\n:::\n\n\nNow we can plot the left panel of Figure 3.1 with\n`ggplot2::geom_point()`. But before we do, we'll need to add a variable\nnumbering the samples.\n\n#### Plot samples distribution\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-scatterplot-samples-b\n#| fig-cap: \"Scatterplot of the drawn samples (Version b)\"\n\nsamples_b %>% \n  mutate(sample_number = 1:n()) %>% \n  \n  ggplot(aes(x = sample_number, y = samples_b)) +\n  geom_point(alpha = 1/10) +\n  scale_y_continuous(\"proportion of water (p)\", limits = c(0, 1)) +\n  xlab(\"sample number\")\n```\n````\n\n::: {.cell-output-display}\n![Scatterplot of the drawn samples (Version b)](03-sampling-the-imaginary_files/figure-html/fig-scatterplot-samples-b-1.png){#fig-scatterplot-samples-b width=672}\n:::\n:::\n\n\nWe'll make the density in the right panel with\n`ggplot2::geom_density()`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-density-samples-b\n#| fig-cap: \"Density estimate of the drawn samples (Version b)\"\n\nsamples_b %>% \n  ggplot(aes(x = samples_b)) +\n  geom_density(fill = \"grey\") +\n  scale_x_continuous(\"proportion of water (p)\", limits = c(0, 1))\n```\n````\n\n::: {.cell-output-display}\n![Density estimate of the drawn samples (Version b)](03-sampling-the-imaginary_files/figure-html/fig-density-samples-b-1.png){#fig-density-samples-b width=672}\n:::\n:::\n\n\nCompare this somewhat smoother @fig-density-samples-b with\n@fig-density-samples-a.\n\nIf we keep increasing the number of samples we will get a better\napproximation to the ideal posterior distribution we have computed via\ngrid approximation. Here's what it looks like with `1e6`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-density-samples-b2\n#| fig-cap: \"Density estimate of 1e6 drawn samples (Version b)\"\n\nset.seed(3)\n\nd_b %>% \n  slice_sample(n = 1e6, weight_by = posterior_b, replace = T) %>% \n  ggplot(aes(x = p_grid_b)) +\n  geom_density(fill = \"grey\") +\n  scale_x_continuous(\"proportion of water (p)\", limits = c(0, 1))\n```\n````\n\n::: {.cell-output-display}\n![Density estimate of 1e6 drawn samples (Version b)](03-sampling-the-imaginary_files/figure-html/fig-density-samples-b2-1.png){#fig-density-samples-b2 width=672}\n:::\n:::\n\n\n## Sampling to Summarize\n\n### Three questions asked {.unnumbered}\n\nAll we have done so far is crudely replicate the posterior density we\nhad already computed in the previous chapter. Now it is time to use\nthese samples to describe and understand the posterior.\n\nThe first step in understanding the posterior distribution is to\nsummarize it. Exactly how it is summarized depends upon your purpose.\nBut common questions include:\n\n-   How much posterior probability lies below some parameter value?\n-   How much posterior probability lies between two parameter values?\n-   Which parameter value marks the lower 5% of the posterior\n    probability?\n-   Which range of parameter values contains 90% of the posterior\n    probability?\n-   Which parameter value has highest posterior probability?\n\nThe above list of questions can be divided into three inquiries:\n\n1.  Questions about intervals of defined boundaries.\n2.  Questions about intervals of defined probability mass.\n3.  Questions about point estimates.\n\n### Intervals of Defined Boundaries\n\n#### Original\n\n##### Grid approach\n\nFor instance: What is the probability that the proportion of water is\nless than 0.5?\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: grid-boundaries-a\n\n## R code 3.6 ##############################\n# add up posterior probability where p < 0.5\nsum(posterior_a[p_grid_a < 0.5])\n```\n````\n\n```\n#> [1] 0.1718746\n```\n:::\n\n\nAbout 17% of the posterior probability is below 0.5. Couldn't be easier.\n\n##### Sampling approach\n\nBut this easy calculation based on grid approximation is often no\npractical when there are more parameters. So let's try the sampling\napproach:\n\nTo use the samples from the posterior you have to add up all of the\nsamples below 0.5, but also divide the resulting count by the total\nnumber of samples. In other words, find the frequency of parameter\nvalues below 0.5:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: sample-boundaries-0.5-a\n\n## R code 3.7 #############################\n(p_boundary_a <- sum(samples_a < 0.5) / 1e4)\n```\n````\n\n```\n#> [1] 0.1629\n```\n:::\n\n\n::: callout-caution\n###### Attention: Different values\n\nIn comparison with the value in the original book (0.1726) our value of\n0.1629 is different. 17%.\n\nThe reason for the difference is that you can't get the same values in\nsampling processes. This is the nature of randomness. And McElreath did\nnot include the set.sedd() function for (exact) reproducibility. What\nadditionally happened is that our `set.seed()`\\` value of 3 results in a\nsomewhat untypical sampling distribution. I will demonstrate this with\nadditional three samples with different `set.seed()`\\` values.\n:::\n\n##### Different sampling distributions\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: different-sample-distributions\n\n# four examples with different seeds\nset.seed(42)\nsamples_a2 <- sample(p_grid_a, prob = posterior_a, size = 1e4, replace = TRUE)\nset.seed(123)\nsamples_a3 <- sample(p_grid_a, prob = posterior_a, size = 1e4, replace = TRUE)\nset.seed(1000)\nsamples_a4 <- sample(p_grid_a, prob = posterior_a, size = 1e4, replace = TRUE)\nset.seed(33)\nsamples_a5 <- sample(p_grid_a, prob = posterior_a, size = 1e4, replace = TRUE)\n\n(p_boundary_a2 <- sum(samples_a2 < 0.5) / 1e4)\n(p_boundary_a3 <- sum(samples_a3 < 0.5) / 1e4)\n(p_boundary_a4 <- sum(samples_a4 < 0.5) / 1e4)\n(p_boundary_a5 <- sum(samples_a5 < 0.5) / 1e4)\n\n# without setting a seed (not reproducible)\nset.seed(0)\nsamples_a6 <- sample(p_grid_a, prob = posterior_a, size = 1e4, replace = TRUE)\nsamples_a7 <- sample(p_grid_a, prob = posterior_a, size = 1e4, replace = TRUE)\n(p_boundary_a6 <- sum(samples_a6 < 0.5) / 1e4)\n(p_boundary_a7 <- sum(samples_a7 < 0.5) / 1e4)\n```\n````\n\n```\n#> [1] 0.1695\n#> [1] 0.1631\n#> [1] 0.1765\n#> [1] 0.1778\n#> [1] 0.1696\n#> [1] 0.1711\n```\n:::\n\n\nAlthough the second figure is near our \"bad\" result, all sampling\nprocedures with the chosen different `base::set.seed() values` return\nbetter values (nearer the true value of 0.1718746) as our sampling\nprocedure fixed with `base::set.seed(3)`. The last two values are done\nwithout setting a `set.seed()` value so my values should differ than\nyours.\n\nUsing the same approach, you can ask how much posterior probability lies\nbetween 0.5 and 0.75:\n\n\n::: {.cell}\n\n````{.cell-code #sample-boundaries2-a lst-cap=\"Find the frequency of parameter values below 0.5 with `base::set.seed(3)`\"}\n```{{r}}\n#| label: sample-boundaries-0.5-0.75-a\n#| attr-source: '#sample-boundaries2-a lst-cap=\"Find the frequency of parameter values below 0.5 with `base::set.seed(3)`\"'\n#|\n## R code 3.8 #########################################\n(p_boundary_a8 <- sum(samples_a > 0.5 & samples_a < 0.75) / 1e4)\n```\n````\n\n```\n#> [1] 0.6061\n```\n:::\n\n\n#### Tidyverse\n\n##### Grid approach\n\n> To get the proportion of water less than some value of `p_grid_b`\n> within the {**tidyverse}**, you might first `filter()` by that value\n> and then take the `sum()` within `summarise()`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: grid-boundaries-b\n\n# add up posterior probability where p < 0.5\nd_b |> filter(p_grid_b < 0.5) |> \n    summarize(sum = sum(posterior_b))\n```\n````\n\n```\n#> # A tibble: 1 × 1\n#>     sum\n#>   <dbl>\n#> 1 0.172\n```\n:::\n\n\n##### Sampling approach\n\nIf what you want a frequency based on filtering by `samples_b`, then you\nmight use `n()` within `summarise()`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: samples-boundaries-b\n\n# add up all posterior probabilities of samples under .5\nsamples_b |> \n    filter(samples_b < .5) |> \n    summarize(sum = n() / n_samples_b)\n```\n````\n\n```\n#> # A tibble: 1 × 1\n#>     sum\n#>   <dbl>\n#> 1 0.163\n```\n:::\n\n\nA more explicit approach for the same computation is to follow up\n`count()` with `mutate()`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: samples-boundaries-b2\n\nsamples_b |> \n    count(samples_b < .5) |> \n    mutate(probability = n / sum(n))\n```\n````\n\n```\n#> # A tibble: 2 × 3\n#>   `samples_b < 0.5`     n probability\n#>   <lgl>             <int>       <dbl>\n#> 1 FALSE              8371       0.837\n#> 2 TRUE               1629       0.163\n```\n:::\n\n\nAn even trickier approach for the same is to insert the logical\nstatement `p_grid < .5` within the `mean()` function.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: samples-boundaries-b3\n\nsamples_b |> \n    summarize(sum = mean(samples_b < .5))\n```\n````\n\n```\n#> # A tibble: 1 × 1\n#>     sum\n#>   <dbl>\n#> 1 0.163\n```\n:::\n\n\nTo determine the posterior probability between 0.5 and 0.75, you can use\n`&` within `filter()`. Just multiply that result by 100 to get the value\nin percent.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: samples-boundaries-b4\n\nsamples_b |> \n    filter(samples_b > .5 & samples_b < .75) |> \n    summarize(sum = n() / n_samples_b)\n```\n````\n\n```\n#> # A tibble: 1 × 1\n#>     sum\n#>   <dbl>\n#> 1 0.606\n```\n:::\n\n\nAnd, of course, you can do that with our `mean()` trick, too.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: samples-boundaries-b5\n\nsamples_b %>%\n  summarise(percent = 100 * mean(samples_b > .5 & samples_b < .75))\n```\n````\n\n```\n#> # A tibble: 1 × 1\n#>   percent\n#>     <dbl>\n#> 1    60.6\n```\n:::\n\n\n##### Plot interval of defined boundaries\n\nTo produce Figure 3.2 of the book we apply following code lines:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-upper-part-3.2\n#| fig-cap: \"Upper part of SR2 Figure 3.2: Posterior distribution produced with {**tidyverse**} tools: Left: The blue area is the posterior probability below a parameter value of 0.5. Right: The posterior probability between 0.5 and 0.75.\"\n\n# upper left panel\np1 <-\n  samples_b %>% \n  ggplot(aes(x = samples_b, y = posterior_samples_b)) +\n  geom_line() +\n  geom_area(data = samples_b %>% filter(samples_b < .5), fill = \"deepskyblue\") +\n  labs(x = \"proportion of water (p)\",\n       y = \"density\")\n\n# upper right panel\np2 <- \n  samples_b %>% \n  ggplot(aes(x = samples_b, y = posterior_samples_b)) +\n  geom_line() +\n  geom_area(data = samples_b %>% filter(samples_b > .5 & samples_b < .75), fill = \"deepskyblue\") +\n  labs(x = \"proportion of water (p)\",\n       y = \"density\")\n\nlibrary(patchwork)\np1 + p2\n```\n````\n\n::: {.cell-output-display}\n![Upper part of SR2 Figure 3.2: Posterior distribution produced with {**tidyverse**} tools: Left: The blue area is the posterior probability below a parameter value of 0.5. Right: The posterior probability between 0.5 and 0.75.](03-sampling-the-imaginary_files/figure-html/fig-upper-part-3.2-1.png){#fig-upper-part-3.2 width=672}\n:::\n:::\n\n\n### Intervals of Defined Probability Mass\n\n#### Original\n\n##### Quantiles\n\n> Reporting an interval of defined mass is usually known as a\n> **CONFIDENCE INTERVAL**. An interval of posterior probability, such as\n> the ones we are working with, may instead be called a **CREDIBLE\n> INTERVAL**.\n\n> We're going to call it a **COMPATIBILITY INTERVAL** instead, in order\n> to avoid the unwarranted implications of \"confidence\" and\n> \"credibility.\" What the interval indicates is a range of parameter\n> values compatible with the model and data. The model and data\n> themselves may not inspire confidence, in which case the interval will\n> not either.\n\n> For this type of interval, it is easier to find the answer by using\n> samples from the posterior than by using a grid approximation. Suppose\n> for example you want to know the boundaries of the lower 80% posterior\n> probability. You know this interval starts at $p = 0$. To find out\n> where it stops, think of the samples as data and ask where the 80th\n> percentile lies:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: samples-quantile-a\n\n## R code 3.9 #######################\nquantile(samples_a, 0.8)\n```\n````\n\n```\n#>       80% \n#> 0.7627628\n```\n:::\n\n\nSimilarly, the middle 80% interval lies between the 10th percentile and\nthe 90th percentile. These boundaries are found using the same approach:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: sample-quantile-a2\n\n## R code 3.10 ###################\nquantile(samples_a, c(0.1, 0.9))\n```\n````\n\n```\n#>       10%       90% \n#> 0.4514515 0.8148148\n```\n:::\n\n\nIntervals of this sort, which assign equal probability mass to each\ntail, are very common in the scientific literature. We'll call them\n**PERCENTILE INTERVALS** (PI). These intervals do a good job of\ncommunicating the shape of a distribution, as long as the distribution\nisn't too asymmetrical. But in terms of describing the shape of the\nposterior distribution---which is really all these intervals are asked\nto do---the percentile interval can be misleading.\n\nConsider the posterior distribution and different intervals in\n@fig-skewed-dist-a. This posterior is consistent with observing three\nwaters in three tosses and a uniform (flat) prior. It is highly skewed,\nhaving its maximum value at the boundary, $p = 1$. You can compute it,\nvia grid approximation.\n\n##### Skewed distribution\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-skewed-dist-a\n#| fig-cap: \"Skewed posterior distribution observing three waters in three tosses and a uniform (flat) prior. It is highly skewed, having its maximum value at the boundary where p equals 1.\"\n\n## R code 3.11 #####################\np_grid_skewed_a <- seq(from = 0, to = 1, length.out = 1000)\nprior_skewed_a <- rep(1, 1000)\nlikelihood_skewed_a <- dbinom(3, size = 3, prob = p_grid_skewed_a)\nposterior_skewed_a <- likelihood_skewed_a * prior_skewed_a\nposterior_skewed_a <- posterior_skewed_a / sum(posterior_skewed_a)\n\nset.seed(3) # added to make sampling distribution reproducible (pb)\nsamples_skewed_a <- sample(p_grid_skewed_a, size = 1e4, replace = TRUE, prob = posterior_skewed_a)\n\n\n\n# added to show the skewed posterior distribution (pb)\nrethinking::dens(samples_skewed_a)\n```\n````\n\n::: {.cell-output-display}\n![Skewed posterior distribution observing three waters in three tosses and a uniform (flat) prior. It is highly skewed, having its maximum value at the boundary where p equals 1.](03-sampling-the-imaginary_files/figure-html/fig-skewed-dist-a-1.png){#fig-skewed-dist-a width=672}\n:::\n:::\n\n\n##### Percentile (compatibility) Intervall (PI)\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: rethinking-PI-a\n\n## R code 3.12 ############################\nrethinking::PI(samples_skewed_a, prob = 0.5)\n```\n````\n\n```\n#>       25%       75% \n#> 0.7087087 0.9349349\n```\n:::\n\n\nThe Percentile compatibility Interval (PI) of `prob = 0.5` assigns 25%\nof the probability mass above and below the interval. So it provides the\ncentral 50% probability.\n\nIt is just a shorthand for the base R `stats::quantile()` function:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: quantiles-PI-a\n\nquantile(samples_skewed_a, prob = c(.25, .75))\n```\n````\n\n```\n#>       25%       75% \n#> 0.7087087 0.9349349\n```\n:::\n\n\n> This \\[percentile compability\\] interval assigns 25% of the\n> probability mass above and below the interval. So it provides the\n> central 50% probability. But in this example, it ends up excluding the\n> most probable parameter values, near $p = 1$. So in terms of\n> describing the shape of the posterior distribution---which is really\n> all these intervals are asked to do---the percentile interval can be\n> misleading.\n\nTo see how it works we make another PI of `prob = 0.6`. We divide always\nthe percentage by 2 and subtract it from 50% respectively add this value\nto 50%. The result is the probability mass between 20-80%.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: rethinking-PI-a2\n\nrethinking::PI(samples_skewed_a, prob = 0.6)\n```\n````\n\n```\n#>       20%       80% \n#> 0.6706707 0.9481481\n```\n:::\n\n\nBut in these examples, we end up excluding the most probable parameter\nvalues, near $p = 1$. So in terms of describing the shape of the\nposterior distribution---which is really all these intervals are asked\nto do---the percentile interval can be misleading.\n\n##### Highest Posterior Density Interval (HPDI)\n\n> \\[In contrast, to PI the HPDI (HIGHEST POSTERIOR DENSITY INTERVAL)\\]\n> displays the narrowest interval containing the specified probability\n> mass. If you think about it, there must be an infinite number of\n> posterior intervals with the same mass. But if you want an interval\n> that best represents the parameter values most consistent with the\n> data, then you want the densest of these intervals. That's what the\n> HPDI is. Compute it from the samples with HPDI (also part of\n> rethinking):\n\n\n::: {.cell}\n\n````{.cell-code #lst-rethinking-HPDI-a lst-cap=\"Compute the HPDI from the samples distribution\"}\n```{{r}}\n#| label: rethinking-HPDI-a\n#| attr-source: '#lst-rethinking-HPDI-a lst-cap=\"Compute the HPDI from the samples distribution\"'\n\n## R code 3.13 ###############################\nrethinking::HPDI(samples_skewed_a, prob = 0.5)\n```\n````\n\n```\n#>      |0.5      0.5| \n#> 0.8418418 0.9989990\n```\n:::\n\n\nThis interval captures the parameters with highest posterior\nprobability, as well as being noticeably narrower: 0.18 in width rather\nthan 0.28 for the percentile interval resp. 0.16 and 0.23 in the\noriginal book version.\n\nSo the HPDI has some advantages over the PI. But in most cases, these\ntwo types of interval are very similar. They only look so different in\nthis case because the posterior distribution is highly skewed. If we\ninstead used samples from the posterior distribution for six waters in\nnine tosses, these intervals would be nearly identical. Try it for\nyourself, using different probability masses, such as prob=0.8 and\nprob=0.95.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: PI-HPDI-in-sym-dist\n\nrethinking::PI(samples_a, prob = 0.8)\nrethinking::HPDI(samples_a, prob = 0.8)\n\nrethinking::PI(samples_a, prob = 0.95)\nrethinking::HPDI(samples_a, prob = 0.95)\n```\n````\n\n```\n#>       10%       90% \n#> 0.4514515 0.8148148 \n#>      |0.8      0.8| \n#> 0.4874875 0.8448448 \n#>        3%       98% \n#> 0.3493493 0.8788789 \n#>     |0.95     0.95| \n#> 0.3703704 0.8938939\n```\n:::\n\n\n##### Difference between PI and HDPI\n\n> When the posterior is bell shaped, it hardly matters which type of\n> interval you use. Remember, we're not launching rockets or calibrating\n> atom smashers, so fetishizing precision to the 5th decimal place will\n> not improve your science.\n\n> The HPDI also has some disadvantages. HPDI is more computationally\n> intensive than PI and suffers from greater simulation variance, which\n> is a fancy way of saying that it is sensitive to how many samples you\n> draw from the posterior. It is also harder to understand and many\n> scientific audiences will not appreciate its features, while they will\n> immediately understand a percentile interval, as ordinary non-Bayesian\n> intervals are typically interpreted (incorrectly) as percentile\n> intervals (although see the Rethinking box below).\n\n> Overall, if the choice of interval type makes a big difference, then\n> you shouldn't be using intervals to summarize the posterior. Remember,\n> the entire posterior distribution is the Bayesian \"estimate.\" It\n> summarizes the relative plausibilities of each possible value of the\n> parameter. Intervals of the distribution are just helpful for\n> summarizing it. If choice of interval leads to different inferences,\n> then you'd be better off just plotting the entire posterior\n> distribution.\n\n##### Intervall mass of 95?\n\n> The most common interval mass in the natural and social sciences is\n> the 95% interval. This interval leaves 5% of the probability outside,\n> corresponding to a 5% chance of the parameter not lying within the\n> interval (although see below). This customary interval also reflects\n> the customary threshold for statistical significance, which is 5% or p\n> \\< 0.05.\n\nIt is just a convention, there are no analytical reasons why you should\nchoose exactly this interval. But convenience is not a serious\ncriterion. So what to do instead?\n\n> If you are trying to say that an interval doesn't include some value,\n> then you might use the widest interval that excludes the value. Often,\n> all compatibility intervals do is communicate the shape of a\n> distribution. In that case, a series of nested intervals may be more\n> useful than any one interval. For example, why not present 67%, 89%,\n> and 97% intervals, along with the median? Why these values? No reason.\n> They are prime numbers, which makes them easy to remember. But all\n> that matters is they be spaced enough to illustrate the shape of the\n> posterior. And these values avoid 95%, since conventional 95%\n> intervals encourage many readers to conduct unconscious hypothesis\n> tests.\n\n##### Defined boundaries and probabilty mass\n\nThe difference between intervals of defined boundaries and intervals of\ndefined probability mass is that in the first case we ask for a\n**probability of frequencies** whereas in the second case we calculate a\nspecified **amount of posterior probability**. As result from the first\nquestion we get the percentage of the probability whereas the result of\nthe second question is the probability value of the percentage of\nfrequencies looked for.\n\nThe boundary intervals are grounded on the prob values ($0-1$) and\nresults in the percentage of the probability whereas the probability\nmass intervals focus on the percentage of probabilities ($0-100$%) and\nresults in probability values.\n\n#### Tidyverse\n\n##### Quantiles\n\nSince we saved our `samples_b` samples within the well-named `samples_b`\ntibble, we'll have to index with `$` within `stats::quantile()`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: sample-quantile-b\n\n(q80 <- quantile(samples_b$samples_b, probs = .8))\n```\n````\n\n```\n#>       80% \n#> 0.7627628\n```\n:::\n\n\nFor an alternative approach, we could `dplyr::select()` the `samples_b`\nvector, extract it from the tibble with `dplyr::pull()`, and then pump\nit into `stats::quantile()`.\n\n> `pull()` is similar to `$`. It's mostly useful because it looks a\n> little nicer in pipes, it also works with remote data frames, and it\n> can optionally name the output.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: sample-quantile-b2\n\nsamples_b |> \n    pull(samples_b) |> \n    quantile(probs = .8)\n    \n```\n````\n\n```\n#>       80% \n#> 0.7627628\n```\n:::\n\n\nWe might also use `stats::quantile()` within `dplyr::summarise()`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: sample-quantile-b3\n\nsamples_b |> \n    summarize(q80_2 = quantile(samples_b, probs = .8))\n```\n````\n\n```\n#> # A tibble: 1 × 1\n#>   q80_2\n#>   <dbl>\n#> 1 0.763\n```\n:::\n\n\nHere's the `summarise()` approach with two probabilities.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: sample-quantile-b4\n\nsamples_b |> \n    summarize(q10 = quantile(samples_b, probs = .1),\n              q90 = quantile(samples_b, probs = .9))\n    \n```\n````\n\n```\n#> # A tibble: 1 × 2\n#>     q10   q90\n#>   <dbl> <dbl>\n#> 1 0.451 0.815\n```\n:::\n\n\nYou can also use the vector feature of R to summarize different\nquantiles with one line. But Kurz's version is in the meanwhile\ndeprecated:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: sample-quantile-b5\n\nsamples_b |> \n    summarize(q10_90 = quantile(samples_b, probs = c(.1, .9)))\n```\n````\n\n```\n#> Warning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\n#> dplyr 1.1.0.\n#> ℹ Please use `reframe()` instead.\n#> ℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n#>   always returns an ungrouped data frame and adjust accordingly.\n#> # A tibble: 2 × 1\n#>   q10_90\n#>    <dbl>\n#> 1  0.451\n#> 2  0.815\n```\n:::\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: sample-quantile-b6\n\nsamples_b |> \n    reframe(q10_90 = quantile(samples_b, probs = c(.1, .9)))\n```\n````\n\n```\n#> # A tibble: 2 × 1\n#>   q10_90\n#>    <dbl>\n#> 1  0.451\n#> 2  0.815\n```\n:::\n\n\nFrom the help file of `dplyr::reframe()`:\n\n> While `summarise()` requires that each argument returns a single\n> value, and `mutate()` requires that each argument returns the same\n> number of rows as the input, `reframe()` is a more general workhorse\n> with no requirements on the number of rows returned per group.\n>\n> `reframe()` creates a new data frame by applying functions to columns\n> of an existing data frame. It is most similar to `summarise()`, with\n> two big differences:\n>\n> -   `reframe()` can return an arbitrary number of rows per group,\n>     while `summarise()` reduces each group down to a single row.\n> -   `reframe()` always returns an ungrouped data frame, while\n>     `summarise()` might return a grouped or rowwise data frame,\n>     depending on the scenario.\n>\n> We expect that you'll use `summarise()` much more often than\n> `reframe()`, but `reframe()` can be particularly helpful when you need\n> to apply a complex function that doesn't return a single summary\n> value.\n\nSee also the appropriate [section in the blog\npost](https://www.tidyverse.org/blog/2023/02/dplyr-1-1-0-pick-reframe-arrange/#reframe)\nabout changes in {**dplyr**} 1.1.0. The name `reframe()` is in\naccordance with `tibble::enframe()` and `tibble::deframe()`:\n\n-   `enframe()`: Takes a vector, returns a data frame\n-   `deframe()`: Takes a data frame, returns a vector\n-   `reframe()`: Takes a data frame, returns a data frame\n\n> The functions of the tidyverse approach typically returns a data\n> frame. But sometimes you just want your values in a numeric vector for\n> the sake of quick indexing. In that case, base R `stats::quantile()`\n> shines:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: sample-quantile-b7\n\n(q10_q90 = quantile(samples_b$samples_b, probs = c(.1, .9)))\n```\n````\n\n```\n#>       10%       90% \n#> 0.4514515 0.8148148\n```\n:::\n\n\n##### Plot intervals of defined mass\n\nNow we have our cutoff values saved as `q80`, respectively `q10` and\n`q90`, we're ready to make the bottom panels of Figure 3.2.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: figure-3-2-lower-part\n\np1 <-\n  samples_b %>% \n  ggplot(aes(x = samples_b, y = posterior_samples_b)) +\n  geom_line() +\n  geom_area(data = samples_b %>% filter(samples_b < q80), fill = \"deepskyblue\") +\n  annotate(geom = \"text\",\n           x = .25, y = .0025,\n           label = \"lower 80%\") +\n  labs(x = \"proportion of water (p)\",\n       y = \"density\")\n\n# upper right panel\np2 <- \n  samples_b %>% \n  ggplot(aes(x = samples_b, y = posterior_samples_b)) +\n  geom_line() +\n  geom_area(data = samples_b %>% filter(samples_b > q10_q90[[1]] & samples_b < q10_q90[[2]]), fill = \"deepskyblue\") +\n  annotate(geom = \"text\",\n           x = .25, y = .0025,\n           label = \"middle 80%\") +\n  labs(x = \"proportion of water (p)\",\n       y = \"density\")\n\nlibrary(patchwork)\np1 + p2\n```\n````\n\n::: {.cell-output-display}\n![](03-sampling-the-imaginary_files/figure-html/figure-3-2-lower-part-1.png){width=672}\n:::\n:::\n\n\n##### Skewed distribution\n\nAgain we will demonstrate the misleading character of Pecentile\nIntervals (PIs) with a very skewed distribution.\n\nWe've already defined `p_grid_b` and `prior_b` within `d_b`, above. Here\nwe'll reuse them and create a new tibble by updating all the columns\nwith the skewed parameters.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: samples-skewed-dist\n\nn_samples_skewed_b <- 1e4\nn_success_skewed_b <- 3\nn_trials_skewed_b  <- 3\n\nd_skewed_b <-\ntibble(p_grid_skewed_b = seq(from = 0, to = 1, length.out = n_samples_skewed_b),\n     # note we're still using a flat uniform prior\n     prior_skewed_b  = 1) %>% \nmutate(likelihood_skewed_b = dbinom(n_success_skewed_b, size = n_trials_skewed_b, prob = p_grid_skewed_b)) %>% \nmutate(posterior_skewed_b = (likelihood_skewed_b * prior_skewed_b) / sum(likelihood_skewed_b * prior_skewed_b))\n\n# make the next part reproducible\nset.seed(3)\n\n# here's our new samples tibble\n(\n  samples_skewed_b <-\n    d_skewed_b %>% \n    slice_sample(n = n_samples_skewed_b, weight_by = posterior_skewed_b, replace = T)\n)\n```\n````\n\n```\n#> # A tibble: 10,000 × 4\n#>    p_grid_skewed_b prior_skewed_b likelihood_skewed_b posterior_skewed_b\n#>              <dbl>          <dbl>               <dbl>              <dbl>\n#>  1           0.874              1               0.669          0.000267 \n#>  2           0.830              1               0.572          0.000229 \n#>  3           0.684              1               0.320          0.000128 \n#>  4           0.903              1               0.735          0.000294 \n#>  5           0.750              1               0.423          0.000169 \n#>  6           0.989              1               0.968          0.000387 \n#>  7           0.875              1               0.670          0.000268 \n#>  8           0.522              1               0.143          0.0000570\n#>  9           0.967              1               0.903          0.000361 \n#> 10           0.967              1               0.903          0.000361 \n#> # ℹ 9,990 more rows\n```\n:::\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: skewed-dist-b\n\n\n# here we update the `dbinom()` parameters \n# for values for a skewed distribution\n# assuming three trials results in 3 W (Water)\nn_success_skewed_b <- 3\nn_trials_skewed_b  <- 3\n\n# update `d_b` to d_skewed_b\nd_skewed_b <-\n  d_b %>% \n  mutate(likelihood_skewed_b = dbinom(n_success_skewed_b, size = n_trials_skewed_b, prob = p_grid_b)) %>% \n  mutate(posterior_skewed_b  = (likelihood_skewed_b * prior_b) / sum(likelihood_skewed_b * prior_b))\n\n# make the next part reproducible\nset.seed(3)\n\n# here's our new samples tibble\n(\n    samples_skewed_b <- \n        d_skewed_b %>% \n        slice_sample(n = n_samples_skewed_b, weight_by = posterior_skewed_b, replace = T) |> \n        rename(p_skewed_b = p_grid_b,\n               prior_skewed_b = prior_b)\n)\n\n# added to see the skewed distribution\nsamples_skewed_b %>% \n  ggplot(aes(x = p_skewed_b, y = posterior_skewed_b)) +\n  geom_line()\n```\n````\n\n```\n#> # A tibble: 10,000 × 6\n#>    p_skewed_b prior_skewed_b likelihood_b posterior_b likelihood_skewed_b\n#>         <dbl>          <dbl>        <dbl>       <dbl>               <dbl>\n#>  1      0.717              1    0.259     0.00259                  0.368 \n#>  2      0.652              1    0.272     0.00272                  0.277 \n#>  3      0.548              1    0.210     0.00210                  0.164 \n#>  4      1                  1    0         0                        1     \n#>  5      0.991              1    0.0000582 0.000000582              0.973 \n#>  6      0.788              1    0.192     0.00192                  0.489 \n#>  7      0.940              1    0.0125    0.000126                 0.830 \n#>  8      0.817              1    0.153     0.00154                  0.545 \n#>  9      0.955              1    0.00582   0.0000583                0.871 \n#> 10      0.449              1    0.116     0.00116                  0.0908\n#> # ℹ 9,990 more rows\n#> # ℹ 1 more variable: posterior_skewed_b <dbl>\n```\n\n::: {.cell-output-display}\n![](03-sampling-the-imaginary_files/figure-html/skewed-dist-b-1.png){width=672}\n:::\n:::\n\n\n##### Reconsideration\n\nTo see the difference how the skewed distribution is different to the\nFigure 3.2 lower part, I will draw the appropriate figure here myself.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: figure-3.2-skewed-lower-part\n\np1 <-\n  samples_skewed_b %>% \n  ggplot(aes(x = p_skewed_b, y = posterior_skewed_b)) +\n  geom_line() +\n  geom_area(data = samples_skewed_b %>% filter(p_skewed_b < q80), fill = \"deepskyblue\") +\n  annotate(geom = \"text\",\n           x = .25, y = .0025,\n           label = \"lower 80%\") +\n  labs(x = \"proportion of water (p)\",\n       y = \"density\")\n\n# upper right panel\np2 <- \n  samples_skewed_b %>% \n  ggplot(aes(x = p_skewed_b, y = posterior_skewed_b)) +\n  geom_line() +\n  geom_area(data = samples_skewed_b %>% filter(p_skewed_b > q10_q90[[1]] & p_skewed_b < q10_q90[[2]]), fill = \"deepskyblue\") +\n  annotate(geom = \"text\",\n           x = .25, y = .0025,\n           label = \"middle 80%\") +\n  labs(x = \"proportion of water (p)\",\n       y = \"density\")\n\nlibrary(patchwork)\np1 + p2\n```\n````\n\n::: {.cell-output-display}\n![](03-sampling-the-imaginary_files/figure-html/figure-3.2-skewed-lower-part-1.png){width=672}\n:::\n:::\n\n\n##### Introducing tidybayes\n\nThe [{**tidybayes**}](https://mjskay.github.io/tidybayes/) package\noffers an array of convenience functions for summarizing Bayesian\nmodels. For an introduction using {**tidybayes**} with {**brms**} see\n[Extracting and visualizing tidy draws from {**brms**}\nmodels](https://mjskay.github.io/tidybayes/articles/tidy-brms.html).\n\n> {**tidybayes**} is an R package that aims to make it easy to integrate\n> popular Bayesian modeling methods into a tidy data + ggplot workflow.\n> It builds on top of (and re-exports) several functions for visualizing\n> uncertainty from its sister package,\n> [{**ggdist**}](https://mjskay.github.io/ggdist/).\n\nI had difficulties to use Kurz's functions because there was an\n[overhaul in the naming\nscheme](https://mjskay.github.io/tidybayes/reference/tidybayes-deprecated.html)\nof {**tidybayes**} version 1.0 and a deprecation of horizontal shortcut\ngeoms and stats in {**tidybayes**} 2.1. Because {**tidybayes**}\nintegrates function of the sister package {**ggdist**} the [function\ndescriptions and references of\n{**ggdist**}](https://mjskay.github.io/ggdist/reference/index.html) are\nalso important to consult.\n\nFor the following parts the section on [Point Summaries and\nIntervals](https://mjskay.github.io/tidybayes/articles/tidy-brms.html#point-summaries-and-intervals)\nand the reference on [Point and interval summaries for tidy data frames\nof draws from\ndistributions](https://mjskay.github.io/ggdist/reference/point_interval.html)\nare especially important.\n\nThe {**tidybayes**} package contains a [family of\nfunctions](https://mjskay.github.io/tidybayes/articles/tidy-brms.html#point-summaries-and-intervals)\nthat make it easy to summarize a distribution with a measure of central\ntendency accompanied by intervals. With `tidybayes::median_qi()`, we\nwill ask for the median and quantile-based intervals --- just like we've\nbeen doing with `stats::quantile()`.\n\n::: callout-caution\nAlthough Kurz uses the `samples` data frame I cannot reproduce it with\n`samples_b` data frame, because has changed it values recently to the\nskewed sampling version. To get the same results as Kurz I have to use\nin my naming scheme the `skewed` version.\n:::\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: median-quantile-interval\n\ntidybayes::median_qi(samples_skewed_b$p_skewed_b, .width = .5)\n```\n````\n\n```\n#>           y      ymin      ymax .width .point .interval\n#> 1 0.8428428 0.7087087 0.9349349    0.5 median        qi\n```\n:::\n\n\nNote how the `.width` argument within `tidybayes::median_qi()` worked\nthe same way the `prob` argument did within `rethinking::PI()`. With\n`.width = .5`, we indicated we wanted a quantile-based 50% interval,\nwhich was returned in the `ymin` and `ymax` columns.\n\nThe {**tidybayes**} framework makes it easy to request multiple types of\nintervals. In the following code chunk we'll request 50%, 80%, and 99%\nintervals.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: multiple-intervals\n\ntidybayes::median_qi(samples_skewed_b$p_skewed_b, .width = c(.5, .8, .99))\n```\n````\n\n```\n#>           y      ymin      ymax .width .point .interval\n#> 1 0.8428428 0.7087087 0.9349349   0.50 median        qi\n#> 2 0.8428428 0.5705706 0.9749750   0.80 median        qi\n#> 3 0.8428428 0.2562563 0.9989990   0.99 median        qi\n```\n:::\n\n\n> The .width column in the output indexed which line presented which\n> interval. The value in the y column remained constant across rows.\n> That's because that column listed the measure of central tendency, the\n> median in this case.\n\n> Now let's use the `rethinking::HPDI()` function to return 50% highest\n> posterior density intervals (HPDIs).\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: rethinking-HPDI-b\n\n\nrethinking::HPDI(samples_skewed_b$p_skewed_b, prob = .5)\n```\n````\n\n```\n#>      |0.5      0.5| \n#> 0.8418418 0.9989990\n```\n:::\n\n\n::: callout-note\nMy results (0.8428428 and 1.0000000) are slightly different from the\noutput in Kurz's version (0.8418418, 0.9989990). I assume that these\ndifferences are rounding errors. This happened although I had used the\n`set.seed(3)` value for the Original and Tidyverse variants. TODO: CHECK\nTHIS OUT IN MORE DETAILS.\n:::\n\n> The reason I introduce {**tidybayes**} now is that the functions of\n> the {**brms**} package only support percentile-based intervals of the\n> type we computed with `quantile()` and `median_qi()`. But\n> {**tidybayes**} also supports HPDIs.\n\n::: callout-warning\nThe line `mode_hdi(samples$p_grid, .width = .5)` in Kurz's version is\nnot correct.\n\nThe correct code line is: `mode_hdci(samples$p_grid, .width = .5)`\n\n> `hdi` yields the highest-density interval(s) (also known as the\n> highest posterior density interval). Note: If the distribution is\n> multimodal, `hdi` may return multiple intervals for each probability\n> level (these will be spread over rows). You may wish to use `hdci` ...\n> instead if you want a single highest-density interval, with the caveat\n> that when the distribution is multimodal `hdci` is not a\n> highest-density interval. (See [Point and interval summaries for tidy\n> data frames of draws from\n> distributions](https://mjskay.github.io/ggdist/reference/point_interval.html))\n\nI have therefore changed my version from `tidybayes::mode_hci` to\n`tidybayes::mode_hdci` .\n:::\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: tidyverse-HPDI-b\n\ntidybayes::mode_hdci(samples_skewed_b$p_skewed_b, .width = .5)\n\n```\n````\n\n```\n#>           y      ymin     ymax .width .point .interval\n#> 1 0.9995616 0.8418418 0.998999    0.5   mode      hdci\n```\n:::\n\n\nThis time we used the mode as the measure of central tendency. With this\nfamily of {**tidybayes**} functions, you specify the measure of central\ntendency in the prefix (i.e., mean, median, or mode) and then the type\nof interval you'd like (i.e., `qi()` or `hdci()`).\n\nIf all you want are the intervals without the measure of central\ntendency or all that other technical information, {**tidybayes**} also\noffers the handy `qi()` and `hdi()` functions.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: tidybayes-qi-skewed-dist\n\ntidybayes::qi(samples_skewed_b$p_skewed_b, .width = .5)\n```\n````\n\n```\n#>           [,1]      [,2]\n#> [1,] 0.7087087 0.9349349\n```\n:::\n\n\nThe `qi()` function worked for me and results in the same values as in\nthe Kurz's version. But with `hdi()` I get an error message in the\nskewed version. (Tt worked in the normal version.) completely different\nresult:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: tidybayes-hdi-skewed-dist-error\n#| error: true\n\n# skewed version: 3 toss -> 3 Water (Success)\ntidybayes::hdi(samples_skewed_b$p_skewed_b, na.rm = TRUE, .width = .5)\n```\n````\n\n```\n#> Error in quantile.default(dist_y, probs = 1 - .width): missing values and NaN's not allowed if 'na.rm' is FALSE\n```\n\n````{.cell-code}\n```{{r}}\n#| label: tidybayes-hdi-skewed-dist-error\n#| error: true\n\n\n# original version: 9 toss -> 6 Water (Success)\ntidybayes::hdi(samples_b$samples_b, na.rm = TRUE, .width = .5)\n```\n````\n\n```\n#>           [,1]      [,2]\n#> [1,] 0.5685686 0.7597598\n```\n:::\n\n\nTo work correctly I am calling always `hdci()` instead of `hdi()`:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: tidybayes-hdci-skewed-dist\n\ntidybayes::hdci(samples_skewed_b$p_skewed_b, .width = .5)\n```\n````\n\n```\n#>           [,1]     [,2]\n#> [1,] 0.8418418 0.998999\n```\n:::\n\n\nThese are the same values as in Kurz's version.\n\n::: callout-warning\n###### Error with `tidybayes::hdi()`\n\nIn the skewed version `tidybayes::hdi()` does not work for me in the\nskewed version. I do not know why this is the case. But if I used\n`tidybayes::hdci()` I've got the same result as Kurz.\n:::\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: figure-3.3\n\n# left panel\np1 <-\n  samples_skewed_b %>% \n  ggplot(aes(x = p_skewed_b, y = posterior_skewed_b)) +\n  # check out our sweet `qi()` indexing\n  geom_area(data = samples_skewed_b %>% \n              filter(p_skewed_b >\n                tidybayes::qi(samples_skewed_b$p_skewed_b, .width = .5)[1] & \n                     p_skewed_b <\n                tidybayes::qi(samples_skewed_b$p_skewed_b, .width = .5)[2]),\n                fill = \"deepskyblue\") +\n  geom_line() +\n  labs(subtitle = \"50% Percentile Interval\",\n       x = \"proportion of water (p)\",\n       y = \"density\")\n\n# right panel\np2 <-\n  samples_skewed_b %>% \n  ggplot(aes(x = p_skewed_b, y = posterior_skewed_b)) +\n  geom_area(data = . %>% \n              filter(p_skewed_b > \n                 tidybayes::hdci(samples_skewed_b$p_skewed_b, .width = .5)[1] & \n                       p_skewed_b < \n                 tidybayes::hdci(samples_skewed_b$p_skewed_b, .width = .5)[2]),\n                 fill = \"deepskyblue\") +\n  geom_line() +\n  labs(subtitle = \"50% HPDI\",\n       x = \"proportion of water (p)\",\n       y = \"density\")\n\n# combine!\nlibrary(patchwork)\np1 | p2\n```\n````\n\n::: {.cell-output-display}\n![](03-sampling-the-imaginary_files/figure-html/figure-3.3-1.png){width=672}\n:::\n:::\n\n\nAgain I had to change Kurz's version for the correct result: Instead of\n`tidybayes::hci()` one must use `tidybayes::hdci()` for the right hand\nplot.\n\nComparing the two panels of the plot you can see that in contrast to the\n50% HPDI the 50% of PI does not include the highest probability value.\n\n##### Dots and the Pipe\n\n> In the geom_area() line for the HPDI plot, did you notice how we\n> replaced `data = samples_skewed_b` with `data = .`? When using the\n> pipe (i.e., `%>%`), you can use the `.` as a placeholder for the\n> original data object. It's an odd and handy trick to know about.\n\n(We could have made this change in both parts (p1 and p2) of the graph.\nThe only condition is that you have used the full name of the data frame\nallready in the same piped statement.)\n\nLearn more of the [pipe function %\\>% of the {**magrittr**}\npackage](https://magrittr.tidyverse.org/reference/pipe.html) and about\nthe [base R native forward pipe\noperator](https://stat.ethz.ch/R-manual/R-devel/library/base/html/pipeOp.html).\n\nThe native pipe is available starting with R 4.1.0. It is constructed\nwith `|` followed by `>` resulting in the symbol `|>` to differentiate\nit from the {**magrittr**} pipe (%\\>%). To understand the details of the\ndifferences of `%>%` and the native R pipe `|>` read this elaborated\n[blog article by Isabella\nVelásquez](https://ivelasq.rbind.io/blog/understanding-the-r-pipe/index.html),\nan employee of [Posit](https://posit.co/) (formerly RStudio).\n\n##### Difference between PI and HPDI\n\nPI and HPDI are only very different if you have a very skewed\ndistribution. Otherwise they are pretty similar. Let's check this\nassertion:\n\n::: callout-note\nIn this file we have named the variables in the version of the data\nframe of 6`W` with 9 trials different than the variables of the version\nof 3`W` with 3 trials. Therefore we do not need (re)calculate (= update)\nthe simulation as in Kurz's version.\n\nFurthermore besides `tidybayes::mean_qi()` I will use\n`tidybayes::mean_hdi()` and `tidybayes::mean_hdci()`.\n:::\n\nTry out my own variable names:\n\na)  Results rounded\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: intervals-6-9-not-skewed-rounded\n\nbind_rows(tidybayes::mean_hdci(samples_b$samples_b, .width = c(.8, .95)),\n          tidybayes::mean_hdi(samples_b$samples_b, .width = c(.8, .95)),\n          tidybayes::mean_qi(samples_b$samples_b,  .width = c(.8, .95))) %>% \n  select(.width, .interval, ymin:ymax) %>% \n  arrange(.width) %>% \n  mutate_if(is.double, round, digits = 2)\n```\n````\n\n```\n#>   .width .interval ymin ymax\n#> 1   0.80      hdci 0.48 0.84\n#> 2   0.80       hdi 0.48 0.84\n#> 3   0.80        qi 0.45 0.81\n#> 4   0.95      hdci 0.37 0.90\n#> 5   0.95       hdi 0.37 0.90\n#> 6   0.95        qi 0.35 0.88\n```\n:::\n\n\nSampling from a somewhat Gaussian distribution shows that there are\nabsolut no differences between `tidybayes::mean_hdci()` and\n`tidybayes::mean_hdi()`.\n\nThere are two differences to Kurz's version: 1. `ymin` of 0.80 `hdi` is\nin my version 0.48 and not 0.49. 2. `ymax` of 0.95 `hdi` is in my\nversion 0.90 and not 0.89.\n\nI believe that this (very small) differences are rounding errors. But\nnot rounding errors in the result values but during the complex `hdi`\ncalculations as the following not rounded display of values demonstrate:\n\nb)  Results not rounded\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: intervals-6-9-not-skewed-not-rounded\n\nbind_rows(tidybayes::mean_hdci(samples_b$samples_b, .width = c(.8, .95)),\n          tidybayes::mean_hdi(samples_b$samples_b, .width = c(.8, .95)),\n          tidybayes::mean_qi(samples_b$samples_b,  .width = c(.8, .95))) %>% \n  select(.width, .interval, ymin:ymax) %>% \n  arrange(.width)\n```\n````\n\n```\n#>   .width .interval      ymin      ymax\n#> 1   0.80      hdci 0.4814815 0.8398398\n#> 2   0.80       hdi 0.4814815 0.8398398\n#> 3   0.80        qi 0.4514515 0.8148148\n#> 4   0.95      hdci 0.3723724 0.8958959\n#> 5   0.95       hdi 0.3723724 0.8958959\n#> 6   0.95        qi 0.3493493 0.8788789\n```\n:::\n\n\nBut these small differences are not important. In McElreath words: \\>\nRemember, we're not launching rockets or calibrating atom smashers, so\nfetishizing precision to the 5th decimal place will not improve your\nscience.\n\nThe same is true of the small differences between `qi` and `hdi`. They\ndifference is only 2 resp. 3 percent.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: intervals-3-9-skewed-version\n\nbind_rows(tidybayes::mean_hdci(samples_skewed_b$p_skewed_b, .width = c(.8, .95)),\n          tidybayes::mean_qi(samples_skewed_b$p_skewed_b,  .width = c(.8, .95))) %>% \n  select(.width, .interval, ymin:ymax) %>% \n  arrange(.width) %>% \n  mutate_if(is.double, round, digits = 2)\n```\n````\n\n```\n#>   .width .interval ymin ymax\n#> 1   0.80      hdci 0.67 1.00\n#> 2   0.80        qi 0.57 0.97\n#> 3   0.95      hdci 0.48 1.00\n#> 4   0.95        qi 0.40 0.99\n```\n:::\n\n\nThe skewed version shows bigger differences of 10% resp. 8% between\n`mean_hdci()` and `mean_qi()`. (This calculation is missing in Kurz's\nversion.)\n\nBecause of the disadvantages of HPDI (more computationally intensive,\ngreater simulation variance and harder to understand) we'll primarily\nstick to the PI-based intervals.\n\n### Point Estimates\n\n#### Original\n\n> The third and final common summary task for the posterior is to\n> produce point estimates of some kind. Given the entire posterior\n> distribution, what value should you report? This seems like an\n> innocent question, but it is difficult to answer. **The Bayesian\n> parameter estimate is precisely the entire posterior distribution,\n> which is not a single number, but instead a function that maps each\n> unique parameter value onto a plausibility value.** So really the most\n> important thing to note is that you don't have to choose a point\n> estimate. It's hardly ever necessary and often harmful. It discards\n> information. (emphasis is mine, pb)\n\nBut if you must do it ... we will take again the globe tossing\nexperiment in which we observe 3 waters out of 3 tosses, e.g. the very\nskewed distribution.\n\n##### Calcualte MAP\n\n> First, it is very common for scientists to report the parameter value\n> with highest posterior probability, a *maximum a posteriori* (MAP)\n> estimate.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: calculate-skewed-MAP-grid-a\n\n### actually the grid is not skewed. I should change the names accordingly.\n\n## R code 3.14 ###############\np_grid_skewed_a[which.max(posterior_skewed_a)]\n```\n````\n\n```\n#> [1] 1\n```\n:::\n\n\nOr if you instead have samples from the posterior, you can still\napproximate the same point:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: calculate-skewed-MAP-samples-a\n\n## R code 3.15 ################\nrethinking::chainmode(samples_skewed_a, adj = 0.01)\n```\n````\n\n```\n#> [1] 0.9938226\n```\n:::\n\n\n##### Calculate measures of central tendency\n\nBut why is the MAP, the mode, so interesting? Why not report the\nposterior mean or median?\n\n\n::: {.cell}\n\n````{.cell-code #lst-mean-median-skewed-samples lst-cap=\"Posterior mean and median of the skewed distribution (all three draws are `W`)\"}\n```{{r}}\n#| label: mean-median-skewed-samples-a\n#| attr-source: '#lst-mean-median-skewed-samples lst-cap=\"Posterior mean and median of the skewed distribution (all three draws are `W`)\"'\n\n## R code 3.16 #############\nmean(samples_skewed_a)\nmedian(samples_skewed_a)\n```\n````\n\n```\n#> [1] 0.8027632\n#> [1] 0.8428428\n```\n:::\n\n\n##### Plot postponend\n\nThe graphical representation as shown in Figure 3.4 will be calculated\nin the tidyverse version of this section. See: @fig-minimum-loss-b for\nthe left panel and @fig-minimum-loss2-b for the right panel of Figure\n3.4.\n\n> These are also point estimates, and they also summarize the posterior.\n> But all three---the mode (MAP), mean, and median---are different in\n> this case. How can we choose?\n\n##### Loss function\n\nOne principled way to go beyond using the entire posterior as the\nestimate is to choose a **LOSS FUNCTION**. A loss function is a rule\nthat tells you the cost associated with using any particular point\nestimate. While statisticians and game theorists have long been\ninterested in loss functions, and how Bayesian inference supports them,\nscientists hardly ever use them explicitly. The key insight is that\n*different loss functions imply different point estimates*.\n\n> Calculating expected loss for any given decision means using the\n> posterior to average over our uncertainty in the true value. Of course\n> we don't know the true value, in most cases. But if we are going to\n> use our model's information about the parameter, that means using the\n> entire posterior distribution. So suppose we decide $p = 0.5$ will be\n> our decision. Then the expected loss will be:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: loss-expected-0.5-a\n\n## R code 3.17 ##################\nsum(posterior_skewed_a * abs(0.5 - p_grid_skewed_a))\n```\n````\n\n```\n#> [1] 0.3128752\n```\n:::\n\n\n> All the code above does is compute the weighted average loss, where\n> each loss is weighted by its corresponding posterior probability.\n> There's a trick for repeating this calculation for every possible\n> decision, using the function sapply.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: loss-function-a\n\n## R code 3.18 #############################\nloss_a <- sapply(p_grid_skewed_a, function(d) sum(posterior_skewed_a * abs(d - p_grid_skewed_a)))\nhead(loss_a)\n```\n````\n\n```\n#> [1] 0.8004001 0.7993991 0.7983981 0.7973971 0.7963961 0.7953951\n```\n:::\n\n\n> Now the symbol `loss_a` contains a list of loss values, one for each\n> possible decision, corresponding the values in `p_grid_skewed_a`. From\n> here, it's easy to find the parameter value that minimizes the loss:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: minimize-loss-a\n\n## R code 3.19 #############################\np_grid_skewed_a[which.min(loss_a)]\n```\n````\n\n```\n#> [1] 0.8408408\n```\n:::\n\n\n> And this is actually the posterior median, the parameter value that\n> splits the posterior density such that half of the mass is above it\n> and half below it.\n\nWe have already calculated the posterior median in\n@lst-mean-median-skewed-samples. Because of sampling variation it is not\nidentical but pretty close (0.8428428 vs. 0.8408408).\n\n**Learnings**:\n\n> In order to decide upon a point estimate, a single-value summary of\n> the posterior distribution, we need to pick a loss function. Different\n> loss functions nominate different point estimates. The two most common\n> examples are the absolute loss as above, which leads to the median as\n> the point estimate, and the quadratic loss $(d - p)^{2}$, which leads\n> to the posterior mean `(mean(samples))` as the point estimate.\n\n> When the posterior distribution is symmetrical and normal-looking,\n> then the median and mean converge to the same point, which relaxes\n> some anxiety we might have about choosing a loss function. For the\n> original globe tossing data (6 waters in 9 tosses), for example, the\n> mean and median are barely different.\n\n**Comparison of mean & median** with globe tossing data (6 waters in 9\ntosses) versus (3 waters in 3 tosses):\n\n6/9: Mean = 0.6400664 Median = 0.6486486 3/3: Mean\n= 0.8027632 Median = 0.8428428\n\n::: callout-important\nInstead of deciding what parameter to report for summarizing the\nposterior distribution it is usually better to communicate as much as\nyou can about the posterior distribution, as well as the data and the\nmodel itself, so that others can build upon your work.\n\nThis advice is also valid if you just want to accept or not to accept an\nhypothesis. Because the challenge then is to say what the relevant costs\nand benefits would be, in terms of the knowledge gained or lost.\n:::\n\n#### Tidyverse\n\n##### Calculate MAP\n\nTo get the MAP of the skewed version (three tosses with thre `W`) we\nhave to `arrange()` our `d_b` tibble in descending order by posterior.\nThen we will see the corresponding p_grid_b value for its MAP estimate.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: calculate-skewed-MAP-grid-b\n\nsamples_skewed_b %>% \n  arrange(desc(posterior_skewed_b))\n```\n````\n\n```\n#> # A tibble: 10,000 × 6\n#>    p_skewed_b prior_skewed_b likelihood_b posterior_b likelihood_skewed_b\n#>         <dbl>          <dbl>        <dbl>       <dbl>               <dbl>\n#>  1          1              1            0           0                   1\n#>  2          1              1            0           0                   1\n#>  3          1              1            0           0                   1\n#>  4          1              1            0           0                   1\n#>  5          1              1            0           0                   1\n#>  6          1              1            0           0                   1\n#>  7          1              1            0           0                   1\n#>  8          1              1            0           0                   1\n#>  9          1              1            0           0                   1\n#> 10          1              1            0           0                   1\n#> # ℹ 9,990 more rows\n#> # ℹ 1 more variable: posterior_skewed_b <dbl>\n```\n:::\n\n\nTo emphasize it, we can use slice() to select the top row. The MAP value\nis the column `posterior_skewed_b`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: calculate-skewed-MAP-grid2-b\n\nsamples_skewed_b %>% \n  arrange(desc(posterior_skewed_b)) |> \n  slice(1)\n```\n````\n\n```\n#> # A tibble: 1 × 6\n#>   p_skewed_b prior_skewed_b likelihood_b posterior_b likelihood_skewed_b\n#>        <dbl>          <dbl>        <dbl>       <dbl>               <dbl>\n#> 1          1              1            0           0                   1\n#> # ℹ 1 more variable: posterior_skewed_b <dbl>\n```\n:::\n\n\n##### Calculate measures of central tendency\n\nWe can get the mode with `mode_hdci()` or `mode_qi()`\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: tidybayes-mode-qi-and-hdci\n\nsamples_skewed_b %>% tidybayes::mode_qi(p_skewed_b)\nsamples_skewed_b %>% tidybayes::mode_hdci(p_skewed_b)\n```\n````\n\n```\n#> # A tibble: 1 × 6\n#>   p_skewed_b .lower .upper .width .point .interval\n#>        <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n#> 1       1.00  0.399  0.994   0.95 mode   qi       \n#> # A tibble: 1 × 6\n#>   p_skewed_b .lower .upper .width .point .interval\n#>        <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n#> 1       1.00  0.475      1   0.95 mode   hdci\n```\n:::\n\n\nThose returned a lot of output in addition to the mode. If all you want\nis the mode itself, you can just use `tidybayes::Mode()`.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: tidybayes-mode\n\ntidybayes::Mode(samples_skewed_b$p_skewed_b)\n```\n````\n\n```\n#> [1] 0.9995616\n```\n:::\n\n\nMedians and means are typical measures of central tendency, too.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: tidybayes-mean-and-median\n\nsamples_skewed_b %>% \n  summarise(mean   = mean(p_skewed_b),\n            median = median(p_skewed_b))\n```\n````\n\n```\n#> # A tibble: 1 × 2\n#>    mean median\n#>   <dbl>  <dbl>\n#> 1 0.803  0.843\n```\n:::\n\n\n##### Plot left panel of figure 3.4\n\n1.  Bundle the three types of estimates into a tibble\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: bundle-skewed-point-estimates\n\n(\n  point_estimates_b <-\n  bind_rows(samples_skewed_b %>% tidybayes::mean_qi(p_skewed_b),\n            samples_skewed_b %>% tidybayes::median_qi(p_skewed_b),\n            samples_skewed_b %>% tidybayes::mode_qi(p_skewed_b)) %>% \n  select(p_skewed_b, .point) %>% \n  # these last two columns will help us annotate  \n  mutate(x = p_skewed_b + c(-.03, .03, -.03),\n         y = c(.0005, .0012, .002))\n)\n```\n````\n\n```\n#> # A tibble: 3 × 4\n#>   p_skewed_b .point     x      y\n#>        <dbl> <chr>  <dbl>  <dbl>\n#> 1      0.803 mean   0.773 0.0005\n#> 2      0.843 median 0.873 0.0012\n#> 3      1.00  mode   0.970 0.002\n```\n:::\n\n\nPlotting the results:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-skewed-point-estimates_b\n#| fig-cap: \"Posterior distribution (blue) after observing 3 water in 3 tosses of the globe. Vertical lines show the locations of the mode, median, and mean. Each point implies a different loss function.\"\n\nsamples_skewed_b %>% \n  ggplot(aes(x = p_skewed_b)) +\n  geom_area(aes(y = posterior_skewed_b),\n            fill = \"deepskyblue\") +\n  geom_vline(xintercept = point_estimates_b$p_skewed_b) +\n  geom_text(data = point_estimates_b,\n            aes(x = x, y = y, label = .point),\n            angle = 90) +\n  labs(x = \"proportion of water (p)\",\n       y = \"density\") +\n  theme(panel.grid = element_blank())\n```\n````\n\n::: {.cell-output-display}\n![Posterior distribution (blue) after observing 3 water in 3 tosses of the globe. Vertical lines show the locations of the mode, median, and mean. Each point implies a different loss function.](03-sampling-the-imaginary_files/figure-html/fig-skewed-point-estimates_b-1.png){#fig-skewed-point-estimates_b width=672}\n:::\n:::\n\n\nIn contrast the other distribution with 6 successes by 9 trials\n(tosses):\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-sym-point-estimates_b\n#| fig-cap: \"Point estimates in the almost symmetrical distribution of 6 successes (`W`) in 9 tosses.\"\n\n(\n    point_estimates_b <-\n      bind_rows(samples_b %>% tidybayes::mean_qi(samples_b),\n                samples_b %>% tidybayes::median_qi(samples_b),\n                samples_b %>% tidybayes::mode_qi(samples_b)) %>% \n      select(samples_b, .point) %>% \n      # these last two columns will help us annotate  \n      mutate(x = samples_b + c(-.03, .03, -.03),\n             y = c(.0005, .0012, .002))\n)\n\nsamples_b %>% \n  ggplot(aes(x = samples_b)) +\n  geom_area(aes(y = posterior_samples_b),\n            fill = \"deepskyblue\") +\n  geom_vline(xintercept = point_estimates_b$samples_b) +\n  geom_text(data = point_estimates_b,\n            aes(x = x, y = y, label = .point),\n            angle = 90) +\n  labs(x = \"proportion of water (p)\",\n       y = \"density\") +\n  theme(panel.grid = element_blank())\n```\n````\n\n```\n#> # A tibble: 3 × 4\n#>   samples_b .point     x      y\n#>       <dbl> <chr>  <dbl>  <dbl>\n#> 1     0.640 mean   0.610 0.0005\n#> 2     0.649 median 0.679 0.0012\n#> 3     0.651 mode   0.621 0.002\n```\n\n::: {.cell-output-display}\n![Point estimates in the almost symmetrical distribution of 6 successes (`W`) in 9 tosses.](03-sampling-the-imaginary_files/figure-html/fig-sym-point-estimates_b-1.png){#fig-sym-point-estimates_b width=672}\n:::\n:::\n\n\n##### Loss function\n\nLet $p$ be the proportion of the Earth covered by water and $d$ be our\nguess. If McElreath pays us \\$100 if we guess exactly right but\nsubtracts money from the prize proportional to how far off we are, then\nour loss is proportional to $d - p$. If we decide $d = .5$, we can\ncompute our expected loss.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: loss-epected-0.5-b\n\nd_skewed_b |> \n    summarise(`expected loss` = sum(posterior_skewed_b * abs(0.5 - p_grid_b)))\n```\n````\n\n```\n#> # A tibble: 1 × 1\n#>   `expected loss`\n#>             <dbl>\n#> 1           0.313\n```\n:::\n\n\nThe `map()` family of the [{**purrr**}\npackage](https://purrr.tidyverse.org/) is the tidyverse alternative to\nthe family of `apply()` functions from the base R framework. You can\nlearn more about how to use the `map()` family on different places:\n\n-   [Purrr reference](https://purrr.tidyverse.org/reference/map.html):\n    Apply a function to each element of a vector\n-   [Purrr\n    tutorial](https://jennybc.github.io/purrr-tutorial/ls01_map-name-position-shortcuts.html):\n    Introduction to map(): extract elements\n-   [University of Virginia\n    Library](https://data.library.virginia.edu/getting-started-with-the-purrr-package-in-r/):\n    Getting Started with the purrr Package in R\n\nCalculate loss function\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: loss-function-b\n\nmake_loss_b <- function(our_d) {\n  d_skewed_b %>% \n    mutate(loss_b = posterior_skewed_b * abs(our_d - p_grid_b)) %>% \n    summarise(weighted_average_loss_b = sum(loss_b))\n}\n\n(\n  l_b <-\n  d_skewed_b %>% \n  select(p_grid_b) %>% \n  rename(decision_b = p_grid_b) %>% \n  mutate(weighted_average_loss_b = purrr::map(decision_b, make_loss_b)) %>% \n  unnest(weighted_average_loss_b) \n)\n```\n````\n\n```\n#> # A tibble: 1,000 × 2\n#>    decision_b weighted_average_loss_b\n#>         <dbl>                   <dbl>\n#>  1    0                         0.800\n#>  2    0.00100                   0.799\n#>  3    0.00200                   0.798\n#>  4    0.00300                   0.797\n#>  5    0.00400                   0.796\n#>  6    0.00501                   0.795\n#>  7    0.00601                   0.794\n#>  8    0.00701                   0.793\n#>  9    0.00801                   0.792\n#> 10    0.00901                   0.791\n#> # ℹ 990 more rows\n```\n:::\n\n\nCalculate the minimum loss\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: minimize-loss-b\n\n# this will help us find the x and y coordinates for the minimum value\n(\n    min_loss_b <-\n      l_b %>% \n      filter(weighted_average_loss_b == min(weighted_average_loss_b)) %>% \n      as.numeric()\n)\n```\n````\n\n```\n#> [1] 0.8408408 0.1273465\n```\n:::\n\n\nNow we're ready for the right panel of Figure 3.4.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-minimum-loss-b\n#| fig-cap: \"Expected loss under the rule that loss is proportional to absolute distance of decision (horizontal axis) from the true value. The point marks the value of `p` that minimizes the expected loss, the posterior median.\"\n\nl_b %>%   \n  ggplot(aes(x = decision_b, y = weighted_average_loss_b)) +\n  geom_area(fill = \"deepskyblue\") +\n  geom_vline(xintercept = min_loss_b[1], color = \"black\", linetype = 3) +\n  geom_hline(yintercept = min_loss_b[2], color = \"black\", linetype = 3) +\n  ylab(\"expected proportional loss\") +\n  theme(panel.grid = element_blank())\n```\n````\n\n::: {.cell-output-display}\n![Expected loss under the rule that loss is proportional to absolute distance of decision (horizontal axis) from the true value. The point marks the value of `p` that minimizes the expected loss, the posterior median.](03-sampling-the-imaginary_files/figure-html/fig-minimum-loss-b-1.png){#fig-minimum-loss-b width=672}\n:::\n:::\n\n\nWe saved the exact minimum value as `min_loss_b[1]`, which is\n0.8408408. Within sampling error, this is the posterior median\nas depicted by our samples.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: posterior-skewed-median_b\n\nsamples_skewed_b %>% \n  summarise(posterior_median_b = median(p_skewed_b))\n```\n````\n\n```\n#> # A tibble: 1 × 1\n#>   posterior_median_b\n#>                <dbl>\n#> 1              0.843\n```\n:::\n\n\nThe quadratic loss $(d−p)^{2}$ suggests we should use the mean instead.\nLet's investigate.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-minimum-loss2-b\n#| fig-cap: \"Expected loss under the rule that loss is quadratic to the distance of decision (horizontal axis) from the true value. The point marks the value of `p` that minimizes the expected loss, the posterior mean\"\n\n# amend our loss function\n\nmake_loss2_b <- function(our_d2) {\n  d_skewed_b %>% \n    mutate(loss2_b = posterior_skewed_b * (our_d2 - p_grid_b)^2) %>% \n    summarise(weighted_average_loss2_b = sum(loss2_b))\n}\n\n\n# remake our `l` data\nl2_b <-\n  d_skewed_b %>% \n  select(p_grid_b) %>% \n  rename(decision2_b = p_grid_b) %>% \n  mutate(weighted_average_loss2_b = purrr::map(decision2_b, make_loss2_b)) %>% \n  unnest(weighted_average_loss2_b)\n\n# update to the new minimum loss coordinates\n\n(\n    min_loss2_b <-\n      l2_b %>% \n      filter(weighted_average_loss2_b == min(weighted_average_loss2_b)) %>% \n      as.numeric()\n)\n\n# update the plot\nl2_b %>%   \n  ggplot(aes(x = decision2_b, y = weighted_average_loss2_b)) +\n  geom_area(fill = \"deepskyblue\") +\n  geom_vline(xintercept = min_loss2_b[1], color = \"black\", linetype = 3) +\n  geom_hline(yintercept = min_loss2_b[2], color = \"black\", linetype = 3) +\n  ylab(\"expected proportional loss\") +\n  theme(panel.grid = element_blank())\n```\n````\n\n```\n#> [1] 0.80080080 0.02669345\n```\n\n::: {.cell-output-display}\n![Expected loss under the rule that loss is quadratic to the distance of decision (horizontal axis) from the true value. The point marks the value of `p` that minimizes the expected loss, the posterior mean](03-sampling-the-imaginary_files/figure-html/fig-minimum-loss2-b-1.png){#fig-minimum-loss2-b width=672}\n:::\n:::\n\n\nBased on quadratic loss $(d−p)^{2}$, the exact minimum value is\n0.8008008. Within sampling error, this is the posterior mean of\nour samples.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: posterior-skewed-mean_b\n\nsamples_skewed_b %>% \n  summarise(posterior_mean_b = mean(p_skewed_b))\n```\n````\n\n```\n#> # A tibble: 1 × 1\n#>   posterior_mean_b\n#>              <dbl>\n#> 1            0.803\n```\n:::\n\n\n## Sampling to simulate prediction\n\n### Original\n\nTo generate implied observations from a model is useful for at least\nfive reasons:\n\n1.  **Model design**: We can sample not only from the posterior, but\n    also from the prior. Seeing what the model expects, before the data\n    arrive, is the best way to understand the implications of the prior.\n    We'll do a lot of this in later chapters, where there will be\n    multiple parameters and so their joint implications are not always\n    very clear.\n2.  **Model checking**: After a model is updated using data, it is worth\n    simulating implied observations, to check both whether the fit\n    worked correctly and to investigate model behavior.\n3.  **Software validation**: In order to be sure that our model fitting\n    software is working, it helps to simulate observations under a known\n    model and then attempt to recover the values of the parameters the\n    data were simulated under.\n4.  **Research design**: If you can simulate observations from your\n    hypothesis, then you can evaluate whether the research design can be\n    effective. In a narrow sense, this means doing power analysis, but\n    the possibilities are much broader.\n5.  **Forecasting**: Estimates can be used to simulate new predictions,\n    for new cases and future observations. These forecasts can be useful\n    as applied prediction, but also for model criticism and revision.\n\n#### Dummy data\n\n> Now note that these assumptions not only allow us to infer the\n> plausibility of each possible value of *p*, after observation. That's\n> what you did in the previous chapter. These assumptions also allow us\n> to simulate the observations that the model implies. They allow this,\n> because likelihood functions work in both directions. Given a realized\n> observation, the likelihood function says how plausible the\n> observation is. And given only the parameters, the likelihood defines\n> a distribution of possible observations that we can sample from, to\n> simulate observation. In this way, Bayesian models are always\n> *generative*, capable of simulating predictions. Many non-Bayesian\n> models are also generative, but many are not.\n>\n> We will call such simulated data **DUMMY DATA**, to indicate that it\n> is a stand-in for actual data.\n\n##### Probability of each globe toss\n\n> Suppose $N = 2$, two tosses of the globe. Then there are only three\n> possible observations: 0 water, 1 water, 2 water. You can quickly\n> compute the probability of each, for any given value of *p*. Let's use\n> $p = 0.7$, which is just about the true proportion of water on the\n> Earth:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: dummy-data-a\n\n## R code 3.20 #############################\ndbinom(0:2, size = 2, prob = 0.7)\n```\n````\n\n```\n#> [1] 0.09 0.42 0.49\n```\n:::\n\n\n##### Simulation of globe tosses\n\n> Now we're going to simulate observations, using these probabilities.\n> This is done by sampling from the distribution just described above.\n> You could use `sample()` to do this, but R provides convenient\n> sampling functions for all the ordinary probability distributions,\n> like the binomial.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: simulate-1-obs-a\n\nset.seed(3) # for reproducibility\n\n## R code 3.21 #############################\nrbinom(1, size = 2, prob = 0.7)\n```\n````\n\n```\n#> [1] 2\n```\n:::\n\n\n(As the outcome results from a random process the above value differs\nfrom the SR2 version. But with `set.seed(3)`\\` you will get the same\nvalue of $2$.)\n\n> That $1$ means \"2 water in 2 tosses.\" The \"`r`\" in `rbinom` stands for\n> \"random.\" It can also generate more than one simulation at a time. A\n> set of 10 simulations can be made by:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: simulate-10-obs-a\n\nset.seed(3)\n## R code 3.22 #############################\nrbinom(10, size = 2, prob = 0.7)\n```\n````\n\n```\n#>  [1] 2 1 2 2 1 1 2 2 1 1\n```\n:::\n\n\n> Let's generate 100,000 dummy observations, just to verify that each\n> value (0, 1, or 2) appears in proportion to its likelihood:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: simulate-1e5-obs-a\n\nset.seed(3)\n## R code 3.23 #############################\ndummy_w_a <- rbinom(1e5, size = 2, prob = 0.7)\ntable(dummy_w_a) / 1e5\n```\n````\n\n```\n#> dummy_w_a\n#>       0       1       2 \n#> 0.09000 0.42051 0.48949\n```\n:::\n\n\n##### Plot simulation of globe tosses\n\nWe could use either the base R `graphics::hist()` or --- as in the SR2\nbook --- the `rethinking::simplehist()` function.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-plot-hist-figure-3.5-a\n#| fig-cap: \"Distribution of simulated sample observations from 9 tosses of the globe. These samples assume the proportion of water is 0.7. The plot uses the base R `hist()` function\"\n\nset.seed(3)\ndummy_w_a <- rbinom(1e5, size = 9, prob = 0.7)\nhist(dummy_w_a, xlab = \"dummy water count\")\n```\n````\n\n::: {.cell-output-display}\n![Distribution of simulated sample observations from 9 tosses of the globe. These samples assume the proportion of water is 0.7. The plot uses the base R `hist()` function](03-sampling-the-imaginary_files/figure-html/fig-plot-hist-figure-3.5-a-1.png){#fig-plot-hist-figure-3.5-a width=672}\n:::\n:::\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-plot-simplehist-figure-3.5-a\n#| fig-cap: \"Distribution of simulated sample observations from 9 tosses of the globe. These samples assume the proportion of water is 0.7. the plot uses the rethinking::simplehist() function\"\n\nset.seed(3)\n## R code 3.24 #############################\ndummy_w_a <- rbinom(1e5, size = 9, prob = 0.7)\nrethinking::simplehist(dummy_w_a, xlab = \"dummy water count\")\n```\n````\n\n::: {.cell-output-display}\n![Distribution of simulated sample observations from 9 tosses of the globe. These samples assume the proportion of water is 0.7. the plot uses the rethinking::simplehist() function](03-sampling-the-imaginary_files/figure-html/fig-plot-simplehist-figure-3.5-a-1.png){#fig-plot-simplehist-figure-3.5-a width=672}\n:::\n:::\n\n\n> Notice that most of the time the expected observation does not contain\n> water in its true proportion, 0.7. That's the nature of observation:\n> There is a one-to-many relationship between data and data-generating\n> processes. You should experiment with sample size, the `size` input in\n> the code above, as well as the prob, to see how the distribution of\n> simulated samples changes shape and location.\n\n> Many readers will already have seen simulated observations. **SAMPLING\n> DISTRIBUTIONS** are the foundation of common non-Bayesian statistical\n> traditions. In those approaches, inference about parameters is made\n> through the sampling distribution. In this book, inference about\n> parameters is never done directly through a sampling distribution. The\n> posterior distribution is not sampled, but deduced logically. Then\n> samples can be drawn from the posterior, as earlier in this chapter,\n> to aid in inference. In neither case is \"sampling\" a physical act. In\n> both cases, it's just a mathematical device and produces only *small\n> world* (@sec-small-and-large-worlds) numbers.\n\n\n#### Model checking\n\n> MODEL CHECKING means (1) ensuring the model fitting worked correctly and (2) evaluating the adequacy of a model for some purpose. Since Bayesian models are always _generative_, able to simulate observations as well as estimate parameters from observations, once you condition a model on data, you can simulate to examine the model’s empirical expectations.\n\n> We’d like to _propagate_ the parameter uncertainty—carry it forward—as we evaluate the implied predictions. All that is required is averaging over the posterior density for `p`, while computing the predictions. For each possible value of the parameter `p`, there is an implied distribution of outcomes. So if you were to compute the sampling distribution of outcomes at each value of `p`, then you could average all of these prediction distributions together, using the posterior probabilities of each value of `p`, to get a POSTERIOR PREDICTIVE DISTRIBUTION.\n\nThe reproduction of FIGURE 3.6 that illustrates this averaging is shown in ref###.\n\n> we need to learn how to combine sampling of simulated observations, as in the previous section, with sampling parameters from the posterior distribution. We expect to do better when we use the entire posterior distribution, not just some point estimate derived from it. \n\nSo how do you actually do the calculations? \n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: sim-pred-values-a\n\nset.seed(3)\n## R code 3.25 #############################\nw_a <- rbinom(1e4, size = 9, prob = 0.6)\nrethinking::simplehist(w_a)\n```\n````\n\n::: {.cell-output-display}\n![](03-sampling-the-imaginary_files/figure-html/sim-pred-values-a-1.png){width=672}\n:::\n:::\n\n> This generates 10,000 (1e4) simulated predictions of 9 globe tosses (size=9), assuming $p = 6$. The predictions are stored as counts of water, so the theoretical minimum is zero and the theoretical maximum is nine. \n\nWe used `rethinking::`simplehist(w_a)` to get a clean histogram of the simulated outcomes.\n\n> All you need to propagate parameter uncertainty into these predictions is replace the value 0.6 with samples from the posterior:\n\n\n::: {.cell}\n\n````{.cell-code #lst-sim-pred-samples-a lst-cap=\"Generate 1e4 random binomial samples to simulate predicted observations for $p = 0.6$\"}\n```{{r}}\n#| label: sim-pred-samples-a\n#| fig-cap: \"Random binomial samples to simulate predicted observations for $p = 0.6$\"\n#| attr-source: '#lst-sim-pred-samples-a lst-cap=\"Generate 1e4 random binomial samples to simulate predicted observations for $p = 0.6$\"'\n\nset.seed(3)\n## R code 3.26 #############################\nw2_a <- rbinom(1e4, size = 9, prob = samples_a)\nrethinking::simplehist(w2_a)\n```\n````\n\n::: {.cell-output-display}\n![Random binomial samples to simulate predicted observations for $p = 0.6$](03-sampling-the-imaginary_files/figure-html/sim-pred-samples-a-1.png){width=672}\n:::\n:::\n\n\nThe symbol `samples_a` above is the same list of random samples from the posterior distribution that we have calculated in @lst-draw-samples-a and used in previous sections. \n\n\n\n> For each sampled value, a random binomial observation is generated. Since the sampled values appear in proportion to their posterior probabilities, the resulting simulated observations are averaged over the posterior. You can manipulate these simulated observations just like you manipulate samples from the posterior—you can compute intervals and point statistics using the same procedures. \n\n> The simulated model predictions are quite consistent with the observed data in this case—the actual count of 6 lies right in the middle of the simulated distribution. … So far, we’ve only viewed the data just as the model views it: Each toss of the globe is completely independent of the others. This assumption is questionable.\n\n> So with the goal of seeking out aspects of prediction in which the model fails, let’s look at the data in two different ways. Recall that the sequence of nine tosses was `W L W W W L W L W`. First, consider the length of the longest run of either water or land. This will provide a crude measure of correlation between tosses. So in the observed data, the longest run is 3 W’s. Second, consider the number of times in the data that the sample switches from water to land or from land to water. This is another measure of correlation between samples. In the observed data, the number of switches is 6. There is nothing special about these two new ways of describing the data. They just serve to inspect the data in new ways. In your own modeling, you’ll have to imagine aspects of the data that are relevant in your context, for your purposes.\n\n\n\n\nFIGURE 3.7 showing the simulated predictions, viewed in these two new ways, is reproduced in ref###. I have also postponed the interpretation of Figure 3.7 to ref###, because it is more understandable viewing the plots.\n\n### Tidyverse\n\n\n\n#### Dummy data\n\n> Dummy data for the globe tossing model arise from the binomial likelihood. \n\n##### Probability of each globe toss\n\nSuppose $N = 2$, two tosses of the globe.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: dummy-data-b\n\ntibble(n      = 2,\n       `p(w)` = .7,\n       w      = 0:n) %>% \n  mutate(density = dbinom(w, size = n, prob = `p(w)`))\n```\n````\n\n```\n#> # A tibble: 3 × 4\n#>       n `p(w)`     w density\n#>   <dbl>  <dbl> <int>   <dbl>\n#> 1     2    0.7     0    0.09\n#> 2     2    0.7     1    0.42\n#> 3     2    0.7     2    0.49\n```\n:::\n\n\n##### Simulation of globe tosses\n\nSimulate one globe toss:\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: simulate-1-obs-b\n\nset.seed(3)\nrbinom(1, size = 2, prob = .7)\n```\n````\n\n```\n#> [1] 2\n```\n:::\n\n\nSimulate 10 globe tosses.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: simulate-10-obs-b\n\nset.seed(3)\nrbinom(10, size = 2, prob = .7)\n```\n````\n\n```\n#>  [1] 2 1 2 2 1 1 2 2 1 1\n```\n:::\n\nNow generate 100,000 (i.e., 1e5) reproducible dummy observations.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: simulate-1e5-obs-b\n\nn_draws_b <- 1e5\n\nset.seed(3)\ndummy_w_b <- tibble(draws = rbinom(n_draws_b, size = 2, prob = .7)) \n    \n    dummy_w_b |> \n        count(draws) |> \n        mutate(proportion = n / nrow(dummy_w_b))\n```\n````\n\n```\n#> # A tibble: 3 × 3\n#>   draws     n proportion\n#>   <int> <int>      <dbl>\n#> 1     0  9000      0.09 \n#> 2     1 42051      0.421\n#> 3     2 48949      0.489\n```\n:::\n\n##### Plot simulation of globe tosses\n\nThe simulation updated to $n=9$ and plotting the tidyverse version of Figure 3.5.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-plot-ggplot2-figure-3.5-b\n#| fig-cap: \"Distribution of simulated sample observations from 9 tosses of the globe. These samples assume the proportion of water is 0.7. The plot uses the {**ggplot2**} functions. The left panel is Kurz's original, the right one is my version slightly changed.\"\n\nn_draws_b <- 1e5\n\nset.seed(3)\ndummy_w2_b <- tibble(draws = rbinom(n_draws_b, size = 9, prob = .7))\n\np1 <- dummy_w2_b |> \n    ggplot(aes(x = draws)) + \n    geom_histogram(binwidth = 1, center = 0,\n                 fill = \"deepskyblue\", color = \"black\", \n                 linewidth = 1/10) +\n    # breaks = 0:10 * 2 = equivalent in Kurz's versions:  breaks = 0:4 * 2\n    scale_x_continuous(\"dummy water count\", breaks = 0:10 * 2) +\n    ylab(\"frequency\") +\n    coord_cartesian(xlim = c(0, 9)) +\n    theme(panel.grid = element_blank())\n\np2 <- dummy_w2_b |> \n    ggplot(aes(x = draws)) + \n    geom_histogram(binwidth = 1, center = 0,\n                 fill = \"deepskyblue\", color = \"black\", \n                 linewidth = 1/10) +\n    ## breaks = 0:10 * 2 = equivalent in Kurz's versions:  breaks = 0:4 * 2\n    ## I decided to set a break at each of the draws: breaks = 0:9 * 1\n    scale_x_continuous(\"dummy water count\", breaks = 0:9 * 1) +\n    ylab(\"frequency\") +\n    ## I did not zoom into the graph because doesn't look so nice\n    ## for instance the last line is not visible\n    # coord_cartesian(xlim = c(0, 9)) +\n    theme(panel.grid = element_blank())\n\nlibrary(patchwork)\np1 + p2\n```\n````\n\n::: {.cell-output-display}\n![Distribution of simulated sample observations from 9 tosses of the globe. These samples assume the proportion of water is 0.7. The plot uses the {**ggplot2**} functions. The left panel is Kurz's original, the right one is my version slightly changed.](03-sampling-the-imaginary_files/figure-html/fig-plot-ggplot2-figure-3.5-b-1.png){#fig-plot-ggplot2-figure-3.5-b width=672}\n:::\n:::\n\n##### Simulating and plotting 9 conditions\n\nMcElreath suggested we play around with different values of `size` and `prob`. The next block of code simulates nine conditions.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-simulate-9-conditions\n#| fig-cap: \"Distribution of 9 simulated sample observations, using different size (number of tosses: 3, 6, 9) and different probabilities (prob: 0.3, 0.6, 0.9).\"\n\nn_draws <- 1e5\n\nsimulate_binom <- function(n, probability) {\n  set.seed(3)\n  rbinom(n_draws, size = n, prob = probability) \n}\n\nd9_b <-\n  crossing(n9_b           = c(3, 6, 9),\n           probability9_b = c(.3, .6, .9)) %>% \n  mutate(draws9_b = map2(n9_b, probability9_b, simulate_binom)) %>% \n  ungroup() %>% \n  mutate(n           = str_c(\"n = \", n9_b),\n         probability = str_c(\"p = \", probability9_b)) %>% \n  unnest(draws9_b)\n\nd9_b |> \n    slice_sample(n = 10) |> \n    arrange(n9_b)\n```\n````\n\n```\n#> # A tibble: 10 × 5\n#>     n9_b probability9_b draws9_b n     probability\n#>    <dbl>          <dbl>    <int> <chr> <chr>      \n#>  1     3            0.3        2 n = 3 p = 0.3    \n#>  2     3            0.9        3 n = 3 p = 0.9    \n#>  3     3            0.9        3 n = 3 p = 0.9    \n#>  4     3            0.9        3 n = 3 p = 0.9    \n#>  5     3            0.6        3 n = 3 p = 0.6    \n#>  6     6            0.9        5 n = 6 p = 0.9    \n#>  7     6            0.9        5 n = 6 p = 0.9    \n#>  8     6            0.6        3 n = 6 p = 0.6    \n#>  9     9            0.6        7 n = 9 p = 0.6    \n#> 10     9            0.3        3 n = 9 p = 0.3\n```\n:::\n\n\nI am still not very experienced with `tidyr::crossing()` and `tidyr::unnest()`:\n\n- **`crossing()`** is a wrapper around `expand_grid()` and therefore creates a tibble from all combination of inputs. In addition to `expand_grid()` it de-duplicates and sorts its input.\n- **`unnest()`** expands a list-column containing data frames into row and columns. In the above case `map2()` returns a list and stores the data in `draws9_b`.\n\nInstead of `head()` I used `dplyr::slice_sample()` and ordered the result by the first column. I think this will get a better glimpse on the data as just the first 6 rows.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-sim-plot-9-cond-b\n\nd9_b %>% \n  ggplot(aes(x = draws9_b)) +\n  geom_histogram(binwidth = 1, center = 0,\n                 color = \"deepskyblue\", linewidth = 1/10) +\n  scale_x_continuous(\"dummy water count\", breaks = 0:4 * 2) +\n  ylab(\"frequency\") +\n  coord_cartesian(xlim = c(0, 9)) +\n  theme(panel.grid = element_blank()) +\n  facet_grid(n9_b ~ probability9_b)\n```\n````\n\n::: {.cell-output-display}\n![](03-sampling-the-imaginary_files/figure-html/fig-sim-plot-9-cond-b-1.png){#fig-sim-plot-9-cond-b width=672}\n:::\n:::\n\n#### Model checking\n\nOn software checking Kurz refers to some material, that is too special for me. So I do not include it here. Maybe I will come later here again when I have more experiences with Bayesian statistics and the necessary tools.\n\n##### Reproduction of Figure 3.6\n\nAt first I thought I do not need to refresh the original grid approximation from @lst-grid-approx-b as I have it stored it with the unique name `d_b`. But it turned out that the above code with `n_grid_b = 1000L` does not work, because it draws no vertical lines by the posterior density. Instead one has to sample 1001 times. \n\nActually I do not know why this (small) difference is necessary, but I noticed that with most sample numbers the plot does not work correctly. It worked with 1071. The sequence 1011, 1021, 1031, 1041, 1051, 1061 misses just one vertical line at .7 Ab exception is 1041, which misses .6.\n\nSo I have to crate the data with 1001 samples again before I produce the plot.\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-repr-figure-3.6-top-b\n#| fig-cap: \"Reproduction of the top part of Figure 3.6\"\n\nn2_b <- 1001L\nn_success <- 6L\nn_trials  <- 9L\n\n(\n  d2_b <-\n  tibble(p_grid2_b = seq(from = 0, to = 1, length.out = n2_b),\n         # note we're still using a flat uniform prior\n         prior2_b  = 1) %>% \n  mutate(likelihood2_b = dbinom(n_success, size = n_trials, prob = p_grid2_b)) %>% \n  mutate(posterior2_b = (likelihood2_b * prior2_b) / sum(likelihood2_b * prior2_b))\n)\n\nd2_b %>% \n  ggplot(aes(x = p_grid2_b, y = posterior2_b)) +\n  geom_area(color = \"deepskyblue\", fill = \"deepskyblue\") +\n  geom_segment(data = . %>% \n                 filter(p_grid2_b %in% c(seq(from = .1, to = .9, by = .1), 3 / 10)),\n               aes(xend = p_grid2_b, yend = 0, linewidth = posterior2_b),\n               color = \"black\", show.legend = F) +\n  geom_point(data = . %>%\n               filter(p_grid2_b %in% c(seq(from = .1, to = .9, by = .1), 3 / 10))) +\n  annotate(geom = \"text\", \n           x = .08, y = .0025,\n           label = \"Posterior probability\") +\n  scale_linewidth_continuous(range = c(0, 1)) +\n  scale_x_continuous(\"probability of water\", breaks = 0:10 / 10) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n```\n````\n\n```\n#> # A tibble: 1,001 × 4\n#>    p_grid2_b prior2_b likelihood2_b posterior2_b\n#>        <dbl>    <dbl>         <dbl>        <dbl>\n#>  1     0            1      0            0       \n#>  2     0.001        1      8.37e-17     8.37e-19\n#>  3     0.002        1      5.34e-15     5.34e-17\n#>  4     0.003        1      6.07e-14     6.07e-16\n#>  5     0.004        1      3.40e-13     3.40e-15\n#>  6     0.005        1      1.29e-12     1.29e-14\n#>  7     0.006        1      3.85e-12     3.85e-14\n#>  8     0.007        1      9.68e-12     9.68e-14\n#>  9     0.008        1      2.15e-11     2.15e-13\n#> 10     0.009        1      4.34e-11     4.34e-13\n#> # ℹ 991 more rows\n```\n\n::: {.cell-output-display}\n![Reproduction of the top part of Figure 3.6](03-sampling-the-imaginary_files/figure-html/fig-repr-figure-3.6-top-b-1.png){#fig-repr-figure-3.6-top-b width=672}\n:::\n:::\n\n\nWe’ll need to do a bit of wrangling before we’re ready to make the plot in the middle panel of Figure 3.6.\n\nI STOPPED HERE! (2023-07-31) THERE IS A GAP BECAUSE THE FOLLOWING CODE CHUNKS (MOSTLY DRAWINGS) ARE FOR MY PURPOSE (LEARNING BAYESIAN STATISTICS) NOT RELEVANT.\n\n\n## I STOPPED HERE! (2023-07-29)\n\n## Practice with brms\n\n### Tidyverse (and {brms})\n\nWith {**brms**}, we’ll fit the primary model of $w=6$ and $n=9$.\n\n#### Fit model with the {brms} package\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: brms-model-fit-b\n\n\nb3.1 <-\n  brms::brm(data = list(w = 6), \n      family = binomial(link = \"identity\"),\n      w | trials(9) ~ 0 + Intercept,\n      # this is a flat prior\n      brms::prior(beta(1, 1), class = b, lb = 0, ub = 1),\n      iter = 5000, warmup = 1000,\n      seed = 3,\n      file = \"fits/b03.01\")\n```\n````\n:::\n\n\nWe'll learn more about the beta distribution in Chapter 12. But for now,\nhere's the posterior summary for `b_Intercept`, the probability of a\n\"w\".\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: posterior-summary-b\n\nbrms::posterior_summary(b3.1)[\"b_Intercept\", ] %>% \n  round(digits = 2)\n```\n````\n\n```\n#>  Estimate Est.Error      Q2.5     Q97.5 \n#>      0.64      0.14      0.34      0.88\n```\n:::\n\n\n#### Simulate probability values\n\nI had problems with the following code chunk. See [my postings at Kurz's repo](https://github.com/ASKurz/Statistical_Rethinking_with_brms_ggplot2_and_the_tidyverse_2_ed/issues/49).\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: sim-prob-values\n\nf <-\n  brms:::fitted.brmsfit(b3.1, \n         summary = F,\n         scale = \"linear\") %>% \n  data.frame() %>% \n  rlang::set_names(\"p\")\n\nglimpse(f)\n```\n````\n\n```\n#> Rows: 16,000\n#> Columns: 1\n#> $ p <dbl> 0.6318994, 0.8105015, 0.7677781, 0.7250286, 0.7265799, 0.7376768, 0.…\n```\n:::\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-brms-model-density\n#| fig-cap: \"Density plot of the fittet {brms} model\"\n\nf %>% \n  ggplot(aes(x = p)) +\n  geom_density(fill = \"deepskyblue\", color = \"deepskyblue\") +\n  annotate(geom = \"text\", x = .08, y = 2.5,\n           label = \"Posterior probability\") +\n  scale_x_continuous(\"probability of water\",\n                     breaks = c(0, .5, 1),\n                     limits = 0:1) +\n  scale_y_continuous(NULL, breaks = NULL) +\n  theme(panel.grid = element_blank())\n```\n````\n\n::: {.cell-output-display}\n![Density plot of the fittet {brms} model](03-sampling-the-imaginary_files/figure-html/fig-brms-model-density-1.png){#fig-brms-model-density width=672}\n:::\n:::\n\n\nThe graphic should look like the top part of Figure 3.6, reproduced as @fig-repr-figure-3.6-top-b. (I am not sure if the differences are important: I have a smaller apex and my curve is more irregular.)\n\n> Much like we did with samples, we can use this distribution of probabilities to predict histograms of $w$ counts. With those in hand, we can make an analogue to the histogram in the bottom panel of Figure 3.6.\n\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: fig-sim-pred-values-b\n#| fig-cap: \"Simulation to predict posterior distribution\"\n\nset.seed(3)\nf <-\n  f %>% \n  mutate(w2 = rbinom(n(), size = 9,  prob = p))\n\n# the plot\nf %>% \n  ggplot(aes(x = w2)) +\n  geom_histogram(binwidth = 1, center = 0, color = \"black\",\n                 fill = \"deepskyblue\", linewidth = 1/10) +\n  scale_x_continuous(\"number of water samples\", breaks = 0:3 * 3) +\n  scale_y_continuous(NULL, breaks = NULL, limits = c(0, 5000)) +\n  ggtitle(\"Posterior predictive distribution\") +\n  coord_cartesian(xlim = c(0, 9)) +\n  theme(panel.grid = element_blank())\n```\n````\n\n::: {.cell-output-display}\n![Simulation to predict posterior distribution](03-sampling-the-imaginary_files/figure-html/fig-sim-pred-values-b-1.png){#fig-sim-pred-values-b width=672}\n:::\n:::\n\n\n## Session info\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| label: session-info\n\nsessionInfo()\n```\n````\n\n```\n#> R version 4.3.1 (2023-06-16)\n#> Platform: x86_64-apple-darwin20 (64-bit)\n#> Running under: macOS Ventura 13.4.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n#> \n#> locale:\n#> [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n#> \n#> time zone: Europe/Vienna\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] patchwork_1.1.2 lubridate_1.9.2 forcats_1.0.0   stringr_1.5.0  \n#>  [5] dplyr_1.1.2     purrr_1.0.1     readr_2.1.4     tidyr_1.3.0    \n#>  [9] tibble_3.2.1    ggplot2_3.4.2   tidyverse_2.0.0\n#> \n#> loaded via a namespace (and not attached):\n#>   [1] tensorA_0.36.2       rstudioapi_0.15.0    jsonlite_1.8.7      \n#>   [4] shape_1.4.6          magrittr_2.0.3       TH.data_1.1-2       \n#>   [7] estimability_1.4.1   farver_2.1.1         nloptr_2.0.3        \n#>  [10] rmarkdown_2.23       vctrs_0.6.3          minqa_1.2.5         \n#>  [13] base64enc_0.1-3      htmltools_0.5.5      distributional_0.3.2\n#>  [16] curl_5.0.1           tidybayes_3.0.4      StanHeaders_2.26.27 \n#>  [19] htmlwidgets_1.6.2    plyr_1.8.8           sandwich_3.0-2      \n#>  [22] emmeans_1.8.7        zoo_1.8-12           igraph_1.5.0.1      \n#>  [25] mime_0.12            lifecycle_1.0.3      pkgconfig_2.0.3     \n#>  [28] colourpicker_1.2.0   Matrix_1.6-0         R6_2.5.1            \n#>  [31] fastmap_1.1.1        shiny_1.7.4.1        digest_0.6.33       \n#>  [34] colorspace_2.1-0     ps_1.7.5             brms_2.19.0         \n#>  [37] crosstalk_1.2.0      projpred_2.6.0       labeling_0.4.2      \n#>  [40] fansi_1.0.4          timechange_0.2.0     mgcv_1.9-0          \n#>  [43] abind_1.4-5          compiler_4.3.1       withr_2.5.0         \n#>  [46] backports_1.4.1      inline_0.3.19        shinystan_2.6.0     \n#>  [49] rethinking_2.31      gamm4_0.2-6          pkgbuild_1.4.2      \n#>  [52] MASS_7.3-60          gtools_3.9.4         loo_2.6.0           \n#>  [55] tools_4.3.1          httpuv_1.6.11        threejs_0.3.3       \n#>  [58] glue_1.6.2           callr_3.7.3          nlme_3.1-162        \n#>  [61] promises_1.2.0.1     grid_4.3.1           cmdstanr_0.5.3      \n#>  [64] checkmate_2.2.0      reshape2_1.4.4       generics_0.1.3      \n#>  [67] gtable_0.3.3         tzdb_0.4.0           hms_1.1.3           \n#>  [70] xml2_1.3.5           utf8_1.2.3           pillar_1.9.0        \n#>  [73] ggdist_3.3.0         markdown_1.7         posterior_1.4.1     \n#>  [76] later_1.3.1          splines_4.3.1        lattice_0.21-8      \n#>  [79] survival_3.5-5       tidyselect_1.2.0     miniUI_0.1.1.1      \n#>  [82] knitr_1.43           arrayhelpers_1.1-0   gridExtra_2.3       \n#>  [85] V8_4.3.3             rversions_2.1.2      stats4_4.3.1        \n#>  [88] xfun_0.39            bridgesampling_1.1-2 matrixStats_1.0.0   \n#>  [91] DT_0.28              rstan_2.26.22        stringi_1.7.12      \n#>  [94] yaml_2.3.7           boot_1.3-28.1        evaluate_0.21       \n#>  [97] codetools_0.2-19     cli_3.6.1            RcppParallel_5.1.7  \n#> [100] shinythemes_1.2.0    xtable_1.8-4         munsell_0.5.0       \n#> [103] processx_3.8.2       Rcpp_1.0.11          coda_0.19-4         \n#> [106] svUnit_1.0.6         parallel_4.3.1       rstantools_2.3.1.1  \n#> [109] ellipsis_0.3.2       prettyunits_1.1.1    dygraphs_1.1.1.6    \n#> [112] bayesplot_1.10.0     Brobdingnag_1.2-9    lme4_1.1-34         \n#> [115] mvtnorm_1.2-2        scales_1.2.1         xts_0.13.1          \n#> [118] crayon_1.5.2         rlang_1.1.1          multcomp_1.4-25     \n#> [121] shinyjs_2.1.0\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}